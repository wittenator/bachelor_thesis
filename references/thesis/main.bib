
@article{devlinBERTPretrainingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04805},
  primaryClass = {cs},
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  journal = {arXiv:1810.04805 [cs]},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/home/tim/Zotero/storage/HVVE6QAK/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/tim/Zotero/storage/3D8HVL8L/1810.html}
}

@article{liptonMythosModelInterpretability2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/K9YF2I9K/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/2ID6BE7W/1606.html}
}

@article{millerExplainableAIBeware2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00547},
  primaryClass = {cs},
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  journal = {arXiv:1712.00547 [cs]},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/SGZP2PCV/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf;/home/tim/Zotero/storage/JTDVCR2I/1712.html}
}

@article{millerExplanationArtificialIntelligence2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07269},
  primaryClass = {cs},
  title = {Explanation in {{Artificial Intelligence}}: {{Insights}} from the {{Social Sciences}}},
  shorttitle = {Explanation in {{Artificial Intelligence}}},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  journal = {arXiv:1706.07269 [cs]},
  author = {Miller, Tim},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/8L9NTSUM/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf;/home/tim/Zotero/storage/YFLV9UKK/1706.html}
}

@article{petersenSystematicMappingStudies,
  title = {Systematic {{Mapping Studies}} in {{Software Engineering}}},
  abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions.
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for doing systematic maps are defined.
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
  language = {en},
  author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
  pages = {10},
  file = {/home/tim/Zotero/storage/VX598A2F/Petersen et al. - Systematic Mapping Studies in Software Engineering.pdf}
}

@misc{IntroducingMuseumFur,
  title = {Introducing the {{Museum}} F{\"u}r {{Naturkunde}} in {{Berlin}}},
  abstract = {An interview with Ulrike Sturm, scientist at the Museum f{\"u}r Naturkunde in Berlin.Tell us about the Museum f{\"u}r Naturkunde.The Museum f{\"u}r Naturkunde \textendash{} \ldots{}},
  language = {en-GB},
  journal = {Europeana Pro},
  howpublished = {https://pro.europeana.eu/post/introducing-the-museum-fur-naturkunde-in-berlin},
  file = {/home/tim/Zotero/storage/H3JQ6NHC/introducing-the-museum-fur-naturkunde-in-berlin.html}
}

@misc{DFGGEPRIS,
  title = {{{DFG}} - {{GEPRIS}}},
  howpublished = {https://gepris.dfg.de/gepris/OCTOPUS?task=showAbout},
  file = {/home/tim/Zotero/storage/3G8PRGJ5/OCTOPUS.html}
}

@misc{dennymatthewpennstateuniversity;spirlingarthurnewyorkuniversityReplicationDataText2017,
  title = {Replication {{Data}} for: {{Text Preprocessing For Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, {{And What To Do About It}}},
  shorttitle = {Replication {{Data}} For},
  abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention. Yet, as we show, such decisions have profound effects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice. To aid researchers working in unsupervised settings, we introduce a statistical procedure and software that examines the sensitivity of findings under alternate preprocessing regimes. This approach complements a researcher's substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset. In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication efforts.},
  language = {en},
  publisher = {{Harvard Dataverse}},
  author = {Denny, Matthew (Penn State University); Spirling, Arthur (New York University)},
  year = {2017},
  file = {/home/tim/Zotero/storage/JRGABUXT/Denny, Matthew (Penn State University)\; Spirling, Arthur (New York University) - 2017 - Replication Data for Text Preprocessing For Unsup.pdf},
  doi = {10.7910/dvn/xrr0hm}
}

@article{deerwesterIndexingLatentSemantic,
  title = {Indexing by Latent Semantic Analysis},
  language = {en},
  journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
  author = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  pages = {17},
  file = {/home/tim/Zotero/storage/6ZJZKBPF/Deerwester et al. - Indexing by latent semantic analysis.pdf}
}

@article{robertsonUnderstandingInverseDocument2004,
  title = {Understanding Inverse Document Frequency: On Theoretical Arguments for {{IDF}}},
  volume = {60},
  issn = {0022-0418},
  shorttitle = {Understanding Inverse Document Frequency},
  abstract = {The term weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon's Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in traditional probabilistic model of information retrieval.},
  language = {en},
  number = {5},
  journal = {Journal of Documentation},
  doi = {10.1108/00220410410560582},
  author = {Robertson, Stephen},
  month = oct,
  year = {2004},
  pages = {503-520},
  file = {/home/tim/Zotero/storage/ZZK4XD5W/Robertson - 2004 - Understanding inverse document frequency on theor.pdf}
}

@inproceedings{li-pingjingImprovedFeatureSelection2002a,
  title = {Improved Feature Selection Approach {{TFIDF}} in Text Mining},
  volume = {2},
  abstract = {This paper describes the feature selection method TFIDF (term frequency, inverse document frequency). With it, we process the data resource and set up the vector space model in order to provide a convenient data structure for text categorization. We calculate the precision of this method with the help of categorization results. According to the empirical results, we analyze its advantages and disadvantages and present a new TFIDF-based feature selection approach to improve its accuracy.},
  booktitle = {Proceedings. {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  doi = {10.1109/ICMLC.2002.1174522},
  author = {{Li-Ping Jing} and {Hou-Kuan Huang} and {Hong-Bo Shi}},
  month = nov,
  year = {2002},
  keywords = {Data mining,data structures,data mining,feature extraction,Indexing,Data structures,Text mining,classification,text categorization,Text categorization,Learning systems,Classification algorithms,text mining,Data preprocessing,data structure,evaluation function,feature selection,Frequency,indexing,inverse document frequency,Mutual information,term frequency,TFIDF method,vector space model},
  pages = {944-946 vol.2},
  file = {/home/tim/Zotero/storage/RZU74A5J/Li-Ping Jing et al. - 2002 - Improved feature selection approach TFIDF in text .pdf;/home/tim/Zotero/storage/6BPY4X38/1174522.html}
}

@article{aizawaInformationtheoreticPerspectiveTf2003,
  title = {An Information-Theoretic Perspective of Tf\textendash{}Idf Measures},
  volume = {39},
  issn = {0306-4573},
  abstract = {This paper presents a mathematical definition of the ``probability-weighted amount of information'' (PWI), a measure of specificity of terms in documents that is based on an information-theoretic view of retrieval events. The proposed PWI is expressed as a product of the occurrence probabilities of terms and their amounts of information, and corresponds well with the conventional term frequency\textendash{}inverse document frequency measures that are commonly used in today's information retrieval systems. The mathematical definition of the PWI is shown, together with some illustrative examples of the calculation.},
  number = {1},
  journal = {Information Processing \& Management},
  doi = {10.1016/S0306-4573(02)00021-3},
  author = {Aizawa, Akiko},
  month = jan,
  year = {2003},
  keywords = {Text categorization,Information theory,Term weighting theories,tf–idf},
  pages = {45-65},
  file = {/home/tim/Zotero/storage/DE62UZ4G/Aizawa - 2003 - An information-theoretic perspective of tf–idf mea.pdf;/home/tim/Zotero/storage/AVANRBHR/S0306457302000213.html}
}

@misc{PivotedDocumentLength,
  title = {Pivoted Document Length Normalisation | {{RARE Technologies}}},
  howpublished = {https://rare-technologies.com/pivoted-document-length-normalisation/},
  file = {/home/tim/Zotero/storage/H2WHRRXR/pivoted-document-length-normalisation.html}
}

@incollection{chenCurseDimensionality2009,
  address = {{Boston, MA}},
  title = {Curse of {{Dimensionality}}},
  isbn = {978-0-387-39940-9},
  language = {en},
  booktitle = {Encyclopedia of {{Database Systems}}},
  publisher = {{Springer US}},
  author = {Chen, Lei},
  editor = {LIU, LING and {\"O}ZSU, M. TAMER},
  year = {2009},
  pages = {545-546},
  doi = {10.1007/978-0-387-39940-9_133}
}

@incollection{aggarwalSurprisingBehaviorDistance2001,
  address = {{Berlin, Heidelberg}},
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  volume = {1973},
  isbn = {978-3-540-41456-8 978-3-540-44503-6},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lk norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L1 norm) is consistently more preferable than the Euclidean distance metric (L2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lk norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  language = {en},
  booktitle = {Database {{Theory}} \textemdash{} {{ICDT}} 2001},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and {Van den Bussche}, Jan and Vianu, Victor},
  year = {2001},
  pages = {420-434},
  file = {/home/tim/Zotero/storage/B36SJCV8/Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf},
  doi = {10.1007/3-540-44503-X_27}
}

@article{liuOverviewTopicModeling2016,
  title = {An Overview of Topic Modeling and Its Current Applications in Bioinformatics},
  volume = {5},
  issn = {2193-1801},
  abstract = {With the rapid accumulation of biological datasets, machine learning methods designed to automate data analysis are urgently needed. In recent years, so-called topic models that originated from the field of natural language processing have been receiving much attention in bioinformatics because of their interpretability. Our aim was to review the application and development of topic models for bioinformatics.},
  number = {1},
  journal = {SpringerPlus},
  doi = {10.1186/s40064-016-3252-8},
  author = {Liu, Lin and Tang, Lin and Dong, Wen and Yao, Shaowen and Zhou, Wei},
  month = sep,
  year = {2016},
  pages = {1608},
  file = {/home/tim/Zotero/storage/HSKQIZ2U/Liu et al. - 2016 - An overview of topic modeling and its current appl.pdf;/home/tim/Zotero/storage/28QGZ63P/s40064-016-3252-8.html}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  volume = {3},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  journal = {J. Mach. Learn. Res.},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  month = mar,
  year = {2003},
  pages = {993--1022},
  file = {/home/tim/Zotero/storage/3J3TBKMU/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf}
}

@article{wuWordMoverEmbedding2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.01713},
  primaryClass = {cs, stat},
  title = {Word {{Mover}}'s {{Embedding}}: {{From Word2Vec}} to {{Document Embedding}}},
  shorttitle = {Word {{Mover}}'s {{Embedding}}},
  abstract = {While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called \textbackslash{}emph\{Word Mover's Distance\} (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the \textbackslash{}emph\{Word Mover's Embedding \} (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.},
  journal = {arXiv:1811.01713 [cs, stat]},
  author = {Wu, Lingfei and Yen, Ian E. H. and Xu, Kun and Xu, Fangli and Balakrishnan, Avinash and Chen, Pin-Yu and Ravikumar, Pradeep and Witbrock, Michael J.},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/HIIMRNPX/Wu et al. - 2018 - Word Mover's Embedding From Word2Vec to Document .pdf;/home/tim/Zotero/storage/4I5C39UB/1811.html}
}

@article{leDistributedRepresentationsSentences2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4053},
  primaryClass = {cs},
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  journal = {arXiv:1405.4053 [cs]},
  author = {Le, Quoc V. and Mikolov, Tomas},
  month = may,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/tim/Zotero/storage/TSC6J4Y9/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/home/tim/Zotero/storage/AMFSB8A6/1405.html}
}

@article{mikolovDistributedRepresentationsWords,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
  language = {en},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  pages = {9},
  file = {/home/tim/Zotero/storage/CWISGIEJ/Mikolov et al. - Distributed Representations of Words and Phrases a.pdf}
}

@incollection{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5998--6008},
  file = {/home/tim/Zotero/storage/ZUT4D9QJ/Vaswani et al. - 2017 - Attention is All you Need.pdf;/home/tim/Zotero/storage/CHRCBFWT/7181-attention-is-all-you-need.html}
}

@article{deboomRepresentationLearningVery2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.00570},
  title = {Representation Learning for Very Short Texts Using Weighted Word Embedding Aggregation},
  volume = {80},
  issn = {01678655},
  abstract = {Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.},
  journal = {Pattern Recognition Letters},
  doi = {10.1016/j.patrec.2016.06.012},
  author = {De Boom, Cedric and Van Canneyt, Steven and Demeester, Thomas and Dhoedt, Bart},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  pages = {150-156},
  file = {/home/tim/Zotero/storage/UH7EALFW/De Boom et al. - 2016 - Representation learning for very short texts using.pdf;/home/tim/Zotero/storage/BMSEITMW/1607.html}
}

@article{mullnerModernHierarchicalAgglomerative2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1109.2378},
  primaryClass = {cs, stat},
  title = {Modern Hierarchical, Agglomerative Clustering Algorithms},
  abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
  journal = {arXiv:1109.2378 [cs, stat]},
  author = {M{\"u}llner, Daniel},
  month = sep,
  year = {2011},
  keywords = {62H30,Computer Science - Data Structures and Algorithms,I.5.3,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/2ZUZST64/Müllner - 2011 - Modern hierarchical, agglomerative clustering algo.pdf;/home/tim/Zotero/storage/MM6UR9AV/1109.html}
}


