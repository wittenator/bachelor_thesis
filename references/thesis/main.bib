
@article{liptonMythosModelInterpretability2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/K9YF2I9K/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/2ID6BE7W/1606.html}
}

@article{millerExplainableAIBeware2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00547},
  primaryClass = {cs},
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  journal = {arXiv:1712.00547 [cs]},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/SGZP2PCV/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf;/home/tim/Zotero/storage/JTDVCR2I/1712.html}
}

@article{millerExplanationArtificialIntelligence2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07269},
  primaryClass = {cs},
  title = {Explanation in {{Artificial Intelligence}}: {{Insights}} from the {{Social Sciences}}},
  shorttitle = {Explanation in {{Artificial Intelligence}}},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  journal = {arXiv:1706.07269 [cs]},
  author = {Miller, Tim},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/8L9NTSUM/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf;/home/tim/Zotero/storage/YFLV9UKK/1706.html}
}

@article{petersenSystematicMappingStudies,
  title = {Systematic {{Mapping Studies}} in {{Software Engineering}}},
  abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions.
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for doing systematic maps are defined.
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
  language = {en},
  author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
  pages = {10},
  file = {/home/tim/Zotero/storage/VX598A2F/Petersen et al. - Systematic Mapping Studies in Software Engineering.pdf}
}

@misc{IntroducingMuseumFur,
  title = {Introducing the {{Museum}} F{\"u}r {{Naturkunde}} in {{Berlin}}},
  abstract = {An interview with Ulrike Sturm, scientist at the Museum f{\"u}r Naturkunde in Berlin.Tell us about the Museum f{\"u}r Naturkunde.The Museum f{\"u}r Naturkunde \textendash{} \ldots{}},
  language = {en-GB},
  journal = {Europeana Pro},
  howpublished = {https://pro.europeana.eu/post/introducing-the-museum-fur-naturkunde-in-berlin},
  file = {/home/tim/Zotero/storage/H3JQ6NHC/introducing-the-museum-fur-naturkunde-in-berlin.html}
}


