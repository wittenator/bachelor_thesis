
@article{arrasWhatRelevantText2017,
  title = {"{{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {"{{What}} Is Relevant in a Text Document?},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  language = {en},
  number = {8},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0181142},
  author = {Arras, Leila and Horn, Franziska and Montavon, Gr\'egoire and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = aug,
  year = {2017},
  keywords = {Machine learning,Semantics,Support vector machines,Neural networks,Imaging techniques,Neurons,Preprocessing,Vector spaces},
  pages = {e0181142},
  file = {/home/tim/Zotero/storage/XUTM7FUA/Arras et al. - 2017 - What is relevant in a text document An interpr.pdf;/home/tim/Zotero/storage/4XVW5AR9/article.html}
}

@inproceedings{benjaminTransparencyMediationMeaning2018,
  title = {Transparency and the {{Mediation}} of {{Meaning}} in {{Algorithmic Systems}}},
  abstract = {Today's algorithmic systems are increasingly ubiquitous, while advancements in machine learning, especially deep learning, have made them nearly incomprehensible in their functioning. With the growing presence of these algorith-mic systems for supporting or even replacing human decision making, contemporary designers and engineers in both academia and industry are confronted with novel challenges , especially regarding the disclosure of the system's inner functioning. Design practitioners are already urged to consider explaining the presence and function of algorithms to end users [2]. However, the dynamic nature of al-gorithmic systems, as described by Redstr\"om and Wiltse [6] for example, complicates current design approaches that evolve around questions such as: What can (or should) be made present to users when considering algorithmic systems, while still maintaining an actionable visual presentation the algorithm, the data, the confidence level of the results? These open questions highlight only some of the challenges that occur when considering transparency in algorithmic systems.},
  author = {Benjamin, Jesse and {M\"uller-Birn}, Claudia and Ginosar, Rony},
  month = oct,
  year = {2018}
}

@article{millerExplanationArtificialIntelligence2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07269},
  primaryClass = {cs},
  title = {Explanation in {{Artificial Intelligence}}: {{Insights}} from the {{Social Sciences}}},
  shorttitle = {Explanation in {{Artificial Intelligence}}},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  journal = {arXiv:1706.07269 [cs]},
  author = {Miller, Tim},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/5AL7RRXN/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf;/home/tim/Zotero/storage/Q3AH6U7I/1706.html}
}

@article{petersenSystematicMappingStudies,
  title = {Systematic {{Mapping Studies}} in {{Software Engineering}}},
  abstract = {BACKGROUND: A software engineering systematic map is a defined method to build
a classification scheme and structure a software engineering field of interest. The
analysis of results focuses on frequencies of publications for categories within the
scheme. Thereby, the coverage of the research field can be determined. Different
facets of the scheme can also be combined to answer more specific research
questions.
OBJECTIVE: We describe how to conduct a systematic mapping study in software
engineering and provide guidelines. We also compare systematic maps and
systematic reviews to clarify how to chose between them. This comparison leads to a
set of guidelines for systematic maps.
METHOD: We have defined a systematic mapping process and applied it to complete
a systematic mapping study. Furthermore, we compare systematic maps with
systematic reviews by systematically analyzing existing systematic reviews.
RESULTS: We describe a process for software engineering systematic mapping
studies and compare it to systematic reviews. Based on this, guidelines for doing
systematic maps are defined.
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth,
validity issues and implications. Thus, they should be used complementarily and
require different methods (e.g., for analysis).},
  author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattson, Michael}
}

@article{gilpinExplainingExplanationsOverview2018,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  abstract = {There has recently been a surge of work in explanatory artificial
intelligence (XAI). This research area tackles the important problem that
complex machines and algorithms often cannot provide insights into their
behavior and thought processes. XAI allows users and parts of the internal
system to be more transparent, providing explanations of their decisions in
some level of detail. These explanations are important to ensure algorithmic
fairness, identify potential bias/problems in the training data, and to ensure
that the algorithms perform as expected. However, explanations produced by
these systems is neither standardized nor systematically assessed. In an effort
to create best practices and identify open challenges, we provide our
definition of explainability and show how it can be used to classify existing
literature. We discuss why current approaches to explanatory methods especially
for deep neural networks are insufficient. Finally, based on our survey, we
conclude with suggested future research directions for explanatory artificial
intelligence.},
  language = {en},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  month = may,
  year = {2018},
  file = {/home/tim/Zotero/storage/2ARPDPK3/Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf;/home/tim/Zotero/storage/XETV5NHP/1806.html}
}

@article{liptonMythosModelInterpretability2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/G56B9GE2/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/CP3V8CD5/1606.html}
}

@incollection{robnik-sikonjaPerturbationBasedExplanationsPrediction2018,
  title = {Perturbation-{{Based Explanations}} of {{Prediction Models}}},
  isbn = {978-3-319-90402-3},
  abstract = {Current research into algorithmic explanation methods for predictive models can be divided into two main approaches: gradient-based approaches limited to neural networks and more general perturbation-based approaches which can be used with arbitrary prediction models. We present an overview of perturbation-based approaches, with focus on the most popular methods (EXPLAIN, IME, LIME). These methods support explanation of individual predictions but can also visualize the model as a whole. We describe their working principles, how they handle computational complexity, their visualizations as well as their advantages and disadvantages. We illustrate practical issues and challenges in applying the explanation methodology in a business context on a practical use case of B2B sales forecasting in a company. We demonstrate how explanations can be used as a what-if analysis tool to answer relevant business questions.},
  author = {{Robnik-Sikonja}, Marko and Bohanec, Marko},
  month = jun,
  year = {2018},
  pages = {159-175},
  file = {/home/tim/Zotero/storage/GJI2XTV2/Robnik-Sikonja and Bohanec - 2018 - Perturbation-Based Explanations of Prediction Mode.pdf},
  doi = {10.1007/978-3-319-90403-0_9}
}

@article{devlinBERTPretrainingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04805},
  primaryClass = {cs},
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  journal = {arXiv:1810.04805 [cs]},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/home/tim/Zotero/storage/9N8HVT47/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/tim/Zotero/storage/KBFJ67K5/1810.html}
}

@article{leDistributedRepresentationsSentences2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.4053},
  primaryClass = {cs},
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  journal = {arXiv:1405.4053 [cs]},
  author = {Le, Quoc V. and Mikolov, Tomas},
  month = may,
  year = {2014},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computation and Language},
  file = {/home/tim/Zotero/storage/KB2NV2IP/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/home/tim/Zotero/storage/DCEUBV5E/1405.html}
}

@article{millerExplainableAIBeware2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00547},
  primaryClass = {cs},
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  journal = {arXiv:1712.00547 [cs]},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/7W26EADJ/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf;/home/tim/Zotero/storage/GGSA8CEF/1712.html}
}


