
@article{devlinBERTPretrainingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04805},
  primaryClass = {cs},
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  journal = {arXiv:1810.04805 [cs]},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/home/tim/Zotero/storage/HVVE6QAK/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/tim/Zotero/storage/3D8HVL8L/1810.html}
}

@article{liptonMythosModelInterpretability2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/K9YF2I9K/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/2ID6BE7W/1606.html}
}

@article{millerExplainableAIBeware2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00547},
  primaryClass = {cs},
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  journal = {arXiv:1712.00547 [cs]},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/SGZP2PCV/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf;/home/tim/Zotero/storage/JTDVCR2I/1712.html}
}

@article{millerExplanationArtificialIntelligence2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07269},
  primaryClass = {cs},
  title = {Explanation in {{Artificial Intelligence}}: {{Insights}} from the {{Social Sciences}}},
  shorttitle = {Explanation in {{Artificial Intelligence}}},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  journal = {arXiv:1706.07269 [cs]},
  author = {Miller, Tim},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/tim/Zotero/storage/8L9NTSUM/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf;/home/tim/Zotero/storage/YFLV9UKK/1706.html}
}

@inproceedings{petersenSystematicMappingStudies2008,
  address = {{Swindon, UK}},
  series = {{{EASE}}'08},
  title = {Systematic {{Mapping Studies}} in {{Software Engineering}}},
  abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  publisher = {{BCS Learning \& Development Ltd.}},
  author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
  year = {2008},
  keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
  pages = {68--77}
}

@incollection{robnik-sikonjaPerturbationBasedExplanationsPrediction2018,
  title = {Perturbation-{{Based Explanations}} of {{Prediction Models}}},
  isbn = {978-3-319-90402-3},
  abstract = {Current research into algorithmic explanation methods for predictive models can be divided into two main approaches: gradient-based approaches limited to neural networks and more general perturbation-based approaches which can be used with arbitrary prediction models. We present an overview of perturbation-based approaches, with focus on the most popular methods (EXPLAIN, IME, LIME). These methods support explanation of individual predictions but can also visualize the model as a whole. We describe their working principles, how they handle computational complexity, their visualizations as well as their advantages and disadvantages. We illustrate practical issues and challenges in applying the explanation methodology in a business context on a practical use case of B2B sales forecasting in a company. We demonstrate how explanations can be used as a what-if analysis tool to answer relevant business questions.},
  author = {{Robnik-Sikonja}, Marko and Bohanec, Marko},
  month = jun,
  year = {2018},
  pages = {159-175},
  doi = {10.1007/978-3-319-90403-0_9}
}

@article{arrasWhatRelevantText2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.07843},
  title = {"{{What}} Is {{Relevant}} in a {{Text Document}}?": {{An Interpretable Machine Learning Approach}}},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {"{{What}} Is {{Relevant}} in a {{Text Document}}?},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  number = {8},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0181142},
  author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Information Retrieval},
  pages = {e0181142},
  file = {/home/tim/Zotero/storage/G6UDH3EC/Arras et al. - 2017 - What is Relevant in a Text Document An Interpr.pdf;/home/tim/Zotero/storage/8GZJMAT2/1612.html}
}

@inproceedings{benjaminTransparencyMediationMeaning2018,
  title = {Transparency and the {{Mediation}} of {{Meaning}} in {{Algorithmic Systems}}},
  abstract = {Today's algorithmic systems are increasingly ubiquitous, while advancements in machine learning, especially deep learning, have made them nearly incomprehensible in their functioning. With the growing presence of these algorith-mic systems for supporting or even replacing human decision making, contemporary designers and engineers in both academia and industry are confronted with novel challenges , especially regarding the disclosure of the system's inner functioning. Design practitioners are already urged to consider explaining the presence and function of algorithms to end users [2]. However, the dynamic nature of al-gorithmic systems, as described by Redstr{\"o}m and Wiltse [6] for example, complicates current design approaches that evolve around questions such as: What can (or should) be made present to users when considering algorithmic systems, while still maintaining an actionable visual presentation the algorithm, the data, the confidence level of the results? These open questions highlight only some of the challenges that occur when considering transparency in algorithmic systems.},
  author = {Benjamin, Jesse and {M{\"u}ller-Birn}, Claudia and Ginosar, Rony},
  month = oct,
  year = {2018},
  file = {/home/tim/Zotero/storage/NVEYMPCQ/Benjamin et al. - 2018 - Transparency and the Mediation of Meaning in Algor.pdf}
}

@inproceedings{christophkinkeldeySupportingInterpretabilityClustering2019,
  title = {Towards {{Supporting Interpretability}} of {{Clustering Results}} with {{Uncertainty Visualization}}},
  abstract = {Interpretation of machine learning results is a major challenge for non-technical experts, with visualization being a common
approach to support this process. For instance, interpretation of clustering results is usually based on scatterplots that provide
information about cluster characteristics implicitly through the relative location of objects. However, the locations and dis-
tances tend to be distorted because of artifacts stemming from dimensionality reduction. This makes interpretation of clusters
difficult and may lead to distrust in the system. Most existing approaches that counter this drawback explain the distances in the
scatterplot (e.g., error visualization) to foster the interpretability of implicit information. Instead, we suggest explicit visualiza-
tion of the uncertainty related to the information needed for interpretation, specifically the uncertain membership of each object
to its cluster. In our approach, we place objects on a grid, and add a continuous ``topography'' in the background, expressing the
distribution of uncertainty over all clusters. We motivate our approach from a use case in which we visualize research projects,
clustered by topics extracted from scientific abstracts. We hypothesize that uncertainty visualization can increase trust in the
system, which we specify as an emergent property of interaction with an interpretable system. We present a first prototype and
outline possible procedures for evaluating if and how the uncertainty visualization approach affects interpretability and trust.},
  booktitle = {{{TrustVis19}}},
  author = {{Christoph Kinkeldey} and {Tim Korjakow} and {Jesse Josua Benjamin}},
  month = jun,
  year = {2019}
}

@article{petersenSystematicMappingStudies,
  title = {Systematic {{Mapping Studies}} in {{Software Engineering}}},
  abstract = {BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions.
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps.
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews.
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for doing systematic maps are defined.
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
  language = {en},
  author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
  pages = {10},
  file = {/home/tim/Zotero/storage/VX598A2F/Petersen et al. - Systematic Mapping Studies in Software Engineering.pdf}
}

@article{hohmanGamutDesignProbe2019,
  title = {Gamut: {{A Design Probe}} to {{Understand How Data Scientists Understand Machine Learning Models}}},
  shorttitle = {Gamut},
  abstract = {Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability.~Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into \ldots{}},
  language = {en-US},
  author = {Hohman, Fred and Head, Andrew and Caruana, Rich and DeLine, Rob and Drucker, Steven},
  month = jan,
  year = {2019},
  file = {/home/tim/Zotero/storage/YXRAE6X4/Hohman et al. - 2019 - Gamut A Design Probe to Understand How Data Scien.pdf;/home/tim/Zotero/storage/WGJULAGJ/gamut-a-design-probe-to-understand-howdata-scientists-understand-machine-learning-models.html}
}

@inproceedings{roderExploringSpaceTopic2015a,
  address = {{Shanghai, China}},
  title = {Exploring the {{Space}} of {{Topic Coherence Measures}}},
  isbn = {978-1-4503-3317-7},
  abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
  language = {en},
  booktitle = {Proceedings of the {{Eighth ACM International Conference}} on {{Web Search}} and {{Data Mining}} - {{WSDM}} '15},
  publisher = {{ACM Press}},
  doi = {10.1145/2684822.2685324},
  author = {R{\"o}der, Michael and Both, Andreas and Hinneburg, Alexander},
  year = {2015},
  pages = {399-408},
  file = {/home/tim/Zotero/storage/8E3D9WJ9/RÃ¶der et al. - 2015 - Exploring the Space of Topic Coherence Measures.pdf}
}


