
@article{liptonMythosModelInterpretability2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/G56B9GE2/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/CP3V8CD5/1606.html},
  annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY}
}

@inproceedings{pop00036,
  title = {What Can {{AI}} Do for Me: {{Evaluating Machine Learning Interpretations}} in {{Cooperative Play}}},
  abstract = {Machine learning is an important tool for decision making, but its ethical and responsible application requires rigorous vetting of its interpretability and utility: an understudied problem, particularly for natural language processing models. We design a task-specific evaluation for a question answering task and evaluate how well a model interpretation improves human performance in a human-machine cooperative setting. We evaluate interpretation methods in a grounded, realistic setting: playing a trivia game as a team. We also provide design guidance for natural language processing human-in-the-loop settings.},
  booktitle = {{{arXiv}} Preprint {{arXiv}}:1810.09648},
  author = {Feng, S and {Boyd-Graber}, J},
  year = {2018},
  keywords = {interpretability,natural language processing,question answering},
  publisher = {{arxiv.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00003,
  title = {Visual Analytics for Explainable Deep Learning},
  abstract = {Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.},
  journal = {IEEE computer graphics and applications},
  author = {Choo, J and Liu, S},
  year = {2018},
  publisher = {{ieeexplore.ieee.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00033,
  title = {Towards {{Explainable Process Predictions}} for {{Industry}} 4.0 in the {{DFKI}}-{{Smart}}-{{Lego}}-{{Factory}}},
  abstract = {With the advent of digitization on the shopfloor and the developments of Industry 4.0, companies are faced with opportunities and challenges alike. This can be illustrated by the example of AI-based process predictions, which can be valuable for real-time process management in a smart factory. However, to constructively collaborate with such a prediction, users need to establish confidence in its decisions. Explainable artificial intelligence (XAI) has emerged as a new research area to enable humans to understand, trust, and manage the AI they work with. In this contribution, we illustrate the opportunities and challenges of process predictions and XAI for Industry 4.0 with the DFKI-Smart-Lego-Factory. This fully automated factory prototype built out of LEGO\textregistered{} bricks demonstrates the potentials of Industry 4.0 in an innovative, yet easily accessible way. It includes a showcase that predicts likely process outcomes and uses state-of-the-art XAI techniques to explain them to its workers and visitors.},
  journal = {KI-K\"unstliche Intelligenz},
  author = {Rehse, JR and Mehdiyev, N and Fettke, P},
  keywords = {Explainable artificial Intelligence,Industry 4.0,Process prediction,Smart factories},
  file = {/home/tim/Zotero/storage/LBKGX9G9/Rehse et al. - 2019 - Towards Explainable Process Predictions for Indust.pdf},
  publisher = {{Springer}},
  note = {Query date: 2019-04-22}
}

@inproceedings{pop00026,
  title = {Toward {{Efficient Automation}} of {{Interpretable Machine Learning}}},
  abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
  booktitle = {\ldots{}~{{Conference}} on {{Big Data}} ({{Big Data}}~\ldots{}},
  author = {Kovalerchuk, B and Neuhaus, N},
  year = {2018},
  keywords = {accuracy,Classification algorithms,classifier,complex multidimensional relations,Computational modeling,DCP algorithm,domain-explainable ML models,dominance classifier and predictor algorithm,dominant intervals,explainability,human perceptual limitations,human-understandable machine learning models,interpretability,interpretable grey box models,interpretable machine learning,interpretable mathematical forms,interpretable methods,learning (artificial intelligence),machine learning,Machine learning,Machine learning algorithms,Mathematical model,Neural networks,noninterpretable models,pattern classification,Prediction algorithms,unintelligible black box models,visual model,visualization,white box models},
  publisher = {{ieeexplore.ieee.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00023,
  title = {Stakeholders in {{Explainable AI}}},
  abstract = {There is general consensus that it is important for artificial intelligence (AI) and machine learning systems to be explainable and/or interpretable. However, there is no general consensus over what is meant by 'explainable' and 'interpretable'. In this paper, we argue that this lack of consensus is due to there being several distinct stakeholder communities. We note that, while the concerns of the individual communities are broadly compatible, they are not identical, which gives rise to different intents and requirements for explainability/interpretability. We use the software engineering distinction between validation and verification, and the epistemological distinctions between knowns/unknowns, to tease apart the concerns of the stakeholder communities and highlight the areas where their foci overlap or diverge. It is not the purpose of the authors of this paper to 'take sides' - we count ourselves as members, to varying degrees, of multiple communities - but rather to help disambiguate what stakeholders mean when they ask 'Why?' of an AI.},
  journal = {arXiv preprint arXiv~\ldots{}},
  author = {Preece, A and Harborne, D and Braines, D and Tomsett, R and {...}},
  year = {2018},
  publisher = {{arxiv.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00044,
  title = {Regularizing {{Black}}-Box {{Models}} for {{Improved Interpretability}}},
  abstract = {Most work on interpretability in machine learning has focused on designing either inherently interpretable models, that typically trade-off interpretability for accuracy, or post-hoc explanation systems, that lack guarantees about their explanation quality. We propose an alternative to these approaches by directly regularizing a black-box model for interpretability at training time. Our approach explicitly connects three key aspects of interpretable machine learning: the model's innate explainability, the explanation system used at test time, and the metrics that measure explanation quality. Our regularization results in substantial (up to orders of magnitude) improvement in terms of explanation fidelity and stability metrics across a range of datasets, models, and black-box explanation systems. Remarkably, our regularizers also slightly improve predictive accuracy on average across the nine datasets we consider. Further, we show that the benefits of our novel regularizers on explanation quality provably generalize to unseen test points.},
  journal = {arXiv preprint arXiv~\ldots{}},
  author = {Plumb, G and {Al-Shedivat}, M and Xing, E and Talwalkar, A},
  year = {2019},
  publisher = {{arxiv.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00040,
  title = {Peeking inside the Black-Box: {{A}} Survey on {{Explainable Artificial Intelligence}} ({{XAI}})},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  journal = {IEEE Access},
  author = {Adadi, A and Berrada, M},
  year = {2018},
  keywords = {AI-based systems,artificial intelligence,Biological system modeling,black-box models,black-box nature,Conferences,explainable AI,explainable artificial intelligence,Explainable artificial intelligence,fourth industrial revolution,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms,XAI},
  publisher = {{ieeexplore.ieee.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00087,
  title = {Modelling {{Machine Learning Models}}},
  abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
  journal = {3rd Conference on"~\ldots{}},
  author = {{Fabra-Boluda}, R and Ferri, C and {Hern\'andez-Orallo}, J and {...}},
  year = {2017},
  publisher = {{Springer}},
  note = {Query date: 2019-04-22}
}

@inproceedings{pop00015,
  title = {Improved Explainability of Capsule Networks: {{Relevance}} Path by Agreement},
  abstract = {Recent advancements in signal processing domain have resulted in a surge of interest in deep neural networks (DNNs) due to their unprecedented performance and high accuracy for challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in medical applications), it is of paramount importance to understand, trust, and in one word "explain" the rational behind deep models' decisions. Generally, DNNs are considered as black-box systems, which do not provide any clue on their internal processing actions. Although some recent efforts have been initiated to explain behavior/decisions of deep networks, explainable artificial intelligence (XAI) domain is still in its infancy. In this regard, we consider capsule networks (referred to as CapsNets), which are novel deep structures; recently proposed as an alternative counterpart to convolutional neural networks (CNNs), and posed to change the future of machine intelligence. In this paper, we investigate and analyze structure and behavior of CapsNets and illustrate potential explainability properties of such networks. Furthermore, we show possibility of transforming deep architectures in to transparent networks via incorporation of capsules in different layers instead of convolution layers of the CNNs.},
  booktitle = {2018 {{IEEE Global}}~\ldots{}},
  author = {Shahroudnejad, A and Afshar, P and {...}},
  year = {2018},
  keywords = {black-box systems,CapsNets,capsule networks,Capsule Networks,CNN,Computer architecture,convolutional neural nets,convolutional neural networks,Convolutional Neural Networks,Couplings,critical decisions,deep architectures,deep learning architectures,deep models,deep networks,deep neural networks,Deep Neural Networks,deep structures,DNN,explainable artificial intelligence domain,Explainable Machine Learning,Feature extraction,human lives,internal processing actions,learning (artificial intelligence),Machine learning,medical applications,Neural networks,potential explainability properties,Predictive models,relevance path,signal processing,signal processing domain,significant engineering importance,Training,transparent networks,unprecedented performance,XAI},
  publisher = {{ieeexplore.ieee.org}},
  note = {Query date: 2019-04-22}
}

@article{pop00029,
  title = {Explainable Neural Networks Based on Additive Index Models},
  abstract = {Machine Learning algorithms are increasingly being used in recent years due to their flexibility in model fitting and increased predictive performance. However, the complexity of the models makes them hard for the data analyst to interpret the results and explain them without additional tools. This has led to much research in developing various approaches to understand the model behavior. In this paper, we present the Explainable Neural Network (xNN), a structured neural network designed especially to learn interpretable features. Unlike fully connected neural networks, the features engineered by the xNN can be extracted from the network in a relatively straightforward manner and the results displayed. With appropriate regularization, the xNN provides a parsimonious explanation of the relationship between the features and the output. We illustrate this interpretable feature--engineering property on simulated examples.},
  journal = {arXiv preprint arXiv~\ldots{}},
  author = {Vaughan, J and Sudjianto, A and Brahimi, E and Chen, J and {...}},
  year = {2018},
  publisher = {{arxiv.org}},
  note = {Query date: 2019-04-22}
}

@inproceedings{pop00108,
  title = {Designing {{Theory}}-{{Driven User}}-{{Centric Explainable AI}}},
  abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
  booktitle = {Proceedings of the {{SIGCHI}}~\ldots{}},
  author = {Wang, D and Yang, Q and Abdul, A and Lim, BY},
  year = {2019},
  keywords = {clinical decision making,decision making,explainable artificial intelligence,explanations,intelligibility},
  type = {PDF},
  publisher = {{brianlim.net}},
  note = {Query date: 2019-04-22}
}

@article{http://arxiv.org/abs/1710.09511v2,
  title = {{{InterpNET}}: {{Neural Introspection}} for {{Interpretable Deep Learning}}},
  abstract = {Humans are able to explain their reasoning. On the contrary, deep neural networks are not. This paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classification, inspired by physiological evidence of the human visual system's inner-workings. This paper proposes a neural network design paradigm, termed InterpNET, which can be combined with any existing classification architecture to generate natural language explanations of the classifications. The success of the module relies on the assumption that the network's computation and reasoning is represented in its internal layer activations. While in principle InterpNET could be applied to any existing classification architecture, it is evaluated via an image classification and explanation task. Experiments on a CUB bird classification and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations. While the current state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a much higher METEOR score of 37.9.},
  journal = {arxiv},
  author = {Barratt, Shane},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1810.00869v1,
  title = {Training {{Machine Learning Models}} by {{Regularizing}} Their {{Explanations}}},
  abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).},
  journal = {arxiv},
  author = {Ross, Andrew Slavin},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
  title = {Local {{Interpretable Model}}-Agnostic {{Explanations}} of {{Bayesian Predictive Models}} via {{Kullback}}-{{Leibler Projections}}},
  abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation fidelity and complexity. We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.},
  journal = {arxiv},
  author = {Peltola, Tomi},
  month = oct,
  year = {2018}
}

@article{shehDefiningExplainableAI2018,
  title = {Defining {{Explainable AI}} for {{Requirements Analysis}}},
  volume = {32},
  issn = {1610-1987},
  abstract = {Explainable artificial intelligence (XAI) has become popular in the last few years. The artificial intelligence (AI) community in general, and the machine learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements? In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.},
  language = {en},
  number = {4},
  journal = {KI - K\"unstliche Intelligenz},
  doi = {10.1007/s13218-018-0559-3},
  author = {Sheh, Raymond and Monteath, Isaac},
  month = nov,
  year = {2018},
  keywords = {Decision trees,Explainable AI,Machine learning,Neural networks,Requirements analysis},
  pages = {261-266},
  file = {/home/tim/Zotero/storage/K9GUDZBX/Sheh and Monteath - 2018 - Defining Explainable AI for Requirements Analysis.pdf}
}

@inproceedings{alonsoBibliometricAnalysisExplainable2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {A {{Bibliometric Analysis}} of the {{Explainable Artificial Intelligence Research Field}}},
  isbn = {978-3-319-91473-2},
  abstract = {This paper presents the results of a bibliometric study of the recent research on eXplainable Artificial Intelligence (XAI) systems. We took a global look at the contributions of scholars in XAI as well as in the subfields of AI that are mostly involved in the development of XAI systems. It is worthy to remark that we found out that about one third of contributions in XAI come from the fuzzy logic community. Accordingly, we went in depth with the actual connections of fuzzy logic contributions with AI to promote and improve XAI systems in the broad sense. Finally, we outlined new research directions aimed at strengthening the integration of different fields of AI, including fuzzy logic, toward the common objective of making AI accessible to people.},
  language = {en},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
  publisher = {{Springer International Publishing}},
  author = {Alonso, Jose M. and Castiello, Ciro and Mencar, Corrado},
  editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2018},
  keywords = {Comprehensibility,Explainable AI,Interpretability,Interpretable Fuzzy Systems,Understandability},
  pages = {3-15}
}

@incollection{mohantyBlackBoxUnderstanding2018,
  address = {Berkeley, CA},
  title = {Inside the {{Black Box}}: {{Understanding AI Decision Making}}},
  isbn = {978-1-4842-3808-0},
  shorttitle = {Inside the {{Black Box}}},
  abstract = {AI's ability to keep improving its predictive capabilities just by learning from the data and without significant involvement from humans to explain exactly how to accomplish the tasks is a big deal. Why?},
  language = {en},
  booktitle = {How to {{Compete}} in the {{Age}} of {{Artificial Intelligence}}: {{Implementing}} a {{Collaborative Human}}-{{Machine Strategy}} for {{Your Business}}},
  publisher = {{Apress}},
  author = {Mohanty, Soumendra and Vyas, Sachin},
  editor = {Mohanty, Soumendra and Vyas, Sachin},
  year = {2018},
  pages = {91-124},
  doi = {10.1007/978-1-4842-3808-0_4}
}

@article{zhangVisualInterpretabilityDeep2018,
  title = {Visual Interpretability for Deep Learning: A Survey},
  volume = {19},
  issn = {2095-9230},
  shorttitle = {Visual Interpretability for Deep Learning},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human\textendash{}computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  language = {en},
  number = {1},
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  doi = {10.1631/FITEE.1700808},
  author = {Zhang, Quan-shi and Zhu, Song-chun},
  month = jan,
  year = {2018},
  keywords = {Artificial intelligence,Deep learning,Interpretable model,TP391},
  pages = {27-39},
  file = {/home/tim/Zotero/storage/9YW5PKAF/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf}
}

@article{carabantesBlackboxArtificialIntelligence2019,
  title = {Black-Box Artificial Intelligence: An Epistemological and Critical Analysis},
  issn = {1435-5655},
  shorttitle = {Black-Box Artificial Intelligence},
  abstract = {The artificial intelligence models with machine learning that exhibit the best predictive accuracy, and therefore, the most powerful ones, are, paradoxically, those with the most opaque black-box architectures. At the same time, the unstoppable computerization of advanced industrial societies demands the use of these machines in a growing number of domains. The conjunction of both phenomena gives rise to a control problem on AI that in this paper we analyze by dividing the issue into two. First, we carry out an epistemological examination of the AI's opacity in light of the latest techniques to remedy it. And second, we evaluate the rationality of delegating tasks in opaque agents.},
  language = {en},
  journal = {AI \& SOCIETY},
  doi = {10.1007/s00146-019-00888-w},
  author = {Carabantes, Manuel},
  month = apr,
  year = {2019},
  keywords = {Artificial intelligence,Deep neural networks,GDPR,Instrumental reason,Machine learning,Philosophy of technology,XAI},
  file = {/home/tim/Zotero/storage/24Q7MRVK/Carabantes - 2019 - Black-box artificial intelligence an epistemologi.pdf}
}

@incollection{abdollahiTransparencyFairMachine2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Transparency in {{Fair Machine Learning}}: The {{Case}} of {{Explainable Recommender Systems}}},
  isbn = {978-3-319-90403-0},
  shorttitle = {Transparency in {{Fair Machine Learning}}},
  abstract = {Machine Learning (ML) models are increasingly being used in many sectors, ranging from health and education to justice and criminal investigation. Therefore, building a fair and transparent model which conveys the reasoning behind its predictions is of great importance. This chapter discusses the role of explanation mechanisms in building fair machine learning models and explainable ML technique. We focus on the special case of recommender systems because they are a prominent example of a ML model that interacts directly with humans. This is in contrast to many other traditional decision making systems that interact with experts (e.g. in the health-care domain). In addition, we discuss the main sources of bias that can lead to biased and unfair models. We then review the taxonomy of explanation styles for recommender systems and review models that can provide explanations for their recommendations. We conclude by reviewing evaluation metrics for assessing the power of explainability in recommender systems.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Abdollahi, Behnoush and Nasraoui, Olfa},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {21-35},
  doi = {10.1007/978-3-319-90403-0_2}
}

@inproceedings{marateaDeepNeuralNetworks2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Deep {{Neural Networks}} and {{Explainable Machine Learning}}},
  isbn = {978-3-030-12544-8},
  abstract = {From a general perspective, the most impressive results [1, 6] in Machine Learning have been recently obtained via black-box models, being Deep Neural Networks (DNNs) the major player in the game.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Maratea, Antonio and Ferone, Alessio},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  keywords = {Deep Neural Networks,Granular Computing,XAI},
  pages = {253-256},
  file = {/home/tim/Zotero/storage/ND6ZGFSS/Maratea and Ferone - 2019 - Deep Neural Networks and Explainable Machine Learn.pdf}
}

@article{ECR2018BOOK2018,
  title = {{{ECR}} 2018 - {{BOOK OF ABSTRACTS}}},
  volume = {9},
  issn = {1869-4101},
  language = {en},
  number = {1},
  journal = {Insights into Imaging},
  doi = {10.1007/s13244-018-0603-8},
  month = apr,
  year = {2018},
  pages = {1-642},
  file = {/home/tim/Zotero/storage/ZMRNQ2FK/2018 - ECR 2018 - BOOK OF ABSTRACTS.pdf}
}

@inproceedings{chimatapuExplainableAIFuzzy2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{AI}} and {{Fuzzy Logic Systems}}},
  isbn = {978-3-030-04070-3},
  abstract = {The recent advances in computing power coupled with the rapid increases in the quantity of available data has led to a resurgence in the theory and applications of Artificial Intelligence (AI). However, the use of complex AI algorithms like Deep Learning, Random Forests, etc., could result in a lack of transparency to users which is termed as black/opaque box models. Thus, for AI to be trusted and widely used by governments and industries, there is a need for greater transparency through the creation of explainable AI (XAI) systems. In this paper, we introduce the concepts of XAI and give an overview of hybrid systems which employ fuzzy logic systems which can hold great promise for creating trusted and explainable AI systems.},
  language = {en},
  booktitle = {Theory and {{Practice}} of {{Natural Computing}}},
  publisher = {{Springer International Publishing}},
  author = {Chimatapu, Ravikiran and Hagras, Hani and Starkey, Andrew and Owusu, Gilbert},
  editor = {Fagan, David and {Mart\'in-Vide}, Carlos and O'Neill, Michael and {Vega-Rodr\'iguez}, Miguel A.},
  year = {2018},
  keywords = {Deep fuzzy systems,Explainable AI,Fuzzy logic systems,XAI},
  pages = {3-20},
  file = {/home/tim/Zotero/storage/QDHC8B6N/Chimatapu et al. - 2018 - Explainable AI and Fuzzy Logic Systems.pdf}
}

@incollection{turnerControllingCreations2019,
  address = {Cham},
  title = {Controlling the {{Creations}}},
  isbn = {978-3-319-96235-1},
  abstract = {Turner explains how in order to implement constraints into AI directly, we will need to address both moral and technical questions: Which norms should be chosen? How can these be implemented? Potential basic laws for robots include: a law of identification, requiring that AI makes its status clear; a law of explanation, requiring that at least some parts of AI's reasoning be divulged; a laws on avoiding bias; and a law setting out any limits to areas where AI can operate. Finally, a kill switch law might make it mandatory that AI systems include a mechanism for safely interrupting their processes or operations, either temporarily or permanently.},
  language = {en},
  booktitle = {Robot {{Rules}} : {{Regulating Artificial Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Turner, Jacob},
  editor = {Turner, Jacob},
  year = {2019},
  pages = {319-369},
  doi = {10.1007/978-3-319-96235-1_8}
}

@incollection{galitskyExplainableMachineLearning2019,
  address = {Cham},
  title = {Explainable {{Machine Learning}} for {{Chatbots}}},
  isbn = {978-3-030-04299-8},
  abstract = {Machine learning (ML) has been successfully applied to a wide variety of fields ranging from information retrieval, data mining, and speech recognition, to computer graphics, visualization, and human-computer interaction. However, most users often treat a machine learning model as a black box because of its incomprehensible functions and unclear working mechanism (Liu et al. 2017). Without a clear understanding of how and why a model works, the development of high performance models for chatbots typically relies on a time-consuming trial-and-error process. As a result, academic and industrial ML chatbot developers are facing challenges that demand more transparent and explainable systems for better understanding and analyzing ML models, especially their inner working mechanisms.In this Chapter we focus on explainability. We first discuss what is explainable ML and how its features are desired by users. We then draw an example chatbot-related classification problem and show how it is solved by a transparent rule-based or ML method. After that we present a decision support-enabled chatbot that shares its explanations to back up its decisions and tackles that of a human peer. We conclude this chapter with a learning framework representing a deterministic inductive approach with complete explainability.},
  language = {en},
  booktitle = {Developing {{Enterprise Chatbots}}: {{Learning Linguistic Structures}}},
  publisher = {{Springer International Publishing}},
  author = {Galitsky, Boris and Goldberg, Saveli},
  editor = {Galitsky, Boris},
  year = {2019},
  pages = {53-83},
  doi = {10.1007/978-3-030-04299-8_3}
}

@incollection{riegerStructuringNeuralNetworks2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Structuring {{Neural Networks}} for {{More Explainable Predictions}}},
  isbn = {978-3-319-98131-4},
  abstract = {Machine learning algorithms such as neural networks are more useful, when their predictions can be explained, e.g. in terms of input variables. Often simpler models are more interpretable than more complex models with higher performance. In practice, one can choose a readily interpretable (possibly less predictive) model. Another solution is to directly explain the original, highly predictive model. In this chapter, we present a middle-ground approach where the original neural network architecture is modified parsimoniously in order to reduce common biases observed in the explanations. Our approach leads to explanations that better separate classes in feed-forward networks, and that also better identify relevant time steps in recurrent neural networks.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Rieger, Laura and Chormai, Pattarawat and Montavon, Gr\'egoire and Hansen, Lars Kai and M\"uller, Klaus-Robert},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Convolutional neural networks,Interpretable machine learning,Recurrent neural networks},
  pages = {115-131},
  doi = {10.1007/978-3-319-98131-4_5}
}

@inproceedings{huExplainableNeuralComputation2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
  isbn = {978-3-030-01234-2},
  abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  publisher = {{Springer International Publishing}},
  author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  keywords = {Interpretable reasoning,Neural module networks,Visual question answering},
  pages = {55-71},
  file = {/home/tim/Zotero/storage/D3U374F3/Hu et al. - 2018 - Explainable Neural Computation via Stack Neural Mo.pdf}
}

@article{vellidoImportanceInterpretabilityVisualization2019,
  title = {The Importance of Interpretability and Visualization in Machine Learning for Applications in Medicine and Health Care},
  issn = {1433-3058},
  abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
  language = {en},
  journal = {Neural Computing and Applications},
  doi = {10.1007/s00521-019-04051-w},
  author = {Vellido, Alfredo},
  month = feb,
  year = {2019},
  keywords = {Explainability,Health care,Interpretability,Machine learning,Medicine,Visualization},
  file = {/home/tim/Zotero/storage/SI2SPSZ7/Vellido - 2019 - The importance of interpretability and visualizati.pdf}
}

@inproceedings{mencarPavingWayExplainable2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Paving the {{Way}} to {{Explainable Artificial Intelligence}} with {{Fuzzy Modeling}}},
  isbn = {978-3-030-12544-8},
  abstract = {Explainable Artificial Intelligence (XAI) is a relatively new approach to AI with special emphasis to the ability of machines to give sound motivations about their decisions and behavior. Since XAI is human-centered, it has tight connections with Granular Computing (GrC) in general, and Fuzzy Modeling (FM) in particular. However, although FM has been originally conceived to provide easily understandable models to users, this property cannot be taken for grant but it requires careful design choices. Furthermore, full integration of FM into XAI requires further processing, such as Natural Language Generation (NLG), which is a matter of current research.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Mencar, Corrado and Alonso, Jos\'e M.},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  pages = {215-227},
  file = {/home/tim/Zotero/storage/B8TUU224/Mencar and Alonso - 2019 - Paving the Way to Explainable Artificial Intellige.pdf}
}

@incollection{aakurInherentExplainabilityPattern2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {On the {{Inherent Explainability}} of {{Pattern Theory}}-{{Based Video Event Interpretations}}},
  isbn = {978-3-319-98131-4},
  abstract = {The ability of artificial intelligence systems to offer explanations for its decisions is central to building user confidence and structuring smart human-machine interactions. Expressing the rationale behind such a system's output is an important aspect of human-machine interaction as AI continues to be prominent in general, everyday use-cases. In this paper, we introduce a novel framework integrating Grenander's pattern theory structures to produce inherently explainable, symbolic representations for activity interpretations. These representations provide semantically rich and coherent interpretations of video activity using connected structures of detected (grounded) concepts, such as objects and actions, that are bound by semantics through background concepts not directly observed, i.e. contextualization cues. We use contextualization cues to establish semantic relationships among concepts to infer a deeper interpretation of events than what can be directly sensed. We propose the use of six questions that can be used to gain insight into the models ability to justify its decision and enhance its ability to interact with humans. The six questions are designed to (1) build an understanding of how the model is able to infer interpretations, (2) enable us to walk through its decision-making process, and (3) understand its drawbacks and possibly address them. We demonstrate the viability of this idea on video data using a dialog model that uses interpretations to generate explanations grounded in both video data and semantics.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Aakur, Sathyanarayanan N. and {de Souza}, Fillipe D. M. and Sarkar, Sudeep},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Activity interpretation,ConceptNet,Explainability,Semantics},
  pages = {277-299},
  doi = {10.1007/978-3-319-98131-4_11}
}

@incollection{liemPsychologyMeetsMachine2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Psychology {{Meets Machine Learning}}: {{Interdisciplinary Perspectives}} on {{Algorithmic Job Candidate Screening}}},
  isbn = {978-3-319-98131-4},
  shorttitle = {Psychology {{Meets Machine Learning}}},
  abstract = {In a rapidly digitizing world, machine learning algorithms are increasingly employed in scenarios that directly impact humans. This also is seen in job candidate screening. Data-driven candidate assessment is gaining interest, due to high scalability and more systematic assessment mechanisms. However, it will only be truly accepted and trusted if explainability and transparency can be guaranteed. The current chapter emerged from ongoing discussions between psychologists and computer scientists with machine learning interests, and discusses the job candidate screening problem from an interdisciplinary viewpoint. After introducing the general problem, we present a tutorial on common important methodological focus points in psychological and machine learning research. Following this, we both contrast and combine psychological and machine learning approaches, and present a use case example of a data-driven job candidate assessment system, intended to be explainable towards non-technical hiring specialists. In connection to this, we also give an overview of more traditional job candidate assessment approaches, and discuss considerations for optimizing the acceptability of technology-supported hiring solutions by relevant stakeholders. Finally, we present several recommendations on how interdisciplinary collaboration on the topic may be fostered.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Liem, Cynthia C. S. and Langer, Markus and Demetriou, Andrew and Hiemstra, Annemarie M. F. and Sukma Wicaksana, Achmadnoer and Born, Marise Ph. and K\"onig, Cornelius J.},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainability,Interdisciplinarity,Job candidate screening,Machine learning,Methodology,Multimodal analysis,Psychology},
  pages = {197-253},
  doi = {10.1007/978-3-319-98131-4_9}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
  isbn = {978-3-319-98131-4},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Accountability,Interpretability,Machine learning,Transparency},
  pages = {3-17},
  doi = {10.1007/978-3-319-98131-4_1}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
  series = {International {{Centre}} for {{Mechanical Sciences}}},
  title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
  isbn = {978-3-7091-2668-4},
  shorttitle = {Machine {{Learning}}},
  abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
  language = {en},
  booktitle = {Learning, {{Networks}} and {{Statistics}}},
  publisher = {{Springer Vienna}},
  author = {Bratko, I.},
  editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
  year = {1997},
  keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
  pages = {163-177}
}

@inproceedings{otteSafeInterpretableMachine2013,
  series = {Studies in {{Computational Intelligence}}},
  title = {Safe and {{Interpretable Machine Learning}}: {{A~Methodological Review}}},
  isbn = {978-3-642-32378-2},
  shorttitle = {Safe and {{Interpretable Machine Learning}}},
  abstract = {When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.},
  language = {en},
  booktitle = {Computational {{Intelligence}} in {{Intelligent Data Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Otte, Clemens},
  editor = {Moewes, Christian and N\"urnberger, Andreas},
  year = {2013},
  keywords = {Input Space,Methodological Review,Multivariate Adaptive Regression Spline,Symbolic Model,Symbolic Regression},
  pages = {111-122}
}

@incollection{jinInterpretabilityImprovementRBFbased2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Interpretability Improvement of {{RBF}}-Based Neurofuzzy Systems Using Regularized Learning},
  isbn = {978-3-540-37057-4},
  abstract = {Radial-basis-function (RBF) networks are mathematically equivalent to a class of fuzzy systems under mild conditions. Therefore, RBF networks have widely been used in learning of neurofuzzy systems to improve the performance. However, in most cases, the interpretability of fuzzy system will get lost after neural network learning. This chapter proposes a learning method using interpretability based regularization for neurofuzzy systems. This method can either be used in extracting interpretable fuzzy rules from RBF networks or in improving the interpretability of RBF-based neurofuzzy systems. Two simulation examples are presented to show the effectiveness of the proposed method.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jin, Yaochu},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Fuzzy Inference System,Fuzzy Rule,Fuzzy System,Membership Function,Radial Basis Function Network},
  pages = {605-620},
  doi = {10.1007/978-3-540-37057-4_26}
}

@inproceedings{oitaReverseEngineeringCreativity2020,
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  title = {Reverse {{Engineering Creativity}} into {{Interpretable Neural Networks}}},
  isbn = {978-3-030-12385-7},
  abstract = {In the field of AI the ultimate goal is to achieve generic intelligence, also called ``true AI'', but which depends on the successful enablement of imagination and creativity in artificial agents. To address this problem, this paper presents a novel deep learning framework for creativity, called INNGenuity. Pursuing an interdisciplinary implementation of creativity conditions, INNGenuity aims at the resolution of the various flaws of current AI learning architectures, which stem from the opacity of their models. Inspired by the neuroanatomy of the brain during creative cognition, the proposed framework's hybrid architecture blends both symbolic and connectionist AI, inline with Minsky's ``society of mind''. At its core, semantic gates are designed to facilitate an input/output flow of semantic structures and enable the usage of aligning mechanisms between neural activation clusters and semantic graphs. Having as goal alignment maximization, such a system would enable interpretability through the creation of labeled patterns of computation, and propose unaligned but relevant computation patterns as novel and useful, therefore creative.},
  language = {en},
  booktitle = {Advances in {{Information}} and {{Communication}}},
  publisher = {{Springer International Publishing}},
  author = {Oita, Marilena},
  editor = {Arai, Kohei and Bhatia, Rahul},
  year = {2020},
  keywords = {Creativity,Imagination,Interpretability,Knowledge,Neural architecture,Neural networks,Semantic networks},
  pages = {235-247}
}

@incollection{russellHumanInformationInteraction2017,
  address = {Cham},
  title = {Human {{Information Interaction}}, {{Artificial Intelligence}}, and {{Errors}}},
  isbn = {978-3-319-59719-5},
  abstract = {In a time of pervasive and increasingly transparent computing, humans will interact more with information objects, and less with the computing devices that define them. Artificial Intelligence (AI) will be the proxy for humans' interaction with information. Because interaction creates opportunities for error, the trend towards AI-augmented human information interaction (HII) will mandate an increased emphasis on cognition-oriented information science research, and new ways of thinking about errors and error handling. In this chapter, a review of HII and its relationship to AI is presented, with a focus on errors in this context.},
  language = {en},
  booktitle = {Autonomy and {{Artificial Intelligence}}: {{A Threat}} or {{Savior}}?},
  publisher = {{Springer International Publishing}},
  author = {Russell, Stephen and Moskowitz, Ira S. and Raglin, Adrienne},
  editor = {Lawless, W.F. and Mittu, Ranjeev and Sofge, Donald and Russell, Stephen},
  year = {2017},
  pages = {71-101},
  doi = {10.1007/978-3-319-59719-5_4}
}

@article{zerilliTransparencyAlgorithmicHuman2018,
  title = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}: {{Is There}} a {{Double Standard}}?},
  issn = {2210-5441},
  shorttitle = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}},
  abstract = {We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems, show in what respects AI fares little worse or better and argue that at least some regulatory proposals for explainable AI could end up setting the bar higher than is necessary or indeed helpful. The demands of practical reason require the justification of action to be pitched at the level of practical reason. Decision tools that support or supplant practical reasoning should not be expected to aim higher than this. We cast this desideratum in terms of Daniel Dennett's theory of the ``intentional stance'' and argue that since the justification of action for human purposes takes the form of intentional stance explanation, the justification of algorithmic decisions should take the same form. In practice, this means that the sorts of explanations for algorithmic decisions that are analogous to intentional stance explanations should be preferred over ones that aim at the architectural innards of a decision tool.},
  language = {en},
  journal = {Philosophy \& Technology},
  doi = {10.1007/s13347-018-0330-6},
  author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
  month = sep,
  year = {2018},
  keywords = {Algorithmic decision-making,Explainable AI,Intentional stance,Transparency},
  file = {/home/tim/Zotero/storage/DJLZ77QS/Zerilli et al. - 2018 - Transparency in Algorithmic and Human Decision-Mak.pdf}
}

@incollection{kayaMultimodalPersonalityTrait2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Multimodal {{Personality Trait Analysis}} for {{Explainable Modeling}} of {{Job Interview Decisions}}},
  isbn = {978-3-319-98131-4},
  abstract = {Automatic analysis of job interview screening decisions is useful for establishing the nature of biases that may play a role in such decisions. In particular, assessment of apparent personality gives insights into the first impressions evoked by a candidate. Such analysis tools can be used for training purposes, if they can be configured to provide appropriate and clear feedback. In this chapter, we describe a multimodal system that analyzes a short video of a job candidate, producing apparent personality scores and a prediction about whether the candidate will be invited for a further job interview or not. This system provides a visual and textual explanation about its decision, and was ranked first in the ChaLearn 2017 Job Candidate Screening Competition. We discuss the application scenario and the considerations from a broad perspective.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kaya, Heysem and Salah, Albert Ali},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable machine learning,Job candidate screening,Multimodal affective computing,Personality trait analysis},
  pages = {255-275},
  doi = {10.1007/978-3-319-98131-4_10}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
  isbn = {978-3-319-99740-7},
  shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
  abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial intelligence,Explainable AI,Knowledge extraction,Machine learning,Privacy},
  pages = {1-8},
  file = {/home/tim/Zotero/storage/UPAN5NTZ/Holzinger et al. - 2018 - Current Advances, Trends and Challenges of Machine.pdf}
}

@inproceedings{goebelExplainableAINew2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{AI}}: {{The New}} 42?},
  isbn = {978-3-319-99740-7},
  shorttitle = {Explainable {{AI}}},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning},
  pages = {295-303},
  file = {/home/tim/Zotero/storage/2PBCRQ7H/Goebel et al. - 2018 - Explainable AI The New 42.pdf}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
  isbn = {978-3-319-03200-9},
  abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Lisboa, P. J. G.},
  editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
  year = {2013},
  keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
  pages = {15-21},
  file = {/home/tim/Zotero/storage/LRZ3MW87/Lisboa - 2013 - Interpretability in Machine Learning – Principles .pdf}
}

@incollection{jinSimultaneousGenerationAccurate2006,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Simultaneous {{Generation}} of {{Accurate}} and {{Interpretable Neural Network Classifiers}}},
  isbn = {978-3-540-33019-6},
  abstract = {Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.},
  language = {en},
  booktitle = {Multi-{{Objective Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jin, Yaochu and Sendhoff, Bernhard and K\"orner, Edgar},
  editor = {Jin, Yaochu},
  year = {2006},
  keywords = {Hide Neuron,Mean Square Error,Multiobjective Optimization,Neural Network,Pareto Front},
  pages = {291-312},
  file = {/home/tim/Zotero/storage/TJXC8RBG/Jin et al. - 2006 - Simultaneous Generation of Accurate and Interpreta.pdf},
  doi = {10.1007/3-540-33019-4_13}
}

@incollection{kochGroupCognitionCollaborative2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Group {{Cognition}} and {{Collaborative AI}}},
  isbn = {978-3-319-90403-0},
  abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Koch, Janin and Oulasvirta, Antti},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {293-312},
  doi = {10.1007/978-3-319-90403-0_15}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
  isbn = {978-3-030-04503-6},
  abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
  language = {en},
  booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
  editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
  year = {2018},
  keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
  pages = {295-308},
  file = {/home/tim/Zotero/storage/4NFATM85/Zhou et al. - 2018 - Measuring Interpretability for Different Types of .pdf}
}

@inproceedings{hsuMultivariateTimeSeries2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Multivariate {{Time Series Early Classification}} with {{Interpretability Using Deep Learning}} and {{Attention Mechanism}}},
  isbn = {978-3-030-16142-2},
  abstract = {Multivariate time-series early classification is an emerging topic in data mining fields with wide applications like biomedicine, finance, manufacturing, etc. Despite of some recent studies on this topic that delivered promising developments, few relevant works can provide good interpretability. In this work, we consider simultaneously the important issues of model performance, earliness, and interpretability to propose a deep-learning framework based on the attention mechanism for multivariate time-series early classification. In the proposed model, we used a deep-learning method to extract the features among multiple variables and capture the temporal relation that exists in multivariate time-series data. Additionally, the proposed method uses the attention mechanism to identify the critical segments related to model performance, providing a base to facilitate the better understanding of the model for further decision making. We conducted experiments on three real datasets and compared with several alternatives. While the proposed method can achieve comparable performance results and earliness compared to other alternatives, more importantly, it can provide interpretability by highlighting the important parts of the original data, rendering it easier for users to understand how the prediction is induced from the data.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Hsu, En-Yu and Liu, Chien-Liang and Tseng, Vincent S.},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  keywords = {Attention,Deep neural network,Early classification on time-series},
  pages = {541-553},
  file = {/home/tim/Zotero/storage/F5VFRTAP/Hsu et al. - 2019 - Multivariate Time Series Early Classification with.pdf}
}

@inproceedings{zhangInterpretableNeuralModel2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {An {{Interpretable Neural Model}} with {{Interactive Stepwise Influence}}},
  isbn = {978-3-030-16142-2},
  abstract = {Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhang, Yin and Liu, Ninghao and Ji, Shuiwang and Caverlee, James and Hu, Xia},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  keywords = {Interpretation,Neural network,Stepwise Influence},
  pages = {528-540},
  file = {/home/tim/Zotero/storage/L3NSU5YW/Zhang et al. - 2019 - An Interpretable Neural Model with Interactive Ste.pdf}
}

@incollection{kimExplainableDeepDriving2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Explainable {{Deep Driving}} by {{Visualizing Causal Attention}}},
  isbn = {978-3-319-98131-4},
  abstract = {Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable\textemdash{}they should provide easy-to-interpret rationales for their behavior\textemdash{}so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here, we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolutional network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 h of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network highlights interpretable features that are used by humans while driving, and causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kim, Jinkyu and Canny, John},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable AI,Self-driving vehicles,Visual attention},
  pages = {173-193},
  doi = {10.1007/978-3-319-98131-4_8}
}

@incollection{ivancevicIntroductionHumanComputational2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Introduction: {{Human}} and {{Computational Mind}}},
  isbn = {978-3-540-71561-0},
  shorttitle = {Introduction},
  language = {en},
  booktitle = {Computational {{Mind}}: {{A Complex Dynamics Perspective}}},
  publisher = {{Springer Berlin Heidelberg}},
  editor = {Ivancevic, Vladimir G. and Ivancevic, Tijana T.},
  year = {2007},
  keywords = {Adaptive Resonance Theory,Cellular Automaton,Horn Clause,Human Mind},
  pages = {1-269},
  file = {/home/tim/Zotero/storage/5Y4AXSEY/Ivancevic and Ivancevic - 2007 - Introduction Human and Computational Mind.pdf},
  doi = {10.1007/978-3-540-71561-0_1}
}

@incollection{nissanAccountingSocialSpatial2012,
  address = {Dordrecht},
  series = {Law, {{Governance}} and {{Technology Series}}},
  title = {Accounting for {{Social}}, {{Spatial}}, and {{Textual Interconnections}}},
  isbn = {978-90-481-8990-8},
  abstract = {This is a chapter about what link analysis and data mining can do for criminal investigation. It is a long and complex chapter, in which a variety of techniques and topics are accommodated. It is divided in two parts, one about methods, and the other one about real-case studies. We begin by discussing social networks and their visualisation, as well as what unites them with or distinguishes them from link analysis (which itself historically arose from the disciplinary context of ergonomics). Having considered applications of link analysis to criminal investigation, we turn to crime risk assessment, to geographic information systems for mapping crimes, to detection, and then to multiagent architectures and their application to policing. We then turn to the challenge of handling a disparate mass of data, and introduce the reader to data warehousing, XML, ontologies, legal ontologies, and financial fraud ontology. A section about automated summarisation and its application to law is followed by a discussion of text mining and its application to law, and by a section on support vector machines for information retrieval, text classification, and matching. A section follows, about stylometrics, determining authorship, handwriting identification and its automation, and questioned documents evidence. We next discuss classification, clustering, series analysis, and association in knowledge discovery from legal databases; then, inconsistent data; rule induction (including in law); using neural networks in the legal context; fuzzy logic; and genetic algorithms. Before turning to case studies of link analysis and data mining, we take a broad view of digital resources and uncovering perpetration: email mining, computer forensics, and intrusion detection. We consider the Enron email database; the discovery of social coalitions with the SIGHTS text mining system, and recursive data mining. We discuss digital forensics, digital steganography, and intrusion detection (the use of learning techniques, the detection of masquerading, and honeypots for trapping intruders). Case studies include, for example: investigating Internet auction fraud with NetProbe; graph mining for malware detection with Polonium; link analysis with Coplink; a project of the U.S. Federal Defense Financial Accounting Service; information extraction tools for integration with a link analysis tool; the Poznan ontology model for the link analysis of fuel fraud; and fiscal fraud detection with the Pisa SNIPER project.},
  language = {en},
  booktitle = {Computer {{Applications}} for {{Handling Legal Evidence}}, {{Police Investigation}} and {{Case Argumentation}}},
  publisher = {{Springer Netherlands}},
  author = {Nissan, Ephraim},
  editor = {Nissan, Ephraim},
  year = {2012},
  pages = {483-765},
  doi = {10.1007/978-90-481-8990-8_6}
}

@incollection{thompsonArtificialIntelligenceArtificial1996,
  address = {Dordrecht},
  series = {Boston {{Studies}} in the {{Philosophy}} of {{Science}}},
  title = {Artificial {{Intelligence}}, {{Artificial Life}}, and the {{Symbol}}-{{Matter Problem}}},
  isbn = {978-94-009-0113-1},
  abstract = {What is the relation between matter and form? This question is of course as old as philosophy itself. But it also arises at the foundations of two recent scientific endeavours \textemdash{} the computational approach to the mind-brain in cognitive science and artificial intelligence (AI), and the synthetic approach to living systems in theoretical biology and artificial life (AL). In these fields the question arises primarily in connection with the status of symbols, that is, items that are physically realized, formally identified, and semantically interpretable.},
  language = {en},
  booktitle = {Qu\'ebec {{Studies}} in the {{Philosophy}} of {{Science}}: {{Part II}}: {{Biology}}, {{Psychology}}, {{Cognitive Science}} and {{Economics Essays}} in {{Honor}} of {{Hugues Leblanc}}},
  publisher = {{Springer Netherlands}},
  author = {Thompson, Evan},
  editor = {Marion, Mathieu and Cohen, Robert S.},
  year = {1996},
  keywords = {Artificial Life,Genetic Code,Intentional Stance,Multiple Realizability,Syntactic Feature},
  pages = {63-80},
  doi = {10.1007/978-94-009-0113-1_5}
}

@incollection{jinKnowledgeDiscoveryExtracting2003,
  address = {Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Knowledge {{Discovery}} by {{Extracting Interpretable Fuzzy Rules}}},
  isbn = {978-3-7908-1771-3},
  abstract = {Data, information and knowledge are three closely related but different concepts cepts. The following are the definitions abstracted from Webster's dictionary: Data: 1. factual information (as measurements or statistics) used as a basis for reasoning, discussion, or calculation 2. information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful 3. information in numerical form that can be digitally transmitted or processed. Information: 1. the communication or reception of knowledge or intelligence 2. knowledge obtained from investigation, study, or instruction. Knowledge: 1. the fact or condition of knowing something with familiarity gained through experience or association 2. acquaintance with or understanding of a science, art, or technique 3. the fact or condition of being aware of something.},
  language = {en},
  booktitle = {Advanced {{Fuzzy Systems Design}} and {{Applications}}},
  publisher = {{Physica-Verlag HD}},
  author = {Jin, Yaochu},
  editor = {Jin, Yaochu},
  year = {2003},
  keywords = {Fuzzy Rule,Fuzzy Subset,Fuzzy System,Membership Function,Radial Basis Function Network},
  pages = {173-204},
  doi = {10.1007/978-3-7908-1771-3_7}
}

@incollection{browneCriticalChallengesVisual2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
  isbn = {978-3-319-90403-0},
  abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {119-136},
  doi = {10.1007/978-3-319-90403-0_7}
}

@incollection{alonsoRoleInterpretableFuzzy2019,
  address = {Cham},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  title = {The {{Role}} of {{Interpretable Fuzzy Systems}} in {{Designing Cognitive Cities}}},
  isbn = {978-3-030-00317-3},
  abstract = {In recent years, there has been a huge effort connecting all kind of devices to Internet. From small devices (e.g., e-health monitoring sensors or mobile phones) that we carry daily in what is called the body-area-network, to big devices (such as cars), passing by all devices (e.g., TVs or refrigerators) at home. In modern cities, everything (at work, at home, and even in the streets) is connected to Internet. Accordingly, the amount of data in Internet grows dramatically every day. With this regard, humans face two main challenges: (1) to extract valuable knowledge from the given Big Data and (2) to become part of the equation, i.e., to become active actors in the Internet of Things. To do so, researchers and developers have created a novel generation of intelligent systems which are producing more and more intelligent devices, yielding what is called smart cities. Fuzzy systems are used in many applications in the context of Smart Cities. Now, it is time to address the effective interaction between intelligent systems and citizens with the aim of passing from smart to Cognitive Cities. Moreover, the use of interpretable fuzzy systems can facilitate such interaction and pave the way towards Cognitive Cities.},
  language = {en},
  booktitle = {Designing {{Cognitive Cities}}},
  publisher = {{Springer International Publishing}},
  author = {Alonso, Jos\'e M. and Castiello, Ciro and Mencar, Corrado},
  editor = {Portmann, Edy and Tabacchi, Marco E. and Seising, Rudolf and Habenstein, Astrid},
  year = {2019},
  keywords = {Cognitive City,Collaborative intelligence,Computational Theory of Perceptions,Fuzzy Logic,Human-machine communication,Interpretability,Linguistic descriptions of complex phenomena},
  pages = {131-152},
  doi = {10.1007/978-3-030-00317-3_6}
}

@incollection{liuInterpretabilityComputationalModels2016,
  address = {Cham},
  series = {Studies in {{Computational Intelligence}}},
  title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
  isbn = {978-3-319-30319-2},
  abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
  language = {en},
  booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  year = {2016},
  keywords = {Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Machine learning,Rule based networks,Rule based systems,Sentiment prediction},
  pages = {199-220},
  doi = {10.1007/978-3-319-30319-2_9}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
  isbn = {978-3-319-71746-3},
  shorttitle = {Interpretable {{Probabilistic Embeddings}}},
  abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
  publisher = {{Springer International Publishing}},
  author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
  editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
  year = {2018},
  pages = {167-180}
}

@article{curchoeArtificialIntelligenceMachine2019,
  title = {Artificial Intelligence and Machine Learning for Human Reproduction and Embryology Presented at {{ASRM}} and {{ESHRE}} 2018},
  issn = {1573-7330},
  abstract = {Sixteen artificial intelligence (AI) and machine learning (ML) approaches were reported at the 2018 annual congresses of the American Society for Reproductive Biology (9) and European Society for Human Reproduction and Embryology (7). Nearly every aspect of patient care was investigated, including sperm morphology, sperm identification, identification of empty or oocyte containing follicles, predicting embryo cell stages, predicting blastocyst formation from oocytes, assessing human blastocyst quality, predicting live birth from blastocysts, improving embryo selection, and for developing optimal IVF stimulation protocols. This represents a substantial increase in reports over 2017, where just one abstract each was reported at ASRM (AI) and ESHRE (ML). Our analysis reveals wide variability in how AI and ML methods are described (from not at all or very generic to fully describing the architectural framework) and large variability on accepted dataset sizes (from just 3 patients with 16 follicles in the smallest dataset to 661,060 images of 11,898 human embryos in one of the largest). AI and ML are clearly burgeoning methodologies in human reproduction and embryology and would benefit from early application of reporting standards.},
  language = {en},
  journal = {Journal of Assisted Reproduction and Genetics},
  doi = {10.1007/s10815-019-01408-x},
  author = {Curchoe, Carol Lynn and Bormann, Charles L.},
  month = jan,
  year = {2019},
  keywords = {Artificial intelligence,ASHRE,ASRM,Embryology,Human reproduction,Machine learning},
  file = {/home/tim/Zotero/storage/3QTHNKUM/Curchoe and Bormann - 2019 - Artificial intelligence and machine learning for h.pdf}
}

@incollection{dengJointIntroductionNatural2018,
  address = {Singapore},
  title = {A {{Joint Introduction}} to {{Natural Language Processing}} and to {{Deep Learning}}},
  isbn = {978-981-10-5209-5},
  abstract = {In this chapter, we set up the fundamental framework for the book. We first provide an introduction to the basics of natural language processing (NLP) as an integral part of artificial intelligence. We then survey the historical development of NLP, spanning over five decades, in terms of three waves. The first two waves arose as rationalism and empiricism, paving ways to the current deep learning wave. The key pillars underlying the deep learning revolution for NLP consist of (1) distributed representations of linguistic entities via embedding, (2) semantic generalization due to the embedding, (3) long-span deep sequence modeling of natural language, (4) hierarchical networks effective for representing linguistic levels from low to high, and (5) end-to-end deep learning methods to jointly solve many NLP tasks. After the survey, several key limitations of current deep learning technology for NLP are analyzed. This analysis leads to five research directions for future advances in NLP.},
  language = {en},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {1-22},
  doi = {10.1007/978-981-10-5209-5_1}
}

@incollection{pace-siggeWhereCorpusLinguistics2018,
  address = {Cham},
  title = {Where {{Corpus Linguistics}} and {{Artificial Intelligence}} ({{AI}}) {{Meet}}},
  isbn = {978-3-319-90719-2},
  abstract = {This chapter will provide a platform to showcase the more recent developments that have grown out of the early laid groundwork. The latest theories in the field of linguistics will be presented, based on empirical data taken from naturally occurring language. In particular, the lexical priming theory will be introduced as a way to explain structures of language that corpus linguists have uncovered. Furthermore, the chapter will discuss the development of increasingly sophisticated algorithms that also deal with the use of language. Here, the focus will be on key achievements in the 1980s by IBM which created a solid foundation for applications that are now widely used in mobile and desktop devices\textemdash{}namely ``assistants'' like Amazon's Echo, Apple's SIRI or Google's (and Android's) Google Go.},
  language = {en},
  booktitle = {Spreading {{Activation}}, {{Lexical Priming}} and the {{Semantic Web}}: {{Early Psycholinguistic Theories}}, {{Corpus Linguistics}} and {{AI Applications}}},
  publisher = {{Springer International Publishing}},
  author = {{Pace-Sigge}, Michael},
  editor = {{Pace-Sigge}, Michael},
  year = {2018},
  keywords = {Digital translators,Hoey,Lexical priming,LSTM,N-gram model,Norvig,Quillian},
  pages = {29-82},
  doi = {10.1007/978-3-319-90719-2_3}
}

@incollection{mainzerComplexSystemsEvolution2007,
  address = {Berlin, Heidelberg},
  title = {Complex {{Systems}} and the {{Evolution}} of {{Artificial Life}} and {{Intelligence}}},
  isbn = {978-3-540-72228-1},
  abstract = {All kinds of complex dynamical systems can be modeled by computational systems (Sect. 6.1). Their concepts are inspired by the successful technical applications of nonlinear dynamics to solid-state physics, spin-glass physics, chemical parallel computers, optical parallel computers, laser systems, and the human brain (Sect. 6.2). The cellular neural network (CNN) model has recently become an influential paradigm in complexity research (Sect. 6.3). Like the universal Turing machine model for digital computers, there is a universal CNN machine for modeling analog neural computers. CNNs are used not only for pattern recognition, but to simulate various types of pattern formation (Sect. 6.4). Exciting applications of artificial neural networks already exist in the fields of organic computing, neurobionics, medicine, and robotics (Sect. 6.5). Natural life and intelligence depends decisively on the evolution of organisms and brains. Therefore, embodied life and mind lead to embodied artificial intelligence and embodied artificial life of embodied robotics (Sect. 6.6).},
  language = {en},
  booktitle = {Thinking in {{Complexity}}: {{The Computional Dynamics}} of {{Matter}}, {{Mind}} and {{Mankind}}},
  publisher = {{Springer Berlin Heidelberg}},
  editor = {Mainzer, Klaus},
  year = {2007},
  keywords = {Boolean Function,Cellular Automaton,Cellular Neural Network,Organic Computing,Truth Table},
  pages = {227-310},
  doi = {10.1007/978-3-540-72228-1_6}
}

@incollection{mainzerComplexSystemsEvolution1997,
  address = {Berlin, Heidelberg},
  title = {Complex {{Systems}} and the {{Evolution}} of {{Artificial Intelligence}}},
  isbn = {978-3-662-13214-2},
  abstract = {Can machines think? This famous question from Turing has new topicality in the framework of complex systems. The chapter starts with a short history of computer science since Leibniz and his program for mechanizing thinking (mathesis universalis) (Sect. 5.1). The modern theory of computability enables us to distinguish complexity classes of problems, meaning the order of corresponding functions describing the computational time of their algorithms or computational programs. Modern computer science is interested not only in the complexity of universal problem solving but also in the complexity of knowledge-based programs. Famous examples are expert systems simulating the problem solving behavior of human experts in their specialized fields. Further on, we ask if a higher efficiency of problem solving may be expected from quantum computers and quantum complexity theory (Sect. 5.2).},
  language = {en},
  booktitle = {Thinking in {{Complexity}}: {{The Complex Dynamics}} of {{Matter}}, {{Mind}}, and {{Mankind}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mainzer, Klaus},
  editor = {Mainzer, Klaus},
  year = {1997},
  keywords = {Cellular Automaton,Certainty Factor,Computer Virus,Expert System,Necker Cube,Turing Machine},
  pages = {171-252},
  doi = {10.1007/978-3-662-13214-2_5}
}

@inproceedings{muzzioliFutureFuzzySets2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {The {{Future}} of {{Fuzzy Sets}} in {{Finance}}: {{New Challenges}} in {{Machine Learning}} and {{Explainable AI}}},
  isbn = {978-3-030-12544-8},
  shorttitle = {The {{Future}} of {{Fuzzy Sets}} in {{Finance}}},
  abstract = {Traditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as possible before modeling, and, as a result, part of the original information is lost. On the other hand, machine learning does not impose rigid pre-assumptions about the problem and data distributions since the underlying ratio is to ``learn from data'', without the need for data reduction or a priori knowledge before the learning.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Muzzioli, Silvia},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  keywords = {Big data,Explainable AI,Finance,Fuzzy sets,Machine learning},
  pages = {265-268},
  file = {/home/tim/Zotero/storage/Y9XPTH5V/Muzzioli - 2019 - The Future of Fuzzy Sets in Finance New Challenge.pdf}
}

@inproceedings{brideDependableExplainableMachine2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
  isbn = {978-3-030-02450-5},
  abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
  language = {en},
  booktitle = {Formal {{Methods}} and {{Software Engineering}}},
  publisher = {{Springer International Publishing}},
  author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
  editor = {Sun, Jing and Sun, Meng},
  year = {2018},
  pages = {412-416},
  file = {/home/tim/Zotero/storage/8Y3ZXFCV/Bride et al. - 2018 - Towards Dependable and Explainable Machine Learnin.pdf}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
  isbn = {978-3-030-02628-8},
  abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
  language = {en},
  booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
  editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
  year = {2018},
  keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
  pages = {133-140},
  file = {/home/tim/Zotero/storage/GR58FCWK/Silva et al. - 2018 - Towards Complementary Explanations Using Deep Neur.pdf}
}

@incollection{liaoMiningHumanInterpretable2006,
  address = {Boston, MA},
  series = {Massive {{Computing}}},
  title = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}: {{An Overview}}},
  isbn = {978-0-387-34296-2},
  shorttitle = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}},
  abstract = {This chapter focuses on one particular class of data mining methodologies that expresses the mined knowledge in the form of fuzzy If-Then rules or fuzzy decision trees that can be easily understood by a human. Past studies on generating fuzzy If-Then rules (mostly from exemplar crisp data and a few from exemplar fuzzy data) are grouped into six major categories: grid partitioning, fuzzy clustering, genetic algorithms, neural networks, hybrid methods, and others. The representative method in each category is detailed. The latest improvements and advancements in each category are also reviewed. Similarly, past studies on generating fuzzy decision trees (from exemplar nominal and/or numeric data as well as from exemplar fuzzy data) are surveyed. The essence of each method is presented. Moreover, we discuss selected studies that address most of the necessary conditions for a fuzzy model to be interpretable and highlight areas for future studies. To give an idea of where fuzzy modeling methods have been applied, major application areas are also summarized.},
  language = {en},
  booktitle = {Data {{Mining}} and {{Knowledge Discovery Approaches Based}} on {{Rule Induction Techniques}}},
  publisher = {{Springer US}},
  author = {Liao, T. Warren},
  editor = {Triantaphyllou, Evangelos and Felici, Giovanni},
  year = {2006},
  keywords = {Data mining,Fuzzy clustering,Fuzzy decision trees,Fuzzy If-Then rules,Fuzzy modeling,Fuzzy-neural networks,Genetic algorithms,Neural networks},
  pages = {495-550},
  doi = {10.1007/0-387-34296-6_15}
}

@article{schubbachJudgingMachinesPhilosophical2019,
  title = {Judging Machines: Philosophical Aspects of Deep Learning},
  issn = {1573-0964},
  shorttitle = {Judging Machines},
  abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
  language = {en},
  journal = {Synthese},
  doi = {10.1007/s11229-019-02167-z},
  author = {Schubbach, Arno},
  month = mar,
  year = {2019},
  keywords = {Algorithm,Artificial intelligence,Computation,Deep learning,Explanation,Judgment,Justification,Kant,Machine learning},
  file = {/home/tim/Zotero/storage/DAHFWILU/Schubbach - 2019 - Judging machines philosophical aspects of deep le.pdf}
}

@article{zhongArtificialIntelligenceDrug2018,
  title = {Artificial Intelligence in Drug Design},
  volume = {61},
  issn = {1869-1889},
  abstract = {Thanks to the fast improvement of the computing power and the rapid development of the computational chemistry and biology, the computer-aided drug design techniques have been successfully applied in almost every stage of the drug discovery and development pipeline to speed up the process of research and reduce the cost and risk related to preclinical and clinical trials. Owing to the development of machine learning theory and the accumulation of pharmacological data, the artificial intelligence (AI) technology, as a powerful data mining tool, has cut a figure in various fields of the drug design, such as virtual screening, activity scoring, quantitative structure-activity relationship (QSAR) analysis, de novo drug design, and in silico evaluation of absorption, distribution, metabolism, excretion and toxicity (ADME/T) properties. Although it is still challenging to provide a physical explanation of the AI-based models, it indeed has been acting as a great power to help manipulating the drug discovery through the versatile frameworks. Recently, due to the strong generalization ability and powerful feature extraction capability, deep learning methods have been employed in predicting the molecular properties as well as generating the desired molecules, which will further promote the application of AI technologies in the field of drug design.},
  language = {en},
  number = {10},
  journal = {Science China Life Sciences},
  doi = {10.1007/s11427-018-9342-2},
  author = {Zhong, Feisheng and Xing, Jing and Li, Xutong and Liu, Xiaohong and Fu, Zunyun and Xiong, Zhaoping and Lu, Dong and Wu, Xiaolong and Zhao, Jihui and Tan, Xiaoqin and Li, Fei and Luo, Xiaomin and Li, Zhaojun and Chen, Kaixian and Zheng, Mingyue and Jiang, Hualiang},
  month = oct,
  year = {2018},
  keywords = {ADME/T,artificial intelligence,deep learning,drug design,QSAR},
  pages = {1191-1204},
  file = {/home/tim/Zotero/storage/TM3SYSZ9/Zhong et al. - 2018 - Artificial intelligence in drug design.pdf}
}

@incollection{panesarEthicsIntelligence2019,
  address = {Berkeley, CA},
  title = {Ethics of {{Intelligence}}},
  isbn = {978-1-4842-3799-1},
  abstract = {From supermarket checkouts to airport check-ins and digital healthcare to Internet banking, the use of data and AI for decision-making is ubiquitous. There has been an astronomic growth in data availability over the last two decades, fueled by, first, connectivity, and now the Internet of Things. Traditional data science teams focus on the use of data for the creation, implementation, validation, and evaluation of machine learning models that can be for predictive analytics.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{AI}} for {{Healthcare}}	: {{Big Data}} for {{Improved Health Outcomes}}},
  publisher = {{Apress}},
  author = {Panesar, Arjun},
  editor = {Panesar, Arjun},
  year = {2019},
  pages = {207-254},
  doi = {10.1007/978-1-4842-3799-1_6}
}

@article{jinExtractingInterpretableFuzzy2003,
  title = {Extracting {{Interpretable Fuzzy Rules}} from {{RBF Networks}}},
  volume = {17},
  issn = {1573-773X},
  abstract = {Radial basis function networks and fuzzy rule systems are functionally equivalent under some mild conditions. Therefore, the learning algorithms developed in the field of artificial neural networks can be used to adapt the parameters of fuzzy systems. Unfortunately, after the neural network learning, the structure of the original fuzzy system is changed and interpretability, which is considered to be one of the most important features of fuzzy systems, is usually impaired. This Letter discusses the differences between RBF networks and interpretable fuzzy systems. Based on these discussions, a method for extracting interpretable fuzzy rules from RBF networks is suggested. Simulation examples are given to embody the idea of this paper.},
  language = {en},
  number = {2},
  journal = {Neural Processing Letters},
  doi = {10.1023/A:1023642126478},
  author = {Jin, Yaochu and Sendhoff, Bernhard},
  month = apr,
  year = {2003},
  keywords = {Artificial Intelligence,Artificial Neural Network,Basis Function,Complex System,Neural Network},
  pages = {149-164},
  file = {/home/tim/Zotero/storage/8JJ8MLNZ/Jin and Sendhoff - 2003 - Extracting Interpretable Fuzzy Rules from RBF Netw.pdf}
}

@incollection{dengEpilogueFrontiersNLP2018,
  address = {Singapore},
  title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
  isbn = {978-981-10-5209-5},
  shorttitle = {Epilogue},
  abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
  language = {en},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {309-326},
  doi = {10.1007/978-981-10-5209-5_11}
}

@article{waltlIncreasingTransparencyAlgorithmic2018,
  title = {Increasing {{Transparency}} in {{Algorithmic}}- {{Decision}}-{{Making}} with {{Explainable AI}}},
  volume = {42},
  issn = {1862-2607},
  abstract = {Systems that can autonomously make decisions based on artificial intelligence are becoming ubiquitous. These decisions can affect sensitive areas in our daily life. For example, the price of goods in e-commerce transactions (e.g., via dynamic pricing), or the credit rating of customers (e.g., via automatic credit scoring). The need for more transparency of algorithmic-decision-making (ADM) is not only moving into the focus of lawmakers, but it is also desirable from an engineering perspective. This brief paper investigates different levels of transparency in ADM, and discusses how and to which degree auditing and testing can increase transparency of ADM.},
  language = {en},
  number = {10},
  journal = {Datenschutz und Datensicherheit - DuD},
  doi = {10.1007/s11623-018-1011-4},
  author = {Waltl, Bernhard and Vogl, Roland},
  month = oct,
  year = {2018},
  pages = {613-617},
  file = {/home/tim/Zotero/storage/HF98RF7S/Waltl and Vogl - 2018 - Increasing Transparency in Algorithmic- Decision-M.pdf}
}

@incollection{bikdashInterpretabilityComplexityModular2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Interpretability, {{Complexity}}, and {{Modular Structure}} of {{Fuzzy Systems}}},
  isbn = {978-3-540-37057-4},
  abstract = {Zadeh's original motivation for fuzzy logic and the Fuzzy Rule-Based System (FRBS) was linguistic and hence possessed highly interpretable components. But as the complexity of a typical FRBS increases, it often becomes more like an uninterpretable neural network, and the Principle of Incompatibility predicts a degradation in interpretability for the same accuracy. This is particularly true of the so-called Takagi-Sugeno-Kang (TSK), or simply the Sugeno, approximator. We argue that imposing additional structure on the TSK system can significantly improve the tradeoff inherent in the Principle of Incompatibility. A promising structure was proposed recently in which the membership functions are local and sufficiently differentiable, and the consequent polynomials are rule-centered. This structure leads to the general interpretation that the consequent polynomials are Taylor series expansions. On this interpretation, a foundation for an algebra and a calculus of FRBSs can be built. We will illustrate these aspects of the proposed structure and discuss issues of modularity, functionality, and scalability of FRBSs.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bikdash, Marwan},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Fuzzy Inference System,Fuzzy System,Local Learning,Membership Function,Taylor Series Expansion},
  pages = {355-378},
  doi = {10.1007/978-3-540-37057-4_15}
}

@incollection{zhouMultipleObjectiveLearning2006,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Multiple {{Objective Learning}} for {{Constructing Interpretable Takagi}}-{{Sugeno Fuzzy Model}}},
  isbn = {978-3-540-33019-6},
  abstract = {This chapter discusses the interpretability of Takagi-Sugeno (TS) fuzzy systems. A new TS fuzzy model, whose membership functions are characterized by linguistic modifiers, is presented. The tradeoff between global approximation and local model interpretation has been achieved by minimizing a multiple objective performance measure. In the proposed model, the local models match the global model well and the erratic behaviors of local models are remedied effectively. Furthermore, the transparency of partitioning of input space has been improved during parameter adaptation.},
  language = {en},
  booktitle = {Multi-{{Objective Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zhou, Shang-Ming and Gan, John Q.},
  editor = {Jin, Yaochu},
  year = {2006},
  keywords = {Consequent Parameter,Fuzziness Measure,Fuzzy Model,Fuzzy System,Local Model},
  pages = {385-403},
  file = {/home/tim/Zotero/storage/PV76WSLV/Zhou and Gan - 2006 - Multiple Objective Learning for Constructing Inter.pdf},
  doi = {10.1007/3-540-33019-4_17}
}

@article{ziemkeConstructionRealityRobot2001,
  title = {The {{Construction}} of `{{Reality}}' in the {{Robot}}: {{Constructivist Perspectives}} on {{Situated Artificial Intelligence}} and {{Adaptive Robotics}}},
  volume = {6},
  issn = {1572-8471},
  shorttitle = {The {{Construction}} of `{{Reality}}' in the {{Robot}}},
  abstract = {This paper discusses different approaches incognitive science and artificial intelligenceresearch from the perspective of radicalconstructivism, addressing especially theirrelation to the biologically based theories ofvon Uexk\"ull, Piaget as well as Maturana andVarela. In particular recent work in `New AI' and adaptive robotics on situated and embodiedintelligence is examined, and we discuss indetail the role of constructive processes asthe basis of situatedness in both robots andliving organisms.},
  language = {en},
  number = {1},
  journal = {Foundations of Science},
  doi = {10.1023/A:1011394317088},
  author = {Ziemke, Tom},
  month = mar,
  year = {2001},
  keywords = {adaptive robotics,artificial intelligence,embodied cognition,radical constructivism,situatedness},
  pages = {163-233},
  file = {/home/tim/Zotero/storage/5NCIIVIG/Ziemke - 2001 - The Construction of ‘Reality’ in the Robot Constr.pdf}
}

@article{dimidukPerspectivesImpactMachine2018,
  title = {Perspectives on the {{Impact}} of {{Machine Learning}}, {{Deep Learning}}, and {{Artificial Intelligence}} on {{Materials}}, {{Processes}}, and {{Structures Engineering}}},
  volume = {7},
  issn = {2193-9772},
  abstract = {The fields of machining learning and artificial intelligence are rapidly expanding, impacting nearly every technological aspect of society. Many thousands of published manuscripts report advances over the last 5 years or less. Yet materials and structures engineering practitioners are slow to engage with these advancements. Perhaps the recent advances that are driving other technical fields are not sufficiently distinguished from long-known informatics methods for materials, thereby masking their likely impact to the materials, processes, and structures engineering (MPSE). Alternatively, the diverse nature and limited availability of relevant materials data pose obstacles to machine-learning implementation. The glimpse captured in this overview is intended to draw focus to selected distinguishing advances, and to show that there are opportunities for these new technologies to have transformational impacts on MPSE. Further, there are opportunities for the MPSE fields to contribute understanding to the emerging machine-learning tools from a physics basis. We suggest that there is an immediate need to expand the use of these new tools throughout MPSE, and to begin the transformation of engineering education that is necessary for ongoing adoption of the methods.},
  language = {en},
  number = {3},
  journal = {Integrating Materials and Manufacturing Innovation},
  doi = {10.1007/s40192-018-0117-8},
  author = {Dimiduk, Dennis M. and Holm, Elizabeth A. and Niezgoda, Stephen R.},
  month = sep,
  year = {2018},
  keywords = {Artificial intelligence,Deep learning,Digital engineering,ICME,Machine learning,MGI,Multiscale modeling},
  pages = {157-172},
  file = {/home/tim/Zotero/storage/I992PVW9/Dimiduk et al. - 2018 - Perspectives on the Impact of Machine Learning, De.pdf}
}

@inproceedings{maesPolicySearchSpace2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Policy {{Search}} in a {{Space}} of {{Simple Closed}}-Form {{Formulas}}: {{Towards Interpretability}} of {{Reinforcement Learning}}},
  isbn = {978-3-642-33492-4},
  shorttitle = {Policy {{Search}} in a {{Space}} of {{Simple Closed}}-Form {{Formulas}}},
  abstract = {In this paper, we address the problem of computing interpretable solutions to reinforcement learning (RL) problems. To this end, we propose a search algorithm over a space of simple closed-form formulas that are used to rank actions. We formalize the search for a high-performance policy as a multi-armed bandit problem where each arm corresponds to a candidate policy canonically represented by its shortest formula-based representation. Experiments, conducted on standard benchmarks, show that this approach manages to determine both efficient and interpretable solutions.},
  language = {en},
  booktitle = {Discovery {{Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Maes, Francis and Fonteneau, Raphael and Wehenkel, Louis and Ernst, Damien},
  editor = {Ganascia, Jean-Gabriel and Lenca, Philippe and Petit, Jean-Marc},
  year = {2012},
  keywords = {Formula Discovery,Interpretability,Reinforcement Learning},
  pages = {37-51},
  file = {/home/tim/Zotero/storage/3WX72E8P/Maes et al. - 2012 - Policy Search in a Space of Simple Closed-form For.pdf}
}

@article{kuwajimaImprovingTransparencyDeep2019,
  title = {Improving Transparency of Deep Neural Inference Process},
  issn = {2192-6360},
  abstract = {Deep learning techniques are rapidly advanced recently and becoming a necessity component for widespread systems. However, the inference process of deep learning is black box and is not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of (1) structural feature analysis: lists of the features contributing to inference process, (2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and (3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75\% workers answer that input data and result of our feature analysis are consistent, and 70\% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provides suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.},
  language = {en},
  journal = {Progress in Artificial Intelligence},
  doi = {10.1007/s13748-019-00179-x},
  author = {Kuwajima, Hiroshi and Tanaka, Masayuki and Okutomi, Masatoshi},
  month = apr,
  year = {2019},
  keywords = {Black box,Deep neural network,Explainable AI,Transparency,Visual attribute,Visualization},
  file = {/home/tim/Zotero/storage/PUQL2N9V/Kuwajima et al. - 2019 - Improving transparency of deep neural inference pr.pdf}
}

@incollection{lughoferModelExplanationInterpretation2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
  isbn = {978-3-319-90403-0},
  abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Lughofer, Edwin},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {177-221},
  doi = {10.1007/978-3-319-90403-0_10}
}

@inproceedings{mayrRegularInferenceArtificial2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Regular {{Inference}} on {{Artificial Neural~Networks}}},
  isbn = {978-3-319-99740-7},
  abstract = {This paper explores the general problem of explaining the behavior of artificial neural networks (ANN). The goal is to construct a representation which enhances human understanding of an ANN as a sequence classifier, with the purpose of providing insight on the rationale behind the classification of a sequence as positive or negative, but also to enable performing further analyses, such as automata-theoretic formal verification. In particular, a probabilistic algorithm for constructing a deterministic finite automaton which is approximately correct with respect to an artificial neural network is proposed.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Mayr, Franz and Yovine, Sergio},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial neural networks,Deterministic finite automata,Probably Approximately Correct learning,Sequence classification},
  pages = {350-369},
  file = {/home/tim/Zotero/storage/X2Z44SXK/Mayr and Yovine - 2018 - Regular Inference on Artificial Neural Networks.pdf}
}

@incollection{vandegevelNexusArtificialIntelligence2013,
  address = {Berlin, Heidelberg},
  series = {{{SpringerBriefs}} in {{Economics}}},
  title = {The {{Nexus Between Artificial Intelligence}} and {{Economics}}},
  isbn = {978-3-642-33648-5},
  abstract = {We review recent developments in artificial intelligence and relate them to economics. Artificial intelligence represents the technology most likely to lead to a singularity, an infinite rate of innovation and productivity growth. This could occur via dramatic increases in life expectancy, the development of whole brain emulation, and innovations in robotics. We argue that there is no reason to believe that artificial intelligence would increase human happiness. We describe some recent development in agent-based modeling in economics, which can be interpreted as the introduction of artificially intelligent agents into economics. We argue that classical economic theory, which assumes that all agents are rational and have infinite computational ability, is very relevant in describing the behavior of future artificially intelligent entities. Economic implications of accelerating innovation, greater longevity, and the introduction of robot labor are considered.},
  language = {en},
  booktitle = {The {{Nexus}} between {{Artificial Intelligence}} and {{Economics}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{van de Gevel}, Ad J. W. and Noussair, Charles N.},
  editor = {{van de Gevel}, Ad. J. W. and Noussair, Charles N.},
  year = {2013},
  keywords = {Artificial consciousness,Artificial economics,Artificial happiness,Artificial intelligence,Artificial life,Logistic growth,Methuselarity,Robotics,Singularity,Whole brain emulation},
  pages = {1-110},
  file = {/home/tim/Zotero/storage/XKKUQ4TZ/van de Gevel and Noussair - 2013 - The Nexus Between Artificial Intelligence and Econ.pdf},
  doi = {10.1007/978-3-642-33648-5_1}
}

@inproceedings{atzmuellerDeclarativeAspectsExplicative2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Declarative {{Aspects}} in {{Explicative Data Mining}} for {{Computational Sensemaking}}},
  isbn = {978-3-030-00801-7},
  abstract = {Computational sensemaking aims to develop methods and systems to ``make sense'' of complex data and information. The ultimate goal is then to provide insights and enhance understanding for supporting subsequent intelligent actions. Understandability and interpretability are key elements of that process as well as models and patterns captured therein. Here, declarativity helps to include guiding knowledge structures into the process, while explication provides interpretability, transparency, and explainability. This paper provides an overview of the key points and important developments in these areas, and outlines future potential and challenges.},
  language = {en},
  booktitle = {Declarative {{Programming}} and {{Knowledge Management}}},
  publisher = {{Springer International Publishing}},
  author = {Atzmueller, Martin},
  editor = {Seipel, Dietmar and Hanus, Michael and Abreu, Salvador},
  year = {2018},
  keywords = {Computational sensemaking,Data mining,Declarative modeling,Domain knowledge,Explicative data analysis,Knowledge graph,Statistical relational learning},
  pages = {97-114},
  file = {/home/tim/Zotero/storage/744VFK8M/Atzmueller - 2018 - Declarative Aspects in Explicative Data Mining for.pdf}
}

@incollection{menciaLearningInterpretableRules2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Learning {{Interpretable Rules}} for {{Multi}}-{{Label Classification}}},
  isbn = {978-3-319-98131-4},
  abstract = {Multi-label classification (MLC) is a supervised learning problem in which, contrary to standard multiclass classification, an instance can be associated with several class labels simultaneously. In this chapter, we advocate a rule-based approach to multi-label classification. Rule learning algorithms are often employed when one is not only interested in accurate predictions, but also requires an interpretable theory that can be understood, analyzed, and qualitatively evaluated by domain experts. Ideally, by revealing patterns and regularities contained in the data, a rule-based theory yields new insights in the application domain. Recently, several authors have started to investigate how rule-based models can be used for modeling multi-label data. Discussing this task in detail, we highlight some of the problems that make rule learning considerably more challenging for MLC than for conventional classification. While mainly focusing on our own previous work, we also provide a short overview of related work in this area.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Menc\'ia, Eneldo Loza and F\"urnkranz, Johannes and H\"ullermeier, Eyke and Rapp, Michael},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Label-dependencies,Multi-label classification,Rule learning,Separate-and-conquer},
  pages = {81-113},
  doi = {10.1007/978-3-319-98131-4_4}
}

@incollection{bundyArtificialIntelligenceTechniques1997,
  address = {Berlin, Heidelberg},
  title = {Artificial {{Intelligence Techniques}}},
  isbn = {978-3-642-60359-4},
  abstract = {A viewer-centred representation making explicit the depths, local orientations and discontinuities of visible surfaces, created and maintained from a number of cues, e.g., Stereopsis261 and Optical Flow190. It was thought by Marr to be at the limit of pure perception, i.e., subsequent processes are no longer completely data-driven, and for him it provides a representation of objective physical reality that precedes the decomposition of the scene into objects.},
  language = {en},
  booktitle = {Artificial {{Intelligence Techniques}}: {{A Comprehensive Catalogue}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bundy, Alan},
  editor = {Bundy, Alan},
  year = {1997},
  keywords = {Certainty Factor,Horn Clause,Inductive Logic Program,Natural Deduction,Phrase Structure Grammar},
  pages = {1-129},
  doi = {10.1007/978-3-642-60359-4_1}
}

@inproceedings{ponceVersatilityArtificialHydrocarbon2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Versatility of {{Artificial Hydrocarbon Networks}} for {{Supervised Learning}}},
  isbn = {978-3-030-02837-4},
  abstract = {Surveys on supervised machine show that each technique has strengths and weaknesses that make each of them more suitable for a particular domain or learning task. No technique is capable to tackle every supervised learning task, and it is difficult to comply with all possible desirable features of each particular domain. However, it is important that a new technique comply with the most requirements and desirable features of as many domains and learning tasks as possible. In this paper, we presented artificial hydrocarbon networks (AHN) as versatile and efficient supervised learning method. We determined the ability of AHN to solve different problem domains, with different data-sources and to learn different tasks. The analysis considered six applications in which AHN was successfully applied.},
  language = {en},
  booktitle = {Advances in {{Soft Computing}}},
  publisher = {{Springer International Publishing}},
  author = {Ponce, Hiram and {Mart\'inez-Villase\~nor}, Ma Lourdes},
  editor = {Castro, F\'elix and {Miranda-Jim\'enez}, Sabino and {Gonz\'alez-Mendoza}, Miguel},
  year = {2018},
  keywords = {Artificial organic networks,Interpretability,Machine learning,Versatility},
  pages = {3-16},
  file = {/home/tim/Zotero/storage/Z6RFFT9J/Ponce and Martínez-Villaseñor - 2018 - Versatility of Artificial Hydrocarbon Networks for.pdf}
}

@article{27thAnnualComputational2018,
  title = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018): {{Part One}}},
  volume = {19},
  issn = {1471-2202},
  shorttitle = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018)},
  language = {en},
  number = {2},
  journal = {BMC Neuroscience},
  doi = {10.1186/s12868-018-0452-x},
  month = oct,
  year = {2018},
  pages = {64},
  file = {/home/tim/Zotero/storage/W6XXQSHX/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf}
}

@incollection{sutherlandDictionary1991,
  address = {London},
  series = {Dictionary {{Series}}},
  title = {The {{Dictionary}}},
  isbn = {978-1-349-12428-2},
  abstract = {A. An abbreviation for 1. AMPLITUDE; 2. AMPERE; 3. RESPONSE AMPLITUDE.},
  language = {en},
  booktitle = {Macmillan {{Dictionary}} of {{Psychology}}},
  publisher = {{Palgrave Macmillan UK}},
  author = {Sutherland, Stuart},
  editor = {Sutherland, Stuart},
  year = {1991},
  keywords = {Conditioned Stimulus,Hair Cell,Intelligence Quotient,Noun Phrase,Vocal Tract},
  pages = {1-486},
  doi = {10.1007/978-1-349-12428-2_1}
}

@incollection{harveyDeepLearningBreast2019,
  address = {Cham},
  title = {Deep {{Learning}} in {{Breast Cancer Screening}}},
  isbn = {978-3-319-94878-2},
  abstract = {Traditional computer aided detection (CAD) systems for breast cancer screening relied on machine learning with human-coded feature-engineering. They have largely failed to fulfill the promise of improving screening accuracy and workflow efficiency, and are often associated with increased recall rates and avoidable screening costs due to high instances of false positive markings. Advances in machine learning (such as deep learning) are on the cusp of providing more effective, more efficient, and even more patient-centric breast cancer screening support than ever before. By leveraging the consistent high sensitivity and specificity performance of autonomous systems, in combination with expert human oversight, the potential for efficient single-reader software-supported screening programs with low recall rates is on the horizon.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Medical Imaging}}: {{Opportunities}}, {{Applications}} and {{Risks}}},
  publisher = {{Springer International Publishing}},
  author = {Harvey, Hugh and Heindl, Andreas and Khara, Galvin and Korkinof, Dimitrios and O'Neill, Michael and Yearsley, Joseph and Karpati, Edith and Rijken, Tobias and Kecskemethy, Peter and Forrai, Gabor},
  editor = {Ranschaert, Erik R. and Morozov, Sergey and Algra, Paul R.},
  year = {2019},
  keywords = {Breast cancer,CAD,Deep learning,Mammography,Screening},
  pages = {187-215},
  doi = {10.1007/978-3-319-94878-2_14}
}

@article{harnadComputationJustInterpretable1994,
  title = {Computation Is Just Interpretable Symbol Manipulation; Cognition Isn't},
  volume = {4},
  issn = {1572-8641},
  abstract = {Computation is interpretable symbol manipulation. Symbols are objects that are manipulated on the basis of rules operating only on theirshapes, which are arbitrary in relation to what they can be interpreted as meaning. Even if one accepts the Church/Turing Thesis that computation is unique, universal and very near omnipotent, not everything is a computer, because not everything can be given a systematic interpretation; and certainly everything can't be givenevery systematic interpretation. But even after computers and computation have been successfully distinguished from other kinds of things, mental states will not just be the implementations of the right symbol systems, because of the symbol grounding problem: The interpretation of a symbol system is not intrinsic to the system; it is projected onto it by the interpreter. This is not true of our thoughts. We must accordingly be more than just computers. My guess is that the meanings of our symbols are grounded in the substrate of our robotic capacity to interact with that real world of objects, events and states of affairs that our symbols are systematically interpretable as being about.},
  language = {en},
  number = {4},
  journal = {Minds and Machines},
  doi = {10.1007/BF00974165},
  author = {Harnad, Stevan},
  month = nov,
  year = {1994},
  keywords = {Causality,cognition,computation,consciousness,continuity,implementation,robotics,semantics,sensorimotor transduction,symbol systems,syntax,Turing Machine,Turing Test},
  pages = {379-390},
  file = {/home/tim/Zotero/storage/KW5ZV7MH/Harnad - 1994 - Computation is just interpretable symbol manipulat.pdf}
}

@article{memmiConnectionismArtificialIntelligence1990,
  title = {Connectionism and Artificial Intelligence as Cognitive Models},
  volume = {4},
  issn = {1435-5655},
  abstract = {The current renewal of connectionist techniques using networks of neuron-like units has started to have an influence on cognitive modelling. However, compared with classical artificial intelligence methods, the position of connectionism is still not clear. In this article artificial intelligence and connectionism are systematically compared as cognitive models so as to bring out the advantages and shortcomings of each. The problem of structured representations appears to be particularly important, suggesting likely research directions.},
  language = {en},
  number = {2},
  journal = {AI \& SOCIETY},
  doi = {10.1007/BF01889639},
  author = {Memmi, Daniel},
  month = apr,
  year = {1990},
  keywords = {Artificial intelligence,Cognitive modelling,Epistemology,Neural networks,Representations},
  pages = {115-136},
  file = {/home/tim/Zotero/storage/43PQQWMN/Memmi - 1990 - Connectionism and artificial intelligence as cogni.pdf}
}

@inproceedings{laugelComparisonBasedInverseClassification2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}}},
  isbn = {978-3-319-91473-2},
  abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
  language = {en},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
  publisher = {{Springer International Publishing}},
  author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2018},
  keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
  pages = {100-111}
}

@article{kunapuliDecisionSupportToolRenal2018,
  title = {A {{Decision}}-{{Support Tool}} for {{Renal Mass Classification}}},
  volume = {31},
  issn = {1618-727X},
  abstract = {We investigate the viability of statistical relational machine learning algorithms for the task of identifying malignancy of renal masses using radiomics-based imaging features. Features characterizing the texture, signal intensity, and other relevant metrics of the renal mass were extracted from multiphase contrast-enhanced computed tomography images. The recently developed formalism of relational functional gradient boosting (RFGB) was used to learn human-interpretable models for classification. Experimental results demonstrate that RFGB outperforms many standard machine learning approaches as well as the current diagnostic gold standard of visual qualification by radiologists.},
  language = {en},
  number = {6},
  journal = {Journal of Digital Imaging},
  doi = {10.1007/s10278-018-0100-0},
  author = {Kunapuli, Gautam and Varghese, Bino A. and Ganapathy, Priya and Desai, Bhushan and Cen, Steven and Aron, Manju and Gill, Inderbir and Duddalwar, Vinay},
  month = dec,
  year = {2018},
  keywords = {Clinical decision support,Multiphase CT,Radiomics,Renal mass,Statistical relational learning},
  pages = {929-939},
  file = {/home/tim/Zotero/storage/Z6CV6ADF/Kunapuli et al. - 2018 - A Decision-Support Tool for Renal Mass Classificat.pdf}
}

@incollection{fiordalisoTradeoffAccuracyInterpretability2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {About the Trade-off between Accuracy and Interpretability of {{Takagi}}-{{Sugeno}} Models in the Context of Nonlinear Time Series Forecasting},
  isbn = {978-3-540-37057-4},
  abstract = {The focus of this chapter is related to the question of how to find appropriate Takagi-Sugeno (TS) rules in the framework of (chaotic) time series forecasting. We propose a generalization of the conventional TS system (GTS) allowing to evaluate the importance of any rule in the inference process. The added value of this feature has been put in light on two well-known chaotic time series.Local (clustering-based) and global (gradient-based) learning strategies for GTS systems are compared in terms of interpretability and accuracy. It appears that there is an unavoidable compromise between these two objectives. Global learning seems to be superior in term of accuracy but cannot achieve the same level of interpretability as the local approach.We also present an application of the GTS model in the field of forecasts combination with the target of generating interpretable final rules. This has led us to put additional constraints in the model. More precisely, we force the local linear models (rules conclusions) to achieve convex combination of the forecasts (input patterns). The proposed system may be useful for a wide range of applications where a consensus is required, including forecasts synthesis, controllers and patterns classifiers aggregation.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Fiordaliso, Antonio},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Chaotic Time Series,Fuzzy Model,Fuzzy System,Radial Basis Function,Time Series Forecast},
  pages = {406-430},
  doi = {10.1007/978-3-540-37057-4_17}
}

@incollection{ishibuchiInterpretabilityIssuesFuzzy2003,
  address = {Berlin, Heidelberg},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretability {{Issues}} in {{Fuzzy Genetics}}-{{Based Machine Learning}} for {{Linguistic Modelling}}},
  isbn = {978-3-540-39906-3},
  abstract = {This chapter discusses several issues related to the design of linguistic models with high interpretability using fuzzy genetics-based machine learning (GBML) algorithms. We assume that a set of linguistic terms has been given for each variable. Thus our modelling task is to find a small number of fuzzy rules from possible combinations of the given linguistic terms. First we formulate a three-objective optimization problem, which simultaneously minimizes the total squared error, the number of fuzzy rules, and the total rule length. Next we show how fuzzy GBML algorithms can be applied to our problem in the framework of multi-objective optimization as well as single-objective optimization. Then we point out a possibility that misleading fuzzy rules can be generated when general and specific fuzzy rules are simultaneously used in a single linguistic model. Finally we show that non-standard inclusion-based fuzzy reasoning removes such an undesirable possibility.},
  language = {en},
  booktitle = {Modelling with {{Words}}: {{Learning}}, {{Fusion}}, and {{Reasoning}} within a {{Formal Linguistic Represntation Framework}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ishibuchi, Hisao and Yamamoto, Takashi},
  editor = {Lawry, Jonathan and Shanahan, Jimi and L. Ralescu, Anca},
  year = {2003},
  pages = {209-228},
  file = {/home/tim/Zotero/storage/TKJTL9YH/Ishibuchi and Yamamoto - 2003 - Interpretability Issues in Fuzzy Genetics-Based Ma.pdf},
  doi = {10.1007/978-3-540-39906-3_11}
}

@inproceedings{keneseiCombinationofToolsMethodLearning2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Combination}}-of-{{Tools Method}} for {{Learning Interpretable Fuzzy Rule}}-{{Based Classifiers}} from {{Support Vector Machines}}},
  isbn = {978-3-540-77226-2},
  abstract = {A new approach is proposed for the data-based identification of transparent fuzzy rule-based classifiers. It is observed that fuzzy rule-based classifiers work in a similar manner as kernel function-based support vector machines (SVMs) since both model the input space by nonlinearly maps into a feature space where the decision can be easily made. Accordingly, trained SVM can be used for the construction of fuzzy rule-based classifiers. However, the transformed SVM does not automatically result in an interpretable fuzzy model because the SVM results in a complex rule-base, where the number of rules is approximately 40-60\% of the number of the training data. Hence, reduction of the SVM-initialized classifier is an essential task. For this purpose, a three-step reduction algorithm is developed based on the combination of previously published model reduction techniques. In the first step, the identification of the SVM is followed by the application of the Reduced Set method to decrease the number of kernel functions. The reduced SVM is then transformed into a fuzzy rule-based classifier. The interpretability of a fuzzy model highly depends on the distribution of the membership functions. Hence, the second reduction step is achieved by merging similar fuzzy sets based on a similarity measure. Finally, in the third step, an orthogonal least-squares method is used to reduce the number of rules and re-estimate the consequent parameters of the fuzzy rule-based classifier. The proposed approach is applied for the Wisconsin Breast Cancer, Iris and Wine classification problems to compare its performance to other methods.},
  language = {en},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} - {{IDEAL}} 2007},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kenesei, Tamas and Roubos, Johannes A. and Abonyi, Janos},
  editor = {Yin, Hujun and Tino, Peter and Corchado, Emilio and Byrne, Will and Yao, Xin},
  year = {2007},
  keywords = {Classification,Fuzzy classifier,Model Reduction,Support Vector Machine},
  pages = {477-486},
  file = {/home/tim/Zotero/storage/TITG8KZZ/Kenesei et al. - 2007 - A Combination-of-Tools Method for Learning Interpr.pdf}
}

@incollection{mencarInterpretabilityFuzzyInformation2009,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Interpretability of {{Fuzzy Information Granules}}},
  isbn = {978-3-540-92916-1},
  abstract = {Human-Centric Information Processing requires tight communication processes between users and computers. These two actors, however, traditionally use different paradigms for representing and manipulating information. Users are more inclined in managing perceptual information, usually expressed in natural language, whilst computers are formidable number-crunching systems, capable of manipulating information expressed in precise form. Fuzzy information granules could be used as a common interface for communicating information and knowledge, because of their ability of representing perceptual information in a computer manageable form. Nonetheless, this connection could be established only if information granules are interpretable, i.e. they are semantically co-intensive with human knowledge. Interpretable information granulation opens several methodological issues, regarding the representation and manipulation of information granules, the interpretability constraints and the granulation processes. By taking into account all such issues, effective Information Processing systems could be designed with a strong Human-Centric imprint.},
  language = {en},
  booktitle = {Human-{{Centric Information Processing Through Granular Modelling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mencar, Corrado},
  editor = {Bargiela, Andrzej and Pedrycz, Witold},
  year = {2009},
  keywords = {Fuzzy Information,Linguistic Term,Membership Function,Natural Language,Perceptual Information},
  pages = {95-118},
  doi = {10.1007/978-3-540-92916-1_5}
}

@inproceedings{benrimohAifredHealthDeep2018,
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Aifred {{Health}}, a {{Deep Learning Powered Clinical Decision Support System}} for {{Mental Health}}},
  isbn = {978-3-319-94042-7},
  abstract = {Aifred Health, one of the top two teams in the first round of the IBM Watson AI XPRIZE competition, is using deep learning to solve the problem of treatment selection and prognosis prediction in mental health, starting with depression. Globally, depression affects over 300 million people and is the leading cause of disability. While a range of effective treatments do exist, patients' responses to treatments vary to a large degree. Some patients spend years going through a frustrating `trial-and-error' process in order to find an effective treatment. The Aifred Health solution is a deep learning-powered Clinical Decision Support System (CDSS) aimed at helping clinicians select the most effective treatment plans for depression in collaboration with their patients. In this chapter, we discuss problem of treatment selection in depression and explore the technical, clinical, and ethical dimensions of building a CDSS for mental health based on deep learning technology.},
  language = {en},
  booktitle = {The {{NIPS}} '17 {{Competition}}: {{Building Intelligent Systems}}},
  publisher = {{Springer International Publishing}},
  author = {Benrimoh, David and Fratila, Robert and Israel, Sonia and Perlman, Kelly and Mirchi, Nykan and Desai, Sneha and Rosenfeld, Ariel and Knappe, Sabrina and Behrmann, Jason and Rollins, Colleen and You, Raymond Penh and Aifred Health Team, The},
  editor = {Escalera, Sergio and Weimer, Markus},
  year = {2018},
  pages = {251-287}
}

@article{wangLearningPerformancePrediction2019,
  title = {Learning Performance Prediction via Convolutional {{GRU}} and Explainable Neural Networks in E-Learning Environments},
  volume = {101},
  issn = {1436-5057},
  abstract = {Students learning performance prediction is a challenging task due to the dynamic, virtual environments and the personalized needs for different individuals. To ensure that learners' potential problems can be identified as early as possible, this paper aim to develop a predictive model for effective learning feature extracting, learning performance predicting and result reasoning. We first proposed a general learning feature quantification method to convert the raw data from e-learning systems into sets of independent learning features. Then, a weighted avg-pooling is chosen instead of typical max-pooling in a novel convolutional GRU network for learning performance prediction. Finally, an improved parallel xNN is provided to explain the prediction results. The relevance of positive/negative between features and result could help students find out which part should be improved. Experiments have been carried out over two real online courses data. Results show that our proposed approach performs favorably compared with several other state-of-the-art methods.},
  language = {en},
  number = {6},
  journal = {Computing},
  doi = {10.1007/s00607-018-00699-9},
  author = {Wang, Xizhe and Wu, Pengze and Liu, Guang and Huang, Qionghao and Hu, Xiaoling and Xu, Haijiao},
  month = jun,
  year = {2019},
  keywords = {68T01,68U35,Deep neural network,E-learning environments,Learning feature quantification,Learning performance prediction},
  pages = {587-604},
  file = {/home/tim/Zotero/storage/7IXL7NYP/Wang et al. - 2019 - Learning performance prediction via convolutional .pdf}
}

@inproceedings{weijtersInterpretableNeuralNetworks1998,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretable Neural Networks with {{BP}}-{{SOM}}},
  isbn = {978-3-540-69781-7},
  abstract = {Interpretation of models induced by artificial neural networks is often a difficult task. In this paper we focus on a relatively novel neural network architecture and learning algorithm, bp-som that offers possibilities to overcome this difficulty. It is shown that networks trained with BP-SOM show interesting regularities, in that hidden-unit activations become restricted to discrete values, and that the som part can be exploited for automatic rule extraction.},
  language = {en},
  booktitle = {Machine {{Learning}}: {{ECML}}-98},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Weijters, Ton and {van den Bosch}, Antal and {van den Herik}, Jaap},
  editor = {N\'edellec, Claire and Rouveirol, C\'eline},
  year = {1998},
  keywords = {Generalization Performance,Hide Layer,Hide Unit,Learning Algorithm,Neural Network Architecture,Rule Extraction,Test Instance,Training Instance},
  pages = {406-411},
  file = {/home/tim/Zotero/storage/2LXNZQEA/Weijters and van den Bosch - 1998 - Interpretable neural networks with BP-SOM.pdf;/home/tim/Zotero/storage/LPIVRYRQ/Weijters et al. - 1998 - Interpretable neural networks with BP-SOM.pdf}
}

@inproceedings{maymiHumanMachineTeamingCyberspace2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Human-{{Machine Teaming}} and {{Cyberspace}}},
  isbn = {978-3-319-91470-1},
  abstract = {Artificial Intelligence is becoming the key enabler of solutions to a variety of problems including those associated with cyberspace operations. Based on our analysis of cyber threats and opportunities in the coming years, we assess it as very likely that teams consisting of humans and synthetic agents will routinely work together in many if not most organizations. To fully leverage the potential of these teams, we must continue to develop new paradigms in human-machine teaming. Specifically, we must address three areas that are currently in their infancy. Firstly, we need interfaces that allow all teammates to communicate effectively with each other and seamlessly transfer tasks among them. This must be true regardless of whether the endpoints are human or not. Secondly, we will need cybersecurity operators with broad knowledge and skills. They must know how their synthetic teammates ``think,'' when to task them and when to question their reports. Thirdly, our AI systems must be able to explain their decision-making processes to their human teammates. This paper provides an overview of cyberspace threats and opportunities in the next ten years and how these will impact human-machine teaming. We then apply the key lessons we have learned while working a multitude of advanced research projects at the intersection of human and AI agents to cyberspace operations. Finally, we propose areas of research that will allow humans and machines to better collaborate in the future.},
  language = {en},
  booktitle = {Augmented {{Cognition}}: {{Intelligent Technologies}}},
  publisher = {{Springer International Publishing}},
  author = {Maym\'i, Fernando J. and Thomson, Robert},
  editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
  year = {2018},
  keywords = {Artificial intelligence,Cyberspace,Human-machine teaming},
  pages = {299-315},
  file = {/home/tim/Zotero/storage/4T95BJQU/Maymí and Thomson - 2018 - Human-Machine Teaming and Cyberspace.pdf;/home/tim/Zotero/storage/UWNLW9WQ/Maymí and Thomson - 2018 - Human-Machine Teaming and Cyberspace.pdf}
}

@article{yankovskayaTradeoffSearchMethods2017,
  title = {Tradeoff Search Methods between Interpretability and Accuracy of the Identification Fuzzy Systems Based on Rules},
  volume = {27},
  issn = {1555-6212},
  abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
  language = {en},
  number = {2},
  journal = {Pattern Recognition and Image Analysis},
  doi = {10.1134/S1054661817020134},
  author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
  month = apr,
  year = {2017},
  keywords = {accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,machine learning,metaheuristic,pattern recognition,synergy},
  pages = {243-265},
  file = {/home/tim/Zotero/storage/X2YDZAVK/Yankovskaya et al. - 2017 - Tradeoff search methods between interpretability a.pdf}
}

@inproceedings{munkhdalaiAdvancedNeuralNetwork2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Advanced {{Neural Network Approach}}, {{Its Explanation}} with {{LIME}} for {{Credit Scoring Application}}},
  isbn = {978-3-030-14802-7},
  abstract = {Neural network models have achieved a human-level performance in many application domains, including image classification, speech recognition and machine translation. However, in credit scoring application, neural network approach has been useless because of its black box nature that the relationship between contextual input and output cannot be completely understood. In this study, we investigate the advanced neural network approach and its' explanation for credit scoring. We use the LIME technique to interpret the black box of such neural network and verify its' trustworthiness by comparing a high interpretable logistic model. The results show that neural network models give higher accuracy and equivalent explanation with the logistic model.},
  language = {en},
  booktitle = {Intelligent {{Information}} and {{Database Systems}}},
  publisher = {{Springer International Publishing}},
  author = {Munkhdalai, Lkhagvadorj and Wang, Ling and Park, Hyun Woo and Ryu, Keun Ho},
  editor = {Nguyen, Ngoc Thanh and Gaol, Ford Lumban and Hong, Tzung-Pei and Trawi\'nski, Bogdan},
  year = {2019},
  keywords = {Credit scoring,LIME,Neural network},
  pages = {407-419},
  file = {/home/tim/Zotero/storage/7UDYI2UH/Munkhdalai et al. - 2019 - Advanced Neural Network Approach, Its Explanation .pdf}
}

@incollection{schetininAdvancedFeatureRecognition2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Advanced {{Feature Recognition}} and {{Classification Using Artificial Intelligence Paradigms}}},
  isbn = {978-3-540-47518-7},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Recognition}} and {{Classification}} of {{Astrophysical}} and {{Medical Images}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Schetinin, V. and Zharkova, Valentina and Brazhnikov, A. and Zharkov, S. I. and Salerno, Emanuele and Bedini, Luigi and Kuruoglu, Ercan E. and Tonazzini, Anna and Zazula, Damjan and Cigale, Boris and Yoshida, Hiroyuki},
  editor = {Zharkova, Valentina and Jain, Lakhmi C.},
  year = {2007},
  keywords = {Cellular Neural Network,Compute Tomography Colonography,Independent Component Analysis,Source Separation},
  pages = {151-338},
  file = {/home/tim/Zotero/storage/32C6JQ4M/Schetinin et al. - 2007 - Advanced Feature Recognition and Classification Us.pdf},
  doi = {10.1007/978-3-540-47518-7_4}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
  address = {Cham},
  title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
  isbn = {978-3-319-91596-8},
  abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
  publisher = {{Springer International Publishing}},
  author = {Wodecki, Andrzej},
  editor = {Wodecki, Andrzej},
  year = {2019},
  pages = {133-246},
  file = {/home/tim/Zotero/storage/59B7SQNM/Wodecki - 2019 - Influence of Artificial Intelligence on Activities.pdf},
  doi = {10.1007/978-3-319-91596-8_3}
}

@incollection{aguirreDescriptionSeveralCharacteristics2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {A Description of Several Characteristics for Improving the Accuracy and Interpretability of Inductive Linguistic Rule Learning Algorithms},
  isbn = {978-3-540-37058-1},
  abstract = {The learning algorithms can be an useful tool for helping to the humans to understand the behavior of phenomena from a set of samples. In particular, those algorithms that represent the knowledge obtained by linguistic fuzzy rules are appropriate for this task. However, it is not sufficient that the knowledge representation is close to the humans comprehension. Furthermore, it is necessary that the knowledge is expressed as simple as possible.In this chapter, we classify some techniques, models and tools for improving the knowledge obtained by inductive linguistic rule learning algorithms from three different points of view: those that increase the knowledge interpretability, those that increase the knowledge accuracy keeping its interpretability and those that simultaneously increase the accuracy and interpretability of the knowledge.In this study, we have considered fuzzy rules expressed by the Disjunctive Normal Form (DNF).},
  language = {en},
  booktitle = {Accuracy {{Improvements}} in {{Linguistic Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Aguirre, Eugenio and Gonzalez, Antonio and P\'erez, Ra\'ul},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Feature Selection,Feature Subset,Fuzzy Logic Controller,Fuzzy Rule,Learning Algorithm},
  pages = {249-276},
  doi = {10.1007/978-3-540-37058-1_11}
}

@incollection{aggarwalModelBasedCollaborativeFiltering2016,
  address = {Cham},
  title = {Model-{{Based Collaborative Filtering}}},
  isbn = {978-3-319-29659-3},
  abstract = {The neighborhood-based methods of the previous chapter can be viewed as generalizations of k-nearest neighbor classifiers, which are commonly used in machine learning.},
  language = {en},
  booktitle = {Recommender {{Systems}}: {{The Textbook}}},
  publisher = {{Springer International Publishing}},
  author = {Aggarwal, Charu C.},
  editor = {Aggarwal, Charu C.},
  year = {2016},
  pages = {71-138},
  doi = {10.1007/978-3-319-29659-3_3}
}

@incollection{rovatsosAnticipatoryArtificialIntelligence2017a,
  address = {Cham},
  title = {Anticipatory {{Artificial Intelligence}}},
  isbn = {978-3-319-31737-3},
  abstract = {Anticipation occupies a special role in Artificial Intelligence (AI), not only because replicating human anticipatory processes is naturally a part of the aim to replicate human intelligence, but also because the design, implementation, and evaluation of AI systems involves a mix of several, interdependent anticipatory processes, some of which are carried out by human designers and users, and others by the AI system. This chapter provides an introduction to key AI technologies, investigates to what extent they involve anticipatory processes, and explores the consequences of such anticipation on the predictability of AI systems for humans. Our analysis suggests that the relationship between human and AI anticipation is complementary - the more pronounced the anticipatory capabilities of the AI system, the harder it may be for humans to anticipate their behaviour. If this turns out to be correct, building safe, responsible, and ethically sound AI will require developing more understandable and explainable AI methods in the future.},
  language = {en},
  booktitle = {Handbook of {{Anticipation}}: {{Theoretical}} and {{Applied Aspects}} of the {{Use}} of {{Future}} in {{Decision Making}}},
  publisher = {{Springer International Publishing}},
  author = {Rovatsos, Michael},
  editor = {Poli, Roberto},
  year = {2017},
  keywords = {Anticipation,Anticipatory process,Artificial Intelligence,Computational modelling},
  pages = {1-33},
  file = {/home/tim/Zotero/storage/EWZIPQNT/Rovatsos - 2017 - Anticipatory Artificial Intelligence.pdf;/home/tim/Zotero/storage/JV3LRTJQ/Rovatsos - 2017 - Anticipatory Artificial Intelligence.pdf},
  doi = {10.1007/978-3-319-31737-3_45-1}
}

@incollection{quekCognitiveInterpretationThermographic2010,
  address = {Berlin, Heidelberg},
  series = {Intelligent {{Systems Reference Library}}},
  title = {A {{Cognitive Interpretation}} of {{Thermographic Images Using Novel Fuzzy Learning Semantic Memories}}},
  isbn = {978-3-642-13639-9},
  abstract = {Fuzzy neural network (FNN), a hybrid of fuzzy logic and neural network, is computationally powerful, robust and able to model a complex nonlinear problem domain via the extraction and self-tuning of fuzzy IF-THEN rules. This yields a powerful semantic learning memory system that is useful to build intelligent decision support tools. Thermal imaging has been effectively used in the detection of infrared spectrum for the screening of potential SARS patients. This chapter proposes a cognitive approach in the study of the correlation and semantic interpretation of superficial thermal images against the true internal body temperature. Comparison between global and local semantic memories as well as Mamdani and TSK model of FNNs are presented. Existing infrared systems are commonly used at various boarder checkpoints and these have high false-negative rate. The use of FNN as a back-end of the system can significantly improves and hence serves the role of an intelligent medical decision support tool with high degree of accuracy. Extensive experimentations are conducted on real-life data taken from the Emergency Department (A\&E), Tan Tock Seng Hospital (the designated SARS center in Singapore). The performance of FNN as the thermal analysis decision support system, providing plausible semantic interpretation and understanding, is highly encouraging.},
  language = {en},
  booktitle = {Handbook on {{Decision Making}}: {{Vol}} 1: {{Techniques}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Quek, C. and Irawan, W. and Ng, E.},
  editor = {Jain, Lakhmi C. and Lim, Chee Peng},
  year = {2010},
  keywords = {brain-inspired,cognitive approach,fuzzy rules,intelligent decision support system,learning memories,local and global semantic networks,Mamdani and TSK models,neural fuzzy networks,Semantic interpretation,temperature correlation,Thermal imaging,thermographs},
  pages = {427-452},
  doi = {10.1007/978-3-642-13639-9_17}
}

@article{nadinMachineIntelligenceChimera2018,
  title = {Machine Intelligence: A Chimera},
  issn = {1435-5655},
  shorttitle = {Machine Intelligence},
  abstract = {The notion of computation has changed the world more than any previous expressions of knowledge. However, as know-how in its particular algorithmic embodiment, computation is closed to meaning. Therefore, computer-based data processing can only mimic life's creative aspects, without being creative itself. AI's current record of accomplishments shows that it automates tasks associated with intelligence, without being intelligent itself. Mistaking the abstract (computation) for the concrete (computer) has led to the religion of ``everything is an output of computation''\textemdash{}even the humankind that conceived the computer. The hypostatized role of computers explains the increased dependence on them. The convergence machine called deep learning is only the most recent form through which the deterministic theology of the machine claims more than what it actually is: extremely effective data processing. A proper understanding of complexity, as well as the need to distinguish between the reactive nature of the artificial and the anticipatory nature of the living are suggested as practical responses to the challenges posed by machine theology.},
  language = {en},
  journal = {AI \& SOCIETY},
  doi = {10.1007/s00146-018-0842-8},
  author = {Nadin, Mihai},
  month = apr,
  year = {2018},
  keywords = {Anticipatory,Convergence,G-complexity,Hypostatize,Meaning},
  file = {/home/tim/Zotero/storage/3D6LEH4E/Nadin - 2018 - Machine intelligence a chimera.pdf}
}

@article{muggletonUltraStrongMachineLearning2018,
  title = {Ultra-{{Strong Machine Learning}}: Comprehensibility of Programs Learned with {{ILP}}},
  volume = {107},
  issn = {1573-0565},
  shorttitle = {Ultra-{{Strong Machine Learning}}},
  abstract = {During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell's, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper we provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. We present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment we directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie's sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. We believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.},
  language = {en},
  number = {7},
  journal = {Machine Learning},
  doi = {10.1007/s10994-018-5707-3},
  author = {Muggleton, Stephen H. and Schmid, Ute and Zeller, Christina and {Tamaddoni-Nezhad}, Alireza and Besold, Tarek},
  month = jul,
  year = {2018},
  keywords = {Comprehensibility,Inductive logic programming,Ultra-strong machine learning},
  pages = {1119-1140},
  file = {/home/tim/Zotero/storage/8TKKSVDG/Muggleton et al. - 2018 - Ultra-Strong Machine Learning comprehensibility o.pdf}
}

@incollection{ben-menahemDemiseDogmaticUniverse2009,
  address = {Berlin, Heidelberg},
  title = {Demise of the {{Dogmatic Universe}}},
  isbn = {978-3-540-68832-7},
  language = {en},
  booktitle = {Historical {{Encyclopedia}} of {{Natural}} and {{Mathematical Sciences}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{Ben-Menahem}, Ari},
  editor = {{Ben-Menahem}, Ari},
  year = {2009},
  pages = {2801-5079},
  doi = {10.1007/978-3-540-68832-7_8}
}

@inproceedings{bansalExplanatoryDialogsActionable2018,
  address = {New York, NY, USA},
  series = {{{AIES}} '18},
  title = {Explanatory {{Dialogs}}: {{Towards Actionable}}, {{Interactive Explanations}}},
  isbn = {978-1-4503-6012-8},
  shorttitle = {Explanatory {{Dialogs}}},
  abstract = {Adoption of AI systems in high-stakes domains (e.g., transportation, law, and healthcare) demands that human users trust these systems. A desiderata for establishing trust is that the users understand the system's decision process. However, a high-performing system may use a complex decision process, which may not be interpretable by itself. We argue that existing solutions for generating interpretable explanations have limitations and as a solution, propose developing new explanation systems that enable interactive and actionable dialogs between the user and the system.},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278795},
  author = {Bansal, Gagan},
  year = {2018},
  keywords = {AI,Explainable,Explainable AI},
  pages = {356--357}
}

@inproceedings{ehsanRationalizationNeuralMachine2018,
  address = {New York, NY, USA},
  series = {{{AIES}} '18},
  title = {Rationalization: {{A Neural Machine Translation Approach}} to {{Generating Natural Language Explanations}}},
  isbn = {978-1-4503-6012-8},
  shorttitle = {Rationalization},
  abstract = {We introduce \textbackslash{}em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278736},
  author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
  year = {2018},
  keywords = {ai rationalization,artificial intelligence,explainable ai,interpretability,machine learning,transparency,user perception},
  pages = {81--87}
}

@inproceedings{abdulTrendsTrajectoriesExplainable2018,
  address = {New York, NY, USA},
  series = {{{CHI}} '18},
  title = {Trends and {{Trajectories}} for {{Explainable}}, {{Accountable}} and {{Intelligible Systems}}: {{An HCI Research Agenda}}},
  isbn = {978-1-4503-5620-6},
  shorttitle = {Trends and {{Trajectories}} for {{Explainable}}, {{Accountable}} and {{Intelligible Systems}}},
  abstract = {Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3173574.3174156},
  author = {Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y. and Kankanhalli, Mohan},
  year = {2018},
  keywords = {explainable artificial intelli-gence,explanations,intelligibility,interpretable machine learning},
  pages = {582:1--582:18}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  address = {New York, NY, USA},
  series = {{{KDD}} '16},
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  isbn = {978-1-4503-4232-2},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/2939672.2939778},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning},
  pages = {1135--1144}
}

@inproceedings{pastorExplainingBlackBox2019,
  address = {New York, NY, USA},
  series = {{{SAC}} '19},
  title = {Explaining {{Black Box Models}} by {{Means}} of {{Local Rules}}},
  isbn = {978-1-4503-5933-7},
  abstract = {Many high performance machine learning methods produce black box models, which do not disclose their internal logic yielding the prediction. However, in many application domains understanding the motivation of a prediction is becoming a requisite to trust the prediction itself. We propose a novel rule-based method that explains the prediction of any classifier on a specific instance by analyzing the joint effect of feature subsets on the classifier prediction. The relevant subsets are identified by learning a local rule-based model in the neighborhood of the prediction to explain. While local rules give a qualitative insight of the local behavior, their relevance is quantified by using the concept of prediction difference. Preliminary experiments show that, despite the approximation introduced by the local model, the explanations provided by our method are effective in detecting the effects of attribute correlation. Our method is model-agnostic. Hence, experts can compare explanations and local behaviors of the predictions for the same instance made by different classifiers.},
  booktitle = {Proceedings of the 34th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/3297280.3297328},
  author = {Pastor, Eliana and Baralis, Elena},
  year = {2019},
  keywords = {interpretability,local model,prediction explanation},
  pages = {510--517}
}

@inproceedings{singhEXSExplainableSearch2019,
  address = {New York, NY, USA},
  series = {{{WSDM}} '19},
  title = {{{EXS}}: {{Explainable Search Using Local Model Agnostic Interpretability}}},
  isbn = {978-1-4503-5940-5},
  shorttitle = {{{EXS}}},
  abstract = {Retrieval models in information retrieval are used to rank documents for typically under-specified queries. Today machine learning is used to learn retrieval models from click logs and/or relevance judgments that maximizes an objective correlated with user satisfaction. As these models become increasingly powerful and sophisticated, they also become harder to understand. Consequently, it is hard for to identify artifacts in training, data specific biases and intents from a complex trained model like neural rankers even if trained purely on text features. EXS is a search system designed specifically to provide its users with insight into the following questions: "What is the intent of the query according to the ranker?'', "Why is this document ranked higher than another?'' and "Why is this document relevant to the query?''. EXS uses a version of a popular posthoc explanation method for classifiers -- LIME, adapted specifically to answer these questions. We show how such a system can effectively help a user understand the results of neural rankers and highlight areas of improvement.},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290620},
  author = {Singh, Jaspreet and Anand, Avishek},
  year = {2019},
  keywords = {explainable search,interpretability,neural ranking models},
  pages = {770--773}
}

@inproceedings{strobelAxiomaticApproachExplain2018,
  address = {New York, NY, USA},
  series = {{{AIES}} '18},
  title = {An {{Axiomatic Approach}} to {{Explain Computer Generated Decisions}}: {{Extended Abstract}}},
  isbn = {978-1-4503-6012-8},
  shorttitle = {An {{Axiomatic Approach}} to {{Explain Computer Generated Decisions}}},
  abstract = {Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278788},
  author = {Strobel, Martin},
  year = {2018},
  keywords = {axiomatic approach,explainable machine learning},
  pages = {380--381}
}

@inproceedings{ehsanAutomatedRationaleGeneration2019,
  address = {New York, NY, USA},
  series = {{{IUI}} '19},
  title = {Automated {{Rationale Generation}}: {{A Technique}} for {{Explainable AI}} and {{Its Effects}} on {{Human Perceptions}}},
  isbn = {978-1-4503-6272-6},
  shorttitle = {Automated {{Rationale Generation}}},
  abstract = {Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations, (b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated rationales with regard to confidence in the autonomous agent, communicating failure and unexpected behavior. Overall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users. Moreover, context permitting, participants preferred detailed rationales to form a stable mental model of the agent's behavior.},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302316},
  author = {Ehsan, Upol and Tambwekar, Pradyumna and Chan, Larry and Harrison, Brent and Riedl, Mark O.},
  year = {2019},
  keywords = {algorithmic decision-making,algorithmic explanation,artificial intelligence,explainable AI,interpretability,machine learning,rationale generation,transparency,user perception},
  pages = {263--274}
}

@article{biecekDALEXExplainersComplex2018,
  title = {{{DALEX}}: {{Explainers}} for {{Complex Predictive Models}} in {{R}}},
  volume = {19},
  issn = {1532-4435},
  shorttitle = {{{DALEX}}},
  abstract = {Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  author = {Biecek, Przemys\textbackslash{}law},
  month = jan,
  year = {2018},
  keywords = {explainable artificial intelligence,interpretable machine learning,model visualization,predictive modelling},
  pages = {3245--3249}
}

@inproceedings{bridgeExplainingRecommendationsFidelity2017,
  address = {New York, NY, USA},
  series = {{{WebMedia}} '17},
  title = {Explaining {{Recommendations}}: {{Fidelity Versus Interpretability}}},
  isbn = {978-1-4503-5096-9},
  shorttitle = {Explaining {{Recommendations}}},
  abstract = {A hungry academic who is attending the WebMedia conference in Gramado uses a mobile phone app to obtain a recommendation for a place-to-eat. The restaurant that the app recommends is not within walking distance and serves a fusion-style cuisine with which the academic is unfamiliar. Should she accept the recommendation? Her confidence in the recommendation might be improved by an explanation of the recommender system's decision-making. But the explanations that recommender systems provide at present are often post hoc: fidelity to the system's decision-making is sometimes sacrificed for interpretability. Are fidelity and interpretability always in conflict? Or can they can be reconciled without damaging the quality of the recommendations themselves? This talk will review the kinds of explanations given by recommender systems. It will describe a new generation of recommender systems in which recommendation and explanation are more intimately connected and which seeks to maintain the quality of the recommendations while providing explanations that are both intelligible and reasonably faithful to the system's operation.},
  booktitle = {Proceedings of the 23rd {{Brazillian Symposium}} on {{Multimedia}} and the {{Web}}},
  publisher = {{ACM}},
  doi = {10.1145/3126858.3133312},
  author = {Bridge, Derek},
  year = {2017},
  keywords = {explanations,fidelity,recommendations},
  pages = {5--5}
}

@inproceedings{abdollahiUsingExplainabilityConstrained2017,
  address = {New York, NY, USA},
  series = {{{RecSys}} '17},
  title = {Using {{Explainability}} for {{Constrained Matrix Factorization}}},
  isbn = {978-1-4503-4652-8},
  abstract = {Accurate model-based Collaborative Filtering (CF) approaches, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations have been shown to improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user's trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on factorization models and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3109859.3109913},
  author = {Abdollahi, Behnoush and Nasraoui, Olfa},
  year = {2017},
  keywords = {explanations,interpretable models,latent factor models,matrix factorization,recommender systems},
  pages = {79--83},
  file = {/home/tim/Zotero/storage/M76U3KM7/Abdollahi and Nasraoui - 2017 - Using Explainability for Constrained Matrix Factor.pdf}
}

@inproceedings{gilpinReasonablePerceptionConnecting2018,
  address = {New York, NY, USA},
  series = {{{HRI}} '18},
  title = {Reasonable {{Perception}}: {{Connecting Vision}} and {{Language Systems}} for {{Validating Scene Descriptions}}},
  isbn = {978-1-4503-5615-2},
  shorttitle = {Reasonable {{Perception}}},
  abstract = {Understanding explanations of machine perception is an important step towards developing accountable, trustworthy machines. Furthermore, speech and vision are the primary modalities by which humans collect information about the world, but the linking of visual and natural language domains is a relatively new pursuit in computer vision, and it is difficult to test performance in a safe environment. To couple human visual understanding and machine perception, we present an explanatory system for creating a library of possible context-specific actions associated with 3D objects in immersive virtual worlds. We also contribute a novel scene description dataset, generated natively in virtual reality containing speech, image, gaze, and acceleration data. We discuss the development of a hybrid machine learning algorithm linking vision data with environmental affordances in natural language. Our findings demonstrate that it is possible to develop a model which can generate interpretable verbal descriptions of possible actions associated with recognized 3D objects within immersive VR environments.},
  booktitle = {Companion of the 2018 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3173386.3176994},
  author = {Gilpin, Leilani H. and Zaman, Cagri and Olson, Danielle and Yuan, Ben Z.},
  year = {2018},
  keywords = {commonsense reasoning,explainable ai,explainable robotic systems,virtual reality},
  pages = {115--116}
}

@inproceedings{costaAutomaticGenerationNatural2018,
  address = {New York, NY, USA},
  series = {{{IUI}} '18 {{Companion}}},
  title = {Automatic {{Generation}} of {{Natural Language Explanations}}},
  isbn = {978-1-4503-5571-1},
  abstract = {An interesting challenge for explainable recommender systems is to provide successful interpretation of recommendations using structured sentences. It is well known that user-generated reviews, have strong influence on the users' decision. Recent techniques exploit user reviews to generate natural language explanations. In this paper, we propose a character-level attention-enhanced long short-term memory model to generate natural language explanations. We empirically evaluated this network using two real-world review datasets. The generated text present readable and similar to a real user's writing, due to the ability of reproducing negation, misspellings, and domain-specific vocabulary.},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3180308.3180366},
  author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
  year = {2018},
  keywords = {Explainability,Explanations,Natural Language Generation,Neural Network,Recommender systems},
  pages = {57:1--57:2},
  file = {/home/tim/Zotero/storage/PT5T5JT3/Costa et al. - 2018 - Automatic Generation of Natural Language Explanati.pdf}
}

@inproceedings{amirAgentStrategySummarization2018,
  address = {Richland, SC},
  series = {{{AAMAS}} '18},
  title = {Agent {{Strategy Summarization}}},
  abstract = {Intelligent agents and AI-based systems are becoming increasingly prevalent. They support people in different ways, such as providing users with advice, working with them to achieve goals or acting on users' behalf. One key capability missing in such systems is the ability to present their users with an effective summary of their strategy and expected behaviors under different conditions and scenarios. This capability, which we see as complimentary to those currently under development in the context of "interpretable machine learning'' and "explainable AI'', is critical in various settings. In particular, it is likely to play a key role whenever a user needs to understand the strategy of an agent she is working along with, when having to choose between different available agents to act on her behalf, or when requested to determine the level of autonomy to be granted to the agent or approve its strategy. In this paper, we pose the challenge of developing capabilities for strategy summarization, which is not addressed by current theories and methods in the field. We propose a conceptual framework for strategy summarization, which we envision as a collaborative process that involves both agents and people. Last, we suggest possible testbeds that could be used to evaluate progress in research on strategy summarization.},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Amir, Ofra and {Doshi-Velez}, Finale and Sarne, David},
  year = {2018},
  keywords = {explainable ai,strategy summarization},
  pages = {1203--1207}
}

@inproceedings{nguyenExplainableModelingAnnotations2019,
  address = {New York, NY, USA},
  series = {{{IUI}} '19},
  title = {Explainable {{Modeling}} of {{Annotations}} in {{Crowdsourcing}}},
  isbn = {978-1-4503-6272-6},
  abstract = {Aggregation models for improving the quality of annotations collected via crowdsourcing have been widely studied, but far less has been done to explain why annotators make the mistakes that they do. To this end, we propose a joint aggregation and worker clustering model that detects patterns underlying crowd worker labels to characterize varieties of labeling errors. We evaluate our approach on a Named Entity Recognition dataset labeled by Mechanical Turk workers in both a retrospective experiment and a small human study. The former shows that our joint model improves the quality of clusters vs. aggregation followed by clustering. Results of the latter suggest that clusters aid human sense-making in interpreting worker labels and predicting worker mistakes. By enabling better explanation of annotator mistakes, our model creates a new opportunity to help Requesters improve task instructions and to help crowd annotators learn from their mistakes. Source code, data, and supplementary material is shared online.},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302276},
  author = {Nguyen, An T. and Lease, Matthew and Wallace, Byron C.},
  year = {2019},
  keywords = {clustering,crowdsourcing,explainable},
  pages = {575--579}
}

@inproceedings{guoLEMNAExplainingDeep2018,
  address = {New York, NY, USA},
  series = {{{CCS}} '18},
  title = {{{LEMNA}}: {{Explaining Deep Learning Based Security Applications}}},
  isbn = {978-1-4503-5693-0},
  shorttitle = {{{LEMNA}}},
  abstract = {While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks ( e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications ( e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  doi = {10.1145/3243734.3243792},
  author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
  year = {2018},
  keywords = {binary analysis,deep recurrent neural networks,explainable AI},
  pages = {364--379}
}

@article{hindExplainingExplainableAI2019,
  title = {Explaining {{Explainable AI}}},
  volume = {25},
  issn = {1528-4972},
  abstract = {How good are you at explaining your decisions? Are you better than a machine? Today, AI systems are being asked to explain their decisions. This article explores the challenges in solving this problem and approaches researchers are pursuing.},
  number = {3},
  journal = {XRDS},
  doi = {10.1145/3313096},
  author = {Hind, Michael},
  month = apr,
  year = {2019},
  pages = {16--19}
}

@inproceedings{ahmadInterpretableMachineLearning2018,
  address = {New York, NY, USA},
  series = {{{BCB}} '18},
  title = {Interpretable {{Machine Learning}} in {{Healthcare}}},
  isbn = {978-1-4503-5794-4},
  abstract = {This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
  booktitle = {Proceedings of the 2018 {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}}, and {{Health Informatics}}},
  publisher = {{ACM}},
  doi = {10.1145/3233547.3233667},
  author = {Ahmad, Muhammad Aurangzeb and Eckert, Carly and Teredesai, Ankur},
  year = {2018},
  keywords = {explainable ai,explainable machine learning,interpretable machine learning,machine learning in healthcare},
  pages = {559--560}
}

@inproceedings{baralReELReviewAware2018,
  address = {New York, NY, USA},
  series = {{{UMAP}} '18},
  title = {{{ReEL}}: {{Review Aware Explanation}} of {{Location Recommendation}}},
  isbn = {978-1-4503-5589-6},
  shorttitle = {{{ReEL}}},
  abstract = {The Location-Based Social Networks (LBSN) (e.g., Facebook, etc.) have many attributes (e.g., ratings, reviews, etc.) that play a crucial role for the Point-of-Interest (POI) recommendations. Unlike ratings, the reviews can help users to elaborate their consumption experience in terms of relevant factors of interest (aspects). Though some of the existing systems have exploited user reviews, most of them are less transparent and non-interpretable (as they conceal the reason behind recommendation). These reasons have motivated us towards explainable and interpretable recommendation. To the best of our knowledge, only a few of the researchers have exploited user reviews to incorporate the sentiment and opinions on different aspects for personalized and explainable POI recommendation. This paper proposes a model termed as ReEL (Review aware Explanation of Location Recommendation) which models the review-aspect correlation by exploiting deep neural network, formulates user-aspect bipartite relation as a bipartite graph, and models the explainable recommendation by using dense subgraph extraction and ranking-based techniques. The major contributions of this paper are: (i) it models users and POIs using the aspects posted on user reviews, and it provisions incorporation of multiple contexts (e.g., categorical, spatial, etc.) in POI recommendation, (ii) it formulates preference of users' on aspects as a bipartite relation, represents it as a location-aspect bipartite graph, and models the explainable recommendation with the notion of ordered dense subgraph extraction using bipartite cores, shingles, and ranking-based techniques, and (iii) it extensively evaluates the proposed models using three real-world datasets and demonstrates an improvement of 5.8\% to 29.5\% on F-score metric, when compared to the relevant studies.},
  booktitle = {Proceedings of the 26th {{Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  publisher = {{ACM}},
  doi = {10.1145/3209219.3209237},
  author = {Baral, Ramesh and Zhu, XiaoLong and Iyengar, S. S. and Li, Tao},
  year = {2018},
  keywords = {explainable recommendation,information retrieval,social networks},
  pages = {23--32}
}

@inproceedings{Schuessler:2019:MEC:3290607.3312823,
  series = {{{CHI EA}} '19},
  title = {Minimalistic {{Explanations}}: {{Capturing}} the {{Essence}} of {{Decisions}}},
  isbn = {978-1-4503-5971-9},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3290607.3312823},
  author = {Schuessler, Martin and Wei\ss, Philipp},
  year = {2019},
  keywords = {deep neural networks,explanations,image classification,interpretable machine learning},
  pages = {LBW2810:1-LBW2810:6},
  articleno = {LBW2810},
  numpages = {6},
  acmid = {3312823}
}

@inproceedings{Cai:2019:EEE:3301275.3302289,
  series = {{{IUI}} '19},
  title = {The {{Effects}} of {{Example}}-Based {{Explanations}} in a {{Machine Learning Interface}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302289},
  author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
  year = {2019},
  keywords = {example-based explanations,explainable AI,human-AI interaction,machine learning},
  pages = {258-262},
  numpages = {5},
  acmid = {3302289}
}

@inproceedings{Nobrega:2019:TER:3297280.3297443,
  series = {{{SAC}} '19},
  title = {Towards {{Explaining Recommendations Through Local Surrogate Models}}},
  isbn = {978-1-4503-5933-7},
  booktitle = {Proceedings of the 34th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/3297280.3297443},
  author = {N\'obrega, Caio and Marinho, Leandro},
  year = {2019},
  keywords = {explanations,factorization machines,recommender systems,transparency},
  pages = {1671-1678},
  numpages = {8},
  acmid = {3297443}
}

@inproceedings{Madumal:2018:IDE:3292147.3293450,
  series = {{{OzCHI}} '18},
  title = {Interaction {{Design}} for {{Explainable AI}}: {{Workshop Proposal}}},
  isbn = {978-1-4503-6188-0},
  booktitle = {Proceedings of the 30th {{Australian Conference}} on {{Computer}}-{{Human Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3292147.3293450},
  author = {Madumal, Prashan and Singh, Ronal and Newn, Joshua and Vetere, Frank},
  year = {2018},
  keywords = {explainable AI,explainable interfaces},
  pages = {607-608},
  numpages = {2},
  acmid = {3293450}
}

@inproceedings{Dominguez:2019:EEA:3301275.3302274,
  series = {{{IUI}} '19},
  title = {The {{Effect}} of {{Explanations}} and {{Algorithmic Accuracy}} on {{Visual Recommender Systems}} of {{Artistic Images}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302274},
  author = {Dominguez, Vicente and Messina, Pablo and {Donoso-Guzm\'an}, Ivania and Parra, Denis},
  year = {2019},
  keywords = {art,explainable AI,visual recommender systems},
  pages = {408-416},
  numpages = {9},
  acmid = {3302274}
}

@inproceedings{Iyer:2018:TED:3278721.3278776,
  series = {{{AIES}} '18},
  title = {Transparency and {{Explanation}} in {{Deep Reinforcement Learning Neural Networks}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278776},
  author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
  year = {2018},
  keywords = {deep reinforcement learning,explainable ai,human factors,human-ai interaction,system transparency},
  pages = {144-150},
  numpages = {7},
  acmid = {3278776}
}

@inproceedings{Bock:2018:VNN:3281505.3281605,
  series = {{{VRST}} '18},
  title = {Visualization of {{Neural Networks}} in {{Virtual Reality Using Unreal Engine}}},
  isbn = {978-1-4503-6086-9},
  booktitle = {Proceedings of the 24th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  publisher = {{ACM}},
  doi = {10.1145/3281505.3281605},
  author = {Bock, Marcel and Schreiber, Andreas},
  year = {2018},
  keywords = {deep learning,explainable ai,neural networks,visualization},
  pages = {132:1-132:2},
  articleno = {132},
  numpages = {2},
  acmid = {3281605}
}

@inproceedings{Tan:2018:IAD:3278721.3278802,
  series = {{{AIES}} '18},
  title = {Interpretable {{Approaches}} to {{Detect Bias}} in {{Black}}-{{Box Models}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278802},
  author = {Tan, Sarah},
  year = {2018},
  keywords = {algorithmic fairness,black-box models,interpretability,model distillation,transparency},
  pages = {382-383},
  numpages = {2},
  acmid = {3278802}
}

@inproceedings{Hicks:2018:MAR:3204949.3208129,
  series = {{{MMSys}} '18},
  title = {Mimir: {{An Automatic Reporting}} and {{Reasoning System}} for {{Deep Learning Based Analysis}} in the {{Medical Domain}}},
  isbn = {978-1-4503-5192-8},
  booktitle = {Proceedings of the 9th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM}},
  doi = {10.1145/3204949.3208129},
  author = {Hicks, Steven Alexander and Eskeland, Sigrun and Lux, Mathias and {de Lange}, Thomas and Randel, Kristin Ranheim and Jeppsson, Mattis and Pogorelov, Konstantin and Halvorsen, P\aa{}l and Riegler, Michael},
  year = {2018},
  keywords = {automatic disease detection,deep learning,interpretable neural networks,medical documentation},
  pages = {369-374},
  numpages = {6},
  acmid = {3208129}
}

@inproceedings{Fogg:2003:PTE:765891.765951,
  series = {{{CHI EA}} '03},
  title = {Prominence-Interpretation {{Theory}}: {{Explaining How People Assess Credibility Online}}},
  isbn = {1-58113-637-4},
  booktitle = {{{CHI}} '03 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/765891.765951},
  author = {Fogg, B. J.},
  year = {2003},
  keywords = {e-commerce,e-learning and education,health care,interaction design,theory,user and cognitive models,world wide web and hypermedia},
  pages = {722-723},
  numpages = {2},
  acmid = {765951}
}

@inproceedings{Kouki:2019:PEH:3301275.3302306,
  series = {{{IUI}} '19},
  title = {Personalized {{Explanations}} for {{Hybrid Recommender Systems}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302306},
  author = {Kouki, Pigi and Schaffer, James and Pujara, Jay and O'Donovan, John and Getoor, Lise},
  year = {2019},
  keywords = {explainable artificial intelligence,explainable intelligent user interfaces,explainable recommender systems,hybrid recommender systems},
  pages = {379-390},
  numpages = {12},
  acmid = {3302306}
}

@inproceedings{Wang:2018:TTE:3178876.3186066,
  series = {{{WWW}} '18},
  title = {{{TEM}}: {{Tree}}-Enhanced {{Embedding Model}} for {{Explainable Recommendation}}},
  isbn = {978-1-4503-5639-8},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186066},
  author = {Wang, Xiang and He, Xiangnan and Feng, Fuli and Nie, Liqiang and Chua, Tat-Seng},
  year = {2018},
  keywords = {embedding-based model,explainable recommendation,neural attention network,tree-based model},
  pages = {1543-1552},
  numpages = {10},
  acmid = {3186066}
}

@article{Cheng:2019:MER:3306215.3291060,
  title = {{{MMALFM}}: {{Explainable Recommendation}} by {{Leveraging Reviews}} and {{Images}}},
  volume = {37},
  issn = {1046-8188},
  number = {2},
  journal = {ACM Trans. Inf. Syst.},
  doi = {10.1145/3291060},
  author = {Cheng, Zhiyong and Chang, Xiaojun and Zhu, Lei and Kanjirathinkal, Rose C. and Kankanhalli, Mohan},
  month = jan,
  year = {2019},
  keywords = {Aspect,explainable recommendation,latent factor model,multi-modal,rating prediction},
  pages = {16:1-16:28},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {March 2019},
  articleno = {16},
  numpages = {28},
  acmid = {3291060}
}

@article{Siddiqui:2019:SFE:3301280.3230666,
  title = {Sequential {{Feature Explanations}} for {{Anomaly Detection}}},
  volume = {13},
  issn = {1556-4681},
  number = {1},
  journal = {ACM Trans. Knowl. Discov. Data},
  doi = {10.1145/3230666},
  author = {Siddiqui, Md Amran and Fern, Alan and Dietterich, Thomas G. and Wong, Weng-Keen},
  month = jan,
  year = {2019},
  keywords = {Anomaly detection,anomaly explanation,anomaly interpretation,explanation evaluation},
  pages = {1:1-1:22},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {January 2019},
  articleno = {1},
  numpages = {22},
  acmid = {3230666}
}

@inproceedings{Du:2018:TED:3219819.3220099,
  series = {{{KDD}} '18},
  title = {Towards {{Explanation}} of {{DNN}}-Based {{Prediction}} with {{Guided Feature Inversion}}},
  isbn = {978-1-4503-5552-0},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \&\#38; {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220099},
  author = {Du, Mengnan and Liu, Ninghao and Song, Qingquan and Hu, Xia},
  year = {2018},
  keywords = {deep learning,guided feature inversion,intermediate layers,machine learning interpretation},
  pages = {1358-1367},
  numpages = {10},
  acmid = {3220099}
}

@inproceedings{Liu:2019:RIS:3289600.3290960,
  series = {{{WSDM}} '19},
  title = {Representation {{Interpretation}} with {{Spatial Encoding}} and {{Multimodal Analytics}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290960},
  author = {Liu, Ninghao and Du, Mengnan and Hu, Xia},
  year = {2019},
  keywords = {interpretation,recommender systems,representation learning},
  pages = {60-68},
  numpages = {9},
  acmid = {3290960}
}

@inproceedings{Hu:2017:IWI:3132847.3133198,
  series = {{{CIKM}} '17},
  title = {{{IDM}} 2017: {{Workshop}} on {{Interpretable Data Mining}} -- {{Bridging}} the {{Gap Between Shallow}} and {{Deep Models}}},
  isbn = {978-1-4503-4918-5},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/3132847.3133198},
  author = {Hu, Xia and Ji, Shuiwang},
  year = {2017},
  keywords = {data mining,deep models,interpretability,machine learning,shallow models},
  pages = {2565-2566},
  numpages = {2},
  acmid = {3133198}
}

@inproceedings{Zhang:2018:SWE:3209978.3210193,
  series = {{{SIGIR}} '18},
  title = {{{SIGIR}} 2018 {{Workshop}} on {{ExplainAble Recommendation}} and {{Search}} ({{EARS}} 2018)},
  isbn = {978-1-4503-5657-2},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \&\#38; {{Development}} in {{Information Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3209978.3210193},
  author = {Zhang, Yongfeng and Zhang, Yi and Zhang, Min},
  year = {2018},
  keywords = {explainable recommendation,explainable search,information retrieval,recommendation systems},
  pages = {1411-1413},
  numpages = {3},
  acmid = {3210193}
}

@inproceedings{Wu:2017:ICD:3107411.3107447,
  series = {{{ACM}}-{{BCB}} '17},
  title = {Infer {{Cause}} of {{Death}} for {{Population Health Using Convolutional Neural Network}}},
  isbn = {978-1-4503-4722-8},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}},and {{Health Informatics}}},
  publisher = {{ACM}},
  doi = {10.1145/3107411.3107447},
  author = {Wu, Hang and Wang, May D.},
  year = {2017},
  keywords = {causal inference,deep learning,interpretability},
  pages = {526-535},
  numpages = {10},
  acmid = {3107447}
}

@inproceedings{Amir:2018:HSA:3237383.3237869,
  series = {{{AAMAS}} '18},
  title = {{{HIGHLIGHTS}}: {{Summarizing Agent Behavior}} to {{People}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Amir, Dan and Amir, Ofra},
  year = {2018},
  keywords = {explainable ai,strategy summarization},
  pages = {1168-1176},
  numpages = {9},
  acmid = {3237869}
}

@inproceedings{Dalmia:2018:TIN:3184558.3191523,
  series = {{{WWW}} '18},
  title = {Towards {{Interpretation}} of {{Node Embeddings}}},
  isbn = {978-1-4503-5640-4},
  booktitle = {Companion {{Proceedings}} of the {{The Web Conference}} 2018},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3184558.3191523},
  author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
  year = {2018},
  keywords = {graph representation,model interpretability,neural networks},
  pages = {945-952},
  numpages = {8},
  acmid = {3191523}
}

@inproceedings{Hooker:2018:TNM:3278721.3278753,
  series = {{{AIES}} '18},
  title = {Toward {{Non}}-{{Intuition}}-{{Based Machine}} and {{Artificial Intelligence Ethics}}: {{A Deontological Approach Based}} on {{Modal Logic}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278753},
  author = {Hooker, John N. and Kim, Tae Wan N.},
  year = {2018},
  keywords = {accountable ai,artificial intelligence ethics,autonomous machine ethics,deontology,explainable ai,kantian ai,machine ethics,modal logic},
  pages = {130-136},
  numpages = {7},
  acmid = {3278753}
}

@inproceedings{Balachandran:2009:IRC:1645953.1646227,
  series = {{{CIKM}} '09},
  title = {Interpretable and {{Reconfigurable Clustering}} of {{Document Datasets}} by {{Deriving Word}}-Based {{Rules}}},
  isbn = {978-1-60558-512-3},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/1645953.1646227},
  author = {Balachandran, Vipin and P, Deepak and Khemani, Deepak},
  year = {2009},
  keywords = {interpretable clustering},
  pages = {1773-1776},
  numpages = {4},
  acmid = {1646227}
}

@inproceedings{Zurek:2013:MTI:2514601.2514619,
  series = {{{ICAIL}} '13},
  title = {Modeling {{Teleological Interpretation}}},
  isbn = {978-1-4503-2080-1},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  publisher = {{ACM}},
  doi = {10.1145/2514601.2514619},
  author = {Zurek, Tomasz and Araszkiewicz, Micha\l{}},
  year = {2013},
  keywords = {argumentation,reasoning,teleological interpretation},
  pages = {160-168},
  numpages = {9},
  acmid = {2514619}
}

@inproceedings{Bellini:2018:KAE:3270323.3270327,
  series = {{{DLRS}} 2018},
  title = {Knowledge-Aware {{Autoencoders}} for {{Explainable Recommender Systems}}},
  isbn = {978-1-4503-6617-5},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Deep Learning}} for {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3270323.3270327},
  author = {Bellini, Vito and Schiavone, Angelo and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
  year = {2018},
  keywords = {Autoencoder Neural Networks,Deep Learning,Explainable Models,Explanation,Recommender Systems},
  pages = {24-31},
  numpages = {8},
  acmid = {3270327}
}

@inproceedings{Chen:2018:NAR:3178876.3186070,
  series = {{{WWW}} '18},
  title = {Neural {{Attentional Rating Regression}} with {{Review}}-Level {{Explanations}}},
  isbn = {978-1-4503-5639-8},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186070},
  author = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
  year = {2018},
  keywords = {explainable recommendation,neural attention network,recommender systems,review usefulness},
  pages = {1583-1592},
  numpages = {10},
  acmid = {3186070}
}

@inproceedings{Croitoru:2010:GEN:1838206.1838404,
  series = {{{AAMAS}} '10},
  title = {Graphically {{Explaining Norms}}},
  isbn = {978-0-9826571-1-9},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}: {{Volume}} 1 - {{Volume}} 1},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Croitoru, Madalina and Oren, Nir and Miles, Simon and Luck, Michael},
  year = {2010},
  keywords = {conceptual graphs,norms},
  pages = {1405-1406},
  numpages = {2},
  acmid = {1838404}
}

@inproceedings{Tamagnini:2017:IBC:3077257.3077260,
  series = {{{HILDA}}'17},
  title = {Interpreting {{Black}}-{{Box Classifiers Using Instance}}-{{Level Visual Explanations}}},
  isbn = {978-1-4503-5029-7},
  booktitle = {Proceedings of the {{2Nd Workshop}} on {{Human}}-{{In}}-the-{{Loop Data Analytics}}},
  publisher = {{ACM}},
  doi = {10.1145/3077257.3077260},
  author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
  year = {2017},
  keywords = {classification,explanation,machine learning,visual analytics},
  pages = {6:1-6:6},
  articleno = {6},
  numpages = {6},
  acmid = {3077260}
}

@inproceedings{Eiband:2019:IPE:3290607.3312787,
  series = {{{CHI EA}} '19},
  title = {The {{Impact}} of {{Placebic Explanations}} on {{Trust}} in {{Intelligent Systems}}},
  isbn = {978-1-4503-5971-9},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3290607.3312787},
  author = {Eiband, Malin and Buschek, Daniel and Kremer, Alexander and Hussmann, Heinrich},
  year = {2019},
  keywords = {explainability,explanations,intelligent systems,transparency,XAI},
  pages = {LBW0243:1-LBW0243:6},
  articleno = {LBW0243},
  numpages = {6},
  acmid = {3312787}
}

@inproceedings{Hayes:2017:IRC:2909824.3020233,
  series = {{{HRI}} '17},
  title = {Improving {{Robot Controller Transparency Through Autonomous Policy Explanation}}},
  isbn = {978-1-4503-4336-7},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/2909824.3020233},
  author = {Hayes, Bradley and Shah, Julie A.},
  year = {2017},
  keywords = {human-robot collaboration,human-robot interaction,human-robot teaming,interpretable machine learning},
  pages = {303-312},
  numpages = {10},
  acmid = {3020233}
}

@inproceedings{Simmons:2009:LLA:1480945.1480949,
  series = {{{PEPM}} '09},
  title = {Linear {{Logical Approximations}}},
  isbn = {978-1-60558-327-3},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/1480945.1480949},
  author = {Simmons, Robert J. and Pfenning, Frank},
  year = {2009},
  keywords = {abstract interpretation,bottom-up linear logic programming,operational semantics},
  pages = {9-20},
  numpages = {12},
  acmid = {1480949}
}

@inproceedings{Kasneci:2016:LLW:2983323.2983746,
  series = {{{CIKM}} '16},
  title = {{{LICON}}: {{A Linear Weighting Scheme}} for the {{Contribution ofInput Variables}} in {{Deep Artificial Neural Networks}}},
  isbn = {978-1-4503-4073-1},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/2983323.2983746},
  author = {Kasneci, Gjergji and Gottron, Thomas},
  year = {2016},
  keywords = {artificial neural networks,contribution,explanation,input variables,linear weighting scheme},
  pages = {45-54},
  numpages = {10},
  acmid = {2983746}
}

@inproceedings{Wang:2016:TCW:2906831.2906852,
  series = {{{HRI}} '16},
  title = {Trust {{Calibration Within}} a {{Human}}-{{Robot Team}}: {{Comparing Automatically Generated Explanations}}},
  isbn = {978-1-4673-8370-7},
  booktitle = {The {{Eleventh ACM}}/{{IEEE International Conference}} on {{Human Robot Interaction}}},
  publisher = {{IEEE Press}},
  author = {Wang, Ning and Pynadath, David V. and Hill, Susan G.},
  year = {2016},
  keywords = {explainable a.i.,human-robot interaction,pomdp,trust},
  pages = {109-116},
  numpages = {8},
  acmid = {2906852}
}

@inproceedings{Dodge:2018:EAE:3173574.3174136,
  series = {{{CHI}} '18},
  title = {How the {{Experts Do It}}: {{Assessing}} and {{Explaining Agent Behaviors}} in {{Real}}-{{Time Strategy Games}}},
  isbn = {978-1-4503-5620-6},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3173574.3174136},
  author = {Dodge, Jonathan and Penney, Sean and Hilderbrand, Claudia and Anderson, Andrew and Burnett, Margaret},
  year = {2018},
  keywords = {explainable ai,information foraging,intelligent agents,rts games,starcraft},
  pages = {562:1-562:12},
  articleno = {562},
  numpages = {12},
  acmid = {3174136}
}

@inproceedings{Millecamp:2019:EEE:3301275.3302313,
  series = {{{IUI}} '19},
  title = {To {{Explain}} or {{Not}} to {{Explain}}: {{The Effects}} of {{Personal Characteristics}} When {{Explaining Music Recommendations}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302313},
  author = {Millecamp, Martijn and Htun, Nyi Nyi and Conati, Cristina and Verbert, Katrien},
  year = {2019},
  keywords = {explanations,music,need for cognition,personal characteristics,recommender system,spotify,user characteristics},
  pages = {397-407},
  numpages = {11},
  acmid = {3302313}
}

@inproceedings{Holcomb:2018:ODA:3206157.3206174,
  series = {{{ICBDE}} '18},
  title = {Overview on {{DeepMind}} and {{Its AlphaGo Zero AI}}},
  isbn = {978-1-4503-6358-7},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Big Data}} and {{Education}}},
  publisher = {{ACM}},
  doi = {10.1145/3206157.3206174},
  author = {Holcomb, Sean D. and Porter, William K. and Ault, Shaun V. and Mao, Guifen and Wang, Jin},
  year = {2018},
  keywords = {AI,AlphaGo Zero,Deep Learning,Deep Mind,Neural Networks,Reinforcement Learning},
  pages = {67-71},
  numpages = {5},
  acmid = {3206174}
}

@article{Sanderson:1991:HSL:116890.116911,
  title = {The {{Hierarchical Simulation Language HSL}}: {{A Versatile Tool}} for {{Process}}-Oriented {{Simulation}}},
  volume = {1},
  issn = {1049-3301},
  number = {2},
  journal = {ACM Trans. Model. Comput. Simul.},
  doi = {10.1145/116890.116911},
  author = {Sanderson, D. P. and Sharma, R. and Rozin, R. and Treu, S.},
  month = apr,
  year = {1991},
  keywords = {C++,hierarchy,HSL,inheritance,interpreter,modularity,process,simulation programming language},
  pages = {113-153},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {April 1991},
  numpages = {41},
  acmid = {116911}
}

@inproceedings{Wang:2018:LCM:3219819.3220070,
  series = {{{KDD}} '18},
  title = {Learning {{Credible Models}}},
  isbn = {978-1-4503-5552-0},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \&\#38; {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220070},
  author = {Wang, Jiaxuan and Oh, Jeeheh and Wang, Haozhu and Wiens, Jenna},
  year = {2018},
  keywords = {model interpretability,regularization},
  pages = {2417-2426},
  numpages = {10},
  acmid = {3220070}
}

@inproceedings{Feng:2018:IPE:3206025.3206048,
  series = {{{ICMR}} '18},
  title = {Interpretable {{Partitioned Embedding}} for {{Customized Multi}}-Item {{Fashion Outfit Composition}}},
  isbn = {978-1-4503-5046-4},
  booktitle = {Proceedings of the 2018 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3206025.3206048},
  author = {Feng, Zunlei and Yu, Zhenyun and Yang, Yezhou and Jing, Yongcheng and Jiang, Junxiao and Song, Mingli},
  year = {2018},
  keywords = {adversarial,embedding,interpretable,outfit composition},
  pages = {143-151},
  numpages = {9},
  acmid = {3206048}
}

@article{http://arxiv.org/abs/1803.07980v2,
  title = {Information {{Theoretic Interpretation}} of {{Deep}} Learning},
  abstract = {We interpret part of the experimental results of Shwartz-Ziv and Tishby [2017]. Inspired by these results, we established a conjecture of the dynamics of the machinary of deep neural network. This conjecture can be used to explain the counterpart result by Saxe et al. [2018].},
  journal = {arxiv},
  author = {Zhao, Tianchen},
  month = mar,
  year = {2018}
}

@article{http://arxiv.org/abs/1902.02384v1,
  title = {Global {{Explanations}} of {{Neural Networks}}: {{Mapping}} the {{Landscape}} of {{Predictions}}},
  abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.},
  journal = {arxiv},
  author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.12069v1,
  title = {The {{Virtual Doctor}}: {{An Interactive Artificial Intelligence}} Based on {{Deep Learning}} for {{Non}}-{{Invasive Prediction}} of {{Diabetes}}},
  abstract = {Artificial intelligence (AI) will pave the way to a new era in medicine. However, currently available AI systems do not interact with a patient, e.g., for anamnesis, and thus are only used by the physicians for predictions in diagnosis or prognosis. However, these systems are widely used, e.g., in diabetes or cancer prediction. In the current study, we developed an AI that is able to interact with a patient (virtual doctor) by using a speech recognition and speech synthesis system and thus can autonomously interact with the patient, which is particularly important for, e.g., rural areas, where the availability of primary medical care is strongly limited by low population densities. As a proof-of-concept, the system is able to predict type 2 diabetes mellitus (T2DM) based on non-invasive sensors and deep neural networks. Moreover, the system provides an easy-to-interpret probability estimation for T2DM for a given patient. Besides the development of the AI, we further analyzed the acceptance of young people for AI in healthcare to estimate the impact of such system in the future.},
  journal = {arxiv},
  author = {Sp\"anig, Sebastian and {Emberger-Klein}, Agnes and Sowa, Jan-Peter and Canbay, Ali and Menrad, Klaus and Heider, Dominik},
  month = mar,
  year = {2019}
}

@inproceedings{Mittelstadt:2019:EEA:3287560.3287574,
  series = {{{FAT}}* '19},
  title = {Explaining {{Explanations}} in {{AI}}},
  isbn = {978-1-4503-6125-5},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  publisher = {{ACM}},
  doi = {10.1145/3287560.3287574},
  author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  year = {2019},
  keywords = {Accountability,Explanations,Interpretability,Philosophy of Science},
  pages = {279-288},
  file = {/home/tim/Zotero/storage/DEDJBXHE/Mittelstadt et al. - 2019 - Explaining Explanations in AI.pdf},
  numpages = {10},
  acmid = {3287574}
}

@article{Israelsen:2019:XAY:3303862.3267338,
  title = {\&\#{{x201C}};{{Dave}}...{{I Can Assure You}} ...{{That It}}\&\#x2019;s {{Going}} to {{Be All Right}} ...\&\#{{x201D}}; {{A Definition}}, {{Case}} for, and {{Survey}} of {{Algorithmic Assurances}} in {{Human}}-{{Autonomy Trust Relationships}}},
  volume = {51},
  issn = {0360-0300},
  number = {6},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/3267338},
  author = {Israelsen, Brett W. and Ahmed, Nisar R.},
  month = jan,
  year = {2019},
  keywords = {accountability,algorithmic assurances,explainable artificial intelligence,fairness,Human-computer trust,interpretable machine learning,transparency},
  pages = {113:1-113:37},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {February 2019},
  articleno = {113},
  numpages = {37},
  acmid = {3267338}
}

@inproceedings{Ghazimatin:2019:FFU:3289600.3290990,
  series = {{{WSDM}} '19},
  title = {{{FAIRY}}: {{A Framework}} for {{Understanding Relationships Between Users}}' {{Actions}} and {{Their Social Feeds}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290990},
  author = {Ghazimatin, Azin and Saha Roy, Rishiraj and Weikum, Gerhard},
  year = {2019},
  keywords = {explanation paths,interpretability,social feeds,user actions},
  pages = {240-248},
  numpages = {9},
  acmid = {3290990}
}

@inproceedings{Popat:2018:CCL:3184558.3186967,
  series = {{{WWW}} '18},
  title = {{{CredEye}}: {{A Credibility Lens}} for {{Analyzing}} and {{Explaining Misinformation}}},
  isbn = {978-1-4503-5640-4},
  booktitle = {Companion {{Proceedings}} of the {{The Web Conference}} 2018},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3184558.3186967},
  author = {Popat, Kashyap and Mukherjee, Subhabrata and Str\"otgen, Jannik and Weikum, Gerhard},
  year = {2018},
  keywords = {credibility analysis,fact checking,interpretable learning},
  pages = {155-158},
  numpages = {4},
  acmid = {3186967}
}

@inproceedings{Kuo:2008:FEA:1486927.1486968,
  series = {{{WI}}-{{IAT}} '08},
  title = {Finding {{Explanations}} for {{Assisting Pattern Interpretation}}},
  isbn = {978-0-7695-3496-1},
  booktitle = {Proceedings of the 2008 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}} - {{Volume}} 01},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WIIAT.2008.330},
  author = {Kuo, Yen-Ting and Sonenberg, Liz and Lonie, Andrew},
  year = {2008},
  keywords = {explanation generation,pattern interpretation},
  pages = {48-51},
  numpages = {4},
  acmid = {1486968}
}

@inproceedings{Wannous:2013:EAO:2568493.2569296,
  series = {{{WI}}-{{IAT}} '13},
  title = {Explaining {{Argumentation}} over {{Alignment Agreements}}},
  isbn = {978-0-7695-5145-6},
  booktitle = {Proceedings of the 2013 {{IEEE}}/{{WIC}}/{{ACM International Joint Conferences}} on {{Web Intelligence}} ({{WI}}) and {{Intelligent Agent Technologies}} ({{IAT}}) - {{Volume}} 02},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WI-IAT.2013.94},
  author = {Wannous, Rouaa and Trojahn, C\'assia},
  year = {2013},
  keywords = {argumentation,explanation,ontology matching},
  pages = {80-85},
  numpages = {6},
  acmid = {2569296}
}

@inproceedings{Paulheim:2012:EUL:2166966.2167029,
  series = {{{IUI}} '12},
  title = {Explain-a-{{LOD}}: {{Using Linked Open Data}} for {{Interpreting Statistics}}},
  isbn = {978-1-4503-1048-2},
  booktitle = {Proceedings of the 2012 {{ACM International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/2166966.2167029},
  author = {Paulheim, Heiko},
  year = {2012},
  keywords = {data analysis,linked open data,semantic web,statistics},
  pages = {313-314},
  numpages = {2},
  acmid = {2167029}
}

@article{Miller:2019:BWU:3325198.3313107,
  title = {"{{But Why}}?" {{Understanding Explainable Artificial Intelligence}}},
  volume = {25},
  issn = {1528-4972},
  number = {3},
  journal = {XRDS},
  doi = {10.1145/3313107},
  author = {Miller, Tim},
  month = apr,
  year = {2019},
  pages = {20-25},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {Spring 2019},
  numpages = {6},
  acmid = {3313107}
}

@inproceedings{Gad-Elrab:2019:EFE:3289600.3290996,
  series = {{{WSDM}} '19},
  title = {{{ExFaKT}}: {{A Framework}} for {{Explaining Facts}} over {{Knowledge Graphs}} and {{Text}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290996},
  author = {{Gad-Elrab}, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
  year = {2019},
  keywords = {explainable evidence,fact-checking,knowledge graph,reasoning},
  pages = {87-95},
  numpages = {9},
  acmid = {3290996}
}

@inproceedings{Zheng:2018:DDL:3232565.3232569,
  series = {{{APNet}} '18},
  title = {Demystifying {{Deep Learning}} in {{Networking}}},
  isbn = {978-1-4503-6395-2},
  booktitle = {Proceedings of the {{2Nd Asia}}-{{Pacific Workshop}} on {{Networking}}},
  publisher = {{ACM}},
  doi = {10.1145/3232565.3232569},
  author = {Zheng, Ying and Liu, Ziyu and You, Xinyu and Xu, Yuedong and Jiang, Junchen},
  year = {2018},
  keywords = {Interpretability,Neural networks,Resource allocation},
  pages = {1-7},
  numpages = {7},
  acmid = {3232569}
}

@inproceedings{Gunning:2019:DEA:3301275.3308446,
  series = {{{IUI}} '19},
  title = {{{DARPA}}'s {{Explainable Artificial Intelligence}} ({{XAI}}) {{Program}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3308446},
  author = {Gunning, David},
  year = {2019},
  keywords = {artificial intelligence,evaluation,explanation,machine learning},
  pages = {ii-ii},
  acmid = {3308446}
}

@inproceedings{Haddouchi:2018:AIC:3289402.3289549,
  series = {{{SITA}}'18},
  title = {Assessing {{Interpretation Capacity}} in {{Machine Learning}}: {{A Critical Review}}},
  isbn = {978-1-4503-6462-1},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Intelligent Systems}}: {{Theories}} and {{Applications}}},
  publisher = {{ACM}},
  doi = {10.1145/3289402.3289549},
  author = {Haddouchi, Maissae and Berrado, Abdelaziz},
  year = {2018},
  keywords = {Interpretability,measures,ML,scoring},
  pages = {49:1-49:6},
  articleno = {49},
  numpages = {6},
  acmid = {3289549}
}

@inproceedings{Wang:2016:IPE:2936924.2937071,
  series = {{{AAMAS}} '16},
  title = {The {{Impact}} of {{POMDP}}-{{Generated Explanations}} on {{Trust}} and {{Performance}} in {{Human}}-{{Robot Teams}}},
  isbn = {978-1-4503-4239-1},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Autonomous Agents}} \&\#38; {{Multiagent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Wang, Ning and Pynadath, David V. and Hill, Susan G.},
  year = {2016},
  keywords = {explainable ai,human-robot interaction,pomdps,trust},
  pages = {997-1005},
  numpages = {9},
  acmid = {2937071}
}

@inproceedings{Liu:2018:INE:3219819.3220001,
  series = {{{KDD}} '18},
  title = {On {{Interpretation}} of {{Network Embedding}} via {{Taxonomy Induction}}},
  isbn = {978-1-4503-5552-0},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \&\#38; {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220001},
  author = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
  year = {2018},
  keywords = {machine learning interpretation,network embedding,taxonomy},
  pages = {1812-1820},
  numpages = {9},
  acmid = {3220001}
}

@inproceedings{Sklar:2018:ETA:3284432.3284470,
  series = {{{HAI}} '18},
  title = {Explanation {{Through Argumentation}}},
  isbn = {978-1-4503-5953-5},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Human}}-{{Agent Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3284432.3284470},
  author = {Sklar, Elizabeth I. and Azhar, Mohammad Q.},
  year = {2018},
  keywords = {computational argumentation,explainable ai,human-robot interaction},
  pages = {277-285},
  numpages = {9},
  acmid = {3284470}
}

@inproceedings{Card:2019:DWA:3287560.3287595,
  series = {{{FAT}}* '19},
  title = {Deep {{Weighted Averaging Classifiers}}},
  isbn = {978-1-4503-6125-5},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  publisher = {{ACM}},
  doi = {10.1145/3287560.3287595},
  author = {Card, Dallas and Zhang, Michael and Smith, Noah A.},
  year = {2019},
  keywords = {conformal methods,interpretability credibility},
  pages = {369-378},
  numpages = {10},
  acmid = {3287595}
}

@inproceedings{Bharadhwaj:2019:ERS:3308557.3308699,
  series = {{{IUI}} '19},
  title = {Explainable {{Recommender System That Maximizes Exploration}}},
  isbn = {978-1-4503-6673-1},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}: {{Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3308557.3308699},
  author = {Bharadhwaj, Homanga},
  year = {2019},
  keywords = {explainable AI,recommender systems,RNN},
  pages = {1-2},
  numpages = {2},
  acmid = {3308699}
}

@inproceedings{Ha:2018:DEA:3183654.3183683,
  series = {{{TechMindSociety}} '18},
  title = {Designing {{Explainability}} of an {{Artificial Intelligence System}}},
  isbn = {978-1-4503-5420-2},
  booktitle = {Proceedings of the {{Technology}}, {{Mind}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3183654.3183683},
  author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
  year = {2018},
  keywords = {Anthropomorphism,Attribution theory,Explainability,User perception},
  pages = {14:1-14:1},
  articleno = {14},
  numpages = {1},
  acmid = {3183683}
}

@inproceedings{Dodge:2019:EME:3301275.3302310,
  series = {{{IUI}} '19},
  title = {Explaining {{Models}}: {{An Empirical Study}} of {{How Explanations Impact Fairness Judgment}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302310},
  author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
  year = {2019},
  keywords = {empirical studies,explanation,fairness,machine learning},
  pages = {275-285},
  numpages = {11},
  acmid = {3302310}
}

@inproceedings{Zhang:2018:DAP:3301551.3301588,
  series = {{{ICIT}} 2018},
  title = {Detecting {{Adversarial Perturbations}} with {{Salieny}}},
  isbn = {978-1-4503-6629-8},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Information Technology}}: {{IoT}} and {{Smart City}}},
  publisher = {{ACM}},
  doi = {10.1145/3301551.3301588},
  author = {Zhang, Chiliang and Yang, Zhimou and Ye, Zuochang},
  year = {2018},
  keywords = {Adversarial Examples,Convolutional Neural Networks,Model Interpretation,Saliency},
  pages = {25-30},
  numpages = {6},
  acmid = {3301588}
}

@inproceedings{Pynadath:2018:CBR:3237383.3237923,
  series = {{{AAMAS}} '18},
  title = {Clustering {{Behavior}} to {{Recognize Subjective Beliefs}} in {{Human}}-{{Agent Teams}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Pynadath, David V. and Wang, Ning and Rovira, Ericka and Barnes, Michael J.},
  year = {2018},
  keywords = {affect recognition,explainable ai,human-agent teams,trust},
  pages = {1495-1503},
  numpages = {9},
  acmid = {3237923}
}

@inproceedings{Lim:2019:EES:3308557.3313112,
  series = {{{IUI}} '19},
  title = {{{ExSS}}: {{Explainable Smart Systems}} 2019},
  isbn = {978-1-4503-6673-1},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}: {{Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3308557.3313112},
  author = {Lim, Brian and Sarkar, Advait and {Smith-Renner}, Alison and Stumpf, Simone},
  year = {2019},
  keywords = {explanations,intelligent systems,intelligibility,machine learning,transparency,visualizations},
  pages = {125-126},
  numpages = {2},
  acmid = {3313112}
}

@article{Wyner:2017:ESA:3122009.3153004,
  title = {Explaining the {{Success}} of {{Adaboost}} and {{Random Forests As Interpolating Classifiers}}},
  volume = {18},
  issn = {1532-4435},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  author = {Wyner, Abraham J. and Olson, Matthew and Bleich, Justin and Mease, David},
  month = jan,
  year = {2017},
  keywords = {adaboost,classification,overfitting,random forests,tree-ensembles},
  pages = {1558-1590},
  publisher = {{JMLR.org}},
  issue_date = {January 2017},
  numpages = {33},
  acmid = {3153004}
}

@inproceedings{Green:2009:GTS:1639714.1639768,
  series = {{{RecSys}} '09},
  title = {Generating {{Transparent}}, {{Steerable Recommendations}} from {{Textual Descriptions}} of {{Items}}},
  isbn = {978-1-60558-435-5},
  booktitle = {Proceedings of the {{Third ACM Conference}} on {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/1639714.1639768},
  author = {Green, Stephen J. and Lamere, Paul and Alexander, Jeffrey and Maillet, Fran{\c c}ois and Kirk, Susanna and Holt, Jessica and Bourque, Jackie and Mak, Xiao-Wen},
  year = {2009},
  keywords = {explainable recommender,steerable recommender},
  pages = {281-284},
  numpages = {4},
  acmid = {1639768}
}

@inproceedings{Xiong:2017:ENA:3077136.3080809,
  series = {{{SIGIR}} '17},
  title = {End-to-{{End Neural Ad}}-Hoc {{Ranking}} with {{Kernel Pooling}}},
  isbn = {978-1-4503-5022-8},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3077136.3080809},
  author = {Xiong, Chenyan and Dai, Zhuyun and Callan, Jamie and Liu, Zhiyuan and Power, Russell},
  year = {2017},
  keywords = {embedding,kernel pooling,neural ir,ranking,relevance model},
  pages = {55-64},
  numpages = {10},
  acmid = {3080809}
}

@inproceedings{St.Amant:2003:BEI:604045.604074,
  series = {{{IUI}} '03},
  title = {Balancing {{Efficiency}} and {{Interpretability}} in an {{Interactive Statistical Assistant}}},
  isbn = {1-58113-586-6},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/604045.604074},
  author = {St. Amant, Robert and Dinardo, Michael D. and Buckner, Nickie},
  year = {2003},
  keywords = {data mountain,efficiency,interpretability,navigation,user interface design},
  pages = {181-188},
  numpages = {8},
  acmid = {604074}
}

@inproceedings{Beaton:2018:CAH:3173386.3173391,
  series = {{{HRI}} '18},
  title = {Crucial {{Answers About Humanoid Capital}}},
  isbn = {978-1-4503-5615-2},
  booktitle = {Companion of the 2018 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3173386.3173391},
  author = {Beaton, Brian},
  year = {2018},
  keywords = {capital,explainable artificial intelligence,humanoid robots,xai},
  pages = {5-12},
  numpages = {8},
  acmid = {3173391}
}

@article{Sidner:1981:FIP:972911.972912,
  title = {Focusing for {{Interpretation}} of {{Pronouns}}},
  volume = {7},
  issn = {0891-2017},
  number = {4},
  journal = {Comput. Linguist.},
  author = {Sidner, Candace L.},
  month = oct,
  year = {1981},
  pages = {217-231},
  publisher = {{MIT Press}},
  location = {Cambridge, MA, USA},
  issue_date = {October-December 1981},
  numpages = {15},
  acmid = {972912}
}

@inproceedings{Tsai:2018:ESR:3180308.3180368,
  series = {{{IUI}} '18 {{Companion}}},
  title = {Explaining {{Social Recommendations}} to {{Casual Users}}: {{Design Principles}} and {{Opportunities}}},
  isbn = {978-1-4503-5571-1},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3180308.3180368},
  author = {Tsai, Chun-Hua and Brusilovsky, Peter},
  year = {2018},
  keywords = {Explainable Intelligent User Interface,Social Recommender},
  pages = {59:1-59:2},
  articleno = {59},
  numpages = {2},
  acmid = {3180368}
}

@inproceedings{Winnemoller:2004:CTS:1628275.1628278,
  series = {{{TextMean}} '04},
  title = {Constructing {{Text Sense Representations}}},
  booktitle = {Proceedings of the {{2Nd Workshop}} on {{Text Meaning}} and {{Interpretation}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Winnem\"oller, Ronald},
  year = {2004},
  pages = {17-24},
  numpages = {8},
  acmid = {1628278}
}

@inproceedings{Chelmis:2017:AAS:3091478.3091518,
  series = {{{WebSci}} '17},
  title = {{{ASSIST}}: {{Automatic Summarization}} of {{Significant Structural Changes}} in {{Large Temporal Graphs}}},
  isbn = {978-1-4503-4896-6},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Web Science Conference}}},
  publisher = {{ACM}},
  doi = {10.1145/3091478.3091518},
  author = {Chelmis, Charalampos and Dani, Reshul},
  year = {2017},
  keywords = {anomaly summarization,change attribution,dynamic graph,structural change interpretation},
  pages = {201-205},
  numpages = {5},
  acmid = {3091518}
}

@inproceedings{Brun:2014:LFI:2682648.2682851,
  series = {{{WI}}-{{IAT}} '14},
  title = {Can {{Latent Features Be Interpreted As Users}} in {{Matrix Factorization}}-{{Based Recommender Systems}}?},
  isbn = {978-1-4799-4143-8},
  booktitle = {Proceedings of the 2014 {{IEEE}}/{{WIC}}/{{ACM International Joint Conferences}} on {{Web Intelligence}} ({{WI}}) and {{Intelligent Agent Technologies}} ({{IAT}}) - {{Volume}} 02},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WI-IAT.2014.102},
  author = {Brun, Armelle and Aleksandrova, Marharyta and Boyer, Anne},
  year = {2014},
  keywords = {Cold-start problem,Feature interpretation,Matrix factorization,Recommender systems},
  pages = {226-233},
  numpages = {8},
  acmid = {2682851}
}

@inproceedings{Hicks:2018:CRA:3204949.3208113,
  series = {{{MMSys}} '18},
  title = {Comprehensible {{Reasoning}} and {{Automated Reporting}} of {{Medical Examinations Based}} on {{Deep Learning Analysis}}},
  isbn = {978-1-4503-5192-8},
  booktitle = {Proceedings of the 9th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM}},
  doi = {10.1145/3204949.3208113},
  author = {Hicks, Steven Alexander and Pogorelov, Konstantin and {de Lange}, Thomas and Lux, Mathias and Jeppsson, Mattis and Randel, Kristin Ranheim and Eskeland, Sigrun and Halvorsen, P\aa{}l and Riegler, Michael},
  year = {2018},
  keywords = {automatic disease detection,deep learning,interpretable neural networks,medical documentation},
  pages = {490-493},
  numpages = {4},
  acmid = {3208113}
}

@inproceedings{Weisz:2019:BTS:3301275.3302290,
  series = {{{IUI}} '19},
  title = {{{BigBlueBot}}: {{Teaching Strategies}} for {{Successful Human}}-Agent {{Interactions}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302290},
  author = {Weisz, Justin D. and Jain, Mohit and Joshi, Narendra Nath and Johnson, James and Lange, Ingrid},
  year = {2019},
  keywords = {conversational agents,explainable AI,mechanical turk},
  pages = {448-459},
  numpages = {12},
  acmid = {3302290}
}

@article{Ness:1970:CEG:362007.362042,
  title = {Computer {{Education}} in a {{Graduate School}} of {{Management}}},
  volume = {13},
  issn = {0001-0782},
  number = {2},
  journal = {Commun. ACM},
  doi = {10.1145/362007.362042},
  author = {Ness, D. N. and Green, R. S. and Martin, W. A.},
  month = feb,
  year = {1970},
  keywords = {education,interpreters,machine language,management information systems,simulation},
  pages = {110-114},
  publisher = {{ACM}},
  location = {New York, NY, USA},
  issue_date = {Feb 1970},
  numpages = {5},
  acmid = {362042}
}

@inproceedings{Chen:2014:FFD:2623330.2623627,
  series = {{{KDD}} '14},
  title = {Fast {{Flux Discriminant}} for {{Large}}-Scale {{Sparse Nonlinear Classification}}},
  isbn = {978-1-4503-2956-9},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/2623330.2623627},
  author = {Chen, Wenlin and Chen, Yixin and Weinberger, Kilian Q.},
  year = {2014},
  keywords = {classification,interpretability,sparsity,submodularity},
  pages = {621-630},
  numpages = {10},
  acmid = {2623627}
}

@inproceedings{Barria-Pineda:2019:EER:3308557.3308690,
  series = {{{IUI}} '19},
  title = {Explaining {{Educational Recommendations Through}} a {{Concept}}-Level {{Knowledge Visualization}}},
  isbn = {978-1-4503-6673-1},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}: {{Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3308557.3308690},
  author = {{Barria-Pineda}, Jordan and Brusilovsky, Peter},
  year = {2019},
  keywords = {educational recommendations,explainability,open learner models},
  pages = {103-104},
  numpages = {2},
  acmid = {3308690}
}

@article{http://arxiv.org/abs/1401.5390v1,
  title = {Learning to {{Win}} by {{Reading Manuals}} in a {{Monte}}-{{Carlo Framework}}},
  abstract = {Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34\% absolute improvement and winning over 65\% of games when playing against the built-in AI of Civilization.},
  journal = {arxiv},
  author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
  month = jan,
  year = {2014}
}

@article{http://arxiv.org/abs/1608.08974v2,
  title = {Towards {{Transparent AI Systems}}: {{Interpreting Visual Question Answering Models}}},
  abstract = {Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.},
  journal = {arxiv},
  author = {Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
  month = aug,
  year = {2016}
}

@article{http://arxiv.org/abs/1609.09869v2,
  title = {Structured {{Inference Networks}} for {{Nonlinear State Space Models}}},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
  journal = {arxiv},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  month = sep,
  year = {2016}
}

@article{http://arxiv.org/abs/1611.07429v1,
  title = {{{TreeView}}: {{Peeking}} into {{Deep Neural Networks Via Feature}}-{{Space Partitioning}}},
  abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior mythos. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
  journal = {arxiv},
  author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
  month = nov,
  year = {2016}
}

@article{http://arxiv.org/abs/1701.04489v1,
  title = {Towards a {{New Interpretation}} of {{Separable Convolutions}}},
  abstract = {In recent times, the use of separable convolutions in deep convolutional neural network architectures has been explored. Several researchers, most notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in their deep architectures and have demonstrated state of the art or close to state of the art performance. However, the underlying mechanism of action of separable convolutions are still not fully understood. Although their mathematical definition is well understood as a depthwise convolution followed by a pointwise convolution, deeper interpretations such as the extreme Inception hypothesis (Chollet, 2016) have failed to provide a thorough explanation of their efficacy. In this paper, we propose a hybrid interpretation that we believe is a better model for explaining the efficacy of separable convolutions.},
  journal = {arxiv},
  author = {Ghosh, Tapabrata},
  month = jan,
  year = {2017}
}

@article{http://arxiv.org/abs/1702.08608v2,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  journal = {arxiv},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  month = feb,
  year = {2017}
}

@article{http://arxiv.org/abs/1704.03296v3,
  title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
  abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
  journal = {arxiv},
  author = {Fong, Ruth and Vedaldi, Andrea},
  month = apr,
  year = {2017}
}

@article{http://arxiv.org/abs/1707.01154v1,
  title = {Interpretable \& {{Explorable Approximations}} of {{Black Box Models}}},
  abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
  journal = {arxiv},
  author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
  month = jul,
  year = {2017}
}

@article{http://arxiv.org/abs/1708.04988v1,
  title = {Warp: A Method for Neural Network Interpretability Applied to Gene Expression Profiles},
  abstract = {We show a proof of principle for warping, a method to interpret the inner working of neural networks in the context of gene expression analysis. Warping is an efficient way to gain insight to the inner workings of neural nets and make them more interpretable. We demonstrate the ability of warping to recover meaningful information for a given class on a samplespecific individual basis. We found warping works well in both linearly and nonlinearly separable datasets. These encouraging results show that warping has a potential to be the answer to neural networks interpretability in computational biology.},
  journal = {arxiv},
  author = {Assya, Trofimov and Sebastien, Lemieux and Claude, Perreault},
  month = aug,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.00794v1,
  title = {What {{Does Explainable AI Really Mean}}? {{A New Conceptualization}} of {{Perspectives}}},
  abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  journal = {arxiv},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.06431v2,
  title = {Using {{KL}}-Divergence to Focus {{Deep Visual Explanation}}},
  abstract = {We present a method for explaining the image classification predictions of deep convolution neural networks, by highlighting the pixels in the image which influence the final class prediction. Our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction, and here we use Kullback-Leibler divergence to provide this focus. Overall, our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks. In this brief paper, our experiments evaluate the performance of two popular networks in this context of interpretability.},
  journal = {arxiv},
  author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.02034v2,
  title = {{{SMILES2Vec}}: {{An Interpretable General}}-{{Purpose Deep Neural Network}} for {{Predicting Chemical Properties}}},
  abstract = {Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88\%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.},
  journal = {arxiv},
  author = {Goh, Garrett B. and Hodas, Nathan O. and Siegel, Charles and Vishnu, Abhinav},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.06302v3,
  title = {Visual {{Explanation}} by {{Interpretation}}: {{Improving Visual Feedback Capabilities}} of {{Deep Neural Networks}}},
  abstract = {Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by stridded operations in deconvNet-based visualizations. Moreover, we introduce an8Flower, a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest},
  journal = {arxiv},
  author = {Oramas, Jose and Wang, Kaili and Tuytelaars, Tinne},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.06657v1,
  title = {Towards the {{Augmented Pathologist}}: {{Challenges}} of {{Explainable}}-{{AI}} in {{Digital Pathology}}},
  abstract = {Digital pathology is not only one of the most promising fields of diagnostic medicine, but at the same time a hot topic for fundamental research. Digital pathology is not just the transfer of histopathological slides into digital representations. The combination of different data sources (images, patient records, and *omics data) together with current advances in artificial intelligence/machine learning enable to make novel information accessible and quantifiable to a human expert, which is not yet available and not exploited in current medical settings. The grand goal is to reach a level of usable intelligence to understand the data in the context of an application task, thereby making machine decisions transparent, interpretable and explainable. The foundation of such an "augmented pathologist" needs an integrated approach: While machine learning algorithms require many thousands of training examples, a human expert is often confronted with only a few data points. Interestingly, humans can learn from such few examples and are able to instantly interpret complex patterns. Consequently, the grand goal is to combine the possibilities of artificial intelligence with human intelligence and to find a well-suited balance between them to enable what neither of them could do on their own. This can raise the quality of education, diagnosis, prognosis and prediction of cancer and other diseases. In this paper we describe some (incomplete) research issues which we believe should be addressed in an integrated and concerted effort for paving the way towards the augmented pathologist.},
  journal = {arxiv},
  author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and M\"uller, Heimo and Reihs, Robert and Zatloukal, Kurt},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1801.09808v1,
  title = {The {{Intriguing Properties}} of {{Model Explanations}}},
  abstract = {Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions. In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs). We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect the performance of the model. Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous.},
  journal = {arxiv},
  author = {{Al-Shedivat}, Maruan and Dubey, Avinava and Xing, Eric P.},
  month = jan,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.03043v1,
  title = {{{PoTrojan}}: Powerful Neural-Level Trojan Designs in Deep Learning Models},
  abstract = {With the popularity of deep learning (DL), artificial intelligence (AI) has been applied in many areas of human life. Neural network or artificial neural network (NN), the main technique behind DL, has been extensively studied to facilitate computer vision and natural language recognition. However, the more we rely on information technology, the more vulnerable we are. That is, malicious NNs could bring huge threat in the so-called coming AI era. In this paper, for the first time in the literature, we propose a novel approach to design and insert powerful neural-level trojans or PoTrojan in pre-trained NN models. Most of the time, PoTrojans remain inactive, not affecting the normal functions of their host NN models. PoTrojans could only be triggered in very rare conditions. Once activated, however, the PoTrojans could cause the host NN models to malfunction, either falsely predicting or classifying, which is a significant threat to human society of the AI era. We would explain the principles of PoTrojans and the easiness of designing and inserting them in pre-trained deep learning models. PoTrojans doesn't modify the existing architecture or parameters of the pre-trained models, without re-training. Hence, the proposed method is very efficient.},
  journal = {arxiv},
  author = {Zou, Minhui and Shi, Yang and Wang, Chengliang and Li, Fangyu and Song, WenZhan and Wang, Yu},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.03788v2,
  title = {Influence-{{Directed Explanations}} for {{Deep Convolutional Networks}}},
  abstract = {We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the "essence" of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.},
  journal = {arxiv},
  author = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.08235v1,
  title = {Vector {{Field Based Neural Networks}}},
  abstract = {A novel Neural Network architecture is proposed using the mathematically and physically rich idea of vector fields as hidden layers to perform nonlinear transformations in the data. The data points are interpreted as particles moving along a flow defined by the vector field which intuitively represents the desired movement to enable classification. The architecture moves the data points from their original configuration to anew one following the streamlines of the vector field with the objective of achieving a final configuration where classes are separable. An optimization problem is solved through gradient descent to learn this vector field.},
  journal = {arxiv},
  author = {Vieira, Daniel and Rangel, Fabio and Firmino, Fabricio and Paixao, Joao},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1803.04263v3,
  title = {The {{Challenge}} of {{Crafting Intelligible Intelligence}}},
  abstract = {Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.},
  journal = {arxiv},
  author = {Weld, Daniel S. and Bansal, Gagan},
  month = mar,
  year = {2018}
}

@article{http://arxiv.org/abs/1803.11261v1,
  title = {How an {{Electrical Engineer Became}} an {{Artificial Intelligence Researcher}}, a {{Multiphase Active Contours Analysis}}},
  abstract = {This essay examines how what is considered to be artificial intelligence (AI) has changed over time and come to intersect with the expertise of the author. Initially, AI developed on a separate trajectory, both topically and institutionally, from pattern recognition, neural information processing, decision and control systems, and allied topics by focusing on symbolic systems within computer science departments rather than on continuous systems in electrical engineering departments. The separate evolutions continued throughout the author's lifetime, with some crossover in reinforcement learning and graphical models, but were shocked into converging by the virality of deep learning, thus making an electrical engineer into an AI researcher. Now that this convergence has happened, opportunity exists to pursue an agenda that combines learning and reasoning bridged by interpretable machine learning models.},
  journal = {arxiv},
  author = {Varshney, Kush R.},
  month = mar,
  year = {2018}
}

@article{http://arxiv.org/abs/1805.10820v1,
  title = {Local {{Rule}}-{{Based Explanations}} of {{Black Box Decision Systems}}},
  abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. \%Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
  journal = {arxiv},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  month = may,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.00050v1,
  title = {Interpretable {{Set Functions}}},
  abstract = {We propose learning flexible but interpretable functions that aggregate a variable-length set of permutation-invariant feature vectors to predict a label. We use a deep lattice network model so we can architect the model structure to enhance interpretability, and add monotonicity constraints between inputs-and-outputs. We then use the proposed set function to automate the engineering of dense, interpretable features from sparse categorical features, which we call semantic feature engine. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, and is easier to debug and understand.},
  journal = {arxiv},
  author = {Cotter, Andrew and Gupta, Maya and Jiang, Heinrich and Muller, James and Narayan, Taman and Wang, Serena and Zhu, Tao},
  month = may,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.01261v3,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  journal = {arxiv},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.05337v2,
  title = {Hierarchical Interpretations for Neural Network Predictions},
  abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.},
  journal = {arxiv},
  author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.07538v2,
  title = {Towards {{Robust Interpretability}} with {{Self}}-{{Explaining Neural Networks}}},
  abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating \emph{a posteriori} explanations for previously trained models around specific predictions. \emph{Self-explaining} models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
  journal = {arxiv},
  author = {{Alvarez-Melis}, David and Jaakkola, Tommi S.},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.08340v1,
  title = {Interpretable {{Discovery}} in {{Large Image Data Sets}}},
  abstract = {Automated detection of new, interesting, unusual, or anomalous images within large data sets has great value for applications from surveillance (e.g., airport security) to science (observations that don't fit a given theory can lead to new discoveries). Many image data analysis systems are turning to convolutional neural networks (CNNs) to represent image content due to their success in achieving high classification accuracy rates. However, CNN representations are notoriously difficult for humans to interpret. We describe a new strategy that combines novelty detection with CNN image features to achieve rapid discovery with interpretable explanations of novel image content. We applied this technique to familiar images from ImageNet as well as to a scientific image collection from planetary science.},
  journal = {arxiv},
  author = {Wagstaff, Kiri L. and Lee, Jake},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.03418v1,
  title = {Interpreting and {{Explaining Deep Neural Networks}} for {{Classification}} of {{Audio Signals}}},
  abstract = {Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions. In this paper, two neural network architectures are trained on spectrogram and raw waveform data for audio classification tasks on a newly created audio dataset and layer-wise relevance propagation (LRP), a previously proposed interpretability method, is applied to investigate the models' feature selection and decision making. It is demonstrated that the networks are highly reliant on feature marked as relevant by LRP through systematic manipulation of the input data. Our results show that by making deep audio classifiers interpretable, one can analyze and compare the properties and strategies of different models beyond classification accuracy, which potentially opens up new ways for model improvements.},
  journal = {arxiv},
  author = {Becker, S\"oren and Ackermann, Marcel and Lapuschkin, Sebastian and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.00033v2,
  title = {Techniques for {{Interpretable Machine Learning}}},
  abstract = {Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.},
  journal = {arxiv},
  author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.01591v1,
  title = {{{LISA}}: {{Explaining Recurrent Neural Network Judgments}} via {{Layer}}-{{wIse Semantic Accumulation}} and {{Example}} to {{Pattern Transformation}}},
  abstract = {Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: "How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response" (2) Example2pattern: "How the saliency patterns look like for each category in the data according to the network in decision making". We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.},
  journal = {arxiv},
  author = {Gupta, Pankaj and Sch\"utze, Hinrich},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
  title = {Learning {{Explanations}} from {{Language Data}}},
  abstract = {PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.},
  journal = {arxiv},
  author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1809.10315v2,
  title = {Smooth {{Inter}}-Layer {{Propagation}} of {{Stabilized Neural Networks}} for {{Classification}}},
  abstract = {Recent work has studied the reasons for the remarkable performance of deep neural networks in image classification. We examine batch normalization on the one hand and the dynamical systems view of residual networks on the other hand. Our goal is in understanding the notions of stability and smoothness of the inter-layer propagation of ResNets so as to explain when they contribute to significantly enhanced performance. We postulate that such stability is of importance for the trained ResNet to transfer.},
  journal = {arxiv},
  author = {Zhang, Jingfeng and Wynter, Laura},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.02689v2,
  title = {Hows and {{Whys}} of {{Artificial Intelligence}} for {{Public Sector Decisions}}: {{Explanation}} and {{Evaluation}}},
  abstract = {Evaluation has always been a key challenge in the development of artificial intelligence (AI) based software, due to the technical complexity of the software artifact and, often, its embedding in complex sociotechnical processes. Recent advances in machine learning (ML) enabled by deep neural networks has exacerbated the challenge of evaluating such software due to the opaque nature of these ML-based artifacts. A key related issue is the (in)ability of such systems to generate useful explanations of their outputs, and we argue that the explanation and evaluation problems are closely linked. The paper models the elements of a ML-based AI system in the context of public sector decision (PSD) applications involving both artificial and human intelligence, and maps these elements against issues in both evaluation and explanation, showing how the two are related. We consider a number of common PSD application patterns in the light of our model, and identify a set of key issues connected to explanation and evaluation in each case. Finally, we propose multiple strategies to promote wider adoption of AI/ML technologies in PSD, where each is distinguished by a focus on different elements of our model, allowing PSD policy makers to adopt an approach that best fits their context and concerns.},
  journal = {arxiv},
  author = {Preece, Alun and Ashelford, Rob and Armstrong, Harry and Braines, Dave},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.04053v1,
  title = {The 30-{{Year Cycle In The AI Debate}}},
  abstract = {In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.},
  journal = {arxiv},
  author = {Chauvet, Jean-Marie},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.10862v4,
  title = {Multiparty {{Dynamics}} and {{Failure Modes}} for {{Machine Learning}} and {{Artificial Intelligence}}},
  abstract = {An important challenge for safety in machine learning and artificial intelligence systems is a~set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart's or Campbell's law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.},
  journal = {arxiv},
  author = {Manheim, David},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.13192v4,
  title = {Nearly-Tight Bounds on Linear Regions of Piecewise Linear Neural Networks},
  abstract = {The developments of deep neural networks (DNN) in recent years have ushered a brand new era of artificial intelligence. DNNs are proved to be excellent in solving very complex problems, e.g., visual recognition and text understanding, to the extent of competing with or even surpassing people. Despite inspiring and encouraging success of DNNs, thorough theoretical analyses still lack to unravel the mystery of their magics. The design of DNN structure is dominated by empirical results in terms of network depth, number of neurons and activations. A few of remarkable works published recently in an attempt to interpret DNNs have established the first glimpses of their internal mechanisms. Nevertheless, research on exploring how DNNs operate is still at the initial stage with plenty of room for refinement. In this paper, we extend precedent research on neural networks with piecewise linear activations (PLNN) concerning linear regions bounds. We present (i) the exact maximal number of linear regions for single layer PLNNs; (ii) a upper bound for multi-layer PLNNs; and (iii) a tighter upper bound for the maximal number of liner regions on rectifier networks. The derived bounds also indirectly explain why deep models are more powerful than shallow counterparts, and how non-linearity of activation functions impacts on expressiveness of networks.},
  journal = {arxiv},
  author = {Hu, Qiang and Zhang, Hao},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.00196v1,
  title = {Towards {{Explainable NLP}}: {{A Generative Explanation Framework}} for {{Text Classification}}},
  abstract = {Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information is often ignored, and the systems do not explicitly generate the human-readable explanations. To better alleviate this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.},
  journal = {arxiv},
  author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.02783v1,
  title = {{{YASENN}}: {{Explaining Neural Networks}} via {{Partitioning Activation Sequences}}},
  abstract = {We introduce a novel approach to feed-forward neural network interpretation based on partitioning the space of sequences of neuron activations. In line with this approach, we propose a model-specific interpretation method, called YASENN. Our method inherits many advantages of model-agnostic distillation, such as an ability to focus on the particular input region and to express an explanation in terms of features different from those observed by a neural network. Moreover, examination of distillation error makes the method applicable to the problems with low tolerance to interpretation mistakes. Technically, YASENN distills the network with an ensemble of layer-wise gradient boosting decision trees and encodes the sequences of neuron activations with leaf indices. The finite number of unique codes induces a partitioning of the input space. Each partition may be described in a variety of ways, including examination of an interpretable model (e.g. a logistic regression or a decision tree) trained to discriminate between objects of those partitions. Our experiments provide an intuition behind the method and demonstrate revealed artifacts in neural network decision making.},
  journal = {arxiv},
  author = {Zharov, Yaroslav and Korzhenkov, Denis and Shvechikov, Pavel and Tuzhilin, Alexander},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.06471v2,
  title = {Towards {{Explainable Deep Learning}} for {{Credit Lending}}: {{A Case Study}}},
  abstract = {Deep learning adoption in the financial services industry has been limited due to a lack of model interpretability. However, several techniques have been proposed to explain predictions made by a neural network. We provide an initial investigation into these techniques for the assessment of credit risk with neural networks.},
  journal = {arxiv},
  author = {Modarres, Ceena and Ibrahim, Mark and Louie, Melissa and Paisley, John},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.09725v1,
  title = {Interpretable {{Convolutional Filters}} with {{SincNet}}},
  abstract = {Deep learning is currently playing a crucial role toward higher levels of artificial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal "black-box" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques. This paper summarizes our recent efforts to develop a more interpretable neural model for directly processing speech from the raw waveform. In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover more meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized filter-bank front-end, that only depends on some parameters with a clear physical meaning. Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more interpretable than standard CNNs.},
  journal = {arxiv},
  author = {Ravanelli, Mirco and Bengio, Yoshua},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
  title = {Sensitivity Based {{Neural Networks Explanations}}},
  abstract = {Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque "black boxes". The difficulty of interpreting the predictions of a neural network often prevents its use in fields where explainability is important, such as the financial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.},
  journal = {arxiv},
  author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.01214v2,
  title = {Prototype-Based {{Neural Network Layers}}: {{Incorporating Vector Quantization}}},
  abstract = {Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.},
  journal = {arxiv},
  author = {Saralajew, Sascha and Holdijk, Lars and Rees, Maike and Villmann, Thomas},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.10537v2,
  title = {Prediction of {{Industrial Process Parameters}} Using {{Artificial Intelligence Algorithms}}},
  abstract = {In the present paper, a method of defining the industrial process parameters for a new product using machine learning algorithms will be presented. The study will describe how to go from the product characteristics till the prediction of the suitable machine parameters to produce a good quality of this product, and this is based on an historical training dataset of similar products with their respective process parameters. In the first part of our study, we will focus on the ultrasonic welding process definition, welding parameters and on how it operate. While in second part, we present the design and implementation of the prediction models such multiple linear regression, support vector regression, and we compare them to an artificial neural networks algorithm. In the following part, we present a new application of Convolutional Neural Networks (CNN) to the industrial process parameters prediction. In addition, we will propose the generalization approach of our CNN to any prediction problem of industrial process parameters. Finally the results of the four methods will be interpreted and discussed.},
  journal = {arxiv},
  author = {Khdoudi, Abdelmoula and Masrour, Tawfik},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1901.07538v1,
  title = {Unsupervised {{Learning}} of {{Neural Networks}} to {{Explain Neural Networks}} (Extended Abstract)},
  abstract = {This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., the explainer uses interpretable visual concepts to explain features in middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the explainer performs like an auto-encoder, which decomposes the feature maps into object-part features. The object-part features are learned to reconstruct CNN features without much loss of information. We can consider the disentangled representations of object parts a paraphrase of CNN features, which help people understand the knowledge encoded by the CNN. More crucially, we learn the explainer via knowledge distillation without using any annotations of object parts or textures for supervision. In experiments, our method was widely used to interpret features of different benchmark CNNs, and explainers significantly boosted the feature interpretability without hurting the discrimination power of the CNNs.},
  journal = {arxiv},
  author = {Zhang, Quanshi and Yang, Yu and Wu, Ying Nian},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.08547v1,
  title = {Human-Centric {{Transfer Learning Explanation}} via {{Knowledge Graph}} [{{Extended Abstract}}]},
  abstract = {Transfer learning which aims at utilizing knowledge learned from one problem (source domain) to solve another different but related problem (target domain) has attracted wide research attentions. However, the current transfer learning methods are mostly uninterpretable, especially to people without ML expertise. In this extended abstract, we brief introduce two knowledge graph (KG) based frameworks towards human understandable transfer learning explanation. The first one explains the transferability of features learned by Convolutional Neural Network (CNN) from one domain to another through pre-training and fine-tuning, while the second justifies the model of a target domain predicted by models from multiple source domains in zero-shot learning (ZSL). Both methods utilize KG and its reasoning capability to provide rich and human understandable explanations to the transfer procedure.},
  journal = {arxiv},
  author = {Geng, Yuxia and Chen, Jiaoyan and {Jimenez-Ruiz}, Ernesto and Chen, Huajun},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.10040v1,
  title = {Towards {{Aggregating Weighted Feature Attributions}}},
  abstract = {Current approaches for explaining machine learning models fall into two distinct classes: antecedent event influence and value attribution. The former leverages training instances to describe how much influence a training point exerts on a test point, while the latter attempts to attribute value to the features most pertinent to a given prediction. In this work, we discuss an algorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two explanation classes to form a new approach to feature attribution that not only retrieves local explanations but also captures global patterns learned by a model. Our experimentation convincingly favors weighting and aggregating feature attributions via AVA.},
  journal = {arxiv},
  author = {Bhatt, Umang and Ravikumar, Pradeep and Moura, Jose M. F.},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.11184v1,
  title = {Human-{{Centered Artificial Intelligence}} and {{Machine Learning}}},
  abstract = {Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.},
  journal = {arxiv},
  author = {Riedl, Mark O.},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.04704v2,
  title = {Neural Network Models and Deep Learning - a Primer for Biologists},
  abstract = {Originally inspired by neurobiology, deep neural network models have become a powerful tool of machine learning and artificial intelligence, where they are used to approximate functions and dynamics by learning from examples. Here we give a brief introduction to neural network models and deep learning for biologists. We introduce feedforward and recurrent networks and explain the expressive power of this modeling framework and the backpropagation algorithm for setting the parameters. Finally, we consider how deep neural networks might help us understand the brain's computations.},
  journal = {arxiv},
  author = {Kriegeskorte, Nikolaus and Golan, Tal},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
  title = {{{iBreakDown}}: {{Uncertainty}} of {{Model Explanations}} for {{Non}}-Additive {{Predictive Models}}},
  abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently. Explainability is being presented as a remedy for lack of trust in model predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise instance level interpretability for any complex machine learning model. But how certain are these explanations? Can we rely on additive explanations for non-additive models? In this paper, we examine the behavior of model explainers under the presence of interactions. We define two sources of uncertainty, model level uncertainty, and explanation level uncertainty. We show that adding interactions reduces explanation level uncertainty. We introduce a new method iBreakDown that generates non-additive explanations with local interaction.},
  journal = {arxiv},
  author = {Gosiewska, Alicja and Biecek, Przemyslaw},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.08939v1,
  title = {Understanding {{Neural Networks}} via {{Feature Visualization}}: {{A}} Survey},
  abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
  journal = {arxiv},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1905.00122v1,
  title = {To Believe or Not to Believe: {{Validating}} Explanation Fidelity for Dynamic Malware Analysis},
  abstract = {Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications.},
  journal = {arxiv},
  author = {Chen, Li and Yagemann, Carter and Downing, Evan},
  month = apr,
  year = {2019}
}

@inproceedings{8419428,
  title = {Interpretable {{Machine Learning}} in {{Healthcare}}},
  abstract = {This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
  booktitle = {2018 {{IEEE International Conference}} on {{Healthcare Informatics}} ({{ICHI}})},
  doi = {10.1109/ICHI.2018.00095},
  author = {Ahmad, M. A. and Teredesai, A. and Eckert, C.},
  month = jun,
  year = {2018},
  keywords = {Cancer,explainable artificial intelligence,health care,healthcare,interpretable machine learning,interpretable machine learning algorithm,interpretable machine learning models,learning (artificial intelligence),Machine learning,Machine learning algorithms,Prediction algorithms,Predictive models,Tutorials},
  pages = {447-447},
  issn = {2575-2634}
}

@article{4359216,
  title = {Maxi\textendash{{Min Margin Machine}}: {{Learning Large Margin Classifiers Locally}} and {{Globally}}},
  volume = {19},
  issn = {1045-9227},
  abstract = {In this paper, we propose a novel large margin classifier, called the maxi-min margin machine (M4). This model learns the decision boundary both locally and globally. In comparison, other large margin classifiers construct separating hyperplanes only either locally or globally. For example, a state-of-the-art large margin classifier, the support vector machine (SVM), considers data only locally, while another significant model, the minimax probability machine (MPM), focuses on building the decision hyperplane exclusively based on the global information. As a major contribution, we show that SVM yields the same solution as M4when data satisfy certain conditions, and MPM can be regarded as a relaxation model of M4. Moreover, based on our proposed local and global view of data, another popular model, the linear discriminant analysis, can easily be interpreted and extended as well. We describe the M4model definition, provide a geometrical interpretation, present theoretical justifications, and propose a practical sequential conic programming method to solve the optimization problem. We also show how to exploit Mercer kernels to extend M4for nonlinear classifications. Furthermore, we perform a series of evaluations on both synthetic data sets and real-world benchmark data sets. Comparison with SVM and MPM demonstrates the advantages of our new model.},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2007.905855},
  author = {Huang, K. and Yang, H. and King, I. and Lyu, M. R.},
  month = feb,
  year = {2008},
  keywords = {Algorithms,Artificial Intelligence,Automated,boundary-value problems,Buildings,Classification,Cluster Analysis,computational geometry,decision boundaries,geometrical interpretation,Kernel,kernel methods,large margin,large margin classifier learning,learning (artificial intelligence),learning locally and globally,linear discriminant analysis,Linear discriminant analysis,Machine learning,maxi-min margin machine,Mercer kernels,minimax probability machine,minimax techniques,Minimax techniques,Neural Networks (Computer),nonlinear classification,nonlinear programming,Optimization methods,optimization problem,pattern classification,Pattern Recognition,Performance evaluation,probability,relaxation model,relaxation theory,second-order cone programming,sequential conic programming method,Solid modeling,Support vector machine classification,support vector machines,Support vector machines},
  pages = {260-272}
}

@inproceedings{8679150,
  title = {The {{Structure}} of {{Deep Neural Network}} for {{Interpretable Transfer Learning}}},
  abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  doi = {10.1109/BIGCOMP.2019.8679150},
  author = {Kim, D. and Lim, W. and Hong, M. and Kim, H.},
  month = feb,
  year = {2019},
  keywords = {complex model,Computational modeling,Convolution,Data models,deep neural network,feature extraction,Feature extraction,feature extraction layer,high-quality data,image classification,interpretability,Interpretability,interpretable features,interpretable transfer learning,Korean characters classification,learning (artificial intelligence),Machine Learning,natural language processing,neural nets,Neural networks,pretrained model,Task analysis,Training,Transfer Learning,transfer learning task},
  pages = {1-4},
  issn = {2375-9356}
}

@article{774103,
  title = {Symbolic Interpretation of Artificial Neural Networks},
  volume = {11},
  issn = {1041-4347},
  abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
  number = {3},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  doi = {10.1109/69.774103},
  author = {Taha, I. A. and Ghosh, J.},
  month = may,
  year = {1999},
  keywords = {adjustable detail level,artificial neural networks,Artificial neural networks,binary rules,breast cancer data set,Computer networks,connectionist architecture,Data mining,domain knowledge mapping,domain knowledge representation,embedded knowledge,explanation,explanation power,feedforward networks,feedforward neural nets,Fuzzy neural networks,Fuzzy sets,hidden layer,hybrid intelligent systems,Intelligent systems,iris data set,knowledge based systems,Knowledge based systems,knowledge refinement,knowledge representation,Knowledge representation,knowledge-based systems,learning (artificial intelligence),Military computing,network training,neural net architecture,Neural networks,output decision validation,partial rules,performance measures,rule evaluation technique,rule extraction,rule ordering,sigmoidal units,symbol manipulation,symbolic interpretation,truth maintenance},
  pages = {448-463}
}

@article{728352,
  title = {The Truth Will Come to Light: Directions and Challenges in Extracting the Knowledge Embedded within Trained Artificial Neural Networks},
  volume = {9},
  issn = {1045-9227},
  abstract = {To date, the preponderance of techniques for eliciting the knowledge embedded in trained artificial neural networks (ANN's) has focused primarily on extracting rule-based explanations from feedforward ANN's. The ADT taxonomy for categorizing such techniques was proposed in 1995 to provide a basis for the systematic comparison of the different approaches. This paper shows that not only is this taxonomy applicable to a cross section of current techniques for extracting rules from trained feedforward ANN's but also how the taxonomy can be adapted and extended to embrace a broader range of ANN types (e,g., recurrent neural networks) and explanation structures. In addition we identify some of the key research questions in extracting the knowledge embedded within ANN's including the need for the formulation of a consistent theoretical basis for what has been, until recently, a disparate collection of empirical results.},
  number = {6},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/72.728352},
  author = {Tickle, A. B. and Andrews, R. and Golea, M. and Diederich, J.},
  month = nov,
  year = {1998},
  keywords = {ADT taxonomy,Artificial neural networks,Automata,explanation,explanation structures,feedforward neural nets,feedforward neural networks,Feedforward neural networks,finite automata,finite state automata,Function approximation,fuzzy neural networks,Fuzzy neural networks,Intelligent networks,knowledge acquisition,knowledge insertion,Neural networks,Pattern recognition,recurrent neural nets,recurrent neural networks,Recurrent neural networks,rule extraction,rule refinement,Taxonomy},
  pages = {1057-1068}
}

@inproceedings{8400040,
  title = {Explainable Artificial Intelligence: {{A}} Survey},
  abstract = {In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
  booktitle = {2018 41st {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  doi = {10.23919/MIPRO.2018.8400040},
  author = {Do{\v s}ilovi\'c, F. K. and Br{\v c}i\'c, M. and Hlupi\'c, N.},
  month = may,
  year = {2018},
  keywords = {(super)human performance,artificial general intelligence,comprehensibility,computing power,datasets,Decision trees,explainability,explainable artificial intelligence,finance,healthcare,image recognition,interpretability,learning (artificial intelligence),Machine learning,machine learning systems,Optimization,Predictive models,recent developments,speech analysis,state-of-the-art models,strategic game planning,supervised learning,Supervised learning,Support vector machines,transparency,XAI},
  pages = {0210-0215},
  note = {ISSN:}
}

@article{5337957,
  title = {Regularized {{Negative Correlation Learning}} for {{Neural Network Ensembles}}},
  volume = {20},
  issn = {1045-9227},
  abstract = {Negative correlation learning (NCL) is a neural network ensemble learning algorithm that introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean square error (MSE) together with the correlation of the ensemble. This paper analyzes NCL and reveals that the training of NCL (when \textquestiondown{} = 1) corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This analysis explains the reason why NCL is prone to overfitting the noise in the training set. This paper also demonstrates that tuning the correlation parameter \textquestiondown{} in NCL by cross validation cannot overcome the overfitting problem. The paper analyzes this problem and proposes the regularized negative correlation learning (RNCL) algorithm which incorporates an additional regularization term for the whole ensemble. RNCL decomposes the ensemble's training objectives, including MSE and regularization, into a set of sub-objectives, and each sub-objective is implemented by an individual neural network. In this paper, we also provide a Bayesian interpretation for RNCL and provide an automatic algorithm to optimize regularization parameters based on Bayesian inference. The RNCL formulation is applicable to any nonlinear estimator minimizing the MSE. The experiments on synthetic as well as real-world data sets demonstrate that RNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set.},
  number = {12},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2009.2034144},
  author = {Chen, H. and Yao, X.},
  month = dec,
  year = {2009},
  keywords = {Algorithm design and analysis,Algorithms,Application software,Automatic Data Processing,Bayes Theorem,Bayesian inference,Bayesian interpretation,Bayesian methods,belief networks,Computational intelligence,Computer-Assisted,Cost function,Ensembles,Humans,Inference algorithms,inference mechanisms,Learning,learning (artificial intelligence),learning machine,Machine learning,mean square error,mean square error methods,Mean square error methods,negative correlation learning (NCL),neural nets,neural network ensemble learning algorithm,neural network ensembles,neural networks,Neural networks,Neural Networks (Computer),Noise level,Nonlinear Dynamics,probabilistic model,regularization,regularized negative correlation learning,Signal Processing},
  pages = {1962-1979}
}

@article{7369936,
  title = {The {{SP Theory}} of {{Intelligence}}: {{Distinctive Features}} and {{Advantages}}},
  volume = {4},
  issn = {2169-3536},
  abstract = {This paper aims to highlight distinctive features of the SP theory of intelligence, realized in the SP computer model, and its apparent advantages compared with some AI-related alternatives. Perhaps most importantly, the theory simplifies and integrates observations and concepts in AI-related areas, and has potential to simplify and integrate of structures and processes in computing systems. Unlike most other AI-related theories, the SP theory is itself a theory of computing, which can be the basis for new architectures for computers. Fundamental in the theory is information compression via the matching and unification of patterns and, more specifically, via a concept of multiple alignment. The theory promotes transparency in the representation and processing of knowledge, and unsupervised learning of natural structures via information compression. It provides an interpretation of aspects of mathematics and an interpretation of phenomena in human perception and cognition. concepts in the theory may be realized in terms of neurons and their inter-connections (SP-neural). These features and advantages of the SP system are discussed in relation to AI-related alternatives: the concept of minimum length encoding and related concepts, how computational and energy efficiency in computing may be achieved, deep learning in neural networks, unified theories of cognition and related research, universal search, Bayesian networks and some other models for AI, IBM's Watson, solving problems associated with big data and in the development of intelligence in autonomous robots, pattern recognition and vision, the learning and processing of natural language, exact and inexact forms of reasoning, representation and processing of diverse forms of knowledge, and software engineering. In conclusion, the SP system can provide a firm foundation for the long-term development of AI and related areas, and at the same time, it may deliver useful results on relatively short timescales.},
  journal = {IEEE Access},
  doi = {10.1109/ACCESS.2015.2513822},
  author = {Wolff, J. G.},
  year = {2016},
  keywords = {AI,artificial intelligence,Artificial intelligence,cognition,computational efficiency,Computational modeling,Computer architecture,deep learning,deep-learning,energy efficiency,human perception,information compression,Information compression,knowledge processing,knowledge representation,learning (artificial intelligence),mathematics,Mathematics,minimum length encoding,multiple alignment,neural nets,neural networks,Neural networks,neuron interconnections,pattern matching,pattern unification,perception,reasoning,SP computer model,SP intelligence theory,SP-neural,Turing machines,unsupervised learning,Unsupervised learning},
  pages = {216-246}
}

@inproceedings{8491679,
  title = {Interval {{Type}}-2 {{Fuzzy Logic Based Stacked Autoencoder Deep Neural Network For Generating Explainable AI Models}} in {{Workforce Optimization}}},
  abstract = {In Utility based industries that employ a large mobile workforce, efficient utilization of field engineers is key to optimal service delivery. The utilization of the engineers can be improved by predicting the future performance of work areas by using machine learning tools such as Deep Neural Networks (DNNs).The dramatic success of DNNs has led to an explosion of its applications. However, the effectiveness of DNNs can be limited by the inability to explain how the models arrived at their predictions.In this paper, we present a novel Type-2 Fuzzy Logic System (FLS) whose inputs are preprocessed by a Stacked Autoencoder Neural Network to add some interpretability to a Deep Neural Network model. The proposed type-2 FLS will contain a small rule set with a small number of antecedents per rule to maximize the model's interpretability. We also present an algorithm which can be used to efficiently train the proposed model.We will compare the proposed model with a Standard Stacked Autoencoder Deep Neural Network, a Multi-Layer Perceptron (MLP) neural network and an Interval Type-2 Fuzzy Logic System.The results show that even though the Standard Stacked Autoencoder and MLP Neural Networks have better performance, they do not provide any insight into the reasoning behind the predictions. The Proposed model, on the other hand, provides better result than the standalone type-2 FLS and a comparable performance to the neural networks and provides a little bit of insight into the decision-making process. Without this insight, we cannot be sure why there is a drop in the performance and we need to further analyze the WA before we can take any decision. This leads to quicker decision making and potentially improving the efficiency of the engineers.},
  booktitle = {2018 {{IEEE International Conference}} on {{Fuzzy Systems}} ({{FUZZ}}-{{IEEE}})},
  doi = {10.1109/FUZZ-IEEE.2018.8491679},
  author = {Chimatapu, R. and Hagras, H. and Starkey, A. and Owusu, G.},
  month = jul,
  year = {2018},
  keywords = {Big Bang - Big Crunch,decision making,decision-making process,Deep Neural Networks,DNNs,explainable AI models,Explainable Artificial Intelligence,field engineers,fuzzy logic,Fuzzy logic,fuzzy set theory,Fuzzy sets,interval type-2 fuzzy logic system,learning (artificial intelligence),machine learning tools,MLP neural networks,mobile workforce,multilayer perceptron neural network,multilayer perceptrons,Neural networks,optimal service delivery,optimisation,Optimization,Predictive models,standalone type-2 FLS,standard stacked autoencoder deep neural network,Task analysis,Type-2 fuzzy logic,utility based industries,workforce optimization},
  pages = {1-8},
  note = {ISSN:}
}

@inproceedings{7966169,
  title = {Potential Layer-Wise Supervised Learning for Training Multi-Layered Neural Networks},
  abstract = {The present paper tries to show that the greedy layer-wise supervised learning becomes effective enough to improve generalization and interpretation by the help of potential learning. It has been observed that unsupervised pre-training has a shortcoming of vanishing information as is the case of simple multi-layered network training. When the layer becomes higher, valuable information becomes smaller. Information through many different layers tends to diminish considerably and naturally from the information-theoretic point of view. For this, we use the layer-wise supervised training to prevent information from diminishing. The supervised learning has been said to be not good for pre-training for multi-layered neural networks. However, we have found that the new potential learning can be effectively used to extract valuable information through supervised pre-training. With the help of important components extracted by the potential learning, the supervised pre-training becomes effective for training multi-layered neural networks. We applied the method to two data sets, namely, an artificial and banknote data sets. In both cases, the potential learning proved to be effective in increasing generalization performance. In addition, we could show a possibility that final representation by this method could be clearly understood.},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  doi = {10.1109/IJCNN.2017.7966169},
  author = {Kamimura, R.},
  month = may,
  year = {2017},
  keywords = {Biological neural networks,generalisation (artificial intelligence),generalization,greedy algorithms,greedy layerwise supervised learning,information theory,interpretation,learning (artificial intelligence),Machine learning,Minimization,multilayered neural networks training,neural nets,Neurons,potential layerwise supervised learning,Supervised learning,Training},
  pages = {2568-2575},
  issn = {2161-4407}
}

@inproceedings{8511831,
  title = {Informational {{Privacy}}, {{A Right}} to {{Explanation}}, and {{Interpretable AI}}},
  abstract = {Businesses increasingly utilize secret algorithms and infringe users' informational privacy. We argue that to best protect users' online privacy, the use of an algorithm that assists with decisions or autonomously makes decisions that impact people requires a right to explanation.},
  booktitle = {2018 {{IEEE Symposium}} on {{Privacy}}-{{Aware Computing}} ({{PAC}})},
  doi = {10.1109/PAC.2018.00013},
  author = {Kim, T. W. and Routledge, B. R.},
  month = sep,
  year = {2018},
  keywords = {A right to explanation,artificial intelligence,Artificial intelligence,business data processing,Companies,data protection,Decision making,Explainable AI,GDPR,informational privacy,interpretable AI,Law,Loans and mortgages,Mathematical model,online privacy protection,Privacy,Trust},
  pages = {64-74},
  note = {ISSN:}
}

@inproceedings{170480,
  title = {Neural Networks That Teach Themselves through Genetic Discovery of Novel Examples},
  abstract = {The authors introduce an active learning paradigm for neural networks. In contrast to the passive paradigm, the learning in the active paradigm is initiated by the machine learner instead of its environment or teacher. The authors present a learning algorithm that uses a genetic algorithm for creating novel examples to teach multilayer feedforward networks. The creative learning networks, based on their own knowledge, discover new examples, criticize and select useful ones, train themselves, and thereby extend their existing knowledge. Experiments on function extrapolation show that the self-teaching neural networks not only reduce the teaching efforts of the human, but the genetically created examples also contribute robustly to the improvement of generalization performance and the interpretation of the connectionist knowledge.{$<$}{$>$}},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  doi = {10.1109/IJCNN.1991.170480},
  author = {{-. Zhang}, B. and Veenker, G.},
  month = nov,
  year = {1991},
  keywords = {active learning paradigm,Artificial intelligence,Artificial neural networks,Computer science,connectionist knowledge,extrapolation,Extrapolation,function extrapolation,generalization performance,Genetic algorithms,genetic discovery,learning systems,Learning systems,machine learner,Multi-layer neural network,multilayer feedforward networks,neural nets,neural networks,Neural networks,Supervised learning,teaching efforts,Unsupervised learning},
  pages = {690-695 vol.1},
  note = {ISSN:}
}

@inproceedings{200004,
  title = {{{GRAIL}}: An Integrated Artificial Intelligence System for Gene Recognition and Interpretation},
  abstract = {The development of an integrated artificial intelligence system, GRAIL (gene recognition and analysis Internet link) is described. This system uses a combination of a multi-sensor/neural network, expert system, and parallel search tools to recognize and interpret genes in DNA sequences. A simple electronic mail (E-mail) interface makes the system accessible through Internet. The strength of the system in recognizing and interpreting genes in DNA sequences and the simple E-mail interface have already attracted more than 150 users. The success of the system is largely due to the multi-sensor/neural network approach and the integration of several AI tools. The modular development and flexible framework have made it easier to incorporate new knowledge and tools into the existing system.{$<$}{$>$}},
  booktitle = {Proceedings {{Eighth Conference}} on {{Artificial Intelligence}} for {{Applications}}},
  doi = {10.1109/CAIA.1992.200004},
  author = {Guan, X. and Mural, R. J. and Einstein, J. R. and Mann, R. C. and Uberbacher, E. C.},
  month = mar,
  year = {1992},
  keywords = {AI tools,Artificial intelligence,Bioinformatics,biology computing,cellular biophysics,Data mining,DNA,DNA sequences,E-mail interface,electronic mail,expert system,expert systems,Expert systems,flexible framework,gene recognition,Genomics,GRAIL,Humans,integrated artificial intelligence system,Internet link,modular development,multi-sensor/neural network,neural nets,Neural networks,parallel processing,parallel search tools,Proteins,Sequences},
  pages = {9-13},
  note = {ISSN:}
}

@article{4523947,
  title = {Individual {{Stable Space}}: {{An Approach}} to {{Face Recognition Under Uncontrolled Conditions}}},
  volume = {19},
  issn = {1045-9227},
  abstract = {There usually exist many kinds of variations in face images taken under uncontrolled conditions, such as changes of pose, illumination, expression, etc. Most previous works on face recognition (FR) focus on particular variations and usually assume the absence of others. Instead of such a ldquodivide and conquerrdquo strategy, this paper attempts to directly address \emph{face} \emph{recognition} \emph{under} \emph{uncontrolled} \emph{conditions}. The key is the individual stable space (ISS), which only expresses personal characteristics. A neural network named ISNN is proposed to map a raw face image into the ISS. After that, three ISS-based algorithms are designed for FR under uncontrolled conditions. There are no restrictions for the images fed into these algorithms. Moreover, unlike many other FR techniques, they do not require any extra training information, such as the view angle. These advantages make them practical to implement under uncontrolled conditions. The proposed algorithms are tested on three large face databases with vast variations and achieve superior performance compared with other 12 existing FR techniques.},
  number = {8},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2008.2000275},
  author = {Geng, X. and Zhou, Z. and {Smith-Miles}, K.},
  month = aug,
  year = {2008},
  keywords = {Algorithm design and analysis,Algorithms,Artificial Intelligence,Automated,Computer-Assisted,Face,face databases,face recognition,Face recognition,Face recognition (FR),Humans,Image databases,Image Enhancement,Image Interpretation,Image recognition,individual stable space,individual stable space (ISS),ISNN,Lighting,machine learning,Machine learning algorithms,neural nets,neural network,neural networks,Neural networks,Neural Networks (Computer),pattern recognition,Pattern recognition,Pattern Recognition,Sensitivity and Specificity,Testing},
  pages = {1354-1368}
}

@article{4267862,
  title = {Logistic {{Model Tree Extraction From Artificial Neural Networks}}},
  volume = {37},
  issn = {1083-4419},
  abstract = {Artificial neural networks (ANNs) are a powerful and widely used pattern recognition technique. However, they remain "black boxes" giving no explanation for the decisions they make. This paper presents a new algorithm for extracting a logistic model tree (LMT) from a neural network, which gives a symbolic representation of the knowledge hidden within the ANN. Landwehr's LMTs are based on standard decision trees, but the terminal nodes are replaced with logistic regression functions. This paper reports the results of an empirical evaluation that compares the new decision tree extraction algorithm with Quinlan's C4.5 and ExTree. The evaluation used 12 standard benchmark datasets from the university of California, Irvine machine-learning repository. The results of this evaluation demonstrate that the new algorithm produces decision trees that have higher accuracy and higher fidelity than decision trees created by both C4.5 and ExTree.},
  number = {4},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  doi = {10.1109/TSMCB.2007.895334},
  author = {Dancey, D. and Bandar, Z. A. and McLean, D.},
  month = aug,
  year = {2007},
  keywords = {Algorithms,Artificial intelligence,artificial neural networks,Artificial neural networks,Automated,black boxes,C4.5,Computer Simulation,Data mining,decision making,Decision Support Techniques,decision trees,Decision trees,ExTree,feedforward neural networks,Feedforward neural networks,Intelligent networks,learning (artificial intelligence),logistic model tree extraction,Logistic Models,logistic regression,Logistics,machine-learning repository,Multi-layer neural network,multilayer perceptrons (MPLs),neural nets,neural networks,Neural networks,Neural Networks (Computer),pattern recognition,Pattern recognition,Pattern Recognition,regression analysis,Regression tree analysis},
  pages = {794-802}
}

@inproceedings{191443,
  title = {Safety Critical Neural Computing: Explanation and Verification in Knowledge Augmented Neural Networks},
  abstract = {Conventional neural networks cannot contain a priori knowledge and cannot explain their output. A mathematical theory of black box classifiers is developed which covers most of the best known neural architectures. The limitations of the non-model based computational paradigm are discussed; these include inability to predict the behaviour of systems with multiple-valued, discontinuous, catastrophic and chaotic state spaces. Worse, they include the inability to detect the presence of such systems, when they are working: outside their 'range of competence', and with data of quality outside their range of experience. Neural networks themselves cannot communicate with human decisionmakers in human terms; often the choice is 'take it or leave it'. Knowledge-based computation does not necessarily have these drawbacks, and can therefore augment the powerful neural computing paradigm where it is weakest. The authors consider three fundamental ways of combining the two computational paradigms and show how the explanation facility of knowledge based systems can be used to induce explanation on the output of neural subsystems. They conclude with an architecture which is generic for safety critical neural computation.{$<$}{$>$}},
  booktitle = {{{IEE Colloquium}} on {{Neural Nets}} in {{Human}}-{{Computer Interaction}}},
  author = {Johnson, J. H. and Hallam, N. J. and Picton, P. D.},
  month = dec,
  year = {1990},
  keywords = {architecture,best known neural architectures,black box classifiers,catastrophic,chaotic state spaces,computational paradigms,explanation,Explanation,explanation facility,human decisionmakers,human terms,knowledge augmented neural networks,knowledge based systems,Knowledge based systems,mathematical theory,multiple-valued,neural computing paradigm,neural nets,Neural networks,neural subsystems,non-model based computational paradigm,predict,safety,Safety,safety critical neural computation},
  pages = {1/1-1/8},
  note = {ISSN:}
}

@article{1353291,
  title = {New Dynamical Optimal Learning for Linear Multilayer {{FNN}}},
  volume = {15},
  issn = {1045-9227},
  abstract = {This letter presents a new dynamical optimal learning (DOL) algorithm for three-layer linear neural networks and investigates its generalization ability. The optimal learning rates can be fully determined during the training process. The mean squared error (mse) is guaranteed to be stably decreased and the learning is less sensitive to initial parameter settings. The simulation results illustrate that the proposed DOL algorithm gives better generalization performance and faster convergence as compared to standard error back propagation algorithm.},
  number = {6},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2004.830801},
  author = {Tan, K. C. and Tang, H. J.},
  month = nov,
  year = {2004},
  keywords = {Algorithms,Artificial Intelligence,Automated,Back propagation,Chaos,Computer Simulation,Computer-Assisted,Convergence,Decision Support Techniques,dynamical optimal learning (DOL),dynamical optimal learning algorithm,Feedback,feedforward neural nets,Feedforward neural networks,feedforward neural networks (FNN),Function approximation,Image Interpretation,Information Storage and Retrieval,learning (artificial intelligence),Linear Models,linear multilayer FNN,mean square error methods,mean squared error,Multi-layer neural network,Neural networks,Neural Networks (Computer),Nonhomogeneous media,Numerical Analysis,Pattern recognition,Pattern Recognition,stability,Stability,three-layer linear neural network,Transfer functions},
  pages = {1562-1570}
}

@inproceedings{8621470,
  title = {Prediction of Chromatin Spatial Structure Characteristics Using Machine Learning Methods},
  abstract = {Development of chromosome conformation capture methods boosted progress in the study of the spatial organization of chromatin. Accumulation of large amounts of experimental data provides an opportunity to apply machine learning methods to examine the connection between epigenetics and the three-dimensional structure of chromatin. The aim of this study was to predict the characteristics of the chromatin structure, namely the transitional gamma, from ChIP-Seq experimental data by means of machine learning methods, and also to reveal the properties of epigenetic data influencing prediction. The neural network and the loss function designed for the prediction task are shown to perform with a sufficiently high accuracy. In addition, the genomic size of the chromatin context required for improving the quality of the prediction was assessed. Several neural network visualization techniques were tested as a means for improving interpretability of network, showing the possibility for using visualization to study interrelations in epigenetic data relevant for three-dimensional chromatin structure. To sum up, a close relationship between epigenetic factors and the structure of chromatin has been confirmed.},
  booktitle = {2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  doi = {10.1109/BIBM.2018.8621470},
  author = {Starikov, S. and Khrameeva, E. and Gelfand, M.},
  month = dec,
  year = {2018},
  keywords = {Bioinformatics,biology computing,Biomedical engineering,ChIP-Seq,ChIP-Seq experimental data,chromatin spatial structure characteristics,chromosome conformation capture methods,Conferences,data visualisation,Data visualization,epigenetic data influencing prediction,genetics,genomics,Hi-C,learning (artificial intelligence),Life sciences,machine learning,Machine learning,machine learning methods,neural nets,neural network visualization techniques,neural networks,Neural networks,three-dimensional chromatin structure},
  pages = {2489-2489},
  note = {ISSN:}
}

@inproceedings{1174403,
  title = {Knowledge-Increasable Learning Behaviors Research of Neural Field},
  volume = {2},
  abstract = {In a hierarchical set of systems, a lower order system is included in the parameter space of a large one as a subset. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. Based on the theoretical analysis of information geometry and differential manifold, this paper studies knowledge-increasable learning behaviors of the neural field, and presents a layered knowledge-increasable artificial neural network model which has the knowledge-increasable and structure-extendible ability. The method helps to provide an explanation of the transformation mechanism of human's recognition system and understand the theory of global architecture of neural networks.},
  booktitle = {Proceedings. {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  doi = {10.1109/ICMLC.2002.1174403},
  month = nov,
  year = {2002},
  keywords = {Artificial neural networks,Computer science,differential manifold,geometrical structures,Humans,Information analysis,information geometry,Information geometry,information theory,knowledge-increasable learning,Large-scale systems,learning (artificial intelligence),Machine learning,neural nets,neural network,Neural networks,parallel processing,parameter space,probability,probability distribution,Probability distribution,Solid modeling,structure-extendible ability},
  pages = {586-590 vol.2},
  note = {ISSN:}
}

@inproceedings{713958,
  title = {Self-Organization in Stochastic Neural Networks},
  volume = {1},
  abstract = {The maximization of the mutual information between the stochastic outputs neurons and the clamped inputs is used as an unsupervised criterion for training a Boltzmann machine. The resulting learning rule contains two terms corresponding to the Hebbian and anti-Hebbian learning. The two terms are weighted by the amount of transmitted information in the learning synapse, giving an information-theoretic interpretation to the proportionality constant given in the biological rule of Hebb. The anti-Hebbian term causes the convergence of weights. Simulation for the encoder problem demonstrates optimal performance of this method.},
  booktitle = {Proceedings of 1993 {{International Conference}} on {{Neural Networks}} ({{IJCNN}}-93-{{Nagoya}}, {{Japan}})},
  doi = {10.1109/IJCNN.1993.713958},
  author = {Deco, G. and Parra, L.},
  month = oct,
  year = {1993},
  keywords = {anti-Hebbian learning,Boltzmann machine,Boltzmann machines,Equations,Hebbian learning,information theory,Information theory,information-theory,Intelligent networks,Mutual information,neural nets,Neural networks,Neurons,proportionality constant,Research and development,self-organization,stochastic neural networks,Stochastic processes,Tin,unsupervised learning,Unsupervised learning},
  pages = {479-482 vol.1},
  note = {ISSN:}
}

@article{8440842,
  title = {{{RetainVis}}: {{Visual Analytics}} with {{Interpretable}} and {{Interactive Recurrent Neural Networks}} on {{Electronic Medical Records}}},
  volume = {25},
  issn = {1077-2626},
  abstract = {We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2018.2865027},
  author = {Kwon, B. C. and Choi, M. and Kim, J. T. and Choi, E. and Kim, Y. B. and Kwon, S. and Sun, J. and Choo, J.},
  month = jan,
  year = {2019},
  keywords = {artificial intelligence,artificial intelligence scientists,black-box nature,Computational modeling,data analysis,Data models,data visualisation,design study,electronic medical records,EMR data,Healthcare,increase interactivity,individual medical codes,Interactive Artificial Intelligence,interactive recurrent neural networks,interactive RNN-based model,interactive systems,interactive visual analytic tool,interactively leverage users,interpretable analytics tool,Interpretable Deep Learning,interpretable networks,iterative design process,Machine learning,Medical diagnostic imaging,medical experts,medical information systems,newly improved RNN-based model,prediction tasks,Predictive models,recurrent neural nets,RetainVis,risk predictions,RNN-based model,Task analysis,temporal information,visual analytic researchers,Visual analytics,visual analytics solution,XAI (Explainable Artificial Intelligence)},
  pages = {299-309}
}

@article{8481251,
  title = {Toward {{Human}}-{{Understandable}}, {{Explainable AI}}},
  volume = {51},
  issn = {0018-9162},
  abstract = {Recent increases in computing power, coupled with rapid growth in the availability and quantity of data have rekindled our interest in the theory and applications of artificial intelligence (AI). However, for AI to be confidently rolled out by industries and governments, users want greater transparency through explainable AI (XAI) systems. The author introduces XAI concepts, and gives an overview of areas in need of further exploration-such as type-2 fuzzy logic systems-to ensure such systems can be fully understood and analyzed by the lay user.},
  number = {9},
  journal = {Computer},
  doi = {10.1109/MC.2018.3620965},
  author = {Hagras, H.},
  month = sep,
  year = {2018},
  keywords = {AI,AI systems,artificial intelligence,Artificial intelligence,computing power,explainable artificial intelligence,Future of AI,fuzzy logic,Fuzzy logic,human-understandable,intelligent systems,Intelligent systems,Learning systems,machine leaning,Machine learning,type-2 fuzzy logic systems,Type-2 Fuzzy Logic Systems,XAI},
  pages = {28-36}
}

@inproceedings{8455710,
  title = {Why the {{Failure}}? {{How Adversarial Examples Can Provide Insights}} for {{Interpretable Machine Learning}}},
  abstract = {Recent advances in Machine Learning (ML) have profoundly changed many detection, classification, recognition and inference tasks. Given the complexity of the battlespace, ML has the potential to revolutionise how Coalition Situation Understanding is synthesised and revised. However, many issues must be overcome before its widespread adoption. In this paper we consider two - interpretability and adversarial attacks. Interpretability is needed because military decision-makers must be able to justify their decisions. Adversarial attacks arise because many ML algorithms are very sensitive to certain kinds of input perturbations. In this paper, we argue that these two issues are conceptually linked, and insights in one can provide insights in the other. We illustrate these ideas with relevant examples from the literature and our own experiments.},
  booktitle = {2018 21st {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  doi = {10.23919/ICIF.2018.8455710},
  author = {Tomsett, R. and Widdicombe, A. and Xing, T. and Chakraborty, S. and Julier, S. and Gurram, P. and Rao, R. and Srivastava, M.},
  month = jul,
  year = {2018},
  keywords = {adversarial attacks,adversarial examples,adversarial machine learning,AI alignment,Coalition Situation Understanding,Data models,decision making,deep learning,explainable AI,inference mechanisms,inference tasks,Internet,internet of battlefield things,interpretability,interpretable machine learning,learning (artificial intelligence),Machine learning,Measurement,military computing,military decision-makers,ML algorithms,Sensors,Task analysis,Taxonomy},
  pages = {838-845},
  note = {ISSN:}
}

@inproceedings{8389340,
  title = {Automated Detection of Dermatological Disorders through Image-Processing and Machine Learning},
  abstract = {Dermatological Diseases are one of the biggest medical issues in 21stcentury due to it's highly complex and expensive diagnosis with difficulties and subjectivity of human interpretation. In cases of fatal diseases like Melanoma diagnosis in early stages play a vital role in determining the probability of getting cured. We believe that the application of automated methods will help in early diagnosis especially with the set of images with variety of diagnosis. Hence, in this article we present a completely automated system of dermatological disease recognition through lesion images, a machine intervention in contrast to conventional medical personnel based detection. Our model is designed into three phases compromising of data collection and augmentation, designing model and finally prediction. We have used multiple AI algorithms like Convolutional Neural Network and Support Vector Machine and amalgamated it with image processing tools to form a better structure, leading to higher accuracy of 95.3\%.},
  booktitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  doi = {10.1109/ISS1.2017.8389340},
  author = {Hasija, Y. and Garg, N. and Sourav, S.},
  month = dec,
  year = {2017},
  keywords = {AI algorithm,automated detection,Automated Disease Diagnosis,automated methods,biomedical optical imaging,completely automated system,Computer Vision Techniques,conventional medical personnel,convolutional neural network,data collection,Data models,dermatological disease recognition,Dermatological Diseases,dermatological disorders,Dermatological Disorders,diseases,Diseases,early diagnosis,expensive diagnosis,fatal diseases,Feature extraction,image classification,Image Processing,image processing tools,learning (artificial intelligence),lesion images,machine intervention,Machine learning,Machine Learning,medical image processing,Melanoma diagnosis,multiple AI algorithms,neural nets,Predictive models,skin,Support Vector Machine,support vector machines,Support vector machines,Training},
  pages = {1047-1051},
  note = {ISSN:}
}

@inproceedings{8490530,
  title = {From {{Machine Learning}} to {{Explainable AI}}},
  abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible ``glass-box'' approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
  booktitle = {2018 {{World Symposium}} on {{Digital Intelligence}} for {{Systems}} and {{Machines}} ({{DISA}})},
  doi = {10.1109/DISA.2018.8490530},
  author = {Holzinger, A.},
  month = aug,
  year = {2018},
  keywords = {AI winter,artificial intelligence,big quality data,Cognitive science,computational power,context-adaptive procedures,contextual explanatory models,Data mining,Data visualization,deep learning approaches,engineering effort,Games,glass-box approaches,high-dimensional vectors,knowledge representations,learning (artificial intelligence),logical approaches,Machine learning,mathematical principles,ontologies,ontologies (artificial intelligence),probabilistic learning methods,probability,rule-based approaches,statistical machine learning methods,Uncertainty},
  pages = {55-66},
  note = {ISSN:}
}

@inproceedings{8599838,
  title = {Provisioning {{Robust}} and {{Interpretable AI}}/{{ML}}-{{Based Service Bundles}}},
  abstract = {Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to `unknown unknowns'. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.},
  booktitle = {{{MILCOM}} 2018 - 2018 {{IEEE Military Communications Conference}} ({{MILCOM}})},
  doi = {10.1109/MILCOM.2018.8599838},
  author = {Preece, A. and Harborne, D. and Raghavendra, R. and Tomsett, R. and Braines, D.},
  month = oct,
  year = {2018},
  keywords = {artificial intelligence,coalition operations environments,Cognition,intelligence,Internet,interpretability,interpretable AI-ML-based service,learning (artificial intelligence),machine learning,Machine learning,machine learning technologies,neural nets,reasoning,Reconnaissance,reconnaissance services,reconnaissance tasks,robustness,Robustness,service-provisioning framework,Surveillance,surveillance and reconnaissance,Task analysis,trusted computing},
  pages = {1-9},
  issn = {2155-7586}
}

@article{1687932,
  title = {Training {{Reformulated Radial Basis Function Neural Networks Capable}} of {{Identifying Uncertainty}} in {{Data Classification}}},
  volume = {17},
  issn = {1045-9227},
  abstract = {This paper introduces a learning algorithm that can be used for training reformulated radial basis function neural networks (RBFNNs) capable of identifying uncertainty in data classification. This learning algorithm trains a special class of reformulated RBFNNs, known as cosine RBFNNs, by updating selected adjustable parameters to minimize the class-conditional variances at the outputs of their radial basis functions (RBFs). The experiments verify that quantum neural networks (QNNs) and cosine RBFNNs trained by the proposed learning algorithm are capable of identifying uncertainty in data classification, a property that is not shared by cosine RBFNNs trained by the original learning algorithm and conventional feed-forward neural networks (FFNNs). Finally, this study leads to a simple classification strategy that can be used to improve the classification accuracy of QNNs and cosine RBFNNs by rejecting ambiguous feature vectors based on their responses},
  number = {5},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2006.877538},
  author = {Karayiannis, N. B. and Xiong, Y.},
  month = sep,
  year = {2006},
  keywords = {Algorithms,Artificial Intelligence,Automated,class-conditional variances,Classification algorithms,Cluster Analysis,Computer Simulation,Computing Methodologies,Cosine radial basis function (RBF),data classification,Data Interpretation,feed-forward neural network (FFNN),feedforward neural networks,Feedforward neural networks,Feedforward systems,Function approximation,gradient descent learning,Intelligent networks,learning (artificial intelligence),learning algorithm,Models,Neural networks,Neural Networks (Computer),Pattern Recognition,quantum neural network (QNN),quantum neural networks,radial basis function networks,Radial basis function networks,radial basis function neural network (RBFNN),radial basis function neural networks,Statistical,Training data,uncertain systems,uncertainty,Uncertainty,uncertainty identification},
  pages = {1222-1234}
}

@inproceedings{1380390,
  title = {Information Geometry on Pruning of Neural Network},
  volume = {6},
  abstract = {The problem of determining the proper size of an artificial neural network is recognized to be crucial. One popular approach is pruning which means training a larger than necessary network and removing unnecessary weights/nodes. Though pruning is commonly used in architecture learning of neural network, there is still no theoretical framework about it. We give an information geometric explanation of pruning. In information geometric framework, most kinds of neural networks form exponential or mixture manifold which has a natural hierarchical structure. In a hierarchical set of systems, a lower order system is included in the parameter space of a large one as a submanifold. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. The pruning problem is formulated in iterative m-projections from the current manifold to its submanifold in which the divergence between the two manifolds is minimized, and it means meaning the network performance does not worsen over the entire pruning process. The result gives a geometric understanding and an information geometric guideline of pruning, which has more authentic theoretic foundation.},
  booktitle = {Proceedings of 2004 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}} ({{IEEE Cat}}. {{No}}.{{04EX826}})},
  doi = {10.1109/ICMLC.2004.1380390},
  month = aug,
  year = {2004},
  keywords = {architecture learning,artificial intelligence,artificial neural network,Artificial neural networks,Computer science,Electronic mail,exponential manifold,Guidelines,information geometry,Information geometry,information theory,Information theory,iterative m-projections,iterative methods,Mathematical programming,mixture manifold,neural net architecture,Neural networks,Probability distribution,pruning,Solid modeling},
  pages = {3479-3483 vol.6},
  note = {ISSN:}
}

@inproceedings{170682,
  title = {Designing Neural Network Explanation Facilities Using Genetic Algorithms},
  abstract = {The authors describe the use of genetic algorithms to provide components of explanation facilities for neural network applications. The genetic algorithm implementation, Genesis, uses a trained backpropagation neural network weight matrix as the genetic algorithm fitness function. Using different combinations of Genesis' run-time options, codebook vectors and decision surfaces are defined for the trained neural network. These vectors and surfaces can then be used as components of a facility that explains how the network is trained, and how it differentiates between classes. Two examples of this methodology are presented and briefly discussed. The first is a network trained to solve the XOR problem. The second is a network trained to diagnose appendicitis.{$<$}{$>$}},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  doi = {10.1109/IJCNN.1991.170682},
  author = {Eberhart, R. C. and Dobbins, R. W.},
  month = nov,
  year = {1991},
  keywords = {Abdomen,Algorithm design and analysis,appendicitis,codebook vectors,decision surfaces,diagnosis,Diagnostic expert systems,explanation,fitness function,Genesis,genetic algorithms,Genetic algorithms,Genetic mutations,Laboratories,neural nets,neural network explanation facilities,Neural networks,Pain,Physics,run-time options,Runtime,trained backpropagation neural network weight matrix,XOR problem},
  pages = {1758-1763 vol.2},
  note = {ISSN:}
}

@inproceedings{218477,
  title = {Explanation-Based Learning for the Automated Reuse of Programs},
  abstract = {A new approach for software reuse is presented which allows for the efficient preparation of already available programs, so that they can be automatically reused for novel programming tasks. Explanation-based learning from programs, guided by a domain theory of the semantics of the programming language, was used to acquire skeletal programs. For that purpose, a symbolic trace is constructed as an explanation of the functioning of a program, which may contain different types of control constructs such as sequential execution, conditional execution, and recursion. Explanation-based generalization was extended to explicitly handle these different kinds of execution control which are most essential for computer programs.{$<$}{$>$}},
  booktitle = {{{CompEuro}} 1992 {{Proceedings Computer Systems}} and {{Software Engineering}}},
  doi = {10.1109/CMPEUR.1992.218477},
  author = {Bergmann, R.},
  month = may,
  year = {1992},
  keywords = {Artificial intelligence,automated reuse,Automatic programming,Computer languages,computer programs,Concrete,conditional execution,control constructs,domain theory,execution control,explanation,generalisation (artificial intelligence),high level languages,learning (artificial intelligence),Machine learning,novel programming tasks,Productivity,programming language,recursion,semantics,sequential execution,skeletal programs,Software design,Software engineering,software reusability,Software reusability,software reuse,Software systems,symbolic trace},
  pages = {109-110},
  note = {ISSN:}
}

@inproceedings{1181456,
  title = {Support Vector Machines with Symbolic Interpretation},
  abstract = {In this work, a procedure for rule extraction from support vector machines (SVMs) is proposed. Our method first determines the prototype vectors by using k-means. Then, these vectors are combined with the support vectors using geometric methods to define ellipsoids in the input space, which are later translated to if-then rules. In this way, it is possible to give an interpretation to the knowledge acquired by the SVM. On the other hand, the extracted rules render possible the integration of SVMs with symbolic AI systems.},
  booktitle = {{{VII Brazilian Symposium}} on {{Neural Networks}}, 2002. {{SBRN}} 2002. {{Proceedings}}.},
  doi = {10.1109/SBRN.2002.1181456},
  author = {Nunez, H. and Angulo, C. and Catala, A.},
  month = nov,
  year = {2002},
  keywords = {Artificial intelligence,Clustering algorithms,Data mining,ellipsoids,geometric methods,Intelligent systems,knowledge acquisition,learning (artificial intelligence),neural nets,Neural networks,pattern classification,pattern recognition,Predictive models,Prototypes,rule extraction,Support vector machine classification,support vector machines,Support vector machines,symbol manipulation,symbolic interpretation,Systems engineering and theory},
  pages = {142-147},
  note = {ISSN:}
}

@inproceedings{7099955,
  title = {Artificial Intelligence Approaches for Supervision and Alarm Interpretation in Industrial Environments},
  abstract = {This paper discusses the potential benefits of artificial intelligence techniques. First, the role of alarm interpretation in complex process monitoring is examined followed by an analysis of the alarm interpretation task. The techniques used in several industrial applications are described. They are compared and the common features are highlighted. This paper has been written so as to provide the adequate introductory material to the ECC'99 invited session on Artificial Intelligence for Industrial Process Supervision.},
  booktitle = {1999 {{European Control Conference}} ({{ECC}})},
  doi = {10.23919/ECC.1999.7099955},
  author = {{Trav\'e-Massuy\`es}, L. and Gentil, S.},
  month = aug,
  year = {1999},
  keywords = {alarm interpretation,alarm interpretation task,alarm systems,artificial intelligence,artificial intelligence approach,artificial intelligence for industrial process supervision,Artificial intelligence techniques,Automata,Cognition,complex process monitoring,diagnosis,Expert systems,industrial applications,industrial environments,Monitoring,Numerical models,process monitoring,Sensors,supervision interpretation},
  pages = {3989-3994},
  note = {ISSN:}
}

@inproceedings{8613997,
  title = {Applying {{Internet}} of {{Things}} and {{Machine}}-{{Learning}} for {{Personalized Healthcare}}: {{Issues}} and {{Challenges}}},
  abstract = {Personalized Healthcare (PH) is a new patientoriented healthcare approach which expects to improve the traditional healthcare system. The focus of this new advancement is the patient data collected from patient Electronic health records (EHR), Internet of Things (IoT) sensor devices, wearables and mobile devices, web-based information and social media. PH applies Artificial Intelligence (AI) techniques to the collected dataset to improve disease progression technique, disease prediction, patient selfmanagement and clinical intervention. Machine learning techniques are widely used in this regard to develop analytic models. These models are integrated into different healthcare service applications and clinical decision support systems. These models mainly analyse the collected data from sensor devices and other sources to identify behavioral patterns and clinical conditions of the patient. For example, these models analyse the collected data to identify the patient's improvements, habits and anomaly in daily routine, changes in sleeping and mobility, eating, drinking and digestive pattern. Based on those patterns the healthcare applications and the clinical decision support systems recommend lifestyle advice, special treatment and care plans for the patient. The doctors and caregivers can also be engaged in the care plan process to validate lifestyle advice. However, there are many uncertainties and a grey area when it comes to applying machine learning in this context. Clinical, behaviour and lifestyle data in nature are very sensitive. There could be different types of biased involved in the process of data collection and interpretation. The training data model could have an older version of the dataset. All these could lead to an incorrect decision from the system without the user's knowledge. In this paper, some of the standards of the ML models reported in the recent research trends, identify the reliability issues and propose improvements.},
  booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
  doi = {10.1109/iCMLDE.2018.00014},
  author = {Ahamed, F. and Farid, F.},
  month = dec,
  year = {2018},
  keywords = {artificial intelligence techniques,behavioral patterns,clinical conditions,clinical decision support systems,clinical intervention,data acquisition,data collection,decision support systems,disease prediction,disease progression technique,diseases,electronic health records,health care,healthcare applications,healthcare service applications,healthcare system,Hospitals,Internet of Things,Internet of Things sensor devices,learning (artificial intelligence),lifestyle data,Machine learning,Machine Learning,machine learning techniques,ML models,mobile devices,Monitoring,patient data,patient self-management,patient treatment,personalized healthcare,Personalized Healthcare,Sleep apnea,social media,training data model,Web-based information},
  pages = {19-21},
  note = {ISSN:}
}

@inproceedings{8519384,
  title = {Target {{Aspect Identification}} in {{SAR Image}}: {{A Machine Learning Approach}}},
  abstract = {Identifying the aspect for a given target is an important issue in synthetic aperture radar (SAR) image interpretation. A new SAR target aspect identification method based on machine learning theory is proposed in this paper. First, the aspect angles of the SAR target are discretized, and the spatial relationships of the neighborhoods of the SAR target samples are established. Then an optimal linear mapping is solved based on the proposed subspace aspect discriminant analysis. The samples will be projected into a low-dimensional space and be of a better aspect identifiability than in their original space. Finally, the projected samples are fed into a multilayer neural network, and the aspects of the SAR targets will be indicated. Experimental results have shown the superiority of the proposed method based on the moving and stationary target acquisition and recognition (MSTAR) data set.},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  doi = {10.1109/IGARSS.2018.8519384},
  author = {Pei, J. and Huang, Y. and Huo, W. and Zhang, Y. and Yang, J.},
  month = jul,
  year = {2018},
  keywords = {aspect angles,aspect identifiability,Estimation,image sampling,learning (artificial intelligence),low-dimensional space,machine learning,Machine learning,machine learning theory,moving-and-stationary target acquisition-and-recognition data set,MSTAR data set,multi-layer neural network,Multi-layer neural network,multilayer neural network,multilayer perceptrons,Neurons,optimal linear mapping,projected samples,radar computing,radar imaging,radar target recognition,SAR image,SAR target aspect identification method,SAR target samples,stationary target acquisition,subspace aspect discriminant analysis,synthetic aperture radar,Synthetic aperture radar,synthetic aperture radar image interpretation,target aspect identification,Training},
  pages = {2310-2313},
  issn = {2153-7003}
}

@inproceedings{5621722,
  title = {Evaluation {{Techniques}} for {{Oil Gas Reservoir Based}} on {{Artificial Neural Networks Techniques}}},
  abstract = {By using BP artificial nerve network's error reversion transmission. Summing up various data of comprehensive logging can solve the problem of low accurate rate for identifying oil, gas, water zones. The software provides nerve network reservoir interpretation model by studying and training the initial data of tested oil. Practice proves the overall coincidence rate of interpretation reaches 97\%. It can more efficiently reflects logging technique's advantage of wellsite quick evaluation oil, gas, water zones. The application in this technique improves the level of logging data interpretation and evaluation.},
  booktitle = {2010 {{Third International Conference}} on {{Business Intelligence}} and {{Financial Engineering}}},
  doi = {10.1109/BIFE.2010.17},
  author = {Pan, H. Y. and He, H.},
  month = aug,
  year = {2010},
  keywords = {artificial neural network technique,artificial neural Networks,Artificial neural networks,backpropagation,Biological neural networks,BP artificial nerve network,Data models,error reversion transmission,evaluation technique,Gas and Water Layer,hydrocarbon reservoirs,Hydrocarbon reservoirs,Identification,logging data interpretation,Mud Logging,network nerve reservoir interpretation,neural nets,Oil,oil gas reservoir,petroleum industry,Reservoirs,tested oil,Training,water zone},
  pages = {28-31},
  note = {ISSN:}
}

@article{6877713,
  title = {On {{Equivalence}} of {{FIS}} and {{ELM}} for {{Interpretable Rule}}-{{Based Knowledge Representation}}},
  volume = {26},
  issn = {2162-237X},
  abstract = {This paper presents a fuzzy extreme learning machine (F-ELM) that embeds fuzzy membership functions and rules into the hidden layer of extreme learning machine (ELM). Similar to the concept of ELM that employed the random initialization technique, three parameters of F-ELM are randomly assigned. They are the standard deviation of the membership functions, matrix-C (rule-combination matrix), and matrix-D [don't care (DC) matrix]. Fuzzy if-then rules are formulated by the rule-combination Matrix of F-ELM, and a DC approach is adopted to minimize the number of input attributes in the rules. Furthermore, F-ELM utilizes the output weights of the ELM to form the target class and confidence factor for each of the rules. This is to indicate that the corresponding consequent parameters are determined analytically. The operations of F-ELM are equivalent to a fuzzy inference system. Several benchmark data sets and a real world fault detection and diagnosis problem have been used to empirically evaluate the efficacy of the proposed F-ELM in handling pattern classification tasks. The results show that the accuracy rates of F-ELM are comparable (if not superior) to ELM with distinctive ability of providing explicit knowledge in the form of interpretable rule base.},
  number = {7},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2014.2341655},
  author = {Wong, S. Y. and Yap, K. S. and Yap, H. J. and Tan, S. C. and Chang, S. W.},
  month = jul,
  year = {2015},
  keywords = {Accuracy,Algorithms,Artificial Intelligence,Artificial neural networks,Benchmarking,Classification,Computational modeling,Databases,don't care matrix,Extreme learning machine (ELM),F-ELM,Factual,fault detection,fault diagnosis,fault diagnosis problem,Feedback,FIS,fuzzy extreme learning machine,fuzzy if-then rules,fuzzy inference system,fuzzy inference system (FIS),fuzzy logic,Fuzzy logic,Fuzzy Logic,fuzzy membership functions,fuzzy reasoning,interpretable rule base,interpretable rule-based knowledge representation,knowledge representation,Machine Learning,matrix algebra,matrix-C,matrix-D,Models,Neural Networks (Computer),Neurons,pattern classification,pattern classification tasks,Power Plants,Pragmatics,random initialization technique,Reproducibility of Results,rule based,rule-combination matrix,standard deviation,Statistical,Training},
  pages = {1417-1430}
}

@inproceedings{8258557,
  title = {A Study on Interpretability of Decision of Machine Learning},
  abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2017.8258557},
  author = {Shirataki, S. and Yamaguchi, S.},
  month = dec,
  year = {2017},
  keywords = {Analytical models,Big Data,big data analysis,black boxes,data analysis,decision reasons,DVD,highly weighted words,interpretability,learning (artificial intelligence),machine learning,opinions classification,pattern classification,Predictive models,Support Vector Machine,support vector machines,Support vector machines,SVM,text analysis,Tools,Training},
  pages = {4830-4831},
  note = {ISSN:}
}

@inproceedings{190514,
  title = {Empirical Results from Applying Machine Learning Techniques to Planning},
  abstract = {Outlines an experimental machine learning implementation, called 'FM', that applies both explanation based learning and similarity-based learning to AI planners. The system shell of FM contains techniques for learning application-dependent heuristics, through the experience of using a performance component (a planner) in that application. An application domain is supplied by specifying a set of action schemas, and environmental facts and rules. FM is then fed an initial state, and a sequence of tasks within this application, roughly in ascending order of complexity, which it is expected to solve. After each task has been solved, the system analyses the planning trace, allowing it to learn from experience.{$<$}{$>$}},
  booktitle = {{{IEE Colloquium}} on {{Machine Learning}}},
  author = {McCluskey, T. L.},
  month = jun,
  year = {1990},
  keywords = {AI planners,application-dependent heuristics,artificial intelligence,Artificial intelligence,explanation based learning,learning systems,Learning systems,machine learning,planning,similarity-based learning},
  pages = {6/1-6/3},
  note = {ISSN:}
}

@inproceedings{118339,
  title = {Handling Knowledge in High Order Neural Networks: The Combinatorial Neural Model},
  abstract = {Summary form only given, as follows. A description is given of the combinatorial neural model, a high-order neural network suitable for classification tasks. The model is based on fuzzy set theory, neural sciences studies, and expert knowledge analysis results. It presents interesting properties such as modularity, explanation capacity, knowledge and data representation, high speed of training, incremental learning, generalization capacity, processing of uncertain and incomplete data, and ability to reason nonmonotonically when representing only relevant evidence, and graceful decay.{$<$}{$>$}},
  booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
  doi = {10.1109/IJCNN.1989.118339},
  year = {1989},
  keywords = {classification,combinatorial neural model,data representation,expert knowledge analysis,explanation,Explanation,explanation capacity,fuzzy set theory,Fuzzy sets,generalization capacity,high order neural networks,incomplete data,incremental learning,knowledge representation,Knowledge representation,modularity,neural nets,Neural networks,neural sciences,nonmonotonic reasoning,training,uncertain data},
  pages = {582 vol.2-},
  note = {ISSN:}
}

@inproceedings{7334730,
  title = {Soft {{Sensor Modeling Based}} on {{Extreme Learning Machine}} and {{Case}}-{{Based Reasoning}}},
  volume = {1},
  abstract = {Neural network (NN) and Case-based reasoning (CBR) have common advantages over other learning strategies. NN and CBR can be directly applied to the classification and regression problem without additional transform mechanisms. However, they all have disadvantages. The knowledge representation of NN is unreadable and this black box property restricts the application of NN to areas which needs proper explanations. Meanwhile CBR suffers from the feature-weighting problem, when CBR measures the distance between cases, some input features should be treated more importantly than others. This paper, we propose a hybrid prediction system of extreme learning machine (ELM) and Case-based reasoning (ELM-CBR). In our hybrid system, the feature weight set calculated from the trained ELM network plays the core role in connecting both the learning strategies, and the explanation on prediction can be given by presenting the most similar cases from the case base. Moreover, the prediction value of the Online Sequential Extreme Learning Machine also utilized in conjunction with the neighborhood information. This provides extended information for the query with most similar cases in the database. Finally, we present an application in the sugarcane juice clarification, experiments show that the hybrid system has a better recognition rate compared the k-NN and GA-CBR method.},
  booktitle = {2015 7th {{International Conference}} on {{Intelligent Human}}-{{Machine Systems}} and {{Cybernetics}}},
  doi = {10.1109/IHMSC.2015.39},
  author = {Song, C. and Li, F. and Xiao, L. and Feng, L.},
  month = aug,
  year = {2015},
  keywords = {Artificial neural networks,beverages,black box property,case-based reasoning,CBR,Classification algorithms,Cognition,extreme learning machine,feature weighting,feature-weighting problem,hybrid system,Image color analysis,juice clarification,knowledge representation,learning (artificial intelligence),learning strategies,neural nets,neural network,NN,online sequential extreme learning machine,recognition rate,regression problem,soft sensor modeling,Sugar industry,sugarcane juice clarification,trained ELM network,Training},
  pages = {391-394},
  note = {ISSN:}
}

@inproceedings{343689,
  title = {Representing Acquired Knowledge of Neural Networks by Fuzzy Sets: Control of Internal Information of Neural Networks by Entropy Minimization},
  abstract = {The authors propose an entropy algorithm to extract the internal information of the neural networks, and show that the extracted information is expressed by fuzzy sets. Fuzzy sets representing internal information of neural networks after learning are composed of the competitive hidden unit activities which can be controlled by the entropy method. We apply this method to meaning interpretation of alphabet.{$<$}{$>$}},
  booktitle = {Proceedings of 1994 {{IEEE}} 3rd {{International Fuzzy Systems Conference}}},
  doi = {10.1109/FUZZY.1994.343689},
  author = {Kamimura, R. and Yager, R. R. and Nakanishi, S.},
  month = jun,
  year = {1994},
  keywords = {alphabet interpretation,competitive hidden unit,Data mining,entropy,Entropy,entropy minimization,fuzzy neural nets,fuzzy set theory,Fuzzy sets,Information science,internal information,Knowledge acquisition,knowledge representation,learning,learning (artificial intelligence),Machine intelligence,Machine learning,Minimization methods,neural networks,Neural networks,Shape},
  pages = {58-63 vol.1},
  note = {ISSN:}
}

@inproceedings{8529552,
  title = {{{AANN}}: {{Absolute Artificial Neural Network}}},
  abstract = {This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labeled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.},
  booktitle = {2018 3rd {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  doi = {10.1109/I2CT.2018.8529552},
  author = {Karnewar, A.},
  month = apr,
  year = {2018},
  keywords = {AANN,absolute artificial neural network,absolute valued functions,activation functions,artificial neural networks,Artificial neural networks,Backpropagation,data structures,Generative adversarial networks,input data classification,input data representations,knowledge representation,learning (artificial intelligence),mathematical abs functions,neural net architecture,neural nets,neural network architecture,Neurons,pattern classification,representation vectors,supervised learning,Task analysis,Training,transfer functions},
  pages = {1-6},
  note = {ISSN:}
}

@inproceedings{8666524,
  title = {Improvement of {{Protein Model Scoring Using Grouping}} and {{Interpreter}} for {{Machine Learning}}},
  abstract = {In this paper, we describe our protein folding research with the goal of improving protein model scoring by grouping of protein models and using an interpreter for machine learning (ML). The traditional approach is to use a handful of popular ML algorithms, such as Support Vector Machines (SVM), Random Forest and Neural Networks that are trained on a whole set of models. Our approach is to group the protein models and train the ML algorithms on each group separately. Our framework can be generalized to other application of ML where there is a strong diversification of data set. In this paper, we describe results of comparison of traditional vs. our grouping approach showing that some improvement in the scoring of protein models can be achieved. To further improve the scoring, an interpreter for machine learning is used. The interpreter is based on Local Interpretable Model-Agnostic Explanations (LIME) tool. In this paper it is used to determine feature vector for each group of protein models. Different feature vectors are then used for ML training on different groups of protein models allowing us to improve the ML algorithms. In addition, interpreter of ML can be used in the future to provide feedback for the process of protein models generation.},
  booktitle = {2019 {{IEEE}} 9th {{Annual Computing}} and {{Communication Workshop}} and {{Conference}} ({{CCWC}})},
  doi = {10.1109/CCWC.2019.8666524},
  author = {Czejdo, B. and Bhattacharya, S. and Spooner, C.},
  month = jan,
  year = {2019},
  keywords = {bioinformatics,Bioinformatics,Computational modeling,feature extraction,feature vector,grouping approach,Interpretation-Generating Algorithms,learning (artificial intelligence),LIME,local interpretable model-agnostic explanations tool,machine learning,Machine learning,Machine Learning,Machine learning algorithms,ML algorithms,Model Scoring,neural networks,Predictive models,protein model scoring,Protein Models,proteins,Proteins,random forest,Solid modeling,support vector machines,SVM,Training},
  pages = {0349-0353},
  note = {ISSN:}
}

@inproceedings{128603,
  title = {{{AI}} Research Activities at {{Saint Joseph}}'s {{University}}},
  abstract = {Research activities in artificial intelligence at Saint Joseph's University are described. There has been significant progress in both basic research and system implementation in the last few years. The software developed pertains to search pruning for problem solving, learning from examples (similarity-based learning as it is often called), and explanation-based generalization and self-improving game-playing programs. The work on similarity-based learning can support the automatic formation of expert rules. All of this work is based on the basic research. Activities in the areas of the theory of problem solving, learning algorithms, and software development are described.{$<$}{$>$}},
  booktitle = {Proceedings. 5th {{IEEE International Symposium}} on {{Intelligent Control}} 1990},
  doi = {10.1109/ISIC.1990.128603},
  author = {Hodgson, J. P. E. and Banerji, R. B.},
  month = sep,
  year = {1990},
  keywords = {AI research activities,artificial intelligence,Artificial intelligence,automatic formation,Calculus,Computer science,expert rules,explanation-based generalization,Laboratories,learning algorithms,learning systems,Learning systems,Machine learning,Mathematics,Pattern recognition,Personnel,problem solving,Problem-solving,Saint Joseph's University,search pruning,self-improving game-playing programs,software development,software engineering,system implementation},
  pages = {1178-1179 vol.2},
  issn = {2158-9860}
}

@article{4359988,
  title = {Identification of {{Spinal Deformity Classification With Total Curvature Analysis}} and {{Artificial Neural Network}}},
  volume = {55},
  issn = {0018-9294},
  abstract = {In this paper, a multilayer feed-forward, back-propagation (MLFF/BP) artificial neural network (ANN) was implemented to identify the classification patterns of the scoliosis spinal deformity. At the first step, the simplified 3D spine model was constructed based on the coronal and sagittal X-ray images. The features of the central axis curve of the spinal deformity patterns in 3D space were extracted by the total curvature analysis. The discrete form of the total curvature, including the curvature and the torsion of the central axis of the simplified 3D spine model was derived from the difference quotients. The total curvature values of 17 vertebrae from the first thoracic to the fifth lumbar spine formed a Euclidean space of 17 dimensions. The King classification model was tested on this MLFF/BP ANN identification system. The 17 total curvature values were presented to the input layer of MLFF/BP ANN. In the output layer there were five neurons representing five King classification types. A total of 37 spinal deformity patterns from scoliosis patients were selected. These 37 patterns were divided into two groups. The training group had 25 patterns and testing group had 12 patterns. The 25-pattern training group was further divided into five subsets. Based on the definition of King classification system, each subset contained all five King types. The network training was conducted on these five subsets by the hold-out method, one of cross-validation variants, and the early stop method. In each one of the five cross-validation sessions, four subsets were alternatively used for estimation learning and one subset left was used for validation learning. Final network testing was conducted with remaining 12 patterns in testing group after the MLFF/BP ANN was trained by all five subsets in training group. The performance of the neural network was evaluated by comparing between two network topologies, one with one hidden layer and another with two hidden layers. The results are shown in three tables. The first table shows network errors in estimation learning and the second table shows identification rates in validation learning. The network errors and identification rates in the last round of network training and testing are shown in the third table. Each table has a comparison for both one hidden layer and two hidden layer networks.},
  number = {1},
  journal = {IEEE Transactions on Biomedical Engineering},
  doi = {10.1109/TBME.2007.894831},
  author = {Lin, H.},
  month = jan,
  year = {2008},
  keywords = {3-D spine model,Algorithms,Artificial Intelligence,artificial neural network,Artificial neural network (ANN),Artificial neural networks,Automated,backpropagation,biomechanics,bone,classification,Computer-Assisted,coronal X-ray images,cross-validation variants,diagnostic radiography,difference quotients,estimation learning,Euclidean space,feature extraction,Feedforward systems,hidden layer networks,hold-out method,Humans,image classification,Imaging,King classification model,medical computing,medical image processing,MLFF-BP ANN identification system,Multi-layer neural network,multilayer feed-forward back-propagation,multilayer perceptrons,Network topology,Neural networks,Neural Networks (Computer),Neurons,Pattern analysis,Pattern Recognition,Radiographic Image Enhancement,Radiographic Image Interpretation,Reproducibility of Results,sagittal X-ray images,scoliosis,Scoliosis,scoliosis spinal deformity classification patterns,Sensitivity and Specificity,space curve,Spinal deformity,spinal deformity classification,Spine,System testing,Three-Dimensional,torsion,total curvature analysis,validation learning,X-ray imaging},
  pages = {376-382}
}

@inproceedings{8622073,
  title = {Explainable {{Text Classification}} in {{Legal Document Review A Case Study}} of {{Explainable Predictive Coding}}},
  abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2018.8622073},
  author = {Chhatwal, R. and Gronvall, P. and {Huber-Fliflet}, N. and Keeling, R. and Zhang, J. and Zhao, H.},
  month = dec,
  year = {2018},
  keywords = {data-focused engagements,document classification,electronically-stored documents,explainable AI,explainable AI system,explainable predictive coding,explainable predictive coding methods,Law,law administration,legal document review,machine learning,Machine learning,machine learning researchers,pattern classification,predictive coding,Predictive coding,Predictive models,responsive documents,responsive snippets,supervised learning,supervised learning task,technology-based techniques,text analysis,text categorization,Text categorization,text classification,typical legal document review scenarios},
  pages = {1905-1911},
  note = {ISSN:}
}

@inproceedings{614209,
  title = {Properties of Learning of a Fuzzy {{ART}} Variant},
  volume = {3},
  abstract = {This paper discusses one variation of the fuzzy ART architecture, referred to as fuzzy ART variant. The fuzzy ART variant is a fuzzy ART algorithm, with a very large value for the choice parameter. Based on the geometrical interpretation of templates in fuzzy ART we present and prove useful properties of learning pertaining to the fuzzy ART variant. One of these properties of learning establishes an upper bound on the number of list presentations required by the fuzzy ART variant to learn an arbitrary list of input patterns presented to it. In previously published work, it was shown that the fuzzy ART variant performs as well as a fuzzy ART algorithm with more typical values for the choice parameter. Hence, the fuzzy ART variant is as good a clustering machine as the fuzzy ART algorithm using more typical values of the choice parameter.},
  booktitle = {Proceedings of {{International Conference}} on {{Neural Networks}} ({{ICNN}}'97)},
  doi = {10.1109/ICNN.1997.614209},
  author = {Georgiopoulos, M. and Dagher, I. and Heileman, G. L. and Bebis, G.},
  month = jun,
  year = {1997},
  keywords = {ART neural nets,Clustering algorithms,clustering machine,Computer architecture,Data preprocessing,fuzzy ART variant neural net architecture,Fuzzy logic,fuzzy neural nets,Fuzzy neural networks,geometrical interpretation,geometry,learning (artificial intelligence),learning properties,neural net architecture,Neural networks,Pattern clustering,Subspace constraints,Supervised learning,templates,Upper bound},
  pages = {2012-2016 vol.3},
  note = {ISSN:}
}

@article{7063914,
  title = {Smart {{Information Reconstruction}} via {{Time}}-{{Space}}-{{Spectrum Continuum}} for {{Cloud Removal}} in {{Satellite Images}}},
  volume = {8},
  issn = {1939-1404},
  abstract = {Cloud contamination is a big obstacle when processing satellite images retrieved from visible and infrared spectral ranges for application. Although computational techniques including interpolation and substitution have been applied to recover missing information caused by cloud contamination, these algorithms are subject to many limitations. In this paper, a novel smart information reconstruction (SMIR) method is proposed, in order to reconstruct cloud contaminated pixel values from the time-space-spectrum continuum with the aid of a machine learning tool, namely extreme learning machine (ELM). For the purpose of demonstration, the performance of SMIR is evaluated by reconstructing the missing remote sensing reflectance values derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) on board the Terra satellite over Lake Nicaragua, where is a very cloudy area year round. For comparison, the traditional backpropagation neural network algorithms will also be implemented to reconstruct the missing values. Experimental results show that the ELM outperforms the BP algorithms by an enhanced machine learning capacity with simulated memory effect embedded in MODIS due to linking the complex time-space-spectrum continuum between cloud-free and cloudy pixels. The ELM-based SMIR practice presents a correlation coefficient of 0.88 with root mean squared error of 7.4E - 04sr\textsuperscript{-1} between simulated and observed reflectance values. Finding suggests that the SMIR method is effective to reconstruct all the missing information providing visually logical and quantitatively assured images for further image processing and interpretation in environmental applications.},
  number = {5},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  doi = {10.1109/JSTARS.2015.2400636},
  author = {Chang, N. and Bai, K. and Chen, C.},
  month = may,
  year = {2015},
  keywords = {Artificial neural network,backpropagation neural network algorithm,cloud contaminated pixel reconstruction,cloud contamination,cloud removal,cloud-free,clouds,Clouds,cloudy area,cloudy pixel,computational intelligence,computational technique,Contamination,ELM-based SMIR practice,enhanced machine learning capacity,environmental application,extreme learning machine,geophysical image processing,image reconstruction,Image reconstruction,infrared spectral range,Lake Nicaragua,lakes,learning (artificial intelligence),machine learning,machine learning tool,missing information recovery,missing remote sensing reflectance value,moderate resolution imaging spectroradiometer,neural nets,Neural networks,reflectivity,remote sensing,Remote sensing,root mean squared error,satellite image processing,satellite images,Satellites,smart information reconstruction,SMIR method,Terra satellite,time-space-spectrum continuum,Training,visible spectral range},
  pages = {1898-1912}
}

@inproceedings{1295633,
  title = {Hybrid {{AI}} System for Geometric Pattern Recognition},
  abstract = {The research area of hybrid and neural processing has been actively developing. Hybrid neural systems are computational systems which are based mainly on artificial neural networks but also allow a symbolic interpretation or interaction with symbolic classical artificial intelligence. In this paper we describe a hybrid AI system developed for 2D object recognition. The 2D object recognition system was developed as the initial step for developing a 3D object recognition system for an unmanned aerial vehicle (UAV).},
  booktitle = {Thirty-{{Sixth Southeastern Symposium}} on {{System Theory}}, 2004. {{Proceedings}} of The},
  doi = {10.1109/SSST.2004.1295633},
  author = {Fernando, C. G. and Munasinghe, R.},
  month = mar,
  year = {2004},
  keywords = {2D object recognition,artificial intelligence,Artificial intelligence,artificial neural networks,Artificial neural networks,Biological neural networks,computational systems,Computer networks,computer vision,Expert systems,geometric pattern recognition,Humans,hybrid AI system,hybrid neural systems,neural nets,Neurons,object recognition,Object recognition,Pattern recognition,remotely operated vehicles,Robustness,symbolic interpretation,unmanned aerial vehicle},
  pages = {128-131},
  issn = {0094-2898}
}

@inproceedings{8489490,
  title = {Shepard {{Interpolation Neural Networks}} with {{K}}-{{Means}}: {{A Shallow Learning Method}} for {{Time Series Classification}}},
  abstract = {Deep neural network architectures have redefined benchmark machine learning challenges, from classification to anomaly detection, and have become popular in the time series domain. However, deep learning techniques fall short in time series classification (TSC) because the explainability of deep learning is still abstract, and the training requires vast amounts of data, which utilizes computational power. These obstacles are not the case with Shepard Interpolation Neural Networks (SINN), a shallow learning architecture approach for deep learning tasks. Based on a statistical interpolation technique rather than a biological brain, SINN require little data to achieve high accuracy in its training. Additionally, its explainability can be equated to feature mapping onto hyper surfaces in the feature space. Our proposed algorithm outperforms the other state-of-the-art algorithms on the popular UCR time series classification benchmark data set and outperforms LSTMs on data sets which have significantly smaller training data than testing.},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  doi = {10.1109/IJCNN.2018.8489490},
  author = {Smith, K. E. and Williams, P. and Bryan, K. J. and Solomon, M. and Ble, M. and Haber, R.},
  month = jul,
  year = {2018},
  keywords = {benchmark machine learning challenges,deep learning tasks,deep learning techniques,interpolation,Interpolation,K-means,learning (artificial intelligence),Machine learning,Measurement,neural nets,neural network architectures,Neural networks,Neural Networks,Neurons,pattern classification,Shallow and Deep Learning,shallow learning method,Shepard Interpolation,Shepard interpolation neural networks,Shepard Interpolation Neural Networks,SINN,statistical interpolation technique,Task analysis,time series,Time series analysis,Time Series Classification,time series domain,UCR time series classification benchmark data,Unsupervised Clustering},
  pages = {1-6},
  issn = {2161-4407}
}

@inproceedings{1259707,
  title = {Information Geometry on Extendable Hierarchical Large Scale Neural Network Model},
  volume = {3},
  abstract = {In this paper, an extendable hierarchical large scale neural network model is developed based on the theoretical analysis of information geometry. In a hierarchical set of systems, a lower order system is included in the parameter space of a larger one as a subset. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. Extendable hierarchical large scale neural network divides a task into small tasks, and each task is fulfilled by a small network under the principle of divide and conquer to improve the performance of a single network. By studying the dual manifold architecture for a family of neural networks and analyzing the hierarchical expansion of this model based on information geometry, the paper proposes a new method to construct the extendable hierarchical large scale neural network model that has knowledge-increasable and structure-extendible ability. The method helps to provide explanation of the transformation mechanism of human recognition system and understand the theory of global architecture of neural network.},
  booktitle = {Proceedings of the 2003 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}} ({{IEEE Cat}}. {{No}}.{{03EX693}})},
  doi = {10.1109/ICMLC.2003.1259707},
  month = nov,
  year = {2003},
  keywords = {cognition,Computer science,dual flat manifold architecture,Electronic mail,extendable hierarchical large scale neural network model,geometry,hierarchical systems,human recognition system,Humans,Information analysis,information geometry,Information geometry,Information theory,large-scale systems,Large-scale systems,learning (artificial intelligence),learning behaviors,lower order system,neural nets,Neural networks,parameter space,Probability distribution,Solid modeling,statistical distributions},
  pages = {1380-1384 Vol.3},
  note = {ISSN:}
}

@article{737497,
  title = {A Neural-Network Architecture for Syntax Analysis},
  volume = {10},
  issn = {1045-9227},
  abstract = {Artificial neural networks (ANNs), due to their inherent parallelism, offer an attractive paradigm for implementation of symbol processing systems for applications in computer science and artificial intelligence. The paper explores systematic synthesis of modular neural-network architectures for syntax analysis using a prespecified grammar-a prototypical symbol processing task which finds applications in programming language interpretation, syntax analysis of symbolic expressions, and high-performance compilers. The proposed architecture is assembled from ANN components for lexical analysis, stack, parsing and parse tree construction. Each of these modules takes advantage of parallel content-based pattern matching using a neural associative memory. The proposed neural-network architecture for syntax analysis provides a relatively efficient and high performance alternative to current computer systems for applications that involve parsing of LR grammars which constitute a widely used subset of deterministic context-free grammars. Comparison of quantitatively estimated performance of such a system (implemented using current CMOS VLSI technology) with that of conventional computers demonstrates the benefits of massively parallel neural-network architectures for symbol processing applications.},
  number = {1},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/72.737497},
  author = {Honavar, V.},
  month = jan,
  year = {1999},
  keywords = {Application software,artificial intelligence,Artificial intelligence,Artificial neural networks,CMOS technology,Computer architecture,Computer languages,computer science,Computer science,content-addressable storage,context-free grammars,deterministic context-free grammars,high-performance compilers,inherent parallelism,lexical analysis,LR grammars,modular neural-network architectures,Network synthesis,neural associative memory,neural net architecture,parallel content-based pattern matching,Parallel processing,parse tree construction,parsing,pattern matching,program compilers,programming language interpretation,Prototypes,symbol manipulation,symbol processing systems,symbolic expressions,syntax analysis},
  pages = {94-114}
}

@inproceedings{6016793,
  title = {Application of Neural Network to Identify the Remote Sensing Data of Hillslide},
  volume = {2},
  abstract = {This study presents the results of neural network simulation of hillside area prediction from remote sensing data. Five neural network methods were compared, which were Back Propagation Network (BPN), Extend Neuron Networks (ENN), Fuzzy Neural Network (FNN), Analysis Adjustment Synthesis Network (AASN), and Genetic Algorithm Neural Network (GANN). Three factors were used as the predictor in this study, which were NDVI value, shape factor, and color difference. The result reveals that the BPN is the best choice, because the error is the lowest among the five schemes in this study.},
  booktitle = {2011 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  doi = {10.1109/ICMLC.2011.6016793},
  author = {Wang, T. and Yu, T.},
  month = jul,
  year = {2011},
  keywords = {AASN,analysis adjustment synthesis network,back propagation network,Biological neural networks,BPN,Cybernetics,Data models,ENN,extend neuron networks,FNN,fuzzy neural network,GANN,genetic algorithm neural network,genetic algorithms,geophysics computing,hillside area prediction,hillslide,Image color analysis,Image identification,Image interpretation,image processing,Image variation,Machine learning,neural nets,Neural network,neural network simulation,remote sensing,Remote sensing,remote sensing data,Shape},
  pages = {661-665},
  issn = {2160-1348}
}

@inproceedings{714304,
  title = {A Decision Support System Using Neural Networks in a Glass Furnace Process},
  volume = {3},
  abstract = {A decision support system using artificial neural networks is implemented with real world data of a glass furnace process at Samsung. It provides the functions such as process model identification, set-point control and interpreting input factors. Since a glass furnace process is highly complex, a traditional attempt to develop a model from first principles often proves to be a difficult and costly procedure. However, the decision support system using artificial neural networks does not require a priori knowledge of a glass furnace process and proves to be useful in identifying the model directly by input/output data collected from the plant. This paper shows the method of finding the partial derivative value at some point from trained weights, the conversion method of a 3-layered perceptron network into a 2-layered one, and the interpretation method of neural networks solutions.},
  booktitle = {Proceedings of 1993 {{International Conference}} on {{Neural Networks}} ({{IJCNN}}-93-{{Nagoya}}, {{Japan}})},
  doi = {10.1109/IJCNN.1993.714304},
  month = oct,
  year = {1993},
  keywords = {2-layered network,3-layered perceptron network,Artificial intelligence,artificial neural networks,Artificial neural networks,Chemical processes,decision support system,decision support systems,Decision support systems,furnaces,Furnaces,glass furnace,glass industry,Glass manufacturing,Intelligent networks,multilayer perceptrons,Neural networks,partial derivative value,Petroleum,process control,process model identification,Samsung,set-point control,Thermal variables control},
  pages = {2795-2798 vol.3},
  note = {ISSN:}
}

@inproceedings{8614007,
  title = {Attention {{Visualization}} of {{Gated Convolutional Neural Networks}} with {{Self Attention}} in {{Sentiment Analysis}}},
  abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
  booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
  doi = {10.1109/iCMLDE.2018.00024},
  author = {Yanagimto, H. and Hashimoto, K. and Okada, M.},
  month = dec,
  year = {2018},
  keywords = {Amazon reviews,attention visualization,convolutional neural nets,Convolutional neural networks,deep learning,Deep learning,Gated CNN,gated convolutional neural networks,GCNN,Kernel,learning (artificial intelligence),Logic gates,natural language processing,Natural language processing,neural net architecture,neural network architecture,self-attention mechanism,sentient analysis,sentiment analysis,Sentiment analysis,Task analysis,the self-attention mechanism},
  pages = {77-82},
  note = {ISSN:}
}

@inproceedings{614197,
  title = {D-Entropy Controller for Interpretation and Generalization},
  volume = {3},
  abstract = {In this paper, we propose a method to control D-entropy for better generalization and explicit interpretation of internal representations. By controlling D-entropy, a few hidden units are detected as important units without saturation. In addition, a small number of important input-hidden connections are detected and the majority of the connections are eliminated. Thus, we can obtain much simplified internal representations with better interpretation and generalization. The D-entropy control method was applied to the inference of well-formedness of an artificial language. Experimental results confirmed that by maximizing and minimizing D-entropy, generalization performance can significantly be improved.},
  booktitle = {Proceedings of {{International Conference}} on {{Neural Networks}} ({{ICNN}}'97)},
  doi = {10.1109/ICNN.1997.614197},
  author = {Kamimura, R.},
  month = jun,
  year = {1997},
  keywords = {artificial language,Artificial neural networks,D-entropy controller,Degradation,Difference equations,Entropy,formal languages,generalisation (artificial intelligence),generalization,inference,inference mechanisms,interpretation,Laboratories,learning (artificial intelligence),maximum entropy methods,Minimization methods,minimum entropy methods,Mutual information,neural learning,neural nets,neural networks,Neural networks,Uncertainty},
  pages = {1948-1953 vol.3},
  note = {ISSN:}
}

@inproceedings{5376749,
  title = {Factor {{Sensitivity Analysis}} for {{Multivariable Systems Using Bayesian Neural Networks}}},
  volume = {1},
  abstract = {Neural interpretation is of increasing interest in artificial neural networks and it is potential to reveal the intrinsic mechanism of multivariable systems. This study aims at investigating the efficiency of Bayesian neural networks in neural interpretation. The measures to ensure the stability of the network model are first elaborated and then, two types of Bayesian networks with linear and partly-linear transfer functions are exploited to conduct neural interpretation for simulated multivariable systems. Experimental results show that aided with connection weight calculation, Bayesian neural networks own distinct advantages over other network models in evaluating the relative contribution of independent variables to single dependent one. Therefore, the method proposed in this study is promising to perform factor sensitivity analysis for multivariable systems.},
  booktitle = {2009 {{International Conference}} on {{Computational Intelligence}} and {{Security}}},
  doi = {10.1109/CIS.2009.43},
  author = {Bai, R. and Qiu, X. and Cao, M.},
  month = dec,
  year = {2009},
  keywords = {Artificial neural networks,Bayesian methods,Bayesian neural networks,belief networks,Civil engineering,connection weight approach,connection weight calculation,Cost function,Educational institutions,factor sensitivity analysis,linear transfer functions,MIMO,multivariable systems,neural interpretation,neural nets,Neural networks,Neurons,partly-linear transfer functions,sensitivity analysis,Sensitivity analysis,simulated multivariable systems,Stability,transfer functions},
  pages = {30-33},
  note = {ISSN:}
}

@inproceedings{167089,
  title = {A Neural Network Based Expert System Model},
  abstract = {The architecture of an expert system model using artificial neural networks is proposed. The proposed model effectively supports the necessary components of an expert system such as user interface facility knowledge base, inference engine, and explanation system. The expert system model (ESM) consists of several orders of simple neural networks, each realizing a simple task. These simple neural networks are organized vertically, thereby achieving a second level of parallelism. A novel way to handle both forward and backward chaining reasoning mechanisms is presented. A secondary network model monitors the reasoning patterns of the primary model.{$<$}{$>$}},
  booktitle = {[{{Proceedings}}] {{Third International Conference}} on {{Tools}} for {{Artificial Intelligence}} - {{TAI}} 91},
  doi = {10.1109/TAI.1991.167089},
  author = {Hudli, A. V. and Palakal, M. J. and Zoran, M. J.},
  month = nov,
  year = {1991},
  keywords = {artificial neural networks,Artificial neural networks,backward chaining reasoning,Computer architecture,Computer networks,Engines,expert system model,expert systems,Expert systems,explanation,forward chaining reasoning,Humans,inference engine,inference mechanisms,Information science,Knowledge acquisition,neural nets,Neural networks,parallel processing,Problem-solving,user interface facility knowledge base},
  pages = {145-149},
  note = {ISSN:}
}

@article{5451119,
  title = {Intelligent {{Fabric Hand Prediction System With Fuzzy Neural Network}}},
  volume = {40},
  issn = {1094-6977},
  abstract = {Fabric selection is a crucial step in fashion product development. Prior research works have studied the prediction of fabric specimens based on the fabric hand descriptors via either traditional statistical methods or artificial intelligence methods. Despite showing good prediction accuracy, these methods usually lack an understandable ruleset, which means their ``interpretability'' is low. In this paper, a fuzzy neural network (FNN) based intelligent fabric hand prediction system is explored. Unlike some traditional FNN models in which a full ruleset of the artificial neural network (ANN) is presumed, the proposed FNN system includes a simplification of the network structure and feature selection, so that the number of rules is significantly reduced without big sacrifice on prediction accuracy. Real datasets collected from 30 participants' evaluation on a set of ten fabric specimens are used to train and test the performance of the proposed system. The system's prediction accuracy is found to be over 80\%. Applications of the proposed system are discussed and future research directions are outlined.},
  number = {6},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  doi = {10.1109/TSMCC.2010.2045121},
  author = {Yu, Y. and Hui, C. and Choi, T. and Au, R.},
  month = nov,
  year = {2010},
  keywords = {Accuracy,artificial intelligence,Artificial intelligence,artificial intelligence method,artificial neural network,Artificial neural network (ANN),Artificial neural networks,fabric hand prediction,fabric selection,fabric specimen,fabrics,Fabrics,fashion product development,feature extraction,feature selection,FNN system,fuzzy logic,fuzzy neural nets,fuzzy neural network,fuzzy neural network (FNN),Fuzzy neural networks,intelligent fabric hand prediction system,Intelligent networks,interpretability,network structure,Predictive models,Product development,Statistical analysis,System testing,textile industry},
  pages = {619-629}
}

@article{7552539,
  title = {Evaluating the {{Visualization}} of {{What}} a {{Deep Neural Network Has Learned}}},
  volume = {28},
  issn = {2162-237X},
  abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ``importance'' of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
  number = {11},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2016.2599820},
  author = {Samek, W. and Binder, A. and Montavon, G. and Lapuschkin, S. and M\"uller, K.},
  month = nov,
  year = {2017},
  keywords = {Algorithm design and analysis,Biological neural networks,complex machine learning tasks,Convolutional neural networks,data visualisation,data visualization,Deconvolution,deconvolution method,deep neural network,DNN,explaining classification,Heating,heatmap,ILSVRC2012,image classification,interpretable machine learning,learning (artificial intelligence),Learning systems,MIT Places data sets,multilayer nonlinear structure,neural nets,Neurons,relevance models,relevance propagation algorithm,Sensitivity,sensitivity-based approach,SUN397},
  pages = {2660-2673}
}

@inproceedings{4761819,
  title = {Improving Digital Ink Interpretation through Expected Type Prediction and Dynamic Dispatch},
  abstract = {Interpretation accuracy of current handwriting applications can be improved by providing contextual information about an ink samplepsilas expected type. We have developed a novel approach that uses a classic machine learning technique to predict this expected type from an ink sample. With this approach, we can create a ldquodynamic dispatch interpreterrdquo by biasing interpretation differently according to the predicted expected types of the ink samples. When evaluated in the domain of introductory computer science, our interpreter achieves high interpretation accuracy (87\%), an improvement from Microsoftpsilas default interpreter (62\%), and comparable with other previous interpreters (87-89\%), which, unlike ours, require additional user-specified expected type information for each ink sample.},
  booktitle = {2008 19th {{International Conference}} on {{Pattern Recognition}}},
  doi = {10.1109/ICPR.2008.4761819},
  author = {Tay, K. S. and Koile, K.},
  month = dec,
  year = {2008},
  keywords = {Application software,Artificial intelligence,Artificial neural networks,computer aided instruction,Computer science,contextual information,digital ink interpretation,dynamic dispatch interpreter,expected type prediction,Feature extraction,handwriting applications,Handwriting recognition,Hidden Markov models,Ink,Laboratories,learning (artificial intelligence),Machine learning,machine learning technique,Microsoft default interpreter},
  pages = {1-4},
  issn = {1051-4651}
}

@inproceedings{1001348,
  title = {An Interpretation of {{Preisach}}-{{Krasnoselskii}} Hysteresis Model with the Use of Artificial Neural Networks},
  abstract = {Summary form only given. This paper presents the application of artificial neural networks to implement an accurate magnetic hysteresis model based on the mathematical definition provided Preisach-Krasnoselskii (P-K) model. Accurate modeling of hysteresis is essential for both the design and the performance evaluation of electromagnetic devices. This paper shows that artificial neural networks (ANN) provide the natural setting whereby the P-K model can be successfully implemented.},
  booktitle = {2002 {{IEEE International Magnetics Conference}} ({{INTERMAG}})},
  doi = {10.1109/INTMAG.2002.1001348},
  author = {Sadeghian, A. R.},
  month = apr,
  year = {2002},
  keywords = {Artificial intelligence,artificial neural networks,Artificial neural networks,Computer networks,Context modeling,electromagnetic devices,Lakes,Magnetic analysis,magnetic hysteresis,Magnetic hysteresis,Magnetization processes,neural nets,Neural networks,Physics,physics computing,Preisach-Krasnoselskii hysteresis model},
  pages = {FS5-},
  note = {ISSN:}
}

@inproceedings{8614130,
  title = {Interpretability and {{Reproducability}} in {{Production Machine Learning Applications}}},
  abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  doi = {10.1109/ICMLA.2018.00105},
  author = {Ghanta, S. and Subramanian, S. and Sundararaman, S. and Khermosh, L. and Sridhar, V. and Arteaga, D. and Luo, Q. and Das, D. and Talagala, N.},
  month = dec,
  year = {2018},
  keywords = {Data models,explainability,human understandable machine learning,interpretability,interpretability techniques,learning (artificial intelligence),legal industry requirements,Load modeling,Machine learning,Pipelines,Predictive models,Production,production engineering computing,production machine learning applications,production ML reproducibility solution,production scenario,production viability,reproducability,systems,tracking,Training},
  pages = {658-664},
  note = {ISSN:}
}

@article{8631448,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  journal = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)},
  doi = {10.1109/DSAA.2018.00018},
  author = {Gilpin, L. H. and Bau, D. and Yuan, B. Z. and Bajwa, A. and Specter, M. and Kagal, L.},
  month = oct,
  year = {2018},
  keywords = {algorithmic fairness,artificial intelligence,Artificial intelligence,Biological neural networks,complex machines,Complexity theory,Computational modeling,data analysis,Decision trees,Deep learning and deep analytics,explaining explanations,explanatory artificial intelligence,Fairness and transparency in data science,learning (artificial intelligence),machine learning,Machine learning theories,Models and systems,neural nets,potential bias-problems,suggested future research directions,Taxonomy,training data,XAI},
  pages = {80-89},
  file = {/home/tim/Zotero/storage/2ARPDPK3/Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf;/home/tim/Zotero/storage/XETV5NHP/1806.html},
  note = {ISSN:}
}

@inproceedings{8204039,
  title = {Performance of Convolutional Neural Network and Recurrent Neural Network for Anticipation of Driver's Conduct},
  abstract = {With the advancements in Internet of Things (IoT), we could efficiently improve our daily life activities like health care, monitoring, transportation, smart homes etc. Artificial Intelligence along with Machine learning has played a very supportive role to analyze various situations and take decisions accordingly. Maneuver anticipation supplements existing Advance Driver Assistance Systems (ADAS) by anticipating mishaps and giving drivers more opportunity to respond to road circumstances proactively. The capacity to sort the driver conduct is extremely beneficial for advance driver assistance system (ADAS). Deep learning solutions would further be an endeavor of for driving conduct recognition. A technique for distinguishing driver's conduct is imperative to help operative mode transition between the driver and independent vehicles. We propose a novel approach of dissecting driver's conduct by using Convolutional Neural Network (CNN), Recurrent Neural Network(RNN) and a combination of Convolutional Neural Network with Long-Short Term Memory (LSTM) that would give better results in less response time. We are likewise proposing to concentrate high level features and interpretable features depicting complex driving examples by trying CNN, RNN and then CNN with LSTM. We could improve the system accuracy to 95\% by combining CNN with LSTM.},
  booktitle = {2017 8th {{International Conference}} on {{Computing}}, {{Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  doi = {10.1109/ICCCNT.2017.8204039},
  author = {Virmani, S. and Gite, S.},
  month = jul,
  year = {2017},
  keywords = {ADAS,advance driver assistance system,Advance Driver Assistance Systems,artificial intelligence,Automobiles,CNN,convolutional neural network,daily life activities,Deep learning CNN,deep learning solutions,driver conduct,driver information systems,driving conduct recognition,feedforward neural nets,Hidden Markov models,high level features,independent vehicles,Internet of Things,IoT,ITS,learning (artificial intelligence),long-short term memory,LSTM,machine learning,Machine learning,maneuver anticipation,operative mode transition,recurrent neural nets,Recurrent Neural Network,Recurrent neural networks,RNN,road circumstances,road vehicles,Technological innovation},
  pages = {1-8},
  note = {ISSN:}
}

@inproceedings{713989,
  title = {Explaining Student Grades Predicted by a Neural Network},
  volume = {1},
  abstract = {We have trained a backpropagation trained feedforward neural network to predict student performance in a large undergraduate computer science subject at the University of New South Wales. The prediction uses partial grades from during the teaching session to predict the final grade. The exam mark which is the major component (60\%) of the overall grade is not used. The purpose of this network is to allow students to predict the final grade they are likely to achieve based on current performance, and obviously to improve their performance if the predicted grade is below their expectations. By itself, however, the network is not adequate as it provides no feedback as to why their performance merits a particular grade. We therefore generate an explanation of the conclusion reached by the neural network for predicting particular student grades.},
  booktitle = {Proceedings of 1993 {{International Conference}} on {{Neural Networks}} ({{IJCNN}}-93-{{Nagoya}}, {{Japan}})},
  doi = {10.1109/IJCNN.1993.713989},
  author = {Gedeon, T. D. and Turner, S.},
  month = oct,
  year = {1993},
  keywords = {Aggregates,Australia,backpropagation,Computer science,Education,educational administrative data processing,educational computing,explanation,feedforward neural nets,feedforward neural network,Feedforward neural networks,Feedforward systems,Laboratories,Neural networks,Neurofeedback,Neurons,student grade,student performance prediction,University of New South Wales},
  pages = {609-612 vol.1},
  note = {ISSN:}
}

@article{1501920,
  title = {Computer-Aided Diagnostic Scheme for Distinction between Benign and Malignant Nodules in Thoracic Low-Dose {{CT}} by Use of Massive Training Artificial Neural Network},
  volume = {24},
  issn = {0278-0062},
  abstract = {Low-dose helical computed tomography (LDCT) is being applied as a modality for lung cancer screening. It may be difficult, however, for radiologists to distinguish malignant from benign nodules in LDCT. Our purpose in this study was to develop a computer-aided diagnostic (CAD) scheme for distinction between benign and malignant nodules in LDCT scans by use of a massive training artificial neural network (MTANN). The MTANN is a trainable, highly nonlinear filter based on an artificial neural network. To distinguish malignant nodules from six different types of benign nodules, we developed multiple MTANNs (multi-MTANN) consisting of six expert MTANNs that are arranged in parallel. Each of the MTANNs was trained by use of input CT images and teaching images containing the estimate of the distribution for the "likelihood of being a malignant nodule", i.e., the teaching image for a malignant nodule contains a two-dimensional Gaussian distribution and that for a benign nodule contains zero. Each MTANN was trained independently with ten typical malignant nodules and ten benign nodules from each of the six types. The outputs of the six MTANNs were combined by use of an integration ANN such that the six types of benign nodules could be distinguished from malignant nodules. After training of the integration ANN, our scheme provided a value related to the "likelihood of malignancy" of a nodule, i.e., a higher value indicates a malignant nodule, and a lower value indicates a benign nodule. Our database consisted of 76 primary lung cancers in 73 patients and 413 benign nodules in 342 patients, which were obtained from a lung cancer screening program on 7847 screenees with LDCT for three years in Nagano, Japan. The performance of our scheme for distinction between benign and malignant nodules was evaluated by use of receiver operating characteristic (ROC) analysis. Our scheme achieved an Az (area under the ROC curve) value of 0.882 in a round-robin test. Our scheme correctly identified 100\% (76/76) of malignant nodules as malignant, whereas 48\% (200/413) of benign nodules were identified correctly as benign. Therefore, our scheme may be useful in assisting radiologists in the diagnosis of lung nodules in LDCT.},
  number = {9},
  journal = {IEEE Transactions on Medical Imaging},
  doi = {10.1109/TMI.2005.852048},
  author = {Suzuki, K. and Sone, S. and Doi, K.},
  month = sep,
  year = {2005},
  keywords = {Algorithms,artificial intelligence,Artificial Intelligence,artificial neural network,Artificial neural network,Artificial neural networks,Automated,benign nodules,cancer,Cancer,Coin Lesion,Computed tomography,Computer networks,computer-aided diagnosis (CAD),computer-aided diagnostic scheme,Computer-Assisted,computerised tomography,Databases,dosimetry,Education,Gaussian distribution,helical computed tomography,Humans,likelihood of malignancy,likelihood pf malignancy,low-dose computed tomography,low-dose CT,lung,lung cancer screening,Lung Neoplasms,lung nodule,lung nodules,Lungs,malignant modules,massive training neural network,medical computing,neural nets,Neural Networks (Computer),nonlinear filter,Nonlinear filters,patient diagnosis,Pattern Recognition,Performance analysis,Pulmonary,Radiation Dosage,Radiographic Image Enhancement,Radiographic Image Interpretation,Radiography,receiver operating characteristic analysis,Reproducibility of Results,Retrospective Studies,round-robin test,Sensitivity and Specificity,Spiral Computed,Thoracic,thoracic CT,Tomography,two-dimensional Gaussian distribution},
  pages = {1138-1150}
}

@article{6138313,
  title = {Generalization {{Characteristics}} of {{Complex}}-{{Valued Feedforward Neural Networks}} in {{Relation}} to {{Signal Coherence}}},
  volume = {23},
  issn = {2162-237X},
  abstract = {Applications of complex-valued neural networks (CVNNs) have expanded widely in recent years-in particular in radar and coherent imaging systems. In general, the most important merit of neural networks lies in their generalization ability. This paper compares the generalization characteristics of complex-valued and real-valued feedforward neural networks in terms of the coherence of the signals to be dealt with. We assume a task of function approximation such as interpolation of temporal signals. Simulation and real-world experiments demonstrate that CVNNs with amplitude-phase-type activation function show smaller generalization error than real-valued networks, such as bivariate and dual-univariate real-valued neural networks. Based on the results, we discuss how the generalization characteristics are influenced by the coherence of the signals depending on the degree of freedom in the learning and on the circularity in neural dynamics.},
  number = {4},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2012.2183613},
  author = {Hirose, A. and Yoshida, S.},
  month = apr,
  year = {2012},
  keywords = {Algorithms,amplitude-phase-type activation function,Automated,Biological neural networks,bivariate real-valued neural networks,Coherence,coherent imaging systems,complex-valued feedforward neural networks,Complex-valued neural network,Computer Simulation,Computer-Assisted,dual-univariate real-valued neural networks,Feedback,Feedforward neural networks,function approximation,generalisation (artificial intelligence),generalization,generalization characteristics,Image Interpretation,learning,learning (artificial intelligence),Models,neural dynamics,neural nets,Neural Networks (Computer),Neurons,Optical Coherence,Pattern Recognition,radar systems,real-valued feedforward neural networks,signal coherence,signal processing,Signal to noise ratio,Statistical,supervised learning,temporal signal interpolation,Tomography,Vectors},
  pages = {541-551}
}

@inproceedings{4375533,
  title = {Protein {{Structure Prediction}} and {{Its Understanding Based}} on {{Machine Learning Methods}}},
  abstract = {Understanding protein structures is vital to determining the function of a protein and its interaction with DNA, RNA and enzyme. The information about its conformation can provide essential information for drug design and protein engineering. While there are over a million known protein sequences, only a limited number of protein structures are experimentally determined. Hence, prediction of protein structures from protein sequences using computer programs is an important step to unveil proteins' three dimensional conformation and functions. As a result, prediction of protein structures has profound theoretical and practical influence over biological study. The explanation of how a decision is made during prediction is also important for improving protein structure prediction and guiding the "wet experiments". In this talk, we will show how to use machine learning methods to improve the accuracy of protein structure prediction and to interpret prediction results. We will report our research on using neural networks, support vector machines combined with decision tree and association rule for protein structure prediction, rule extraction and prediction interpretation. Evaluation and comparisons of various prediction and rule extraction systems will be presented and future research direction in this area will also be identified.},
  booktitle = {2007 {{IEEE}} 7th {{International Symposium}} on {{BioInformatics}} and {{BioEngineering}}},
  doi = {10.1109/BIBE.2007.4375533},
  author = {Pan, Y.},
  month = oct,
  year = {2007},
  keywords = {association rule,Biochemistry,biology computing,Biology computing,conformation,data mining,decision tree,decision trees,DNA,Drugs,learning (artificial intelligence),Learning systems,machine learning methods,molecular biophysics,molecular configurations,neural nets,neural networks,Neural networks,Protein engineering,protein sequences,protein structure prediction,proteins,RNA,Sequences,support vector machines,Support vector machines},
  pages = {7-7},
  note = {ISSN:}
}

@article{6555905,
  title = {Multiclass {{Support Vector Machines With Example}}-{{Dependent Costs Applied}} to {{Plankton Biomass Estimation}}},
  volume = {24},
  issn = {2162-237X},
  abstract = {In many applications, the mistakes made by an automatic classifier are not equal, they have different costs. These problems may be solved using a cost-sensitive learning approach. The main idea is not to minimize the number of errors, but the total cost produced by such mistakes. This brief presents a new multiclass cost-sensitive algorithm, in which each example has attached its corresponding misclassification cost. Our proposal is theoretically well-founded and is designed to optimize cost-sensitive loss functions. This research was motivated by a real-world problem, the biomass estimation of several plankton taxonomic groups. In this particular application, our method improves the performance of traditional multiclass classification approaches that optimize the accuracy.},
  number = {11},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2013.2271535},
  author = {Gonz\'alez, P. and \'Alvarez, E. and Barranquero, J. and D\'iez, J. and {Gonz\'alez-Quir\'os}, R. and Nogueira, E. and {L\'opez-Urrutia}, \'A. and {del Coz}, J. J.},
  month = nov,
  year = {2013},
  keywords = {Algorithms,Automated,biology computing,Biomass,Computer-Assisted,Cost-sensitive learning,cost-sensitive learning approach,cost-sensitive loss functions,Estimation,example-dependent costs,Image Interpretation,Kernel,kernel methods,learning (artificial intelligence),Learning systems,Microscopy,multiclass classification approach,multiclass cost-sensitive algorithm,multiclass support vector machines,Optimization,Organisms,pattern classification,Pattern Recognition,Plankton,plankton biomass estimation,plankton recognition,plankton taxonomic groups,Reproducibility of Results,Sensitivity and Specificity,Support Vector Machine,support vector machines,Support vector machines,SVM},
  pages = {1901-1905}
}

@inproceedings{8418593,
  title = {{{AI2}}: {{Safety}} and {{Robustness Certification}} of {{Neural Networks}} with {{Abstract Interpretation}}},
  abstract = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2is precise enough to prove useful specifications (e.g., robustness), (ii) AI2can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2can handle deep convolutional networks, which are beyond the reach of existing methods.},
  booktitle = {2018 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  doi = {10.1109/SP.2018.00058},
  author = {Gehr, T. and Mirman, M. and {Drachsler-Cohen}, D. and Tsankov, P. and Chaudhuri, S. and Vechev, M.},
  month = may,
  year = {2018},
  keywords = {abstract interpretation,Abstract Interpretation,AI2,Biological neural networks,Cats,convolution,convolutional neural network layers,feedforward neural nets,fully connected networks,learning (artificial intelligence),max pooling layers,Neural Networks,neural networks robustness,neural networks safety,Neurons,Perturbation methods,phrase reasoning,program diagnostics,rectified linear unit activations,Reliable Machine Learning,Robustness,Safety,scalable analyzer,sound analyzer,symbolic analysis},
  pages = {3-18},
  issn = {2375-1207}
}

@article{4359184,
  title = {The {{Bayesian ARTMAP}}},
  volume = {18},
  issn = {1045-9227},
  abstract = {In this paper, we modify the fuzzy ARTMAP (FA) neural network (NN) using the Bayesian framework in order to improve its classification accuracy while simultaneously reduce its category proliferation. The proposed algorithm, called Bayesian ARTMAP (BA), preserves the FA advantages and also enhances its performance by the following: (1) representing a category using a multidimensional Gaussian distribution, (2) allowing a category to grow or shrink, (3) limiting a category hypervolume, (4) using Bayes' decision theory for learning and inference, and (5) employing the probabilistic association between every category and a class in order to predict the class. In addition, the BA estimates the class posterior probability and thereby enables the introduction of loss and classification according to the minimum expected loss. Based on these characteristics and using synthetic and 20 real-world databases, we show that the BA outperformes the FA, either trained for one epoch or until completion, with respect to classification accuracy, sensitivity to statistical overlapping, learning curves, expected loss, and category proliferation.},
  number = {6},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2007.900234},
  author = {Vigdor, B. and Lerner, B.},
  month = nov,
  year = {2007},
  keywords = {Algorithms,ART neural nets,Artificial Intelligence,Automated,Bayes decision theory,Bayes methods,Bayes Theorem,Bayes' decision theory,Bayesian ARTMAP,Bayesian methods,category hypervolume,category proliferation,category theory,class posterior probability,classification,classification accuracy,Computer Simulation,Computer-Assisted,Databases,Databases as Topic,decision theory,Decision theory,Diagnosis,expected loss,fuzzy ARTMAP (FA),fuzzy ARTMAP neural network,Fuzzy Logic,fuzzy neural nets,Fuzzy neural networks,Gaussian distribution,generalisation (artificial intelligence),Handwriting recognition,Image Interpretation,inference,Inference algorithms,inference mechanisms,learning (artificial intelligence),learning curves,multidimensional Gaussian distribution,Multidimensional systems,neural network (NN),Neural networks,Neural Networks (Computer),Normal Distribution,pattern classification,Pattern Recognition,probabilistic association,Software,Software Validation,statistical overlapping,Target recognition},
  pages = {1628-1644}
}

@inproceedings{713915,
  title = {A Neural Network for Interpretation of Multi-Meaning {{Chinese}} Words},
  volume = {1},
  abstract = {We proposed a neural network that can interpret multimeaning Chinese words correctly by using context information. The network is generated automatically basing on a Chinese-English dictionary and a knowledge-base of weights and self-organizingly builds a context according to key words of the processed text. Simulation experiment result proved that the network worked as expected.},
  booktitle = {Proceedings of 1993 {{International Conference}} on {{Neural Networks}} ({{IJCNN}}-93-{{Nagoya}}, {{Japan}})},
  doi = {10.1109/IJCNN.1993.713915},
  month = oct,
  year = {1993},
  keywords = {Automatic generation control,Chinese-English dictionary,Computer networks,Concrete,context information,Dictionaries,Helium,Information analysis,key words,knowledge-base,Morphology,multimeaning Chinese word interpretation,natural languages,Natural languages,neural nets,Neural networks,self-organizing neural network,Temperature},
  pages = {291-294 vol.1},
  note = {ISSN:}
}

@inproceedings{8661467,
  title = {Diagnosis of Faults in Power Transformers through the Interpretation of {{FRA}} Testing with Artificial Intelligence},
  abstract = {In the present work, the artificial intelligence is used, through neural networks, in the diagnosis of power transformers for the interpretation of the results obtained in the frequency response analysis test. The results of the classification of three types of failures in this technique are exposed. The characteristics of the statistical indicators that function as input variables of the neural network and the reason for implementing a multilayer network with backpropagation algorithm, in the training are exposed. The results of the discrimination of acceptable or not acceptable state of the transformer are presented and if it has any of the following faults: open winding, short circuit winding and winding with a point to ground. The response of the neural network is determined in case of study of real transformers.},
  booktitle = {2018 {{IEEE International Autumn Meeting}} on {{Power}}, {{Electronics}} and {{Computing}} ({{ROPEC}})},
  doi = {10.1109/ROPEC.2018.8661467},
  author = {Ar\'eu, O. H. and Men\'endez, A. M. G. and S\'anchez, J. I. H. and Vald\'es, E. S.},
  month = nov,
  year = {2018},
  keywords = {artificial intelligence,Artificial intelligence,artificial neuronal network,backpropagation,backpropagation algorithm,fault diagnosis,faults diagnosis,FRA testing,frequency response,frequency response analysis test,multilayer network,neural nets,neural network,open winding,power engineering computing,power transformer testing,power transformers,short circuit winding,statistical analysis,statistical indicators,transformer diagnostic,transformer windings},
  pages = {1-5},
  issn = {2573-0770}
}

@inproceedings{860756,
  title = {Quality Assured Efficient Engineering of Feedforward Neural Networks with Supervised Learning ({{QUEEN}}) Evaluated with the"pima Indians Diabetes Database"},
  volume = {4},
  abstract = {The QUEEN method is based on four main concepts: 1 The QUEEN phase model is derived from the spiral model for the evaluation of the constructed neural network; 2. An overall strategy for the development process enables a continuous supervision, assessment and quality assurance of each step, from the collection of the examples to the evaluation of the constructed neural network. For the assessment of the quality achieved in the development process, a novel quality indicator is introduced. This indicator gives a measure of the complexity of a task in a given representation. This strategy of QUEEN involves the stepwise simplification of the task; 3. The development of the neural networks is structured by the definition of an order over neural networks. The order takes into account the complexity of the interpretation of the neural network by an expert of the application domain. To yield easily interpretable neural networks, and also to get simple models that enable the detection of data artefacts, the development is started with the simplest adequate neural network; 4. The developer is provided with a set of diagnostic methods and tools that will identify and eliminate reasons for difficulties. The novel quality indicator for example, provides the developer with a diagnostic tool that will identify situations where a representation or a network is unnecessarily complex. QUEEN was developed and successfully evaluated in more than 20 projects mostly in the medical application domain. The paper presents the concepts of QUEEN and describes how QUEEN was applied to set-up a feedforward neural network on the pima indians diabetes database, a database that has been used as a benchmark in several studies, QUEEN highlighted several severe data artefacts in this database.},
  booktitle = {Proceedings of the {{IEEE}}-{{INNS}}-{{ENNS International Joint Conference}} on {{Neural Networks}}. {{IJCNN}} 2000. {{Neural Computing}}: {{New Challenges}} and {{Perspectives}} for the {{New Millennium}}},
  doi = {10.1109/IJCNN.2000.860756},
  author = {Waschulzik, T. and Brauer, W. and Castedello, T. and Henery, B.},
  month = jul,
  year = {2000},
  keywords = {assessment,data artefacts,Data engineering,Databases,development process,Diabetes,diagnostic methods,diagnostic tool,feedforward neural nets,feedforward neural networks,Feedforward neural networks,learning (artificial intelligence),Machine learning,medical computing,Neural networks,phase model,pima indians diabetes database,quality assurance,Quality assurance,quality assured efficient engineering,quality indicator,QUEEN method,spiral model,Spirals,Statistics,stepwise simplification,supervised learning,Supervised learning,supervision},
  pages = {97-102 vol.4},
  issn = {1098-7576}
}

@inproceedings{1007533,
  title = {Graffiti Commands Interpretation for {{eBooks}} Using a Self-Structured Neural Network and Genetic Algorithm},
  volume = {3},
  abstract = {This paper presents the interpretation of graffiti commands for electronic books (eBooks). A neural network is employed to perform the graffiti interpretation. By introducing a switch to each link of the neural network, the structure of the neural network can be obtained and tuned automatically by a genetic algorithm (GA) with arithmetic crossover and non-uniform mutation. Simulation results on interpreting graffiti commands for eBooks using the proposed neural network are shown.},
  booktitle = {Proceedings of the 2002 {{International Joint Conference}} on {{Neural Networks}}. {{IJCNN}}'02 ({{Cat}}. {{No}}.{{02CH37290}})},
  doi = {10.1109/IJCNN.2002.1007533},
  author = {Leung, K. F. and Lam, H. K. and Leung, F. H. F. and Tam, P. K. S.},
  month = may,
  year = {2002},
  keywords = {Arithmetic,arithmetic crossover,character recognition equipment,Computational modeling,computer graphic equipment,electronic books,electronic publishing,Electronic publishing,genetic algorithm,genetic algorithms,Genetic algorithms,Genetic engineering,Genetic mutations,graffiti command interpretation,graphical user interfaces,handwritten character recognition,neural net architecture,neural network link switch,neural network structure tuning,Neural networks,nonuniform mutation,notebook computers,Personal digital assistants,self-organising feature maps,self-structured neural network,Signal processing algorithms,simulation,Switches,symbol manipulation,touch sensitive screens,tuning},
  pages = {2487-2492 vol.3},
  issn = {1098-7576}
}

@article{1262340,
  title = {A Probabilistic Active Support Vector Learning Algorithm},
  volume = {26},
  issn = {0162-8828},
  abstract = {The paper describes a probabilistic active learning strategy for support vector machine (SVM) design in large data applications. The learning strategy is motivated by the statistical query model. While most existing methods of active SVM learning query for points based on their proximity to the current separating hyperplane, the proposed method queries for a set of points according to a distribution as determined by the current separating hyperplane and a newly defined concept of an adaptive confidence factor. This enables the algorithm to have more robust and efficient learning capabilities. The confidence factor is estimated from local information using the k nearest neighbor principle. The effectiveness of the method is demonstrated on real-life data sets both in terms of generalization performance, query complexity, and training time.},
  number = {3},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2004.1262340},
  author = {Mitra, P. and Murthy, C. A. and Pal, S. K.},
  month = mar,
  year = {2004},
  keywords = {adaptive confidence factor,Algorithm design and analysis,Algorithms,Artificial Intelligence,Automated,Breast Neoplasms,Cluster Analysis,Computer-Assisted,Computing Methodologies,data mining,Design optimization,Diagnosis,generalization performance,Humans,Image Interpretation,Information Storage and Retrieval,Iterative algorithms,k nearest neighbor principle,Lagrangian functions,Large-scale systems,learning (artificial intelligence),Machine learning,Models,Numerical Analysis,pattern recognition,Pattern Recognition,probabilistic active learning,probability,Quadratic programming,query complexity,real life data sets,Reproducibility of Results,Robustness,Sensitivity and Specificity,Signal Processing,Statistical,statistical query model,Subtraction Technique,support vector learning algorithm,support vector machine,Support vector machine classification,support vector machines,Support vector machines,SVM,training time},
  pages = {413-418}
}

@inproceedings{7966301,
  title = {Interpretability of Artificial Hydrocarbon Networks for Breast Cancer Classification},
  abstract = {In machine learning, interpretability refers to understand the underlying behavior of the prediction of a model in order to identify diagnosis criteria and/or new rules from its output. Interpretability contributes to increase the usability of the method. Also, it is relevant in decision support systems, such as in medical applications. White-box models like tree-based, rule-based and linear models are considered the most comprehensible, but less accurate or simplistic. In contrast, black-box models like nonlinear and ensemble models are more accurate hence more complex to interpret. Thus, a trade-off between accuracy and interpretability is often made when building models to support human experts in a decision-making process. Artificial hydrocarbon networks (AHN) is a supervised learning method that has been proved to be very effective for regression and classification problems. In fact, its training process suggests a kind of interpretability. Thus, the objective of this work is to present first efforts proving the capacity of artificial hydrocarbon networks (AHN) to deliver interpretable models. In order to assess the interpretability of AHN, we address the breast cancer problem using a public dataset. Results showed that AHN can be transformed in treebased and rule-based models preserving high accuracy in the output classification.},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  doi = {10.1109/IJCNN.2017.7966301},
  author = {Ponce, H. and {de Lourdes Martinez-Villase\~nor}, M.},
  month = may,
  year = {2017},
  keywords = {artificial hydrocarbon networks,black-box models,Breast cancer,breast cancer classification,breast cancer problem,cancer,classification problem,Compounds,decision support systems,diagnosis criteria,ensemble model,Hydrocarbons,interpretability,learning (artificial intelligence),linear model,machine learning,medical applications,medical computing,nonlinear model,pattern classification,Predictive models,public dataset,regression problem,rule-based model,supervised learning,Supervised learning,supervised learning method,tree-based model,Usability,white-box models},
  pages = {3535-3542},
  issn = {2161-4407}
}

@article{http://arxiv.org/abs/1604.00289v3,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  journal = {arxiv},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  month = apr,
  year = {2016}
}

@article{http://arxiv.org/abs/1611.07567v1,
  title = {Feature {{Importance Measure}} for {{Non}}-Linear {{Learning Algorithms}}},
  abstract = {Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point).},
  journal = {arxiv},
  author = {Vidovic, Marina M. -C. and G\"ornitz, Nico and M\"uller, Klaus-Robert and Kloft, Marius},
  month = nov,
  year = {2016}
}

@article{http://arxiv.org/abs/1705.06936v1,
  title = {Atari Games and {{Intel}} Processors},
  abstract = {The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions. In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0 machine learning framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.},
  journal = {arxiv},
  author = {Adamski, Robert and Grel, Tomasz and Klimek, Maciej and Michalewski, Henryk},
  month = may,
  year = {2017}
}

@article{http://arxiv.org/abs/1708.08296v1,
  title = {Explainable {{Artificial Intelligence}}: {{Understanding}}, {{Visualizing}} and {{Interpreting Deep Learning Models}}},
  abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
  journal = {arxiv},
  author = {Samek, Wojciech and Wiegand, Thomas and M\"uller, Klaus-Robert},
  month = aug,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.00262v1,
  title = {Fine-Grained {{Event Learning}} of {{Human}}-{{Object Interaction}} with {{LSTM}}-{{CRF}}},
  abstract = {Event learning is one of the most important problems in AI. However, notwithstanding significant research efforts, it is still a very complex task, especially when the events involve the interaction of humans or agents with other objects, as it requires modeling human kinematics and object movements. This study proposes a methodology for learning complex human-object interaction (HOI) events, involving the recording, annotation and classification of event interactions. For annotation, we allow multiple interpretations of a motion capture by slicing over its temporal span, for classification, we use Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field (CRF) for constraints of outputs. Using a setup involving captures of human-object interaction as three dimensional inputs, we argue that this approach could be used for event types involving complex spatio-temporal dynamics.},
  journal = {arxiv},
  author = {Do, Tuan and Pustejovsky, James},
  month = sep,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.04806v2,
  title = {Deep {{Learning}} for {{Case}}-{{Based Reasoning}} through {{Prototypes}}: {{A Neural Network}} That {{Explains Its Predictions}}},
  abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
  journal = {arxiv},
  author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.10967v3,
  title = {Artificial {{Intelligence}} as {{Structural Estimation}}: {{Economic Interpretations}} of {{Deep Blue}}, {{Bonanza}}, and {{AlphaGo}}},
  abstract = {Artificial intelligence (AI) has achieved superhuman performance in a growing number of tasks, but understanding and explaining AI remain challenging. This paper clarifies the connections between machine-learning algorithms to develop AIs and the econometrics of dynamic structural models through the case studies of three famous game AIs. Chess-playing Deep Blue is a calibrated value function, whereas shogi-playing Bonanza is an estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy network" is a deep neural network implementation of Hotz and Miller's (1993) conditional choice probability estimation; its "reinforcement-learning value network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional choice simulation method. Relaxing these AIs' implicit econometric assumptions would improve their structural interpretability.},
  journal = {arxiv},
  author = {Igami, Mitsuru},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.09482v2,
  title = {An {{Introduction}} to {{Deep Visual Explanation}}},
  abstract = {The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.},
  journal = {arxiv},
  author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.09784v1,
  title = {Distilling a {{Neural Network Into}} a {{Soft Decision Tree}}},
  abstract = {Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
  journal = {arxiv},
  author = {Frosst, Nicholas and Hinton, Geoffrey},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.08107v1,
  title = {A {{Deep Learning Interpretable Classifier}} for {{Diabetic Retinopathy Disease Grading}}},
  abstract = {Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.},
  journal = {arxiv},
  author = {de la Torre, Jordi and Valls, Aida and Puig, Domenec},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
  title = {A {{Human}}-{{Grounded Evaluation Benchmark}} for {{Local Explanations}} of {{Machine Learning}}},
  abstract = {In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.},
  journal = {arxiv},
  author = {Mohseni, Sina and Ragan, Eric D.},
  month = jan,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.00541v1,
  title = {Causal {{Learning}} and {{Explanation}} of {{Deep Neural Networks}} via {{Autoencoded Activations}}},
  abstract = {Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.},
  journal = {arxiv},
  author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.01274v1,
  title = {Dream {{Formulations}} and {{Deep Neural Networks}}: {{Humanistic Themes}} in the {{Iconology}} of the {{Machine}}-{{Learned Image}}},
  abstract = {This paper addresses the interpretability of deep learning-enabled image recognition processes in computer vision science in relation to theories in art history and cognitive psychology on the vision-related perceptual capabilities of humans. Examination of what is determinable about the machine-learned image in comparison to humanistic theories of visual perception, particularly in regard to art historian Erwin Panofsky's methodology for image analysis and psychologist Eleanor Rosch's theory of graded categorization according to prototypes, finds that there are surprising similarities between the two that suggest that researchers in the arts and the sciences would have much to benefit from closer collaborations. Utilizing the examples of Google's DeepDream and the Machine Learning and Perception Lab at Georgia Tech's Grad-CAM: Gradient-weighted Class Activation Mapping programs, this study suggests that a revival of art historical research in iconography and formalism in the age of AI is essential for shaping the future navigation and interpretation of all machine-learned images, given the rapid developments in image recognition technologies.},
  journal = {arxiv},
  author = {Spratt, Emily L.},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
  title = {Explanation {{Methods}} in {{Deep Learning}}: {{Users}}, {{Values}}, {{Concerns}} and {{Challenges}}},
  abstract = {Issues regarding explainable AI involve four components: users, laws \& regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.},
  journal = {arxiv},
  author = {Ras, Gabrielle and van Gerven, Marcel and Haselager, Pim},
  month = mar,
  year = {2018},
  keywords = {Artificial intelligence,Deep neural networks,Explainable AI,Explanation methods,Interpretability},
  file = {/home/tim/Zotero/storage/GZ7R6V8C/Ras et al. - 2018 - Explanation Methods in Deep Learning Users, Value.pdf}
}

@article{http://arxiv.org/abs/1804.01396v1,
  title = {Artificial {{Intelligence}} and Its {{Role}} in {{Near Future}}},
  abstract = {AI technology has a long history which is actively and constantly changing and growing. It focuses on intelligent agents, which contain devices that perceive the environment and based on which takes actions in order to maximize goal success chances. In this paper, we will explain the modern AI basics and various representative applications of AI. In the context of the modern digitalized world, AI is the property of machines, computer programs, and systems to perform the intellectual and creative functions of a person, independently find ways to solve problems, be able to draw conclusions and make decisions. Most artificial intelligence systems have the ability to learn, which allows people to improve their performance over time. The recent research on AI tools, including machine learning, deep learning and predictive analysis intended toward increasing the planning, learning, reasoning, thinking and action taking ability. Based on which, the proposed research intends towards exploring on how the human intelligence differs from the artificial intelligence. Moreover, we critically analyze what AI of today is capable of doing, why it still cannot reach human intelligence and what are the open challenges existing in front of AI to reach and outperform human level of intelligence. Furthermore, it will explore the future predictions for artificial intelligence and based on which potential solution will be recommended to solve it within next decades.},
  journal = {arxiv},
  author = {Shabbir, Jahanzaib and Anwer, Tarique},
  month = apr,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.07470v1,
  title = {Contrastive {{Explanations}} with {{Local Foil Trees}}},
  abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like "Why this output (the fact) instead of that output (the foil)?" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.},
  journal = {arxiv},
  author = {van der Waa, Jasper and Robeer, Marcel and van Diggelen, Jurriaan and Brinkhuis, Matthieu and Neerincx, Mark},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.09809v1,
  title = {Generating {{Counterfactual Explanations}} with {{Natural Language}}},
  abstract = {Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classification decision if present in an image (e.g., "This is not a Scarlet Tanager because it does not have black wings.") We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classification decision if present in the image. To demonstrate our method we consider a fine-grained image classification task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.},
  journal = {arxiv},
  author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
  title = {Evaluating {{Feature Importance Estimates}}},
  abstract = {Interpretability methods should be both meaningful to a human and correctly explain model behavior. In this work, we propose a benchmark to evaluate the latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative accuracy of interpretability methods that estimate feature importance in deep neural networks. We evaluate commonly used interpretability methods and a set of recently proposed ensemble-based derivative approaches. Our results across several large-scale image classification datasets are consistent and thought-provoking -- we find that the formal methods we consider produce estimates that are less accurate or on par with a random designation of feature importance. However, certain derivative approaches that ensemble these estimates far outperform such a random guess. The manner of ensembling remains critical, we show that some approaches do no better than the underlying method but carry a far higher computational burden.},
  journal = {arxiv},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.00154v1,
  title = {{{AI}} in {{Education}} Needs Interpretable Machine Learning: {{Lessons}} from {{Open Learner Modelling}}},
  abstract = {Interpretability of the underlying AI representations is a key raison d'\^etre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of learners' cognition and emotions for the purpose of supporting human learning and teaching. Over thirty years of research in ITS (also known as AI in Education) produced important work, which informs about how AI can be used in Education to best effects and, through the OLM research, what are the necessary considerations to make it interpretable and explainable for the benefit of learning. We argue that this work can provide a valuable starting point for a framework of interpretable AI, and as such is of relevance to the application of both knowledge-based and machine learning systems in other high-stakes contexts, beyond education.},
  journal = {arxiv},
  author = {Conati, Cristina and {Porayska-Pomsta}, Kaska and Mavrikis, Manolis},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
  title = {Explanations for {{Temporal Recommendations}}},
  abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for recommendation systems (RS) provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network (RNN) architecture for recommendation and a neighbourhood-based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
  journal = {arxiv},
  author = {Bharadhwaj, Homanga and Joshi, Shruti},
  month = jul,
  year = {2018},
  keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
  file = {/home/tim/Zotero/storage/TLKPAEUT/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf}
}

@article{http://arxiv.org/abs/1807.07404v1,
  title = {Analyzing {{Hypersensitive AI}}: {{Instability}} in {{Corporate}}-{{Scale Machine Learning}}},
  abstract = {Predictive geometric models deliver excellent results for many Machine Learning use cases. Despite their undoubted performance, neural predictive algorithms can show unexpected degrees of instability and variance, particularly when applied to large datasets. We present an approach to measure changes in geometric models with respect to both output consistency and topological stability. Considering the example of a recommender system using word2vec, we analyze the influence of single data points, approximation methods and parameter settings. Our findings can help to stabilize models where needed and to detect differences in informational value of data points on a large scale.},
  journal = {arxiv},
  author = {Regneri, Michaela and Hoffmann, Malte and Kost, Jurij and Pietsch, Niklas and Schulz, Timo and Stamm, Sabine},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1809.02479v1,
  title = {Convolutional {{Neural Network}}: {{Text Classification Model}} for {{Open Domain Question Answering System}}},
  abstract = {Recently machine learning is being applied to almost every data domain one of which is Question Answering Systems (QAS). A typical Question Answering System is fairly an information retrieval system, which matches documents or text and retrieve the most accurate one. The idea of open domain question answering system put forth, involves convolutional neural network text classifiers. The Classification model presented in this paper is multi-class text classifier. The neural network classifier can be trained on large dataset. We report series of experiments conducted on Convolution Neural Network (CNN) by training it on two different datasets. Neural network model is trained on top of word embedding. Softmax layer is applied to calculate loss and mapping of semantically related words. Gathered results can help justify the fact that proposed hypothetical QAS is feasible. We further propose a method to integrate Convolutional Neural Network Classifier to an open domain question answering system. The idea of Open domain will be further explained, but the generality of it indicates to the system of domain specific trainable models, thus making it an open domain.},
  journal = {arxiv},
  author = {Amin, Muhammad Zain and Nadeem, Noman},
  month = sep,
  year = {2018}
}

@inproceedings{8614020,
  title = {Explainable {{A}}.{{I}}.: {{The Promise}} of {{Genetic Programming Multi}}-Run {{Subtree Encapsulation}}},
  abstract = {Deep Learning and other Artificial Neural Network based solutions are rarely transparent, and white-box solutions are often called for. This paper explains how Multirun Subtree Encapsulation can provide equivalent white box solutions to facilitate Explainable Artificial Intelligence.},
  booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
  doi = {10.1109/iCMLDE.2018.00037},
  author = {Howard, D. and Edwards, M. A.},
  month = dec,
  year = {2018},
  keywords = {A.I.,artificial neural network,Artificial neural networks,Artificial Neural Networks,Automatically Defined Functions,black box,data encapsulation,Databases,deep learning,Deep learning,Deep Learning,Encapsulation,Evolutionary Computation,explainable AI,explainable artificial intelligence,Explainable Artificial Intelligence,expression simplification,genetic algorithms,genetic programming,Genetic programming,Genetic Programming,learning (artificial intelligence),modularization,multirun subtree encapsulation,Multirun Subtree Encapsulation,neural nets,Software Evolution,Standards,subtree database,Subtree Encapsulation,trees (mathematics),white box,white-box solutions},
  pages = {158-159},
  note = {ISSN:}
}

@inproceedings{5158992,
  title = {A {{Study}} and {{Application}} on {{Machine Learning}} of {{Artificial Intellligence}}},
  abstract = {This thesis elaborated the concept, significance and main strategy of machine learning as well as the basic structure of machine learning system. By combining several basic ideas of main strategies, great effort are laid on introducing several machine learning methods, such as Rote learning, Explanation-based learning, Learning from instruction, Learning by deduction, Learning by analogy and Inductive learning, etc. Meanwhile, comparison and analysis are made upon their respective advantages and limitations. At the end of the article, it proposes the research objective of machine learning and points out its development trend.Machine learning is a fundamental way that enable the computer to have the intelligence ; Its application which had been used mainly the method of induction and the synthesis, rather than the deduction has already reached many fields of Artificial Intelligence.},
  booktitle = {2009 {{International Joint Conference}} on {{Artificial Intelligence}}},
  doi = {10.1109/JCAI.2009.55},
  author = {Xue, M. and Zhu, C.},
  month = apr,
  year = {2009},
  keywords = {AI,algorithm,Application software,artificial intelligence,Artificial intelligence,Computational modeling,explanation-based learning,Humans,Inductive learning,Intelligent robots,Intelligent systems,learning (artificial intelligence),learning by analogy,learning by deduction,learning from instruction,learning strategy,Learning systems,machine learning,Machine learning,Machine learning algorithms,machine learning system,Physiology,rote learning,system structure},
  pages = {272-274},
  note = {ISSN:}
}

@inproceedings{http://arxiv.org/abs/1811.11705v1,
  title = {An {{Adversarial Approach}} for {{Explainable AI}} in {{Intrusion Detection Systems}}},
  abstract = {Despite the growing popularity of modern machine learning techniques (e.g. Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
  booktitle = {Arxiv},
  author = {Marino, Daniel L. and Wickramasinghe, Chathurika S. and Manic, Milos},
  month = nov,
  year = {2018},
  keywords = {adversarial approach,adversarial machine learning,Adversarial Machine Learning,Adversarial samples,cyber-security,cyber-security applications,data-driven intrusion detection systems,deep neural networks,Estimation,explainable AI,Explainable AI,IDSs,Intrusion detection,learning (artificial intelligence),Machine learning,machine learning techniques,Mathematical model,multilayer perceptron classifiers,multilayer perceptrons,neural nets,pattern classification,security of data,Visualization}
}

@article{http://arxiv.org/abs/1811.11839v2,
  title = {A {{Survey}} of {{Evaluation Methods}} and {{Measures}} for {{Interpretable Machine Learning}}},
  abstract = {The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.},
  journal = {arxiv},
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.02340v4,
  title = {Continual {{Learning Augmented Investment Decisions}}},
  abstract = {Investment decisions can benefit from incorporating an accumulated knowledge of the past to drive future decision making. We introduce Continual Learning Augmentation (CLA) which is based on an explicit memory structure and a feed forward neural network (FFNN) base model and used to drive long term financial investment decisions. We demonstrate that our approach improves accuracy in investment decision making while memory is addressed in an explainable way. Our approach introduces novel remember cues, consisting of empirically learned change points in the absolute error series of the FFNN. Memory recall is also novel, with contextual similarity assessed over time by sampling distances using dynamic time warping (DTW). We demonstrate the benefits of our approach by using it in an expected return forecasting task to drive investment decisions. In an investment simulation in a broad international equity universe between 2003-2017, our approach significantly outperforms FFNN base models. We also illustrate how CLA's memory addressing works in practice, using a worked example to demonstrate the explainability of our approach.},
  journal = {arxiv},
  author = {Philps, Daniel and Weyde, Tillman and d'Avila Garcez, Artur and Batchelor, Roy},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.04801v1,
  title = {Can {{I}} Trust You More? {{Model}}-{{Agnostic Hierarchical Explanations}}},
  abstract = {Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mah\'e, a novel approach to provide Model-agnostic hierarchical \'explanations of how powerful machine learning models, such as deep neural networks, capture these interactions as either dependent on or free of the context of data instances. Specifically, Mah\'e provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. Experimental results show that Mah\'e obtains improved local interaction interpretations over state-of-the-art methods and successfully explains interactions that are context-free.},
  journal = {arxiv},
  author = {Tsang, Michael and Sun, Youbang and Ren, Dongxu and Liu, Yan},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1901.06560v1,
  title = {Explaining {{Explanations}} to {{Society}}},
  abstract = {There is a disconnect between explanatory artificial intelligence (XAI) methods and the types of explanations that are useful for and demanded by society (policy makers, government officials, etc.) Questions that experts in artificial intelligence (AI) ask opaque systems provide inside explanations, focused on debugging, reliability, and validation. These are different from those that society will ask of these systems to build trust and confidence in their decisions. Although explanatory AI systems can answer many questions that experts desire, they often don't explain why they made decisions in a way that is precise (true to the model) and understandable to humans. These outside explanations can be used to build trust, comply with regulatory and policy changes, and act as external validation. In this paper, we focus on XAI methods for deep neural networks (DNNs) because of DNNs' use in decision-making and inherent opacity. We explore the types of questions that explanatory DNN systems can answer and discuss challenges in building explanatory systems that provide outside explanations for societal requirements and benefit.},
  journal = {arxiv},
  author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
  month = jan,
  year = {2019}
}

@inproceedings{8389957,
  title = {Efficiency Enhancement of Food Recognition Using Artificial Neural Network},
  abstract = {In this paper, we have a tendency to apply a artificial neural network (ANN) to the tasks of detective work and recognizing food pictures. Be- explanation for the wide diversity of styles of food, image recognition of food things is usually terribly difficulties. Be that as it may, deep learning has been indicated as of late to be a truly intense image recognition system, and ANN could be a dynamic way to deal with deep learning. We tend to connected ANN to the errands of food location and recognition through parameter change. We tend to made a dataset of the preeminent incessant food things in a publically available food-logging framework, and utilized it to recognition execution. ANN demonstrated fundamentally higher accuracy than did antiquated support-vector-machine-based routes with handmade choices. Furthermore, we tend to establish that the convolution bits demonstrate that shading commands the component extraction strategy. For food image discovery, ANN also indicated fundamentally higher accuracy than a customary procedure. Fundamentally higher accuracy than a customary strategy.},
  booktitle = {2017 {{International Conference}} on {{Energy}}, {{Communication}}, {{Data Analytics}} and {{Soft Computing}} ({{ICECDS}})},
  doi = {10.1109/ICECDS.2017.8389957},
  author = {Lasod, A. and Soni, D.},
  month = aug,
  year = {2017},
  keywords = {ANN,artificial neural network,Artificial neural network,Artificial neural networks,Biological neural networks,convolution bits,deep learning,efficiency enhancement,Feature extraction,food image discovery,Food image recognition,food location,food pictures recognization,food recognition,handmade choices,image recognition,Image recognition,image recognition system,learning (artificial intelligence),Machine learning,neural nets,Neurons,publically available food-logging framework,support vector machines,support-vector-machine,SVM},
  pages = {2758-2762},
  note = {ISSN:}
}

@inproceedings{1279325,
  title = {A Multiple Objective Optimization Based {{GA}} for Designing Interpretable and Comprehensible Neural Network Trees},
  volume = {1},
  abstract = {Neural network tree (NNTree) is a hybrid model for machine learning. The overall structure is a decision tree (DT), and each non-terminal node is an expert neural network (ENN). Generally speaking, NNTrees can achieve better performance than conventional DTs with fewer nodes, and the performance of the tree can be improved through incremental learning. In addition, the NNTrees can be interpreted in polynomial time if the number of inputs for each ENN is limited. In this paper, we propose a multiple objective optimization based genetic algorithm (MOO-GA) for designing interpretable and comprehensible NNTrees. The efficiency of the proposed algorithm is validated by experimental results.},
  booktitle = {International {{Conference}} on {{Neural Networks}} and {{Signal Processing}}, 2003. {{Proceedings}} of the 2003},
  doi = {10.1109/ICNNSP.2003.1279325},
  month = dec,
  year = {2003},
  keywords = {Algorithm design and analysis,decision tree,decision trees,Decision trees,Design optimization,expert neural network,genetic algorithm,genetic algorithms,Genetic algorithms,Helium,Humans,incremental learning,learning (artificial intelligence),machine learning,Machine learning,Machine learning algorithms,multiple objective optimization,neural nets,neural network trees,Neural networks,Polynomials},
  pages = {518-521 Vol.1},
  note = {ISSN:}
}

@article{8489172,
  title = {Interpretable {{Deep Convolutional Neural Networks}} via {{Meta}}-Learning},
  abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for ``algorithmic fairness'' also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
  journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
  doi = {10.1109/IJCNN.2018.8489172},
  author = {Liu, X. and Wang, X. and Matwin, S.},
  month = jul,
  year = {2018},
  keywords = {base learning algorithms,big data,clustering algorithm,CNN-INTE,Computational modeling,convolution,Convolutional Neural Network,deep CNN model,deep learning,feedforward neural nets,interpretability,interpretable deep convolutional neural networks,learning (artificial intelligence),Machine learning,Machine learning algorithms,machine learning approaches,meta-learning,Meta-learning,meta-level test data,meta-level training data,MNIST image dataset,model interpretability,pattern classification,pattern clustering,Prediction algorithms,Predictive models,random forest,random processes,TensorFlow,Training data,Visualization},
  pages = {1-9},
  issn = {2161-4407}
}

@article{http://arxiv.org/abs/1902.02041v1,
  title = {Fooling {{Neural Network Interpretations}} via {{Adversarial Model Manipulation}}},
  abstract = {We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original model. By incorporating the interpretation results directly in the regularization term of the objective function for fine-tuning, we show that the state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled with our model manipulation. We propose two types of fooling, passive and active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.},
  journal = {arxiv},
  author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.03501v1,
  title = {Assessing the {{Local Interpretability}} of {{Machine Learning Models}}},
  abstract = {The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user's ability to run a model on a given input) and "what if" local explainability (a user's ability to correctly indicate the outcome to a model under local changes to the input). Through a user study with 1000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and "what if" local explainability on models that are typically considered locally interpretable. We find evidence consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks. We propose a metric - the runtime operation count on the simulatability task - to indicate the relative interpretability of models and show that as the number of operations increases the users' accuracy on the local interpretability tasks decreases.},
  journal = {arxiv},
  author = {Friedler, Sorelle A. and Roy, Chitradeep Dutta and Scheidegger, Carlos and Slack, Dylan},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
  title = {Aggregating Explainability Methods for Neural Networks Stabilizes Explanations},
  abstract = {Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation. In fact, most works rely on manually assessing the explanation to evaluate the quality of a method. This injects uncertainty in the explanation process along several dimensions: Which explanation method to apply? Who should we ask to evaluate it and which criteria should be used for the evaluation? Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. Our findings show that the aggregation is more robust, well-aligned with human explanations and can attribute relevance to a broader set of features (completeness). Second, we propose a novel way of evaluating explanation methods that circumvents the need for manual evaluation and is not reliant on the alignment of neural networks and humans decision processes.},
  journal = {arxiv},
  author = {Rieger, Laura and Hansen, Lars Kai},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.03894v1,
  title = {{{GNN Explainer}}: {{A Tool}} for {{Post}}-Hoc {{Explanation}} of {{Graph Neural Networks}}},
  abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by using neural networks to pass messages through edges in the graph. However, incorporating both graph structure and feature information leads to complex non-linear models and explaining predictions made by GNNs remains to be a challenging task. Here we propose GnnExplainer, a general model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task (node and graph classification, link prediction). In order to explain a given node's predicted label, GnnExplainer provides a local interpretation by highlighting relevant features as well as an important subgraph structure by identifying the edges that are most relevant to the prediction. Additionally, the model provides single-instance explanations when given a single prediction as well as multi-instance explanations that aim to explain predictions for an entire class of instances/nodes. We formalize GnnExplainer as an optimization task that maximizes the mutual information between the prediction of the full model and the prediction of simplified explainer model. We experiment on synthetic as well as real-world data. On synthetic data we demonstrate that our approach is able to highlight relevant topological structures from noisy graphs. We also demonstrate GnnExplainer to provide a better understanding of pre-trained models on real-world tasks. GnnExplainer provides a variety of benefits, from the identification of semantically relevant structures to explain predictions to providing guidance when debugging faulty graph neural network models.},
  journal = {arxiv},
  author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.12519v1,
  title = {A {{Provable Defense}} for {{Deep Residual Networks}}},
  abstract = {We present a training system, which can provably defend significantly larger neural networks than previously possible, including ResNet-34 and DenseNet-100. Our approach is based on differentiable abstract interpretation and introduces two novel concepts: (i) abstract layers for fine-tuning the precision and scalability of the abstraction, (ii) a flexible domain specific language (DSL) for describing training objectives that combine abstract and concrete losses with arbitrary specifications. Our training method is implemented in the DiffAI system.},
  journal = {arxiv},
  author = {Mirman, Matthew and Singh, Gagandeep and Vechev, Martin},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.09273v1,
  title = {"{{Why}} Did You Do That?": {{Explaining}} Black Box Models with {{Inductive Synthesis}}},
  abstract = {By their nature, the composition of black box models is opaque. This makes the ability to generate explanations for the response to stimuli challenging. The importance of explaining black box models has become increasingly important given the prevalence of AI and ML systems and the need to build legal and regulatory frameworks around them. Such explanations can also increase trust in these uncertain systems. In our paper we present RICE, a method for generating explanations of the behaviour of black box models by (1) probing a model to extract model output examples using sensitivity analysis; (2) applying CNPInduce, a method for inductive logic program synthesis, to generate logic programs based on critical input-output pairs; and (3) interpreting the target program as a human-readable explanation. We demonstrate the application of our method by generating explanations of an artificial neural network trained to follow simple traffic rules in a hypothetical self-driving car simulation. We conclude with a discussion on the scalability and usability of our approach and its potential applications to explanation-critical scenarios.},
  journal = {arxiv},
  author = {Pa{\c c}ac\i, G\"orkem and Johnson, David and McKeever, Steve and Hamfelt, Andreas},
  month = apr,
  year = {2019}
}

@inproceedings{236591,
  title = {Building a Banking System Specification Using Machine Learning},
  abstract = {Transforming user requirements into software specification is a complex and demanding task. Artificial intelligence methods such as machine learning (ML) can assist in the software specification process by providing support to system designers. This paper presents an approach based on explanation-based learning (EBL), a ML technique in which a concept is learned by building an explanation. The approach is presented in the context of the system LISE (Learning in Software Engineering). LISE converts a user requirement for a software module into an operational module definition using EBL with an incomplete theory. An example where LISE is used to build the specification of a banking system is illustrated.{$<$}{$>$}},
  booktitle = {Proceedings {{First International Conference}} on {{Artificial Intelligence Applications}} on {{Wall Street}}},
  doi = {10.1109/AIAWS.1991.236591},
  author = {Genest, J.},
  month = oct,
  year = {1991},
  keywords = {Artificial intelligence,bank data processing,Banking,banking system specification,Buildings,case-based reasoning,explanation,explanation-based learning,formal specification,learning (artificial intelligence),Learning in Software Engineering,LISE,machine learning,Machine learning,Mathematics,Multilevel systems,Programming,Software design,Software engineering,Software libraries,user requirements},
  pages = {263-268},
  note = {ISSN:}
}

@article{http://arxiv.org/abs/1904.11738v1,
  title = {Deep-{{IRT}}: {{Make Deep Learning Based Knowledge Tracing Explainable Using Item Response Theory}}},
  abstract = {Deep learning based knowledge tracing model has been shown to outperform traditional knowledge tracing model without the need for human-engineered features, yet its parameters and representations have long been criticized for not being explainable. In this paper, we propose Deep-IRT which is a synthesis of the item response theory (IRT) model and a knowledge tracing model that is based on the deep neural network architecture called dynamic key-value memory network (DKVMN) to make deep learning based knowledge tracing explainable. Specifically, we use the DKVMN model to process the student's learning trajectory and estimate the student ability level and the item difficulty level over time. Then, we use the IRT model to estimate the probability that a student will answer an item correctly using the estimated student ability and the item difficulty. Experiments show that the Deep-IRT model retains the performance of the DKVMN model, while it provides a direct psychological interpretation of both students and items.},
  journal = {arxiv},
  author = {Yeung, Chun-Kit},
  month = apr,
  year = {2019}
}

@article{1603636,
  title = {Associative Memory Design for 256 Gray-Level Images Using a Multilayer Neural Network},
  volume = {17},
  issn = {1045-9227},
  abstract = {A design procedure is presented for neural associative memories storing gray-scale images. It is an evolution of a previous work based on the decomposition of the image with 2/sup L/ gray levels into L binary patterns, stored in L uncoupled neural networks. In this letter, an L-layer neural network is proposed with both intralayer and interlayer connections. The connections between different layers introduce interactions among all the neurons, increasing the recall performance with respect to the uncoupled case. In particular, the proposed network can store images with the commonly used number of 256 gray levels instead of 16, as in the previous approach.},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2005.863465},
  author = {Costantini, G. and Casali, D. and Perfetti, R.},
  month = mar,
  year = {2006},
  keywords = {2/sup L/ gray levels,Algorithms,Artificial Intelligence,Associative memories,Associative memory,Automated,Biological neural networks,brain-state-in-a-box (BSB) neural networks,Cellular neural networks,Colorimetry,Computer Graphics,Computer Simulation,Computer-Assisted,content-addressable storage,gray-level images,Gray-scale,gray-scale images,Image Interpretation,Image storage,Information Storage and Retrieval,L binary patterns,L uncoupled neural networks,Models,Multi-layer neural network,multilayer architectures,multilayer neural network,neural associative memory design,neural nets,Neural networks,Neural Networks (Computer),Neurons,Numerical Analysis,Pattern Recognition,Pixel,Quantization,Signal Processing,Theoretical},
  pages = {519-522}
}

@article{1356017,
  title = {Digit and Command Interpretation for Electronic Book Using Neural Network and Genetic Algorithm},
  volume = {34},
  issn = {1083-4419},
  abstract = {This work presents the interpretation of digits and commands using a modified neural network and the genetic algorithm. The modified neural network exhibits a node-to-node relationship which enhances its learning and generalization abilities. A digit-and-command interpreter constructed by the modified neural networks is proposed to recognize handwritten digits and commands. A genetic algorithm is employed to train the parameters of the modified neural networks of the digit-and-command interpreter. The proposed digit-and-command interpreter is successfully realized in an electronic book. Simulation and experimental results will be presented to show the applicability and merits of the proposed approach.},
  number = {6},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  doi = {10.1109/TSMCB.2004.834432},
  author = {Lam, H. K. and Leung, F. H. F.},
  month = dec,
  year = {2004},
  keywords = {Algorithms,Artificial Intelligence,artificial intelligence generalization,artificial intelligence learning,Automated,Automatic Data Processing,Backpropagation algorithms,Biological neural networks,Books,command interpretation,Computer Graphics,Computer-Assisted,Digit and command interpretation,digit interpretation,electronic book,electronic publishing,Electronic publishing,Error correction,generalisation (artificial intelligence),genetic algorithm,genetic algorithms,Genetic algorithms,Handwriting,Handwriting recognition,handwritten character recognition,Image Enhancement,Image Interpretation,Information Storage and Retrieval,learning (artificial intelligence),Models,neural nets,neural network,neural networks,Neural networks,Neural Networks (Computer),Neurons,Numerical Analysis,Pattern Recognition,Publishing,Reading,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Signal processing algorithms,Statistical,User-Computer Interface,Word Processing},
  pages = {2273-2283}
}

@inproceedings{8490433,
  title = {Explainable {{AI}} for {{Designers}}: {{A Human}}-{{Centered Perspective}} on {{Mixed}}-{{Initiative Co}}-{{Creation}}},
  abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  doi = {10.1109/CIG.2018.8490433},
  author = {Zhu, J. and Liapis, A. and Risi, S. and Bidarra, R. and Youngblood, G. M.},
  month = aug,
  year = {2018},
  keywords = {AI machine,AI/ML techniques,computer games,explainable AI for designers,explainable artificial intelligence,game design,game designers,Games,human computer interaction,human-centered approach,human-centered perspective,human-computer interaction,learning (artificial intelligence),machine learning,Machine learning,mixed-initiative co-creation,Neurons,Task analysis,Tools,Visualization,XAI,XAID framework},
  pages = {1-8},
  issn = {2325-4289}
}

@article{1359749,
  title = {Artificial Neural Networks for Document Analysis and Recognition},
  volume = {27},
  issn = {0162-8828},
  abstract = {Artificial neural networks have been extensively applied to document analysis and recognition. Most efforts have been devoted to the recognition of isolated handwritten and printed characters with widely recognized successful results. However, many other document processing tasks, like preprocessing, layout analysis, character segmentation, word recognition, and signature verification, have been effectively faced with very promising results. This paper surveys the most significant problems in the area of offline document image processing, where connectionist-based approaches have been applied. Similarities and differences between approaches belonging to different categories are discussed. A particular emphasis is given on the crucial role of prior knowledge for the conception of both appropriate architectures and learning algorithms. Finally, the paper provides a critical analysts on the reviewed approaches and depicts the most promising research guidelines in the field. In particular, a second generation of connectionist-based models are foreseen which are based on appropriate graphical representations of the learning environment.},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2005.4},
  author = {Marinai, S. and Gori, M. and Soda, G.},
  month = jan,
  year = {2005},
  keywords = {Algorithms,Artificial Intelligence,artificial neural networks,Artificial neural networks,Automated,Automatic Data Processing,character recognition,Character recognition,character segmentation,Computer Graphics,Computer-Assisted,connectionist based approach,document image analysis,document image analysis and recognition,document image processing,document image recognition,document preprocessing,Documentation,Face recognition,graphical representations,Handwriting,handwriting recognition,Handwriting recognition,handwritten character recognition,handwritten recognition,Image analysis,Image Enhancement,Image Interpretation,Image recognition,image segmentation,Image segmentation,Index Terms- Character segmentation,Information Storage and Retrieval,layout analysis,learning (artificial intelligence),learning algorithms,neural networks,Neural networks,Neural Networks (Computer),Numerical Analysis,offline document image processing,Optical character recognition software,Pattern Recognition,preprocessing,Reading,recurrent neural nets,recursive neural networks,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,signature verification,Text analysis,User-Computer Interface,word recognition,word recognition.},
  pages = {23-35}
}

@inproceedings{1224095,
  title = {A New Method for Explaining Neural Network Reasoning},
  volume = {4},
  abstract = {This paper presents a new method for explaining the reasoning results of a trained neural network. The method considers the most significant attribute first under the guidance of a relative strength of effect analysis and eliminates irrelevant points. Following the adaptive search in the dynamic state space, a set of relevant points are extracted and form the basis of the explanation of the neural network reasoning. Combining a relative strength of effect analysis with the relevant points, a case based explanation approach is put forward. As an illustration, an experiment with a small data set on the relationship between weather conditions and play decisions is presented to demonstrate the utility of the proposed approach.},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}}, 2003.},
  doi = {10.1109/IJCNN.2003.1224095},
  author = {Hinde, C. and Gillingwater, D.},
  month = jul,
  year = {2003},
  keywords = {adaptive search,Artificial neural networks,case based explanation approach,Computational intelligence,Computer science,Data mining,dynamic state space,explanation,inference mechanisms,Information analysis,Intelligent structures,Knowledge representation,learning (artificial intelligence),neural nets,neural network reasoning,Neural networks,play decisions,relative strength of effect analysis,relevant point extraction,State-space methods,trained neural network,Training data,weather conditions},
  pages = {3256-3260 vol.4},
  issn = {1098-7576}
}

@article{5993545,
  title = {A {{New Formulation}} for {{Feedforward Neural Networks}}},
  volume = {22},
  issn = {1045-9227},
  abstract = {Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
  number = {10},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2011.2163169},
  author = {Razavi, S. and Tolson, B. A.},
  month = oct,
  year = {2011},
  keywords = {Algorithms,Artificial Intelligence,Biological neural networks,black box model,derivative free optimization algorithm,error response surface,Feedback,feedforward neural nets,feedforward neural network,Feedforward neural networks,function approximation,Function approximation,function approximation techniques,generalisation (artificial intelligence),generalization,generalization ability,geometrical interpretation,internal behavior,learning (artificial intelligence),learning ability,measure of regularization,Models,neural network functional geometry,Neural Networks (Computer),Neurological,Neurons,Nickel,optimisation,Optimization,reformulated neural network,ReNN approach,Software Design,training,Training,training method},
  pages = {1588-1598}
}

@inproceedings{287210,
  title = {A Conceptual Interpretation of Spurious Memories in the {{Hopfield}}-Type Neural Network},
  volume = {1},
  abstract = {It is shown that the spurious memories are represented as logical combinations of the learned memories. By assigning a conceptual interpretation to each learned memory, the spurious memories can be interpreted as novel conceptual knowledge created by the network. It is proposed that the generation of spurious memories be considered a primitive creativity that the simple network exhibits in high-level information processing.{$<$}{$>$}},
  booktitle = {[{{Proceedings}} 1992] {{IJCNN International Joint Conference}} on {{Neural Networks}}},
  doi = {10.1109/IJCNN.1992.287210},
  month = jun,
  year = {1992},
  keywords = {Art,Artificial intelligence,Artificial neural networks,Cities and towns,conceptual interpretation,conceptual knowledge,creativity,Hebbian learning,Hopfield neural nets,Hopfield neural networks,Hopfield-type neural network,Information processing,Intelligent networks,knowledge representation,learned memories,Neural networks,Neurons,Samarium,spurious memories},
  pages = {21-26 vol.1},
  note = {ISSN:}
}

@inproceedings{118383,
  title = {A General Explanation and Interrogation System for Neural Networks},
  abstract = {Summary form only given, as follows. The information in a trained neural network is stored as numerical weights in the neural elements and the connectivity pattern of the network. For many applications, it is desirable to have this neural network information converted into symbolic knowledge form for communication with human or machine experts. Techniques are presented for converting the information in a trained network into symbolic form as a set of rules and for obtaining explanations from the network for specific inputs. These two techniques provide the neurocomputer with one advantage of expert systems while retaining the learning and generalization capability of the neural network.{$<$}{$>$}},
  booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
  doi = {10.1109/IJCNN.1989.118383},
  year = {1989},
  keywords = {connectivity pattern,expert systems,Expert systems,explanation,Explanation,generalization capability,interrogation system,learning,neural nets,neural networks,Neural networks,neurocomputer,numerical weights,symbolic knowledge form},
  pages = {594 vol.2-},
  note = {ISSN:}
}

@inproceedings{5209797,
  title = {Application of {{Unascertained Neural Networks}} to {{Financial Early Warning}}},
  volume = {2},
  abstract = {Artificial neural network (ANN) has outstanding characteristics in machine learning, fault, tolerant, parallel reasoning and processing nonlinear problem abilities. Unascertained system that imitates the human brain's thinking logical is a kind of mathematical tools used to deal with imprecise and uncertain knowledge. Integrating unascertained method with neural network technology, the reasoning process of network coding can be tracked, and the output of the network can be given a physical explanation. A unascertained neural network was set up. It can be compared with the fuzzy network, so that their own advantages and shortcomings can be found and further study can be made on the uncertainty network to improve the uncertainty network more complete.},
  booktitle = {2009 {{Second International Symposium}} on {{Electronic Commerce}} and {{Security}}},
  doi = {10.1109/ISECS.2009.133},
  author = {Shi, H.},
  month = may,
  year = {2009},
  keywords = {artificial neural network,Artificial neural networks,Biological neural networks,Civil engineering,Electronic commerce,Electronic mail,financial data processing,financial early warning,fuzzy network,Humans,learning (artificial intelligence),machine learning,Machine learning,network coding,Network coding,neural nets,neural network technology,Neural networks,parallel reasoning,processing nonlinear problem,unascertained method,unascertained neural network,unascertained system,uncertain knowledge,Uncertainty,uncertainty network},
  pages = {365-368},
  note = {ISSN:}
}

@article{6222007,
  title = {Bidirectional {{Extreme Learning Machine}} for {{Regression Problem}} and {{Its Learning Effectiveness}}},
  volume = {23},
  issn = {2162-237X},
  abstract = {It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.},
  number = {9},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2012.2202289},
  author = {Yang, Y. and Wang, Y. and Yuan, X.},
  month = sep,
  year = {2012},
  keywords = {Algorithms,Automated,B-ELM,bidirectional extreme learning machine,Computer architecture,Computer Simulation,Data Interpretation,Equations,Feedforward neural network,Helium,learning (artificial intelligence),learning effectiveness,Learning systems,Machine learning,Models,neural nets,neural networks,Neural Networks (Computer),number of hidden nodes,Pattern Recognition,regression analysis,Regression Analysis,regression problem,Statistical,Testing,Training,universal approximation},
  pages = {1498-1505}
}

@inproceedings{1380384,
  title = {Explanation Based Generalized /Spl Epsi/-{{SVM}} and Its Application in Intelligent Project Management},
  volume = {6},
  abstract = {Support vector machine works well in classifying populations characterized by abrupt decreases in density functions. Its generalization accuracy, however, is not always optimal in dealing with real world problems with neither Gaussian distributions nor sharp boundaries. Incorporating domain theory about problems and excellent intelligent techniques in machine learning into SVM becomes one of promising alternatives. A novel approach, explanation based generalized /spl epsi/-SVM, which synthesizes SVM, prior knowledge, fuzzy logic and neural network, is proposed. Prior knowledge is expressed as a trained fuzzy neural network. An optimal subset of features is obtained by dynamically reducing feature space dimensionality according to the training derivatives extracted from network. By examining a subset of the practical data sampled from Guangdong Natural Science Foundation and testing the remaining set of data, application shows that explanation based generalized /spl epsi/-SVM performs better than that pure SVM and other traditional classifiers.},
  booktitle = {Proceedings of 2004 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}} ({{IEEE Cat}}. {{No}}.{{04EX826}})},
  doi = {10.1109/ICMLC.2004.1380384},
  month = aug,
  year = {2004},
  keywords = {Density functional theory,domain theory,explanation,fuzzy logic,Fuzzy logic,fuzzy neural nets,Gaussian distribution,generalisation (artificial intelligence),generalized /spl epsi/-SVM,intelligent project management,learning (artificial intelligence),Learning systems,machine learning,Machine learning,Network synthesis,neural network,Neural networks,pattern classification,prior knowledge,project management,Project management,support vector machine,Support vector machine classification,support vector machines,Support vector machines,trained fuzzy neural network},
  pages = {3454-3459 vol.6},
  note = {ISSN:}
}

@inproceedings{685683,
  title = {D-{{FANNS}} (Dynamical Functional Artificial Neural Networks)-a New Avenue for Intelligent Analog Signal Processing},
  abstract = {Summary form only given. Intelligent signal processing may be defined as the process of mapping a signal x into a binary vector or matrix y, so that y enables the detection, classification, or interpretation of an event present in x. (In the case of an interpretation in an appropriate language, y would represent a digitally coded relational structure.) We denote by f the input-output map of such an intelligent signal processing filter. In a number of applications, it is possible to naturally implement the nonlinear filter map f by an artificial neural network (ANN). We consider the case in which x is an analog signal (waveform) belonging to L2(I), where I is an appropriate interval of the real line R1 (i.e., L2(I) is the space of square integrable functions on I), and propose the realization of f by an artificial neural network in which the synaptic weight actions of the first layer are implemented by a filter bank. We call such a network a dynamical functional artificial neural network (D-FANN) to distinguish it from a conventional functional artificial neural network (FANN), where a synaptic weight action is implemented by a scalar product (integration) in L2(I), between the incoming waveform x and a "distributed" functional weight. Compared with conventional FANNs, D-FANNs permit simple and meaningful causal realizations of intelligent analog signal processors. A novel element in the present paper is the introduction of a "D-FANN gain equation", in a way analogous to that in Kalman filtering. Applications of D-FANNs to real and simulated data are now in progress and these results are discussed.},
  booktitle = {1998 {{IEEE Symposium}} on {{Advances}} in {{Digital Filtering}} and {{Signal Processing}}. {{Symposium Proceedings}} ({{Cat}}. {{No}}.{{98EX185}})},
  doi = {10.1109/ADFSP.1998.685683},
  author = {{de Figueiredo}, R. J. P.},
  month = jun,
  year = {1998},
  keywords = {Artificial intelligence,Artificial neural networks,band-pass filters,binary vector,classification,D-FANN gain equation,D-FANNS,detection,Digital filters,digitally coded relational structure,distributed functional weight,dynamical functional artificial neural networks,Equations,Event detection,FANN,Filter bank,filtering theory,functional artificial neural network,input-output map,integration,intelligent analog signal processing,intelligent signal processing filter,Intelligent structures,interpretation,Kalman filtering,matrix,neural nets,nonlinear filter map,nonlinear filters,Nonlinear filters,real data,scalar product,Signal mapping,signal processing,Signal processing,simulated data,synaptic weight,waveform},
  pages = {6-},
  note = {ISSN:}
}


