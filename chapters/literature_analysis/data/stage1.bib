@inproceedings{1224095,
 abstract = {This paper presents a new method for explaining the reasoning results of a trained neural network. The method considers the most significant attribute first under the guidance of a relative strength of effect analysis and eliminates irrelevant points. Following the adaptive search in the dynamic state space, a set of relevant points are extracted and form the basis of the explanation of the neural network reasoning. Combining a relative strength of effect analysis with the relevant points, a case based explanation approach is put forward. As an illustration, an experiment with a small data set on the relationship between weather conditions and play decisions is presented to demonstrate the utility of the proposed approach.},
 author = { and C. {Hinde} and D. {Gillingwater}},
 booktitle = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
 doi = {10.1109/IJCNN.2003.1224095},
 issn = {1098-7576},
 keywords = {neural nets;learning (artificial intelligence);explanation;inference mechanisms;neural network reasoning;trained neural network;relative strength of effect analysis;adaptive search;dynamic state space;relevant point extraction;case based explanation approach;weather conditions;play decisions;Neural networks;Data mining;Knowledge representation;Computational intelligence;Intelligent structures;Computer science;State-space methods;Information analysis;Training data;Artificial neural networks},
 month = {July},
 number = {},
 pages = {3256-3260 vol.4},
 title = {A new method for explaining neural network reasoning},
 volume = {4},
 year = {2003}
}

@inproceedings{1259707,
 abstract = {In this paper, an extendable hierarchical large scale neural network model is developed based on the theoretical analysis of information geometry. In a hierarchical set of systems, a lower order system is included in the parameter space of a larger one as a subset. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. Extendable hierarchical large scale neural network divides a task into small tasks, and each task is fulfilled by a small network under the principle of divide and conquer to improve the performance of a single network. By studying the dual manifold architecture for a family of neural networks and analyzing the hierarchical expansion of this model based on information geometry, the paper proposes a new method to construct the extendable hierarchical large scale neural network model that has knowledge-increasable and structure-extendible ability. The method helps to provide explanation of the transformation mechanism of human recognition system and understand the theory of global architecture of neural network.},
 author = { and and and and },
 booktitle = {Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)},
 doi = {10.1109/ICMLC.2003.1259707},
 issn = {},
 keywords = {neural nets;learning (artificial intelligence);large-scale systems;hierarchical systems;statistical distributions;geometry;cognition;extendable hierarchical large scale neural network model;information geometry;lower order system;parameter space;learning behaviors;human recognition system;dual flat manifold architecture;Information geometry;Large-scale systems;Neural networks;Solid modeling;Probability distribution;Information analysis;Computer science;Electronic mail;Humans;Information theory},
 month = {Nov},
 number = {},
 pages = {1380-1384 Vol.3},
 title = {Information geometry on extendable hierarchical large scale neural network model},
 volume = {3},
 year = {2003}
}

@article{1262324,
 abstract = {In this paper, we address the problem of the identification of text in noisy document images. We are especially focused on segmenting and identifying between handwriting and machine printed text because: 1) Handwriting in a document often indicates corrections, additions, or other supplemental information that should be treated differently from the main content and 2) the segmentation and recognition techniques requested for machine printed and handwritten text are significantly different. A novel aspect of our approach is that we treat noise as a separate class and model noise based on selected features. Trained Fisher classifiers are used to identify machine printed text and handwriting from noise and we further exploit context to refine the classification. A Markov Random Field-based (MRF) approach is used to model the geometrical structure of the printed text, handwriting, and noise to rectify misclassifications. Experimental results show that our approach is robust and can significantly improve page segmentation in noisy document collections.},
 author = { and and D. {Doermann}},
 doi = {10.1109/TPAMI.2004.1262324},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {handwriting recognition;image segmentation;document image processing;text analysis;image enhancement;feature extraction;Markov processes;text identification;handwriting identification;noisy document images;machine printed text;recognition techniques;fisher classifiers;Markov random field;page segmentation;Image segmentation;Text recognition;Text analysis;Handwriting recognition;Markov random fields;Image analysis;Context modeling;Solid modeling;Noise robustness;Image enhancement;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Stochastic Processes;Subtraction Technique;User-Computer Interface;Writing},
 month = {March},
 number = {3},
 pages = {337-353},
 title = {Machine printed text and handwriting identification in noisy document images},
 volume = {26},
 year = {2004}
}

@article{1359749,
 abstract = {Artificial neural networks have been extensively applied to document analysis and recognition. Most efforts have been devoted to the recognition of isolated handwritten and printed characters with widely recognized successful results. However, many other document processing tasks, like preprocessing, layout analysis, character segmentation, word recognition, and signature verification, have been effectively faced with very promising results. This paper surveys the most significant problems in the area of offline document image processing, where connectionist-based approaches have been applied. Similarities and differences between approaches belonging to different categories are discussed. A particular emphasis is given on the crucial role of prior knowledge for the conception of both appropriate architectures and learning algorithms. Finally, the paper provides a critical analysts on the reviewed approaches and depicts the most promising research guidelines in the field. In particular, a second generation of connectionist-based models are foreseen which are based on appropriate graphical representations of the learning environment.},
 author = {S. {Marinai} and M. {Gori} and G. {Soda}},
 doi = {10.1109/TPAMI.2005.4},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {document image processing;recurrent neural nets;handwritten character recognition;image segmentation;handwriting recognition;learning (artificial intelligence);artificial neural networks;document image analysis;document image recognition;handwritten recognition;character recognition;layout analysis;character segmentation;word recognition;signature verification;offline document image processing;connectionist based approach;learning algorithms;document preprocessing;recurrent neural nets;graphical representations;Artificial neural networks;Text analysis;Character recognition;Handwriting recognition;Image analysis;Image recognition;Neural networks;Optical character recognition software;Image segmentation;Face recognition;Index Terms- Character segmentation;document image analysis and recognition;layout analysis;neural networks;preprocessing;recursive neural networks;word recognition.;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Neural Networks (Computer);Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;User-Computer Interface},
 month = {Jan},
 number = {1},
 pages = {23-35},
 title = {Artificial neural networks for document analysis and recognition},
 volume = {27},
 year = {2005}
}

@inproceedings{1562962,
 abstract = {A main line of research for introducing explanation capabilities in knowledge-based system supposes that explanations are used to transmit knowledge from the machine to a user and to improve learning ability of the latter. A reasonable explanation should be adapted to different level of users because different users have different knowledge. To describe the difference and then generate a personalized explanation is the main problem. In this paper, a novel method called FUM-CE (fuzzy user model based customized explanation) is proposed, in which a fuzzy user model called FUM is defined, and then several new algorithms are proposed to initialize FUM, update FUM, and extract correlative knowledge for explanation based on the FUM. FUM-CE can provide different and suitable explanation for the different users with different domain knowledge, by which the understandability and acceptability of the expert system for earthquake prediction are improved},
 author = { and },
 booktitle = {17th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'05)},
 doi = {10.1109/ICTAI.2005.54},
 issn = {1082-3409},
 keywords = {earthquakes;expert systems;explanation;fuzzy set theory;geophysics computing;expert system;earthquake prediction;knowledge-based system;fuzzy user model based customized explanation;Expert systems;Earthquake engineering;Knowledge engineering;Machine learning;Artificial intelligence;Humans;Hybrid intelligent systems;Fuzzy systems;Knowledge based systems;Problem-solving},
 month = {Nov},
 number = {},
 pages = {5 pp.-371},
 title = {Customized explanation in expert system for earthquake prediction},
 volume = {},
 year = {2005}
}

@inproceedings{1575741,
 abstract = {A spanned cell in a table is a single, complete unit that physically occupies multiple columns and/or multiple rows. Spanned cells are common in tables, and they are a significant cause of error in the extraction of tables from free text documents. In this paper, we present a model for the detection and merging of vertically spanned cells for tables presented in plain text documents. Our model and algorithm are based purely on the layout features of the tables, and they require no semantic understanding of the documents. When tested on the 98 tables appearing in 40 randomly selected documents from a corpus of company announcements from the Australian Stock Exchange (ASX), our algorithm achieves an accuracy of 86.79% in detecting and merging vertically spanned cells.},
 author = {V. {Long} and R. {Dale} and S. {Cassidy}},
 booktitle = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
 doi = {10.1109/ICDAR.2005.21},
 issn = {1520-5363},
 keywords = {text analysis;vertically spanned table cell merging;vertically spanned table cell detection;plain text documents;free text documents;document semantic understanding;Merging;IEEE news;Australia;Stock markets;Data mining;Terminology;Testing;Robustness;Text analysis},
 month = {Aug},
 number = {},
 pages = {1242-1246 Vol. 2},
 title = {A model for detecting and merging vertically spanned table cells in plain text documents},
 volume = {},
 year = {2005}
}

@article{18thISoPAnnual2018,
 doi = {10.1007/s40264-018-0719-2},
 issn = {1179-1942},
 journal = {Drug Safety},
 language = {en},
 month = {November},
 number = {11},
 pages = {1103-1273},
 title = {18th {{ISoP Annual Meeting}} ``{{Pharmacovigilance}} without Borders'' {{Geneva}}, {{Switzerland}}, 11\textendash{}14 {{November}}, 2018},
 volume = {41},
 year = {2018}
}

@inproceedings{236591,
 abstract = {Transforming user requirements into software specification is a complex and demanding task. Artificial intelligence methods such as machine learning (ML) can assist in the software specification process by providing support to system designers. This paper presents an approach based on explanation-based learning (EBL), a ML technique in which a concept is learned by building an explanation. The approach is presented in the context of the system LISE (Learning in Software Engineering). LISE converts a user requirement for a software module into an operational module definition using EBL with an incomplete theory. An example where LISE is used to build the specification of a banking system is illustrated.<<ETX>>},
 author = {J. {Genest}},
 booktitle = {Proceedings First International Conference on Artificial Intelligence Applications on Wall Street},
 doi = {10.1109/AIAWS.1991.236591},
 issn = {},
 keywords = {bank data processing;case-based reasoning;explanation;formal specification;learning (artificial intelligence);case-based reasoning;banking system specification;user requirements;machine learning;explanation-based learning;LISE;Learning in Software Engineering;Banking;Machine learning;Buildings;Software engineering;Mathematics;Artificial intelligence;Programming;Software design;Multilevel systems;Software libraries},
 month = {Oct},
 number = {},
 pages = {263-268},
 title = {Building a banking system specification using machine learning},
 volume = {},
 year = {1991}
}

@article{25thAnnualConference2018,
 doi = {10.1007/s11136-018-1946-9},
 issn = {1573-2649},
 journal = {Quality of Life Research},
 language = {en},
 month = {October},
 number = {1},
 pages = {1-190},
 title = {25th {{Annual Conference}} of the {{International Society}} for {{Quality}} of {{Life Research}}},
 volume = {27},
 year = {2018}
}

@article{27thAnnualComputational2018,
 doi = {10.1186/s12868-018-0452-x},
 file = {/home/tim/Zotero/storage/I68H3LC3/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf;/home/tim/Zotero/storage/TJ4V7N93/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf},
 issn = {1471-2202},
 journal = {BMC Neuroscience},
 language = {en},
 month = {October},
 number = {2},
 pages = {64},
 shorttitle = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018)},
 title = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018): {{Part One}}},
 volume = {19},
 year = {2018}
}

@inproceedings{346487,
 abstract = {Many of the more ambitious goals of artificial intelligence have proved unattainable because of the failure of the many small, successful systems to scale up. The general use of technologies such as natural language interfaces and expert systems has done little to alleviate the basic difficulties and overwhelming cost of knowledge engineering. At the same time, emerging text processing techniques, including data extraction from text and new text retrieval methods, offer a means of accessing stores of information many times larger than any organized knowledge base or database. Although knowledge acquisition from text is at the heart of the information management problem, interpreting text, paradoxically, requires large amounts of knowledge, mainly about the way words are used in context. In other words, before intelligent text processing systems can be trained to mine for useful knowledge, they must already have enough knowledge to interpret what they read. The point at which there is "enough", is still a matter of debate, as no real program seems close to having enough knowledge to achieve general human-like understanding. Current research in large-scale natural language processing has come, rightly, to focus on lexical acquisition as the key to future progress. Unfortunately, the current state of the art is quite far from the recipe for acquiring knowledge about words, because it leans too heavily on resources that are available, without consideration for what is needed.<<ETX>>},
 author = {P. S. {Jacobs}},
 booktitle = {Proceedings Sixth International Conference on Tools with Artificial Intelligence. TAI 94},
 doi = {10.1109/TAI.1994.346487},
 issn = {},
 keywords = {word processing;natural language interfaces;knowledge acquisition;information retrieval;text-based systems;information management;artificial intelligence;natural language interfaces;expert systems;knowledge engineering;text processing techniques;data extraction;text retrieval methods;knowledge acquisition;intelligent text processing systems;human-like understanding;large-scale natural language processing;lexical acquisition;word processing;Information management;Text processing;Data mining;Artificial intelligence;Natural languages;Expert systems;Costs;Knowledge engineering;Information retrieval;Databases},
 month = {Nov},
 number = {},
 pages = {235-236},
 title = {Text-based systems and information management: artificial intelligence confronts matters of scale},
 volume = {},
 year = {1994}
}

@inproceedings{395650,
 abstract = {The interaction of textual and photographic information in document understanding is explored. Specifically, a computational model whereby textual captions are used as collateral information in the interpretation of the corresponding photographs is presented. The final understanding of the picture and caption reflects a consolidation of the information obtained from each of the two sources and can thus be used in intelligent information retrieval tasks. The problem of performing general-purpose vision without apriori knowledge is very difficult at best. The concept of using collateral information in scene understanding has been explored in systems that use general scene context in the task of object identification. The work described extends this notion by incorporating picture specific information. Finally, as a test of the model, a multi-stage system PICTION which uses captions to identify humans in an accompanying photograph is described. This provides a computationally less expensive alternative to traditional methods of face recognition since it does not require a pre-stored database of face models for all people to be identified.<<ETX>>},
 author = {R. K. {Srihari}},
 booktitle = {Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)},
 doi = {10.1109/ICDAR.1993.395650},
 issn = {},
 keywords = {knowledge based systems;document image processing;face recognition;image recognition;computational complexity;optical character recognition;textual information;intelligent document understanding;photographic information;document understanding;computational model;textual captions;intelligent information retrieval;scene understanding;object identification;picture specific information;multi-stage system;PICTION;captions;face recognition;database;face models;Layout;Information retrieval;Image retrieval;Computer vision;Computational modeling;System testing;Machine vision;Content based retrieval;Text analysis;Information analysis},
 month = {Oct},
 number = {},
 pages = {664-667},
 title = {Intelligent document understanding: Understanding photographs with captions},
 volume = {},
 year = {1993}
}

@inproceedings{395697,
 abstract = {A document analysis system which is capable of extracting the semantics of specific text portions of structured documents is presented. The architecture of the analysis system is based on a knowledge representation scheme, a semantic network, called Resco-Frame Representation of Structured Documents. It allows the definition of knowledge about document components as well as knowledge about analysis algorithms in a uniform, simple, but powerful representation formalism. Hence, this architecture enables the analysis system to exploit the specific power of both the algorithmic knowledge describing the properties of algorithms and the declarative knowledge about properties of text objects in documents. The inference engine and the control algorithm show how these two knowledge sources are combined and utilized. The flexibility of the representation formalism Fresco, the recognition results and the computational complexity of the inference algorithm are presented in two different applications.<<ETX>>},
 author = {T. A. {Bayer}},
 booktitle = {Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)},
 doi = {10.1109/ICDAR.1993.395697},
 issn = {},
 keywords = {document handling;semantic networks;model-based reasoning;structured text documents;model based document analysis system;knowledge representation scheme;semantic network;representation formalism;declarative knowledge;text objects;inference engine;control algorithm;Text analysis;Inference algorithms;Image recognition;Optical character recognition software;Data mining;Algorithm design and analysis;Engines;Computational complexity;Document image processing;Electronics packaging},
 month = {Oct},
 number = {},
 pages = {448-453},
 title = {Understanding structured text documents by a model based document analysis system},
 volume = {},
 year = {1993}
}

@inproceedings{413883,
 abstract = {This study proposes a 3D motion interpretation method which uses a neural network system consisting of three kinds of neural networks. This system estimates the solutions of 3D motion of an object by interpreting three optical flow (OF - motion vector field calculated from images) patterns of the same object obtained from three different view points. Though the interpretation system is trained using only basic 3D motions consisting of a single motion component, the system can interpret unknown multiple 3D motions consisting of several motion components. The generalization capacity of the proposed system is confirmed using diverse test patterns. Also the robustness of the system to noise is proved experimentally. The experimental results show that this method has suitable features for applying to real images.<<ETX>>},
 author = {A. {Miyauchi} and A. {Watanabe} and M. {Miyauchi}},
 booktitle = {Proceedings of 1st International Conference on Image Processing},
 doi = {10.1109/ICIP.1994.413883},
 issn = {},
 keywords = {motion estimation;image sequences;neural nets;learning (artificial intelligence);3D motion interpretation method;neural network system;optical flow normalisation network;motion components;test patterns;noise robustness;experimental results;real images;motion vector field;2D motion interpretation network;3D motion interpretation network;Neural networks;Cameras;Motion estimation;Noise robustness;Parameter estimation;Helium;Optical computing;Image motion analysis;System testing;Computer vision},
 month = {Nov},
 number = {},
 pages = {83-87 vol.3},
 title = {A method to interpret 3D motion using neural networks},
 volume = {3},
 year = {1994}
}

@article{4359385,
 abstract = {Text line segmentation in freestyle handwritten documents remains an open document analysis problem. Curvilinear text lines and small gaps between neighboring text lines present a challenge to algorithms developed for machine printed or hand-printed documents. In this paper, we propose a novel approach based on density estimation and a state-of-the-art image segmentation technique, the level set method. From an input document image, we estimate a probability map, where each element represents the probability that the underlying pixel belongs to a text line. The level set method is then exploited to determine the boundary of neighboring text lines by evolving an initial estimate. Unlike connected component based methods ( [1], [2] for example), the proposed algorithm does not use any script-specific knowledge. Extensive quantitative experiments on freestyle handwritten documents with diverse scripts, such as Arabic, Chinese, Korean, and Hindi, demonstrate that our algorithm consistently outperforms previous methods. Further experiments show the proposed algorithm is robust to scale change, rotation, and noise.},
 author = {Y. {Li} and Y. {Zheng} and D. {Doermann} and S. {Jaeger}},
 doi = {10.1109/TPAMI.2007.70792},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {document image processing;estimation theory;handwritten character recognition;image segmentation;probability;set theory;text analysis;script-independent curvilinear text line segmentation;freestyle handwritten document image segmentation;document analysis problem;machine printed document;hand-printed document;level set method;probability map estimation;connected component based method;Image segmentation;Text analysis;Level set;Image analysis;Piecewise linear approximation;State estimation;Pixel;Noise robustness;Character recognition;Algorithm design and analysis;Document analysis;Handwriting analysis;Document and Text Processing;Document analysis;Handwriting analysis;Document and Text Processing;Algorithms;Artificial Intelligence;Automatic Data Processing;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity},
 month = {Aug},
 number = {8},
 pages = {1313-1329},
 title = {Script-Independent Text Line Segmentation in Freestyle Handwritten Documents},
 volume = {30},
 year = {2008}
}

@article{4492785,
 abstract = {This paper presents a document retrieval technique that is capable of searching document images without optical character recognition (OCR). The proposed technique retrieves document images by a new word shape coding scheme, which captures the document content through annotating each word image by a word shape code. In particular, we annotate word images by using a set of topological shape features including character ascenders/descenders, character holes, and character water reservoirs. With the annotated word shape codes, document images can be retrieved by either query keywords or a query document image. Experimental results show that the proposed document image retrieval technique is fast, efficient, and tolerant to various types of document degradation.},
 author = {S. {Lu} and L. {Li} and C. L. {Tan}},
 doi = {10.1109/TPAMI.2008.89},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {document image processing;image coding;image retrieval;document image retrieval;word shape coding;document image searching;document content;word image annotation;topological shape features;character ascenders;character holes;character water reservoirs;annotated word shape codes;query keywords;query document image;document degradation;character descenders;Image retrieval;Shape;Image coding;Optical character recognition software;Content based retrieval;Degradation;Image segmentation;Character recognition;Water resources;Reservoirs;Image/video retrieval;Shape;Text processing;Document analysism;Document Capture;Document and Text Processing;Computing Methodologies;Shape;Vision and Scene Understanding;Artificial Intelligence;Image/video retrieval;Shape;Text processing;Document analysism;Document Capture;Document and Text Processing;Computing Methodologies;Shape;Vision and Scene Understanding;Artificial Intelligence;Artificial Intelligence;Automatic Data Processing;Database Management Systems;Databases, Factual;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Language;Pattern Recognition, Automated;Reading},
 month = {Nov},
 number = {11},
 pages = {1913-1918},
 title = {Document Image Retrieval through Word Shape Coding},
 volume = {30},
 year = {2008}
}

@inproceedings{4761259,
 abstract = {In real-world digital libraries, artificial intelligence techniques are essential for tackling the automatic document processing task with sufficient flexibility. The great variability in document kind, content and shape requires powerful representation formalisms to catch all the domain complexity. The continuous flow of new documents requires adaptable techniques that can progressively adjust the acquired knowledge on documents as long as new evidence becomes available, even extending if needed the set of recognized document types. Both these issues have not yet been thoroughly studied. This paper presents an incremental first-order logic learning framework for automatically dealing with various kinds of evolution in digital repositories content: evolution in the definition of class definitions, evolution in the set of known classes and evolution by addition of new unknown classes. Experiments show that the approach can be applied to real-world.},
 author = {S. {Ferilli} and M. {Biba} and T. M. A. {Basile} and F. {Esposito}},
 booktitle = {2008 19th International Conference on Pattern Recognition},
 doi = {10.1109/ICPR.2008.4761259},
 issn = {1051-4651},
 keywords = {digital libraries;document image processing;learning (artificial intelligence);incremental machine learning;first-order logic learning;document layout understanding;digital libraries;artificial intelligence;representation formalisms;domain complexity;Machine learning;Software libraries;Artificial intelligence;Shape;Data mining;Learning systems;Automatic logic units;Production systems;Digital systems;Technology management},
 month = {Dec},
 number = {},
 pages = {1-4},
 title = {Incremental machine learning techniques for document layout understanding},
 volume = {},
 year = {2008}
}

@inproceedings{5158992,
 abstract = {This thesis elaborated the concept, significance and main strategy of machine learning as well as the basic structure of machine learning system. By combining several basic ideas of main strategies, great effort are laid on introducing several machine learning methods, such as Rote learning, Explanation-based learning, Learning from instruction, Learning by deduction, Learning by analogy and Inductive learning, etc. Meanwhile, comparison and analysis are made upon their respective advantages and limitations. At the end of the article, it proposes the research objective of machine learning and points out its development trend.Machine learning is a fundamental way that enable the computer to have the intelligence ; Its application which had been used mainly the method of induction and the synthesis, rather than the deduction has already reached many fields of Artificial Intelligence.},
 author = {M. {Xue} and C. {Zhu}},
 booktitle = {2009 International Joint Conference on Artificial Intelligence},
 doi = {10.1109/JCAI.2009.55},
 issn = {},
 keywords = {learning (artificial intelligence);artificial intelligence;machine learning system;rote learning;explanation-based learning;learning from instruction;learning by deduction;learning by analogy;Inductive learning;Machine learning;Artificial intelligence;Learning systems;Application software;Humans;Computational modeling;Machine learning algorithms;Intelligent systems;Intelligent robots;Physiology;machine learning;AI;system structure;learning strategy;algorithm},
 month = {April},
 number = {},
 pages = {272-274},
 title = {A Study and Application on Machine Learning of Artificial Intellligence},
 volume = {},
 year = {2009}
}

@inproceedings{5212590,
 abstract = {Companies in financial distress make the creditors, shareholders, employees, investors and other participants of the related firms suffer great losses. In order to prevent the companies run into bankruptcy, financial distress prediction has been a useful tool for distinguishing companies in financial distress from those healthy. Statistical methods and artificial intelligence techniques have been widely used to deal with this issue. Many studies indicated that artificial neural networks outperform many statistical methods. However, artificial neural networks have the drawback of failing to interpret the classification results. This paper uses an artificial intelligence technique-group method of data handling technique to overcome this drawback. The sample data are collected from Taiwan listed companies in the Taiwan Stock Exchange Corporation. The result illustrates that the accuracy rates of classification of group method of data handling models are larger than 90% and the models of the group method of data handling obtain better accuracy than the models of discriminant analysis and logistic regression.},
 author = { and M. {Liao} and and and and and },
 booktitle = {2009 International Conference on Machine Learning and Cybernetics},
 doi = {10.1109/ICMLC.2009.5212590},
 issn = {2160-133X},
 keywords = {data handling;financial data processing;investment;neural nets;pattern classification;statistical analysis;stock markets;financial distress prediction model;group method;data handling technique;creditor;investor;statistical method;artificial intelligence technique;artificial neural network;Taiwan stock exchange corporation;data classification;Predictive models;Data handling;Artificial neural networks;Logistics;Companies;Statistical analysis;Artificial intelligence;Investments;Neural networks;Machine learning;Financial distress prediction;Group method of data handling;Artificial neural network},
 month = {July},
 number = {},
 pages = {2897-2902},
 title = {Constructing financial distress prediction model using group method of data handling technique},
 volume = {5},
 year = {2009}
}

@inproceedings{5267552,
 abstract = {This paper represents a study using Artificial Neural Networks (ANN) to perform automatic interpretation of lithofacies in a reservoir scale. This technique having been used successfully to interpret lithofacies automatically in the Sha20 Block, Shanian oilfield. Description and interpretation from a cored section in the key well was used to train the Supervised neural network. Having trained the network, it was then used to recognise and interpret the units vertically and laterally in the studied reservoir. The unsupervised neural network was run to classify the cored interval into 2 and 6 classes respectively and the results were then compared with the supervised network output. The results were observed to be over 87% accurate. Then a 3D geological model was built using the sequential indicator simulation method, the excellent results obtained from the developed model shows that the method is quite effective and gets satisfying prediction precision for the lithofacies in reservoir modeling.},
 author = {X. {Ma} and J. {Zhang} and H. {Zhao}},
 booktitle = {2009 ISECS International Colloquium on Computing, Communication, Control, and Management},
 doi = {10.1109/CCCM.2009.5267552},
 issn = {2154-9613},
 keywords = {geology;hydrocarbon reservoirs;neural nets;artificial neural networks;lithofacies interpretation;3D geological modelling;Shanian oilfield;Sha20 Block;supervised neural network;unsupervised neural network;sequential indicator simulation method;Artificial neural networks;Geology;Reservoirs;Predictive models;Petroleum;Neural networks;Computer networks;Costs;Permeability;Cellular neural networks;Artificial Neural Networks;Lithofacies;modelling;training},
 month = {Aug},
 number = {},
 pages = {451-454},
 title = {Application of artificial neural networks in lithofacies interpretation used for 3D geological modelling},
 volume = {4},
 year = {2009}
}

@article{5290725,
 abstract = {With the rapid growth of the World Wide Web and electronic information services, text corpus is becoming available online at an incredible rate. By displaying text data in a logical layout (e.g., color graphs), text visualization presents a direct way to observe the documents as well as understand the relationship between them. In this paper, we propose a novel technique, Exemplar-based visualization (EV), to visualize an extremely large text corpus. Capitalizing on recent advances in matrix approximation and decomposition, EV presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function. The probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding. By selecting the representative exemplars, we obtain a compact approximation of the data. This makes the visualization highly efficient and flexible. In addition, the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of large text corpus. Empirically, we demonstrate the superior performance of EV through extensive experiments performed on the publicly available text data sets.},
 author = {Y. {Chen} and L. {Wang} and M. {Dong} and J. {Hua}},
 doi = {10.1109/TVCG.2009.140},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {biology computing;data visualisation;iterative methods;optimisation;exemplar-based visualization;large document corpus;text corpus;text visualization;matrix approximation;iterative optimization;parameter embedding;Data visualization;Web sites;Multidimensional systems;Drugs;Text mining;Principal component analysis;Computer science;Matrix decomposition;Large-scale systems;Indexing;Exemplar;large-scale document visualization;multidimensional projection.},
 month = {Nov},
 number = {6},
 pages = {1161-1168},
 title = {Exemplar-based Visualization of Large Document Corpus (InfoVis2009-1115)},
 volume = {15},
 year = {2009}
}

@inproceedings{5333385,
 abstract = {This paper describes a comprehensive method to construct fuzzy classification system considering both precision and interpretability. Fuzzy classification system, initialized by modified Gath-Geva fuzzy clustering algorithm, is transformed into neural network. After training the neural network, fuzzy sets similarity measure is adopt to merge redundant fuzzy sets to improve interpretability, and a constraint genetic algorithm is applied to improve precision. The simulation result on Iris data problem demonstrates the effectiveness of the proposed method.},
 author = { and and and },
 booktitle = {2009 ICCAS-SICE},
 doi = {},
 issn = {},
 keywords = {fuzzy set theory;genetic algorithms;neural nets;interpretable fuzzy classification system;neural networks;modified Gath-Geva fuzzy clustering algorithm;constraint genetic algorithm;Iris data problem;fuzzy set theory;Fuzzy neural networks;Fuzzy systems;Neural networks;Fuzzy sets;Genetic algorithms;Electronic mail;Clustering algorithms;Iris;Telecommunication traffic;Transportation;Fuzzy classification system;neural network;interpretability},
 month = {Aug},
 number = {},
 pages = {5318-5321},
 title = {Study on interpretable fuzzy classification system based on neural networks},
 volume = {},
 year = {2009}
}

@inproceedings{5383110,
 abstract = {Orientation detection is an important preprocessing step for accurate recognition of text from document images. Many existing orientation detection techniques are based on the fact that in Roman script text ascenders occur more likely than descenders, but this approach is not applicable to document of other scripts like Urdu, Arabic, etc. In this paper, we propose a discriminative learning approach for orientation detection of Urdu documents with varying layouts and fonts. The main advantage of our approach is that it can be applied to documents of other scripts easily and accurately. Our approach is based on classification of individual connected component orientation in the document image, and then the orientation of the page image is determined via majority count. A convolutional neural network is trained as discriminative learning model for the labeled Urdu books dataset with four target orientations: 0, 90, 180 and 270 degrees. We demonstrate the effectiveness of our method on dataset of Urdu documents categorized into the layouts of book, novel and poetry. We achieved 100% orientation detection accuracy on a test set of 328 document images.},
 author = {S. F. {Rashid} and S. S. {Bukhari} and F. {Shafait} and T. M. {Breuel}},
 booktitle = {2009 IEEE 13th International Multitopic Conference},
 doi = {10.1109/INMIC.2009.5383110},
 issn = {},
 keywords = {classification;document image processing;learning (artificial intelligence);natural language processing;neural nets;text analysis;discriminative learning approach;orientation detection;Urdu document images;text recognition;Roman script;text ascenders;classification;convolutional neural network;Urdu books dataset;Neural networks;Optical character recognition software;Shape;Pattern recognition;Image recognition;Books;Cellular neural networks;Learning;Artificial intelligence;Text recognition},
 month = {Dec},
 number = {},
 pages = {1-5},
 title = {A discriminative learning approach for orientation detection of Urdu document images},
 volume = {},
 year = {2009}
}

@inproceedings{5483913,
 abstract = {Topographic maps contain a small amount of text compared to other forms of printed documents. Furthermore, the text and graphical components typically intersect with one another thus making the extraction of text a very difficult task. Creating training sets with a suitable size from the actual characters in maps would therefore require the laborious processing of many maps with similar features and the manual extraction of character samples. This paper extends the types of defects represented by Baird's document image degradation model in order to create pseudo randomly generated training sets that closely mimic the various artifacts and defects encountered in characters extracted from maps. Two Hidden Markov Models are then trained and used to recognize the text. Tests performed on extracted street labels show an improvement in performance from 88.4% when only the original Baird's model is used to a character recognition rate of 93.2% when the extended defect model is used for training.},
 author = {A. {Pezeshk} and R. L. {Tutwiler}},
 booktitle = {2010 IEEE Southwest Symposium on Image Analysis Interpretation (SSIAI)},
 doi = {10.1109/SSIAI.2010.5483913},
 issn = {},
 keywords = {cartography;document image processing;hidden Markov models;learning (artificial intelligence);text analysis;extended character defect model;text recognition;topographic maps;document image degradation model;pseudo randomly generated training sets;hidden Markov models;Character recognition;Text recognition;Hidden Markov models;Degradation;Graphics;Image recognition;Optical character recognition software;Artificial neural networks;Feature extraction;Data mining;text recognition;document image degradation model;Hidden Markov Models;feature extraction},
 month = {May},
 number = {},
 pages = {85-88},
 title = {Extended character defect model for recognition of text from maps},
 volume = {},
 year = {2010}
}

@inproceedings{5579790,
 abstract = {In order to visualize huge network traffic, data visualization applications are being developed and used to complement network data visualization. With today's network data visualization tools, it is only possible to view small portions of data and consuming lots of time to process the data. The network data process time can be reduced with the innovative artificial intelligence approach, which can effectively accelerate the network data visualization and consequently classify network data into different level of details according diverse computer users' expertise level on network. In the last few years, many visualization tools have been developed; either suffers from time to process the network data or the low understanding from the network data visualization efficiency. In this paper, we proposed an innovative intelligence approach, namely Intelligent Expertise Classification Algorithm (IECA) based on diverse computer users' expertise level in order to improve the network data process time as well as the understanding level among computer users. The approach architecture details and its requirements such as expertise level from diverse computer users and processed data from data mining classification will be discussed. Numbers of experiments have been carried out on 100 computer users from different fields and different level of computer expertise to evaluate the approach effectiveness. It features fast intelligent expertise classification and support network data understanding performance.},
 author = { and S. {Manickam}},
 booktitle = {2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)},
 doi = {10.1109/ICACTE.2010.5579790},
 issn = {2154-7505},
 keywords = {artificial intelligence;computer network security;data mining;data visualisation;pattern classification;telecommunication traffic;intelligent expertise classification approach;innovative artificial intelligence approach;network data visualization;network traffic visualization;network data process;network data classification;data mining classification;Data visualization;Lead;Data mining;Visualization;artificial intelligence approach;intelligent expertise classification algorithm;network data visualization},
 month = {Aug},
 number = {},
 pages = {V6-437-V6-440},
 title = {Intelligent Expertise Classification approach: An innovative artificial intelligence approach to accelerate network data visualization},
 volume = {6},
 year = {2010}
}

@inproceedings{5580484,
 abstract = {Texts in videos provide plenteous information for video analysis such as video indexing, understanding and retrieval. We propose a neural network based method detecting text in the video frames in this work. The proposed method consists of three major steps: feature extraction, text region detection and candidate region refinement. Firstly, we extract texture features from four edge maps yielded from the target video frame. Secondly, a Radial Basis Function Neural Network (RBFNN) optimized by the Localized Generalization Error Model (L-GEM) is applied to detect text candidates. Finally, a false detection of text is applied to fine tune the result. Experimental results demonstrate that the proposed method is efficient for different font-colors, font-sizes and language in complex background.},
 author = {X. {Ma} and W. W. Y. {Ng} and P. P. K. {Chan} and D. S. {Yeung}},
 booktitle = {2010 International Conference on Machine Learning and Cybernetics},
 doi = {10.1109/ICMLC.2010.5580484},
 issn = {2160-133X},
 keywords = {edge detection;feature extraction;radial basis function networks;text analysis;video signal processing;video text detection;localized generalization error model;video indexing;video understanding;video retrieval;texture features extraction;edge maps;radial basis function neural network;Image edge detection;Feature extraction;Training;Computer architecture;Neurons;Classification algorithms;Machine learning;Text detection;Localized generalization error model (LGEM);Radial basis function neural network (RBFNN)},
 month = {July},
 number = {},
 pages = {2161-2166},
 title = {Video text detection and localization based on localized generalization error model},
 volume = {4},
 year = {2010}
}

@inproceedings{5590681,
 abstract = {In document-driven DSS, the decisions are both based on the inheritance among the documents and the acceptance of advices for users. Research in the field of DSS has shown that providing explanations may improve acceptance of decision makers. The most important part of explanations in document-driven DSS lies in tracing the contents and the classification of interrelated documents. But current document-driven DSS is lack of a mechanism to record the citation and cluster the related documents. This paper tries to find out the trace routes among documents to improve the explanations. First, a document lineage model is established to present the citation and delivery mechanism in documents. Second, the document DNA is used to build the routes of documents transferences. Third, the whole lineage in documents is integrated by routes. Finally, a system frame for explanations mechanism in document-driven DSS was described.},
 author = {Z. {Chen} and H. {Dong}},
 booktitle = {2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics},
 doi = {10.1109/IHMSC.2010.67},
 issn = {},
 keywords = {data mining;decision making;decision support systems;information retrieval;pattern clustering;text analysis;document driven DSS;decision support system;decision making;document lineage model;document transference;Decision support systems;DNA;Decision making;Blood;Face;Computers;Document-driven DSS;Explanation;Document lineage},
 month = {Aug},
 number = {},
 pages = {243-248},
 title = {The Effects of Documents Lineage on Use of Explanation in Document-Driven DSS},
 volume = {1},
 year = {2010}
}

@inproceedings{598997,
 abstract = {Intelligent Document Understanding (IDU) is the process of converting scanned document pages into an electronic, processable form. We have previously presented a IDU system architecture suitable for this task which uses a hybrid bottom-up/top-down control strategy. In this paper we focus on a specific subproblem that arises within the chosen framework, concerned with selecting an appropriate page layout structure. A detailed analysis of the problem using an error propagation model, allows computationally simple search strategies to be developed. A multistage layout formation algorithm is proposed and its performance is critically assessed when implemented using two different Layout Object selection criterion. The first selection criterion is based on a maximal column area coverage; the second is based on a probabilistic Layout Object selection. Both techniques have been incorporated into the hybrid IDU system and the results presented indicate its superiority over previously reported systems.},
 author = {G. S. D. {Farrow} and C. S. {Xydeas} and J. P. {Oakley}},
 booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
 doi = {10.1109/ICDAR.1995.598997},
 issn = {},
 keywords = {document image processing;optical character recognition;search problems;intelligent document understanding;model matching;hybrid bottom-up/top-down control strategy;appropriate page layout structure;error propagation model;computationally simple search strategies;maximal column area coverage;probabilistic layout object selection;Control systems;Computer vision;Computer architecture;Performance analysis;Process control;Image databases;Object detection;Computational modeling;Intelligent structures;Degradation},
 month = {Aug},
 number = {},
 pages = {293-296 vol.1},
 title = {Model matching in intelligent document understanding},
 volume = {1},
 year = {1995}
}

@article{5993545,
 abstract = {Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
 author = {S. {Razavi} and B. A. {Tolson}},
 doi = {10.1109/TNN.2011.2163169},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {feedforward neural nets;function approximation;generalisation (artificial intelligence);learning (artificial intelligence);optimisation;function approximation techniques;black box model;neural network functional geometry;geometrical interpretation;reformulated neural network;feedforward neural network;error response surface;learning ability;training method;derivative free optimization algorithm;generalization ability;ReNN approach;Biological neural networks;Training;Nickel;Optimization;Function approximation;Neurons;Feedforward neural networks;generalization;geometrical interpretation;internal behavior;measure of regularization;reformulated neural network;training;Algorithms;Artificial Intelligence;Feedback;Models, Neurological;Neural Networks (Computer);Software Design},
 month = {Oct},
 number = {10},
 pages = {1588-1598},
 title = {A New Formulation for Feedforward Neural Networks},
 volume = {22},
 year = {2011}
}

@inproceedings{6102474,
 abstract = {This paper studies the interpretability of transformations of labeled higher dimensional data into a 2D representation (scatterplots) for visual classification.<sup>1</sup>In this context, the term interpretability has two components: the interpretability of the visualization (the image itself) and the interpretability of the visualization axes (the data transformation functions). We define a data transformation function as any linear or non-linear function of the original variables mapping the data into 1D. Even for a small dataset, the space of possible data transformations is beyond the limit of manual exploration, therefore it is important to develop automated techniques that capture both aspects of interpretability so that they can be used to guide the search process without human intervention. The goal of the search process is to find a smaller number of interpretable data transformations for the users to explore. We briefly discuss how we used such automated measures in an evolutionary computing based data dimensionality reduction application for visual analytics. In this paper, we present a two-part user study in which we separately investigated how humans rated the visualizations of labeled data and comprehensibility of mathematical expressions that could be used as data transformation functions. In the first part, we compared human perception with a number of automated measures from the machine learning and visual analytics literature. In the second part, we studied how various structural properties of an expression related to its interpretability.},
 author = {I. {Icke} and A. {Rosenberg}},
 booktitle = {2011 IEEE Conference on Visual Analytics Science and Technology (VAST)},
 doi = {10.1109/VAST.2011.6102474},
 issn = {},
 keywords = {data analysis;data visualisation;learning (artificial intelligence);pattern classification;interpretable dimensionality reduction;visual classification;transformation interpretability;labeled higher dimensional data;2D representation;visualization axis interpretability;data transformation functions;evolutionary computing;visual analytics;labeled data visualization;mathematical expression comprehensibility;human perception;machine learning;expression structural properties;Indexes;Humans;Visualization;Data visualization;Support vector machines;Particle measurements;Atmospheric measurements;data transformation;visualization;user study},
 month = {Oct},
 number = {},
 pages = {281-282},
 title = {Automated measures for interpretable dimensionality reduction for visual classification: A user study},
 volume = {},
 year = {2011}
}

@inproceedings{6154802,
 abstract = {Automatic Text Classification is a semi-supervised machine learning task that automatically assigns a given text document to a set of pre-defined categories based on the features extracted from its textual content. This paper attempts to automatically classify the textual entries made by bloggers on various sports blogs, to the appropriate category of sport by following steps like pre-processing, feature extraction and naïve Bayesian classification. Empirical evaluation of this technique has resulted in a classification accuracy of approximately 87% over the test set. In addition to classifying the textual entries of sports blogs, it is proposed that the extracted features themselves be further classified under more meaningful heads which results in generation of a semantic resource that lends greater understanding to the classification task. This semantic resource can be used for data mining requirements that arise in the future.},
 author = {M. K. {Dalal} and M. A. {Zaveri}},
 booktitle = {2012 Computing, Communications and Applications Conference},
 doi = {10.1109/ComComAp.2012.6154802},
 issn = {},
 keywords = {Bayes methods;data mining;feature extraction;learning (artificial intelligence);pattern classification;semantic Web;sport;text analysis;Web sites;automatic text classification;sports blog data;semi-supervised machine learning task;text document;feature extraction;naïve Bayesian classification;semantic resource;data mining requirements;Blogs;Feature extraction;Text categorization;Bayesian methods;Training;Semantics;Accuracy;automatic text classification;feature extraction;heuristics;intelligent data mining;machine learning;naïve Bayes classification},
 month = {Jan},
 number = {},
 pages = {219-222},
 title = {Automatic Text Classification of sports blog data},
 volume = {},
 year = {2012}
}

@inproceedings{618903,
 abstract = {When a neural network makes a financial prediction, the user may benefit from knowing which previous time periods are illustrative of the current time period. The authors describe a high-performance neural network that in addition to predicting stock market direction, allows the user to visualize the relationship between current conditions and previous conditions that led to similar predictions. Visualization is accomplished by forming a gated multi-expert network using funnel-shaped multilayer dimensionality reduction networks. The neck of the funnel is a two-neuron layer that displays the training data and the decision boundaries in a two-dimensional space. This architecture facilitates a) interactive design of the decision functions and b) explanation of the relevance of past decisions from the training set to the current decision. They describe a stock market prediction system whose design incorporates a visual neural network for prediction, wavelet transforms and tapped delay lines for feature extraction, and a genetic algorithm for feature selection. This system shows that the visual neural network provides the low error rates (i.e., accurate predictions) of multi-expert networks along with the visual explanatory power of nonlinear dimensionality reduction.},
 author = {C. {Ornes} and J. {Sklansky}},
 booktitle = {Proceedings of the IEEE/IAFE 1997 Computational Intelligence for Financial Engineering (CIFEr)},
 doi = {10.1109/CIFER.1997.618903},
 issn = {},
 keywords = {explanation;prediction theory;neural net architecture;financial data processing;stock markets;multilayer perceptrons;feedforward neural nets;decision support systems;data visualisation;genetic algorithms;feature extraction;financial market behavior explanation;financial market behavior prediction;time periods;high-performance neural network;visualization;current conditions;previous conditions;gated multi-expert network;funnel-shaped multilayer dimensionality reduction networks;two-neuron layer;training data;decision boundaries;2D space;architecture;interactive decision function design;past decision relevance;visual neural network;wavelet transforms;tapped delay lines;feature extraction;genetic algorithm;Neural networks;Stock markets;Data visualization;Multi-layer neural network;Neck;Two dimensional displays;Training data;Algorithm design and analysis;Wavelet transforms;Delay lines},
 month = {March},
 number = {},
 pages = {43-49},
 title = {A neural network that explains as well as predicts financial market behavior},
 volume = {},
 year = {1997}
}

@inproceedings{619849,
 abstract = {In this paper, a qualitative representation for the layout of structured documents as well as classes of documents is presented, which is established by means of supervised learning from a labeled training set of documents. For this formal representation, an inference algorithm has been developed, adopted from error-tolerant subgraph isomorphism, which assigns logic labels to the layout objects of a test document.},
 author = {H. {Walischewski}},
 booktitle = {Proceedings of the Fourth International Conference on Document Analysis and Recognition},
 doi = {10.1109/ICDAR.1997.619849},
 issn = {},
 keywords = {knowledge acquisition;document image processing;learning (artificial intelligence);knowledge representation;inference mechanisms;fault tolerant computing;graph theory;automatic knowledge acquisition;spatial document interpretation;qualitative representation;structured document layout;document classes;supervised learning;labeled training set;formal representation;inference algorithm;error-tolerant subgraph isomorphism;logic labels;layout objects;Knowledge acquisition;Logic testing;Data mining;Inference mechanisms;Inference algorithms;Humans;Information analysis;Writing;Training data;Computer science education},
 month = {Aug},
 number = {},
 pages = {243-247 vol.1},
 title = {Automatic knowledge acquisition for spatial document interpretation},
 volume = {1},
 year = {1997}
}

@inproceedings{6460845,
 abstract = {Document image binarization is an important preprocessing technique for document image analysis that segments the text from the document image backgrounds. Many techniques have been proposed and successfully applied in different applications, such as document image retrieval. However, these techniques may perform poorly on degraded document images. In this paper, we propose a learning framework that makes use of the Markov Random Field to improve the performance of the existing document image binarization methods for those degraded document images. Extensive experiments on the recent Document Image Bina-rization Contest datasets demonstrate that significant improvements of the existing binarization methods when applying our proposed framework.},
 author = {B. {Su} and S. {Lu} and C. L. {Tan}},
 booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
 doi = {},
 issn = {1051-4651},
 keywords = {document image processing;image retrieval;image segmentation;learning (artificial intelligence);Markov processes;random processes;text analysis;learning framework;degraded document image binarization methods;Markov random field;document image analysis;text segmentation;document image backgrounds;document image retrieval;document image binarization contest datasets;Image edge detection;Markov random fields;Vectors;Equations;Mathematical model;Text analysis},
 month = {Nov},
 number = {},
 pages = {3200-3203},
 title = {A learning framework for degraded document image binarization using Markov Random Field},
 volume = {},
 year = {2012}
}

@inproceedings{6486324,
 abstract = {The problem of speech and text understanding and its application to the spoken dialogue systems is investigated in the paper. The Neuro-Fuzzy model has been applied for solution this problem and received satisfactory results. Mathematical model and software developed for building human-computer dialogue system.},
 author = {S. {Rustamov}},
 booktitle = {2012 IV International Conference "Problems of Cybernetics and Informatics" (PCI)},
 doi = {10.1109/ICPCI.2012.6486324},
 issn = {},
 keywords = {fuzzy set theory;human computer interaction;interactive systems;neural nets;speech synthesis;text analysis;neuro-fuzzy model;text understanding systems;speech understanding systems;spoken dialogue systems;mathematical model;human-computer dialogue system;dialogue systems;speech understanding;neuro-fuzzy model;learning user intention},
 month = {Sep.},
 number = {},
 pages = {1-4},
 title = {Application of Neuro-Fuzzy model for text and speech understanding systems},
 volume = {},
 year = {2012}
}

@article{6634108,
 abstract = {We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.},
 author = {T. {Isenberg} and P. {Isenberg} and J. {Chen} and M. {Sedlmair} and T. {Möller}},
 doi = {10.1109/TVCG.2013.126},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;encoding;visualization evaluation;meta-level;coding scheme;IEEE visualization conference;IEEE information visualization;visual tools;requirements analyses;domain-specific work practices;Encoding;Data visualization;History;Systematics;Mathematical model;Encoding;Data visualization;History;Systematics;Mathematical model;information visualization;Evaluation;validation;systematic review;visualization;scientific visualization;Algorithms;Computer Graphics;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface},
 month = {Dec},
 number = {12},
 pages = {2818-2827},
 title = {A Systematic Review on the Practice of Evaluating Visualization},
 volume = {19},
 year = {2013}
}

@article{6634123,
 abstract = {Numerical ensemble forecasting is a powerful tool that drives many risk analysis efforts and decision making tasks. These ensembles are composed of individual simulations that each uniquely model a possible outcome for a common event of interest: e.g., the direction and force of a hurricane, or the path of travel and mortality rate of a pandemic. This paper presents a new visual strategy to help quantify and characterize a numerical ensemble's predictive uncertainty: i.e., the ability for ensemble constituents to accurately and consistently predict an event of interest based on ground truth observations. Our strategy employs a Bayesian framework to first construct a statistical aggregate from the ensemble. We extend the information obtained from the aggregate with a visualization strategy that characterizes predictive uncertainty at two levels: at a global level, which assesses the ensemble as a whole, as well as a local level, which examines each of the ensemble's constituents. Through this approach, modelers are able to better assess the predictive strengths and weaknesses of the ensemble as a whole, as well as individual models. We apply our method to two datasets to demonstrate its broad applicability.},
 author = {L. {Gosink} and K. {Bensema} and T. {Pulsipher} and H. {Obermaier} and M. {Henry} and H. {Childs} and K. I. {Joy}},
 doi = {10.1109/TVCG.2013.138},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {Bayes methods;data visualisation;learning (artificial intelligence);statistical analysis;uncertainty handling;predictive uncertainty characterization;predictive uncertainty visualization;numerical ensemble forecasting;Bayesian model averaging framework;visual strategy;ensemble constituents;event-of-interest prediction;ground truth observations;statistical aggregate;visualization strategy;Bayes methods;Mathematical model;Predictive models;Numerical models;Data visualization;Bayes methods;Mathematical model;Predictive models;Numerical models;Data visualization;statistical visualization;Uncertainty visualization;numerical ensembles;Algorithms;Bayes Theorem;Computer Graphics;Computer Simulation;Data Interpretation, Statistical;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface},
 month = {Dec},
 number = {12},
 pages = {2703-2712},
 title = {Characterizing and Visualizing Predictive Uncertainty in Numerical Ensembles Through Bayesian Model Averaging},
 volume = {19},
 year = {2013}
}

@article{6875959,
 abstract = {Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview-detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus-context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.},
 author = {S. {Koch} and M. {John} and M. {Wörner} and A. {Müller} and T. {Ertl}},
 doi = {10.1109/TVCG.2014.2346677},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;learning (artificial intelligence);text analysis;varifocalreader;in-depth visual analysis;focus-context techniques;text documents;machine learning techniques;intermediate text levels;document analysis;literary analysis;natural language processing;text mining;visual abstraction;Data visualization;Interactive systems;Navigation;Tag clouds;Natural language processing;Text mining;Document handling;Data mining;visual analytics;document analysis;literary analysis;natural language processing;text mining;machine learning;distant reading},
 month = {Dec},
 number = {12},
 pages = {1723-1732},
 title = {VarifocalReader — In-Depth Visual Analysis of Large Text Documents},
 volume = {20},
 year = {2014}
}

@inproceedings{6991412,
 abstract = {To deliver the software product which conforms to customer's actual needs has become an important issue of software development companies. The appropriate Software Development Life Cycle (SDLC) which is the process consists of a sequence of activities performed for developing that software product is selected. During those activities, there are various information related to software product development and are used to communicate among parties involved. This information is often specified in SDLC documents using natural language. Unfortunately, the problems of interpretation and difficulty of understanding are arisen and often caused by characteristic of natural language itself, which is ambiguous, and the inappropriateness of document structure. These problems which are some of the interested open questions in software requirements specifications area may influence on software product discrepancy from customer's actual needs. To mitigate these problems, this paper proposes a method for assessing quality of SDLC documents characteristics focusing on document contents and structure. The measurement process model is used as a guideline for proposing the method and the measurement information model is applied to define metrics which are used to assess SDLC documents characteristics directly. A Software Requirements Specifications (SRS) document was used to illustrate our proposed method as a case study. The result of the proposed method can be used to indicate the quality level of SDLC documents and appeared flaws, which leads to the improvement of SDLC document quality. The improved SDLC documents can enhance the quality of communication among stakeholders and support the software product development to meet customer's actual needs. These results can also be stored as a lesson learned and be applied for the future similar situation.},
 author = {P. {Thitisathienkul} and N. {Prompoon}},
 booktitle = {Ninth International Conference on Digital Information Management (ICDIM 2014)},
 doi = {10.1109/ICDIM.2014.6991412},
 issn = {},
 keywords = {document handling;formal specification;software metrics;software product lines;software quality;quality assessment method;software development process document;software document characteristics metric;software product deliver;customer needs;software development companies;software development life cycle;software product development;SDLC documents;natural language characteristic;document structure;software requirement specifications;software product discrepancy;document contents;document structure;measurement process model;measurement information model;SDLC document characteristics assessment;software requirement specification document;SRS document;SDLC document quality level;communication quality enhancement;Software;Natural languages;Databases;Quality assessment;Software measurement;Standards;Quality Assessment Method;Software Metric;SDLC Document;Characteristics;Document Structure},
 month = {Sep.},
 number = {},
 pages = {182-188},
 title = {Quality assessment method for software development process document based on software document characteristics metric},
 volume = {},
 year = {2014}
}

@inproceedings{7066257,
 abstract = {Text extraction plays an important role in numerous applications. Research on its method still need to be improved in order to achieve better performance, to increase the reliability of text extraction system and to deal with complex cases of text extraction. The majority of the text extraction methods are focusing on horizontal and near horizontal text lines; however, text in natural scene might be in arbitrary line in real time. Thus, this paper aims to solve the issue of extracting the arbitrary oriented text by suggesting a method for text detection and localization based on the Stroke Width Transform. The proposed method is tested on an arbitrary text dataset and ICDAR dataset. The result of the experiment shows that the proposed method adapts well to the arbitrary text.},
 author = {J. {Jameson} and S. N. H. S. {Abdullah}},
 booktitle = {2014 14th International Conference on Intelligent Systems Design and Applications},
 doi = {10.1109/ISDA.2014.7066257},
 issn = {2164-7143},
 keywords = {text detection;transforms;arbitrary text extraction;natural scene image;stroke width transform;text detection;text localization;ICDAR dataset;Image edge detection;Text analysis;Transforms;Optical character recognition software;Licenses;Integrated circuits;Text extraction;Text detection and localization;Stroke Width Transform;Scene understanding},
 month = {Nov},
 number = {},
 pages = {124-128},
 title = {Extraction of arbitrary text in natural scene image based on stroke width transform},
 volume = {},
 year = {2014}
}

@inproceedings{716778,
 abstract = {A novel pattern searching method using neural networks and correlation is presented. This method combines the quickness and adaptiveness of neural networks with the accuracy of the mathematical correlation approach. Images are divided into small sub-images which are presented to the trained neural network. Sub-images that may contain the pattern or partial pattern are selected by the neural network. The neural network also provides the approximate location of the pattern, therefore the selected sub-images can be adjusted to contain the complete pattern. Desired patterns can be located by measuring the new sub-images' correlation values against the reference models in a small area. Experiments show that this superior method is able to find the desired patterns. Moreover, this method is much faster and more adaptable than traditional pattern searching methods.},
 author = {C. {Chiu} and T. {Oki} and P. {Paolellia}},
 booktitle = {Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
 doi = {10.1109/IJCNN.1993.716778},
 issn = {},
 keywords = {image recognition;neural nets;correlation methods;pattern searching method;neural networks;adaptiveness;mathematical correlation;sub-images;Neural networks;Pattern recognition;Brightness;Character recognition;Image recognition;Pixel;Manufacturing automation;Inspection;Printed circuits;Correlation},
 month = {Oct},
 number = {},
 pages = {1277-1280 vol.2},
 title = {A novel pattern searching method using neural networks and correlation},
 volume = {2},
 year = {1993}
}

@inproceedings{7193210,
 abstract = {Text recognition in images is a research area which attempts to develop a computer system with the ability to automatically read the text from images. These days there is a huge demand in storing the information available in paper documents format in to a computer storage disk and then later reusing this information by searching process. One simple way to store information from these paper documents in to computer system is to first scan the documents and then store them as images. But to reuse this information it is very difficult to read the individual contents and searching the contents form these documents line-by-line and word-by-word. The challenges involved in this the font characteristics of the characters in paper documents and quality of images. Due to these challenges, computer is unable to recognize the characters while reading them. Thus there is a need of character recognition mechanisms to perform Document Image Analysis (DIA) which transforms documents in paper format to electronic format. In this paper we have discuss method for text recognition from images. The objective of this paper is to recognition of text from image for better understanding of the reader by using particular sequence of different processing module.},
 author = {P. M. {Manwatkar} and S. H. {Yadav}},
 booktitle = {2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)},
 doi = {10.1109/ICIIECS.2015.7193210},
 issn = {},
 keywords = {character recognition;document image processing;image resolution;image texture;text detection;image text recognition;computer storage disk;computer system;image quality;character recognition;document image analysis;DIA;Text recognition;Biological neural networks;Computers;Image segmentation;Character recognition;Neurons;Feature extraction;Document Image Analysis (DIA);electronic format;text recognition;font characteristics},
 month = {March},
 number = {},
 pages = {1-6},
 title = {Text recognition from images},
 volume = {},
 year = {2015}
}

@inproceedings{7311974,
 abstract = {In this paper we present a semi-automated analysis of student reading performance from the perspective of her text reading level and text understanding. Silences (pauses) between uttered words or read sentences as well as silences between verbalizations given by students are the key points in the analysis of their learning activities. Pause is an essential element in the analysis of a text, which also gives good control over interactions during the processes of text reading and explanation of understanding. This study presents the results specific to pauses in the reading and verbalization using Praat, a tool to analyze spoken productions. Correlations between students' fluency, story understanding, and mean pause duration of reading and explanation phases show consistent results across texts for reading. Results about pauses during explaining yielded low correlations, showing that other variables may influence the pausing behaviors during explaining.},
 author = {S. {Denisleam Molomer} and S. {Trausan-Matu} and P. {Dessus} and M. {Bianco}},
 booktitle = {2015 14th RoEduNet International Conference - Networking in Education and Research (RoEduNet NER)},
 doi = {10.1109/RoEduNet.2015.7311974},
 issn = {2068-1038},
 keywords = {computer aided instruction;linguistics;speech processing;text analysis;students pauses;semiautomated analysis;student reading performance;text reading level;text understanding;uttered words;read sentences;silences;verbalizations;learning activities;text analysis;Praat tool;spoken productions;students fluency;story understanding;mean pause duration;reading phases;explanation phases;pausing behaviors;explaining;Decision support systems;Pause;Reading;Explanation},
 month = {Sep.},
 number = {},
 pages = {90-93},
 title = {Analyzing students pauses during reading and explaining a story},
 volume = {},
 year = {2015}
}

@inproceedings{7333750,
 abstract = {Text line extraction in document images is an important prerequisite for many content based image understanding applications. In this paper, we propose an accurate and robust method for generic text line extraction, which can be applied on large categories of document images, diverse languages, and text lines with different orientations. Firstly, the candidate connected components are extracted from document image using Maximal Stable Extremal Region (MSER) with the noises filtered by Adaboost and Convolution Neural Network (CNN). Then, the coarse text lines are generated from hierarchical edges reconstruction and cut by local linearity of text lines in the document spanning tree. Finally, for accurate text line extraction, the cut multi-components are re-connected based on text line energy minimization in terms of text line consistency and the fitting error. Experimental results on multilingual test dataset demonstrate the effectiveness and robust of the proposed method, which yields higher performance compared with state-of-the-art methods.},
 author = {L. {Wang} and W. {Fan} and J. {Sun} and S. {Naoi} and T. {Hiroshi}},
 booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
 doi = {10.1109/ICDAR.2015.7333750},
 issn = {},
 keywords = {document image processing;edge detection;feature extraction;image reconstruction;learning (artificial intelligence);natural language processing;neural nets;text analysis;trees (mathematics);text line extraction;document images;content based image understanding applications;generic text line extraction;candidate connected components;maximal stable extremal region;noise filtering;Adaboost;convolution neural network;CNN;hierarchical edge reconstruction;document spanning tree;text line energy minimization;text line consistency;fitting error;multilingual test dataset;Robustness;Benchmark testing;Image segmentation;Surveillance;Image recognition;Integrated optics;Optical imaging;generic text line extraction;MSER;hierarchical edge reconstruction and cut;text line energy minimization},
 month = {Aug},
 number = {},
 pages = {191-195},
 title = {Text line extraction in document images},
 volume = {},
 year = {2015}
}

@inproceedings{7333799,
 abstract = {In practical applications of document understanding, if the documents have multiple languages and orientations, the conventional OCR systems can not be directly applied. This is because those OCR systems are usually designed for texts of single language and normal orientation. To solve this problem, many non-character based recognition approaches were proposed. However, the performance of those methods were not comparable with the mature OCR systems. Consequently, a better idea is to recognize the language type and orientation before the OCR is applied. Besides, the characters of different languages have very ambiguous shape, so it is very difficult to extract stable feature for the recognition. Recently, the convolutional neural networks (CNN) have achieved great success in pattern recognition tasks. Therefore, for such difficult tasks, the CNN is one of the best choice. In this paper, we first applied CNN to the recognition of the document properties. A novel sliding window voting process is proposed to reduce the network scale and fully use the information of the text line. In the experiments, our method had very high recognition rate. The results proved the advantage of the proposed method and which also can be applied to create a document understanding system with OCR systems.},
 author = {L. {Chen} and S. {Wang} and W. {Fan} and J. {Sun} and N. {Satoshi}},
 booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
 doi = {10.1109/ICDAR.2015.7333799},
 issn = {},
 keywords = {document image processing;image recognition;neural nets;text detection;deep learning;orientation recognition;language recognition;document analysis;convolutional neural networks;CNN;document properties recognition;sliding window voting process;text line information;recognition rate;document understanding system;Kernel;Optical character recognition software},
 month = {Aug},
 number = {},
 pages = {436-440},
 title = {Deep learning based language and orientation recognition in document analysis},
 volume = {},
 year = {2015}
}

@article{750572,
 abstract = {Presents a methodology for detection of neural-network gaps (NNGs) based on error analysis and the visualization that is applicable to the n-dimensional I/O domain. The generalization problem in artificial neural networks (ANN) training is analyzed and the concept of NNGs is introduced. The NNGs are highly undesirable in ANN generalization and methods for detecting, analyzing, and eliminating them are necessary. Previous methods for NNG detection, based on two-dimensional (2-D) and three dimensional (3-D) visualization, were not applicable for ANNs with more than three inputs. Experiments demonstrate advantages of this new methodology, which allows better understanding of the NNG phenomena using a quantitative approach.},
 author = {M. M. {Kantardzic} and A. A. {Aly} and A. S. {Elmaghraby}},
 doi = {10.1109/72.750572},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {neural nets;generalisation (artificial intelligence);learning (artificial intelligence);error analysis;neural-network gaps;error analysis;visualization;n-dimensional I/O domain;generalization problem;quantitative approach;Visualization;Error analysis;Artificial neural networks;Testing;Neural networks;Two dimensional displays;Particle measurements;Computer networks;Performance evaluation;Supervised learning},
 month = {March},
 number = {2},
 pages = {419-426},
 title = {Visualization of neural-network gaps based on error analysis},
 volume = {10},
 year = {1999}
}

@article{7552539,
 abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
 author = {W. {Samek} and A. {Binder} and G. {Montavon} and S. {Lapuschkin} and K. {Müller}},
 doi = {10.1109/TNNLS.2016.2599820},
 issn = {2162-237X},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;sensitivity-based approach;deconvolution method;heatmap;deep neural network;complex machine learning tasks;multilayer nonlinear structure;DNN;MIT Places data sets;SUN397;ILSVRC2012;relevance propagation algorithm;data visualization;Heating;Neurons;Biological neural networks;Deconvolution;Sensitivity;Learning systems;Algorithm design and analysis;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
 month = {Nov},
 number = {11},
 pages = {2660-2673},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 volume = {28},
 year = {2017}
}

@inproceedings{7557899,
 abstract = {Artificial Intelligence (AI) has infiltrated almost every scientific and social endeavour, including everything from medical research to the sociology of crowd control. But the foundation of AI continues to be based on digital representations of knowledge, and computational reasoning therewith. Because so much of modern knowledge infrastructure and social behaviour is connected to AI, understanding the role of AI in each such endeavour not only helps accelerate progress in those fields in which it applies, but also creates the challenges to extend the foundation for modern AI methods. The simple hypothesis herein is that so-called AI-complete problems have a role in helping to articulate the appropriate integration of AI within other disciplines. With the current growth of interest in "big data" and visualization, we argue that relatively simple formal structures provide a basis for the claim that visualization is an AI-complete problem. The value of confirming this claim is largely to encourage stronger formalizations of the visualization process in terms of the AI foundations of representation and reasoning. This connection will help ensure that relevant components of AI are appropriately applied and integrated, to provide value for a basis of a theory of visualization. The sketch of this claim here is based on the simple idea that visualization is an abstraction process, and that abstractions from partial information, however voluminous, directly confronts the non monotonic reasoning challenge, thus the need for caution in engineering visualization systems without carefully considering the consequences of visual abstraction. This is particularly important with interactive visualization, which has recently formed the basis for such fields as visual analytics.},
 author = {R. {Goebel}},
 booktitle = {2016 20th International Conference Information Visualisation (IV)},
 doi = {10.1109/IV.2016.53},
 issn = {2375-0138},
 keywords = {artificial intelligence;Big Data;data analysis;data structures;data visualisation;AI-complete problem;artificial intelligence;digital knowledge representations;computational reasoning;modern knowledge infrastructure;social behaviour;AI methods;AI integration;Big Data;formal structures;visualization process formalization;partial information abstraction process;monotonic reasoning challenge;engineering visualization systems;visual abstraction;interactive visualization;visual analytics;Data visualization;Visualization;Artificial intelligence;Context;Complexity theory;Cognition;Semantics;AI-complete visualization incomplete knowledge},
 month = {July},
 number = {},
 pages = {27-32},
 title = {Why Visualization is an AI-complete Problem (and Why That Matters)},
 volume = {},
 year = {2016}
}

@article{774103,
 abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
 author = {I. A. {Taha} and J. {Ghosh}},
 doi = {10.1109/69.774103},
 issn = {1041-4347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 keywords = {symbol manipulation;knowledge representation;explanation;truth maintenance;feedforward neural nets;neural net architecture;knowledge based systems;learning (artificial intelligence);symbolic interpretation;hybrid intelligent systems;knowledge-based systems;artificial neural networks;domain knowledge representation;domain knowledge mapping;connectionist architecture;network training;rule extraction;explanation power;output decision validation;knowledge refinement;binary rules;feedforward networks;hidden layer;sigmoidal units;partial rules;embedded knowledge;adjustable detail level;rule evaluation technique;rule ordering;performance measures;iris data set;breast cancer data set;Artificial neural networks;Neural networks;Data mining;Military computing;Knowledge representation;Knowledge based systems;Fuzzy neural networks;Fuzzy sets;Computer networks;Intelligent systems},
 month = {May},
 number = {3},
 pages = {448-463},
 title = {Symbolic interpretation of artificial neural networks},
 volume = {11},
 year = {1999}
}

@inproceedings{777686,
 abstract = {Document Analysis and Understanding (DAU) is a complex AI application with high industrial impact. For the increasing demands upon the bandwidth and quality of the analysis it is crucial to enable different analysis modules to collaborate. For making collaboration possible, we first examine the question of whether there exists a common ontological basis which can serve as a platform for communication of different DAU modules. Once communication is enabled, we investigate the second question, how DAU modules originally designed as stand-alone systems must be modified in order to benefit from collaboration with others.},
 author = {B. {Klein} and A. {Abecker}},
 booktitle = {Proceedings IEEE Forum on Research and Technology Advances in Digital Libraries},
 doi = {10.1109/ADL.1999.777686},
 issn = {1092-9959},
 keywords = {document handling;knowledge based systems;grammars;groupware;distributed knowledge based parsing;document analysis and understanding;DAU;complex AI application;industrial impact;analysis modules;collaboration;common ontological basis;DAU modules;Text analysis;Information analysis;Data mining;Collaboration;Text recognition;SGML;Automata;Artificial intelligence;Bandwidth;Ontologies},
 month = {May},
 number = {},
 pages = {6-15},
 title = {Distributed knowledge-based parsing for document analysis and understanding},
 volume = {},
 year = {1999}
}

@inproceedings{7801719,
 abstract = {Multi-document summarization is to create summaries covering the major information that multiple documents tell in common. For this point, the existing methods are based on hand-crafted features for word and sentence. However, it is difficult to figure out the core contents of each document with the hand-crafted features because they have the limited information presented the given documents. Moreover, there exists a limit to figure out the major information because documents with the same meaning used to be paraphrased depending on their writers. Therefore, it is necessary to represent the semantic meanings of documents as well as sentences through understanding natural language. In this paper, we propose a new multi-document summarization system by creating a synthetic document vector covering the whole documents based on Language Model, whose is well-known for learning the semantic features in text. We experimented with DUC 2004 dataset provided by Document Understanding Conference (DUC) and the results show that our method summarizes multiple documents effectively based on their core contents.},
 author = {D. {Kim} and J. {Lee}},
 booktitle = {2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)},
 doi = {10.1109/SCIS-ISIS.2016.0132},
 issn = {},
 keywords = {document handling;natural language processing;word processing;multidocument summarization system;semantic text feature learning;DUC 2004 dataset;document understanding conference;natural language model;semantic document meanings;hand-crafted features;synthetic document vector;Semantics;Context;Hidden Markov models;Computational modeling;Redundancy;Intelligent systems;Natural languages;Multi-document summarization;Core content;Major Information;Synthetic document vector;Language model},
 month = {Aug},
 number = {},
 pages = {605-609},
 title = {Multi-document Summarization by Creating Synthetic Document Vector Based on Language Model},
 volume = {},
 year = {2016}
}

@inproceedings{7821768,
 abstract = {The problem being addressed in this paper is that using brute force in Natural Language Processing and Machine Learning combined with advanced statistics will only approximate meaning and thus will not deliver in terms of real text understanding. Counting words and tracking word order or parsing by syntax will also result in probability and guesswork at best. Their vendors struggle in delivering accurate quality and this results in ill-functioning applications. The newer generation methodologies like Deep Learning and Cognitive Computing are breaking barriers in the (Big Data) fields of Internet of Things, Robotics and Image/Video Recognition but cannot be successfully deployed for text without huge amounts of training and sample data. In the short term, we believe non-biological Artificial Intelligence will produce the best results for text understanding. Miia applied advanced Linguistic and Semantic Technologies combined with ConceptNet modeling and Machine Learning to successfully cater deep intelligent and cross-language quality to several industries.},
 author = {L. {Stephen} and D. {Geert} and K. {Andreas} and P. {Frank}},
 booktitle = {2016 Future Technologies Conference (FTC)},
 doi = {10.1109/FTC.2016.7821768},
 issn = {},
 keywords = {Big Data;Internet of Things;learning (artificial intelligence);mobile computing;natural language processing;semantic networks;statistics;text analysis;nonbiological AI approach;natural language understanding;natural language processing;machine learning;advanced statistics;meaning approximation;text understanding;word counting;word order tracking;syntax parsing;deep learning;cognitive computing;Big Data;Internet of Things;robotics;image/video recognition;nonbiological artificial intelligence;advanced linguistic technologies;semantic technologies;ConceptNet modeling;cross-language quality;Companies;Natural languages;Artificial intelligence;Semantics;Law;Engines;Natural Language Processing;Natural Language Understanding;Artificial Intelligence;Linguistics;Semantics;Machine Learning;ConceptNet Modelling;Data Mining;Sentiment Analysis;Data Analysis;Big Data;Business Intelligence},
 month = {Dec},
 number = {},
 pages = {1300-1302},
 title = {A non-biological AI approach towards natural language understanding},
 volume = {},
 year = {2016}
}

@inproceedings{7846290,
 abstract = {Machine Learning (ML) techniques have allowed a great performance improvement of different challenging Spoken Language Understanding (SLU) tasks. Among these methods, Neural Networks (NN), or Multilayer Perceptron (MLP), recently received a great interest from researchers due to their representation capability of complex internal structures in a low dimensional subspace. However, MLPs employ document representations based on basic word level or topic-based features. Therefore, these basic representations reveal little in way of document statistical structure by only considering words or topics contained in the document as a “bag-of-words”, ignoring relations between them. We propose to remedy this weakness by extending the complex features based on Quaternion algebra presented in [1] to neural networks called QMLP. This original QMLP approach is based on hyper-complex algebra to take into consideration features dependencies in documents. New document features, based on the document structure itself, used as input of the QMLP, are also investigated in this paper, in comparison to those initially proposed in [1]. Experiments made on a SLU task from a real framework of human spoken dialogues showed that our QMLP approach associated with the proposed document features outperforms other approaches, with an accuracy gain of 2% with respect to the MLP based on real numbers and more than 3% with respect to the first Quaternion-based features proposed in [1]. We finally demonstrated that less iterations are needed by our QMLP architecture to be efficient and to reach promising accuracies.},
 author = {T. {Parcollet} and M. {Morchid} and P. {Bousquet} and R. {Dufour} and G. {Linarès} and R. {De Mori}},
 booktitle = {2016 IEEE Spoken Language Technology Workshop (SLT)},
 doi = {10.1109/SLT.2016.7846290},
 issn = {},
 keywords = {algebra;document handling;learning (artificial intelligence);multilayer perceptrons;natural language processing;quaternion neural networks;machine learning;spoken language understanding;SLU;multilayer perceptron;MLP;document representations;document statistical structure;quaternion algebra;Quaternions;Computational modeling;Neurons;Artificial neural networks;Algebra;Natural language processing;Quaternion;Neural Network;Spoken Language Understanding;Machine Learning},
 month = {Dec},
 number = {},
 pages = {362-368},
 title = {Quaternion Neural Networks for Spoken Language Understanding},
 volume = {},
 year = {2016}
}

@inproceedings{7846312,
 abstract = {Spoken language understanding (SLU) is one of the important problem in natural language processing, and especially in dialog system. Fifth Dialog State Tracking Challenge (DSTC5) introduced a SLU challenge task, which is automatic tagging to speech utterances by two speaker roles with speech acts tag and semantic slots tag. In this paper, we focus on speech acts tagging. We propose local coactivate multi-task learning model for capturing structured speech acts, based on sentence features by recurrent convolutional neural networks. An experiment result, shows that our model outperformed all other submitted entries, and were able to capture coactivated local features of category and attribute, which are parts of speech act.},
 author = {T. {Ushio} and H. {Shi} and M. {Endo} and K. {Yamagami} and N. {Horii}},
 booktitle = {2016 IEEE Spoken Language Technology Workshop (SLT)},
 doi = {10.1109/SLT.2016.7846312},
 issn = {},
 keywords = {feedforward neural nets;learning (artificial intelligence);natural language processing;pattern classification;recurrent neural nets;speech processing;text analysis;attribute feature;category feature;sentence feature;local coactivate multitask learning model;semantic-slot tag;speech-act tag;speech utterances;automatic tagging;SLU challenge task;DSTC5;Fifth-Dialog State Tracking Challenge;dialog system;natural language processing;spoken language understanding;structured speech act tagging;recurrent convolutional neural networks;Speech;Tagging;Neural networks;Hidden Markov models;Training;Text categorization;Neurons;spoken language understanding;speech act tagging;text classification;multi-task learning;neural networks},
 month = {Dec},
 number = {},
 pages = {518-524},
 title = {Recurrent convolutional neural networks for structured speech act tagging},
 volume = {},
 year = {2016}
}

@inproceedings{791799,
 abstract = {HUE (the Handwriting Understanding Environment) is a software framework for handwriting and document analysis built around a two-level programming model in which components are implemented in a system programming language (typically C++) and are connected together into prototype systems using the scripting language Tcl Tk. HUE is an extended version of TABS (a previous handwriting analysis framework), and incorporates the authors' experience of using TABS for around 2 years in data-intensive handwriting and document analysis research and evaluation. HUE currently contains 94 C++ components, 7 native data types, 11 custom-built Tcl Tk packages, a novel dynamic user interface, and several demonstration systems implemented as Tcl scripts.},
 author = {C. {Cracknell} and A. C. {Downton}},
 booktitle = {Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)},
 doi = {10.1109/ICDAR.1999.791799},
 issn = {},
 keywords = {document image processing;object-oriented programming;handwritten character recognition;user interfaces;software prototyping;Handwriting Understanding Environment;rapid prototyping;document analysis;handwriting analysis;software framework;two-level programming model;system programming language;Tcl Tk scripting language;TABS;C++ components;native data types;custom-built Tcl Tk packages;dynamic user interface;demonstration systems;Prototypes;Computer languages;Text analysis;Handwriting recognition;Image processing;Computer vision;Libraries;Programming profession;Design engineering;Systems engineering and theory},
 month = {Sep.},
 number = {},
 pages = {362-365},
 title = {A Handwriting Understanding Environment (HUE) for rapid prototyping in handwriting and document analysis research},
 volume = {},
 year = {1999}
}

@inproceedings{7960721,
 abstract = {The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.},
 author = {İ. Ü. {Oğul} and C. {Özcan} and Ö. {Hakdağlı}},
 booktitle = {2017 25th Signal Processing and Communications Applications Conference (SIU)},
 doi = {10.1109/SIU.2017.7960721},
 issn = {},
 keywords = {Bayes methods;data analysis;distributed processing;Internet of Things;learning (artificial intelligence);pattern classification;public domain software;social networking (online);text analysis;IoT;open source social news site;Reddit;interpretation data;data analysis;fast data storage;Hadoop architecture;distributed data structure;distributed in-memory data collection;Apache Spark architecture;Naive Bayes method;machine learning;meaning extraction;irrelevant data deletion;Internet of Things;fast text classification;Sparks;Java;Internet of Things;Standards;Text categorization;Art;Machine learning;Text mining;Big data;Apache Spark;Classification;Naive Bayes},
 month = {May},
 number = {},
 pages = {1-4},
 title = {Fast text classification with Naive Bayes method on Apache Spark},
 volume = {},
 year = {2017}
}

@inproceedings{7982151,
 abstract = {The accuracy of conventional DGA interpretation methods can be different when each of these methods are used in different places or different circumstances. Rogers Ratio Method (RRM), IEC Ratio Method (IRM) (Basic Gas Ratios Method), GB/T 7252 (National Standard of the People's Republic of China) are popular conventional methods for interpreting the possible faults indicator of transformer in Indonesia and China. This research proposes artificial intelligence to interpret DGA by combining conventional method and artificial intelligence method using weighting factor. The artificial intelligence method which are used in this research is fuzzy logic. DGA practical data which used as refer data for data mining in this research were taken from China and Indonesia. This research also uses Thompson tau method to filter the data from outlier and fuzzy c means clustering to cluster the data to make sure the data are used is valid and good enough to be used to build artificial intelligence through data mining. The output of this research is to create an artificial intelligence and the combination between artificial intelligence which have been built with conventional method to interpret DGA whether there is any fault in transformer or not.},
 author = {C. {Subroto} and and and G. {Zhang}},
 booktitle = {2017 1st International Conference on Electrical Materials and Power Equipment (ICEMPE)},
 doi = {10.1109/ICEMPE.2017.7982151},
 issn = {},
 keywords = {artificial intelligence;fuzzy logic;fuzzy set theory;power engineering computing;power transformers;artificial intelligence;DGA interpretation methods;weighting factor;Rogers ratio method;RRM;IEC Ratio Method;IRM;basic gas ratios method;transformer faults indicator;fuzzy logic;Thompson tau method;fuzzy c means clustering;data mining;Artificial intelligence;Fuzzy logic;Partial discharges;Data mining;Discharges (electric);Power transformer insulation;DGA;Fuzzy Logic;Weighting factor;Thompson tau method;Fuzzy C Means},
 month = {May},
 number = {},
 pages = {85-88},
 title = {Artificial intelligence for DGA interpretation methods using weighting factor},
 volume = {},
 year = {2017}
}

@inproceedings{8047390,
 abstract = {The beginner counselors have more likely to continue counseling in their own interest, they have a high tendency to make great use of the closed-ended question in order to confirm the interpretation with the client. While expert counselors are instructing the counseling skill to beginner counselors, we consider that the reaction of a client for a beginner counselor's question is important to visualize in an appropriate method. To respond the request, we have developed a system for visualizing the flow of conversation in counseling. However, the expert counselor as the system user requires to correct the initial classification result manually, and the work burden is large, because the accuracy of the category classification of conversation data is very low in the current system. To improve this problem, we have implemented on the category classification method of text data with SVM (Support Vector Machine) as machine learning technique to visualize the flow of conversation in counseling. In addition, we have compared and evaluated with results of the initial classification method of the current system. As these results, we have shown that the accuracy rate of the classification method with SVM become higher than the results in the current system.},
 author = {Y. {Hayashida} and T. {Uetsuji} and Y. {Ebara} and K. {Koyamada}},
 booktitle = {2017 Nicograph International (NicoInt)},
 doi = {10.1109/NICOInt.2017.35},
 issn = {},
 keywords = {data visualisation;learning (artificial intelligence);pattern classification;psychology;support vector machines;text analysis;category classification;text data;machine learning;conversation flow visualization;counseling;support vector machine;SVM;Employee welfare;Support vector machines;Data visualization;Psychology;Dictionaries;Data models;Employment;Counseling;Visualization;Machine Learning;Text Classification},
 month = {June},
 number = {},
 pages = {37-40},
 title = {Category Classification of Text Data with Machine Learning Technique for Visualizing Flow of Conversation in Counseling},
 volume = {},
 year = {2017}
}

@inproceedings{8128175,
 abstract = {The use of LiDAR and multiples digital images jointly with 3-D reconstruction techniques for creating 3-D models of natural outcrops and surfaces studies have increased dramatically in the last few years. These techniques have provided an enormous amount of data for interpretation by geoscientists. However, these researchers have no available software capable of offering a user experience comparable to the fieldwork. The majority of solutions have considered desktop systems, which presents inherent limitations due to the 2-D characteristics of displays and loss of immersion into the 3-D model, or up until expensive and complex stereoscopic based approaches to improve the 3-D user experience do not offer well suitable solutions. To address these limitations, this paper presents a low-cost completely disruptive solution for processing, visualizing, sharing and directly handling Digital Outcrop Models with the support of a full interpretation toolset, the MOSIS System. The proposed system provides a fully immersive computational environment, capable of teleporting virtually geoscientists to the fieldwork, giving an awareness of being there physically with an extensible toolset for the DOM's interpretation. Besides, desktop, web and mobile versions of MOSIS have been under development and fulfill the lack of tools for digital outcrop modeling.},
 author = {L. {Gonzaga} and M. R. {Veronez} and D. N. {Alves} and F. {Bordin} and G. L. {Kannenberg} and F. P. {Marson} and F. M. W. {Tognoli} and L. C. {Inocencio}},
 booktitle = {2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
 doi = {10.1109/IGARSS.2017.8128175},
 issn = {2153-7003},
 keywords = {data visualisation;geophysical techniques;image reconstruction;optical radar;radar imaging;stereo image processing;MOSIS;multioutcrop sharing multiples digital images;digital outcrop models;DOM interpretation;digital outcrop modeling;fully immersive computational environment;MOSIS System;interpretation toolset;complex stereoscopic based approaches;natural outcrops;3-D model;3-D reconstruction techniques;Three-dimensional displays;Geology;Data visualization;Solid modeling;Tools;Visualization;Standards;immersive visualization;digital outcop model (DOM);virtual outcrop;interpretation;3-D visualization;GPU Computing},
 month = {July},
 number = {},
 pages = {5209-5212},
 title = {MOSIS — Multi-outcrop sharing interpretation system},
 volume = {},
 year = {2017}
}

@inproceedings{8215791,
 abstract = {Automatic creation of polarity dictionaries is an important issue, as explanations of prediction models are often required in the financial industry. This paper proposes a novel method of developing an interpretable and predictable neural network model. The neural network model we built can extract polarity scores of concepts from documents. Furthermore, we can detect pairwise interactions between concepts, and create polarity concept dictionaries using our neural network model. The model was built using vector representations of words and polarity scores for about 100 words provided by financial professionals, and we obtained about a hundred times more polarity scores for unknown words through backpropagation. First, we analyze the properties of our method from a theoretical point of view. We then confirm its capabilities by conducting simulations of assigning polarity scores to unknown words and detecting interactions using artificial data. We subsequently estimate sentiment tags using real financial textual datasets. Compared with other conventional methods, the proposed approach can forecast sentiments with higher F1 scores. Finally, we develop a polarity concept dictionary based on Yahoo! Finance board textual data.},
 author = {T. {Ito} and H. {Sakaji} and K. {Izumi} and K. {Tsubouchi} and T. {Yamashita}},
 booktitle = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
 doi = {10.1109/ICDMW.2017.159},
 issn = {2375-9259},
 keywords = {backpropagation;data mining;dictionaries;financial data processing;natural language processing;neural nets;text analysis;interpretable neural network model;polarity concept dictionaries;automatic creation;polarity dictionaries;prediction models;interpretable network model;predictable neural network model;polarity scores;unknown words;detecting interactions;financial industry;vector representations;Yahoo! Finance board textual data;Dictionaries;Artificial neural networks;Predictive models;Data mining;Industries;Analytical models;Neural Network Model;Text-mining;Sentiment analysis},
 month = {Nov},
 number = {},
 pages = {1122-1131},
 title = {Development of an Interpretable Neural Network Model for Creation of Polarity Concept Dictionaries},
 volume = {},
 year = {2017}
}

@inproceedings{8256457,
 abstract = {It must be rather difficult for ordinary people to communicate with robots using special technical languages. Therefore, it must be more desirable for them to use natural language (NL) for such a purpose because it is the most conventional among them. This work proposes a methodology for natural language understanding through an AI system named Conversation Management System (CMS) based on Mental Image Directed Semantic Theory proposed by M. Yokota. CMS is intended to enable a robot to understand NL in the same way as people do, and actually can reach the most plausible semantic interpretation of an input text and return desirable outcomes by employing word concepts, postulates, and inference rules. Recently, the authors have applied several spatial terms in English language, for example verbs, prepositions (e.g. between, along, left, right, and so on). We found that the methodology is outstanding from conventional approaches with the attempt to provide robots understand NL based on mental image model. This paper focuses on how CMS understands static spatial (3D) relations expressed in NL.},
 author = {R. {Khummongkol} and M. {Yokota}},
 booktitle = {2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST)},
 doi = {10.1109/ICAwST.2017.8256457},
 issn = {2325-5994},
 keywords = {artificial intelligence;human-robot interaction;natural language processing;mental image based understanding;static relations;dynamic spatial relations;special technical languages;NL;natural language understanding;AI system;Conversation Management System;CMS;Mental Image Directed Semantic Theory;spatial terms;English language;mental image model;robot communication;semantic interpretation;static spatial 3D relations;Robots;Semantics;Natural languages;Rivers;Conferences;Artificial intelligence;Cognition;natural language understanding;human — robot interaction;semantic interpretation},
 month = {Nov},
 number = {},
 pages = {254-259},
 title = {An approach to mental image based understanding of natural language: Focused on static and dynamic spatial relations},
 volume = {},
 year = {2017}
}

@inproceedings{8258557,
 abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
 author = {S. {Shirataki} and S. {Yamaguchi}},
 booktitle = {2017 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData.2017.8258557},
 issn = {},
 keywords = {Big Data;data analysis;learning (artificial intelligence);pattern classification;support vector machines;text analysis;machine learning;decision reasons;Support Vector Machine;big data analysis;SVM;highly weighted words;black boxes;opinions classification;Support vector machines;DVD;Analytical models;Predictive models;Training;Big Data;Tools;SVM;machine learning;interpretability},
 month = {Dec},
 number = {},
 pages = {4830-4831},
 title = {A study on interpretability of decision of machine learning},
 volume = {},
 year = {2017}
}

@inproceedings{8260658,
 abstract = {Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
 author = {K. {Kowsari} and D. E. {Brown} and M. {Heidarysafa} and K. {Jafari Meimandi} and M. S. {Gerber} and L. E. {Barnes}},
 booktitle = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 doi = {10.1109/ICMLA.2017.0-134},
 issn = {},
 keywords = {data mining;learning (artificial intelligence);neural nets;pattern classification;text analysis;document hierarchy;HDLTex;Hierarchical Deep;Text classification;document collections;information processing methods;supervised learning;multiclass classification;hierarchical classification;deep learning architectures;supervised classifiers;text organization;document classification methods;Machine learning;Support vector machines;Computer architecture;Kernel;Mathematical model;Recurrent neural networks;Text Mining;Document Classification;Deep Neural Networks;Hierarchical Learning;Deep Learning},
 month = {Dec},
 number = {},
 pages = {364-371},
 title = {HDLTex: Hierarchical Deep Learning for Text Classification},
 volume = {},
 year = {2017}
}

@inproceedings{8268978,
 abstract = {Deep Neural Networks (DNN) received a great interest from researchers due to their capability to construct robust abstract representations of heterogeneous documents in a latent subspace. Nonetheless, mere real-valued deep neural networks require an appropriate adaptation, such as the convolution process, to capture latent relations between input features. Moreover, real-valued deep neural networks reveal little in way of document internal dependencies, by only considering words or topics contained in the document as an isolate basic element. Quaternion-valued multi-layer perceptrons (QMLP), and autoencoders (QAE) have been introduced to capture such latent dependencies, alongside to represent multidimensional data. Nonetheless, a three-layered neural network does not benefit from the high abstraction capability of DNNs. The paper proposes first to extend the hyper-complex algebra to deep neural networks (QDNN) and, then, introduces pre-trained deep quaternion neural networks (QDNN-AE) with dedicated quaternion encoder-decoders (QAE). The experiments conduced on a theme identification task of spoken dialogues from the DECODA data set show, inter alia, that the QDNN-AE reaches a promising gain of 2.2% compared to the standard real-valued DNN-AE.},
 author = {T. {Parcollet} and M. {Morchid} and G. {Linarès}},
 booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 doi = {10.1109/ASRU.2017.8268978},
 issn = {},
 keywords = {learning (artificial intelligence);multilayer perceptrons;natural language processing;speech recognition;real-valued deep neural networks;document internal dependencies;neural network;deep quaternion neural networks;spoken language understanding;quaternion-valued multilayer perceptrons;QMLP;autoencoders;QAE;spoken dialogues;Quaternions;Speech;Task analysis;Telephone sets;Algebra;Biological neural networks;Quaternions;deep neural networks;spoken language understanding;autoencoders;machine learning},
 month = {Dec},
 number = {},
 pages = {504-511},
 title = {Deep quaternion neural networks for spoken language understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8275810,
 abstract = {In text categorization, feature representation for dimensionality reduction is a key step. Usually, some commonly used methods, e.g., latent semantic analysis (LSA), yield a dense representation or a dense transformation matrix, which is difficult to precisely characterize the document-topic or the topic-word relationship. This paper proposes a novel discriminative topic sparse representation (DTSR) approach for text categorization, in which two stages are included: the topic dictionary construction and sparse representation. Firstly, a discriminative and interpretable dictionary is constructed to characterize the topic-word relationship. The dictionary contains all category center vectors as well as some semantic topic vectors generated by a latent Dirichlet allocation (LDA) model. Furthermore, each document can be represented with a sparse form to obtain a good document-topic relationship. Experimental results on well-known benchmark datasets indicate that the proposed method not only achieves a satisfactory classification performance but also provides a reasonable sparse semantic meaningful.},
 author = {W. {Zheng} and Y. {Liu} and H. {Lu} and H. {Tang}},
 booktitle = {2017 10th International Symposium on Computational Intelligence and Design (ISCID)},
 doi = {10.1109/ISCID.2017.54},
 issn = {2473-3547},
 keywords = {learning (artificial intelligence);pattern classification;text analysis;vectors;semantic topic vectors;latent Dirichlet allocation model;text categorization;feature representation;dimensionality reduction;latent semantic analysis;dense transformation matrix;topic-word relationship;topic dictionary construction;discriminative dictionary;interpretable dictionary;document-topic relationship;discriminative topic sparse representation approach;Semantics;Dictionaries;Text categorization;Training;Sparse matrices;Large scale integration;Resource management;Text Categorization;Sparse Representation;Topic;Discriminative;Semantic},
 month = {Dec},
 number = {},
 pages = {454-457},
 title = {Discriminative Topic Sparse Representation for Text Categorization},
 volume = {1},
 year = {2017}
}

@inproceedings{8276047,
 abstract = {Sentiment analysis is a methodology used to analyse the emotion or view of an individual to a situation or topic. In present scenario, Social media is the source for the collection of individual's feedbacks, user's emotions, reviews and personal experiences which lead to a need for efficient mining of the text to derive knowledge. An optimal classification of text based on emotion is an unsolved problem in text mining. To extract knowledge from text many machine learning tools and techniques were proposed. An onto-based process is proposed to analyse the customer's emotion in this paper. The input emotional text that needs to be classified is given as input to the NLP and processed and an emotional ontology is created for better understanding of the semantics and relationships. When adding new instances, Ontology can be automatically classify them based on emotional relationship. The Emowords from ontology can be further classified using any of the standard machine learning techniques which definitively gives a better performance. This paper is a review of all the machine learning techniques that can be applied on the semantic analysis of sentiments.},
 author = {K. {Saranya} and S. {Jayanthy}},
 booktitle = {2017 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)},
 doi = {10.1109/ICIIECS.2017.8276047},
 issn = {},
 keywords = {data mining;emotion recognition;knowledge based systems;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence);pattern classification;sentiment analysis;social networking (online);text analysis;text mining;machine learning tools;customer;input emotional text;emotional ontology;emotional relationship;standard machine learning techniques;sentiment analysis;Social media;feedbacks;optimal classification;Emowords;Onto-based sentiment classification;NLP;Ontologies;Sentiment analysis;Social network services;Text mining;Semantics;Support vector machines;sentimental analysis;ontology;machine learning;NLP;semantics},
 month = {March},
 number = {},
 pages = {1-5},
 title = {Onto-based sentiment classification using machine learning techniques},
 volume = {},
 year = {2017}
}

@inproceedings{8308186,
 abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
 author = {S. {Albawi} and T. A. {Mohammed} and S. {Al-Zawi}},
 booktitle = {2017 International Conference on Engineering and Technology (ICET)},
 doi = {10.1109/ICEngTechnol.2017.8308186},
 issn = {},
 keywords = {computer vision;feedforward neural nets;image classification;learning (artificial intelligence);natural language processing;fully-connected layers;convolutional connected layers;nonlinearity layer;multiple layers;matrixes called convolution;mathematical linear operation;classical methods performance;deeper hidden layers;multilayers;Artificial Neural Networks;Deep Neural Network;term Deep Learning;convolutional neural network;artificial neural network;largest image classification data;image data;CNN;pooling;Convolution;Neurons;Convolutional neural networks;Feature extraction;Image edge detection;machine learning;artificial neural networks;deep learning;convolutional neural networks;computer vision;Image recognition},
 month = {Aug},
 number = {},
 pages = {1-6},
 title = {Understanding of a convolutional neural network},
 volume = {},
 year = {2017}
}

@inproceedings{8310088,
 abstract = {As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.},
 author = {R. {Raoui-Outach} and C. {Million-Rousseau} and A. {Benoit} and P. {Lambert}},
 booktitle = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
 doi = {10.1109/IPTA.2017.8310088},
 issn = {2154-512X},
 keywords = {data analysis;document image processing;feedforward neural nets;learning (artificial intelligence);sales management;text analysis;Deep learning;automatic sale receipt understanding;data analytics;scanned document analysis;paper damages;scanning quality;industrial context;automatic understanding;essential consumption statistics;accurate consumption statistics;smart-phone;tool chain;essential information;store brand;purchased products;related prices;highest possible confidence;high confidence level;tool-chain;Deep Convolutional Neural Networks;classical image;text processings;double check processing;text analysis;Optical character recognition software;Character recognition;Semantics;Text analysis;Task analysis;Object detection;Machine learning;Receipt image understanding;Deep Convolutional Neural Networks;Object Detection;Semantic Analysis},
 month = {Nov},
 number = {},
 pages = {1-6},
 title = {Deep learning for automatic sale receipt understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8320258,
 abstract = {These days, text summarization is an active research field to identify the relevant information from large documents produced in various domains such as finance, news media, academics, politics, etc. Text summarization is the process of shortening the documents by preserving the important contents of the text. This can be achieved through extractive and abstractive summarization. In this paper, we have proposed an approach to extract a good set of features followed by neural network for supervised extractive summarization. Our experimental results on Document Understanding Conferences 2002 dataset show the effectiveness of the proposed method against various online extractive text summarizers.},
 author = {A. {Jain} and D. {Bhatia} and M. K. {Thakur}},
 booktitle = {2017 International Conference on Machine Learning and Data Science (MLDS)},
 doi = {10.1109/MLDS.2017.12},
 issn = {},
 keywords = {text analysis;news media;abstractive summarization;supervised extractive summarization;Document Understanding Conferences 2002 dataset;online extractive text summarizers;word vector;active research field;Feature extraction;Neural networks;Data mining;Training;Mathematical model;Computational modeling;Testing;Extractive Text Summarization;Neural Network;Machine Learning;Word Vector Embedding},
 month = {Dec},
 number = {},
 pages = {51-55},
 title = {Extractive Text Summarization Using Word Vector Embedding},
 volume = {},
 year = {2017}
}

@inproceedings{8332874,
 abstract = {Semantic Text Similarity plays a major role in natural language processing. In recent years, researchers have paid considerable attention to Semantic Text Similarity. Some breakthroughs have been made in English, but there are two disadvantages when these models are applied to Chinese: Single sequence models don't consider semantic ambiguity such as polysemy, synonym; these models don't consider that Chinese stop words are important for Chinese word segmentation, voice analysis, semantic understanding. Firstly, in order to overcome the first problem, we proposed the double short text sequences model that has two identical LSTM (Long Short-Term Memory) processing two text sequences at the same time. Secondly, in order to overcome the second problem, according to the characteristics of Chinese, we used the Chinese semantic similarity data sets designed by experts to train and test the model, and retained the stop words in the model training process. Finally, the proposed model was compared with the Semantic Text Similarity model based on CNN (Convolution Neural Network) and the Baidu Semantic Text Similarity model. The results show that the model is greater than the previous two in terms of accuracy, recall rate and so on, and the generalization ability is improved also.},
 author = {T. {Shancheng} and B. {Yunyue} and M. {Fuyu}},
 booktitle = {2018 International Conference on Intelligent Transportation, Big Data Smart City (ICITBS)},
 doi = {10.1109/ICITBS.2018.00190},
 issn = {},
 keywords = {feedforward neural nets;learning (artificial intelligence);natural language processing;text analysis;natural language processing;semantic ambiguity;Chinese stop words;Chinese word segmentation;semantic understanding;double short text sequences model;Long Short-Term Memory;Chinese semantic similarity data sets;single sequence models;double short Chinese sequences;Baidu semantic text similarity model;CNN;convolution neural network;Semantics;Training;Computational modeling;Data models;Analytical models;Training data;Encyclopedias;Semantic similarity;Chinese short text;double sequence;deep learning},
 month = {Jan},
 number = {},
 pages = {736-739},
 title = {A Semantic Text Similarity Model for Double Short Chinese Sequences},
 volume = {},
 year = {2018}
}

@inproceedings{8356909,
 abstract = {Deep Learning for the game of Go recently had a tremendous success with the victory of AlphaGo against Ke Jie in May 2017. However, there is no clear understanding of why they perform so well. In this paper, we introduce a visualization technique that performs a sensitivity analysis of the classifier output by occluding portions of the input Go board, revealing which parts of the board are important for predicting the next move. Using this tool, we start with the experiment about the accuracy of the critical area revealed. We also suppose that by showing the critical area, it will allow Go beginners to understand the board visually that they may have been confused about.},
 author = {Y. {Pang} and T. {Ito}},
 booktitle = {2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
 doi = {10.1109/TAAI.2017.42},
 issn = {2376-6824},
 keywords = {computer games;data visualisation;learning (artificial intelligence);pattern classification;sensitivity analysis;sensitivity analysis;classifier output;critical area;visualization method;Ke Jie;deep learning;computer go;Go game;AlphaGo;input Go board;Machine learning;Data visualization;Training;Deep Learning;Computer Go;Visualization},
 month = {Dec},
 number = {},
 pages = {62-65},
 title = {A Proposal of Visualization Method for Critical Area in Computer Go},
 volume = {},
 year = {2017}
}

@inproceedings{8365991,
 abstract = {Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.},
 author = {S. {Nie} and C. {Healey} and K. {Padia} and S. {Leeman-Munk} and J. {Benson} and D. {Caira} and S. {Sethi} and R. {Devarajan}},
 booktitle = {2018 IEEE Pacific Visualization Symposium (PacificVis)},
 doi = {10.1109/PacificVis.2018.00031},
 issn = {2165-8773},
 keywords = {data visualisation;feedforward neural nets;learning (artificial intelligence);natural language processing;text analysis;interaction techniques;DNN models;network activation;training patterns;internal activation patterns;neural network researchers;convolutional connected layers;fully connected layers;deep neural networks;DNNs;visualization system;convolutional layers;visual encodings;text analytics;TNNVis;clustering techniques;summarization techniques;speech models;Visualization;Neurons;Computational modeling;Task analysis;Convolutional neural networks;Biological neural networks;information visualization;deep learning;machine learning;visualization design;human centered computing},
 month = {April},
 number = {},
 pages = {180-189},
 title = {Visualizing Deep Neural Networks for Text Analytics},
 volume = {},
 year = {2018}
}

@inproceedings{8371950,
 abstract = {Most of the technical documents are composed by several modalities, like diagrams, tables, formulas, graphics, pictures and natural language text. Each of these modalities and their associations significantly contribute to the overall deep understanding of the technical document and the knowledge represented in it. Here for us all these modalities, except NL text, are considered as "images". Thus, each technical document mainly is composed by NL text sentences and "images". Thus, in this paper we present a methodology where all these modalities can be expressed into the same two modalities (natural languages text sentences and SPN graphs) for better associations and deeper understanding of a technical document. This deeper understanding will come from two different contributions. The first unique contribution will be an enrichment of the NL text part with additional NL text sentences extracted from the "images" of the technical document. The second unique contribution will come from the SPM models of these images that enrich the main diagram by generating a simulator for the system that technical document describes.},
 author = {N. {Bourbakis}},
 booktitle = {2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)},
 doi = {10.1109/ICTAI.2017.00047},
 issn = {2375-0197},
 keywords = {document image processing;graph theory;natural language processing;Petri nets;stochastic processes;text analysis;technical document;natural languages text sentences;stochastic Petri-net forms;SPN graphs;knowledge representation;Pictures;Graphics;Tables;Formulas;Diagrams;NL text sentences;Databases;Text recognition;Shape;Image recognition;Natural languages;Visualization;SPN;Technical Documents;NL text Sentences},
 month = {Nov},
 number = {},
 pages = {247-254},
 title = {Converting Diagrams, Formulas, Tables, Graphics and Pictures into SPN and NL-text Sentences for Automatic Deep Understanding of Technical Documents},
 volume = {},
 year = {2017}
}

@article{8399509,
 abstract = {In hyperspectral image processing, classification is one of the most popular research topics. In recent years, research progress made in deep-learning-based hierarchical feature extraction and classification has shown a great power in many applications. In this paper, we propose a novel local spatial sequential (LSS) method, which is used in a recurrent neural network (RNN). Using this model, we can extract local and semantic information for hyperspectral image classification. First, we extract low-level features from hyperspectral images, including texture and differential morphological profiles. Second, we combine the low-level features together and propose a method to construct the LSS features. Afterwards, we build an RNN and use the LSS features as the input to train the network for optimizing the system parameters. Finally, the high-level semantic features generated by the RNN is fed into a softmax layer for the final classification. In addition, a nonlocal spatial sequential method is presented for the recurrent neural network model (NLSS-RNN) to further enhance the classification performance. NLSS-RNN finds nonlocal similar structures to a given pixel and extracts corresponding LSS features, which not only preserve the local spatial information, but also integrate the information of nonlocal similar samples. The experimental results on three publicly accessible datasets show that our proposed method can obtain competitive performance compared with several state-of-the-art classifiers.},
 author = {X. {Zhang} and Y. {Sun} and K. {Jiang} and C. {Li} and L. {Jiao} and H. {Zhou}},
 doi = {10.1109/JSTARS.2018.2844873},
 issn = {1939-1404},
 journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
 keywords = {feature extraction;image classification;learning (artificial intelligence);recurrent neural nets;local spatial information;spatial sequential recurrent neural network;hyperspectral image classification;hyperspectral image processing;popular research topics;deep-learning-based hierarchical feature extraction;local spatial sequential method;local information;semantic information;low-level features;hyperspectral images;high-level semantic features;nonlocal spatial sequential method;recurrent neural network model;NLSS-RNN;LSS feature extraction;Feature extraction;Recurrent neural networks;Machine learning;Hyperspectral imaging;Computer architecture;Feedforward neural networks;Deep learning;high-level semantic feature;hyperspectral image (HSI) classification;low-level feature;recurrent neural network (RNN)},
 month = {Nov},
 number = {11},
 pages = {4141-4155},
 title = {Spatial Sequential Recurrent Neural Network for Hyperspectral Image Classification},
 volume = {11},
 year = {2018}
}

@article{8432512,
 abstract = {Hyperspectral unmixing (HU) is a method used to estimate the fractional abundances corresponding to endmembers in each of the mixed pixels in the hyperspectral remote sensing image. In recent times, deep learning has been recognized as an effective technique for hyperspectral image classification. In this letter, an end-to-end HU method is proposed based on the convolutional neural network (CNN). The proposed method uses a CNN architecture that consists of two stages: the first stage extracts features and the second stage performs the mapping from the extracted features to obtain the abundance percentages. Furthermore, a pixel-based CNN and cube-based CNN, which can improve the accuracy of HU, are presented in this letter. More importantly, we also use dropout to avoid overfitting. The evaluation of the complete performance is carried out on two hyperspectral data sets: Jasper Ridge and Urban. Compared with that of the existing method, our results show significantly higher accuracy.},
 author = {X. {Zhang} and Y. {Sun} and J. {Zhang} and P. {Wu} and L. {Jiao}},
 doi = {10.1109/LGRS.2018.2857804},
 issn = {1545-598X},
 journal = {IEEE Geoscience and Remote Sensing Letters},
 keywords = {feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);recurrent neural nets;hyperspectral unmixing;deep convolutional neural networks;hyperspectral remote sensing image;deep learning;hyperspectral image classification;end-to-end HU method;convolutional neural network;CNN architecture;pixel-based CNN;hyperspectral data sets;Jasper Ridge dataset;Urban dataset;Feature extraction;Hyperspectral imaging;Convolution;Artificial neural networks;Indexes;Kernel;Convolutional neural networks (CNNs);end-to-end model;spectral unmixing;spectral–spatial information},
 month = {Nov},
 number = {11},
 pages = {1755-1759},
 title = {Hyperspectral Unmixing via Deep Convolutional Neural Networks},
 volume = {15},
 year = {2018}
}

@article{8440085,
 abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.},
 author = {Y. {Ming} and H. {Qu} and E. {Bertini}},
 doi = {10.1109/TVCG.2018.2864812},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;interactive systems;knowledge representation;learning (artificial intelligence);matrix algebra;pattern classification;rule matrix;black-box model;matrix-based visualization;standardized rule-based knowledge representation;predictive models;interactive visualization technique;machine learning systems;Machine learning;Data visualization;Visualization;Neural networks;Decision trees;Data models;Support vector machines;explainable machine learning;rule visualization;visual analytics},
 month = {Jan},
 number = {1},
 pages = {342-352},
 title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules},
 volume = {25},
 year = {2019}
}

@article{8440091,
 abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
 author = {J. {Zhang} and Y. {Wang} and P. {Molino} and L. {Li} and D. S. {Ebert}},
 doi = {10.1109/TVCG.2018.2864499},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data analysis;data visualisation;iterative methods;learning (artificial intelligence);neural nets;regression analysis;model-agnostic framework;machine learning models;visual analysis techniques;internal logic;specific model type;model development;diagnosis process;potential machine learning;regression tasks;classification tasks;feature discrimination;customizable tabular view;models outcome;scatterplot-based visual summary;refinement;explanation;inspection;generic framework;deep neural networks;debugging;interpretation;manifold;Visualization;Analytical models;Task analysis;Machine learning;Manifolds;Data models;Computational modeling;Interactive machine learning;performance analysis;model comparison;model debugging},
 month = {Jan},
 number = {1},
 pages = {364-373},
 title = {Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models},
 volume = {25},
 year = {2019}
}

@article{8440842,
 abstract = {We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.},
 author = {B. C. {Kwon} and M. {Choi} and J. T. {Kim} and E. {Choi} and Y. B. {Kim} and S. {Kwon} and J. {Sun} and J. {Choo}},
 doi = {10.1109/TVCG.2018.2865027},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {artificial intelligence;data analysis;data visualisation;interactive systems;medical information systems;recurrent neural nets;interactive RNN-based model;EMR data;prediction tasks;RetainVis;individual medical codes;risk predictions;temporal information;increase interactivity;interpretable analytics tool;interpretable networks;interactive recurrent neural networks;electronic medical records;black-box nature;interactively leverage users;design study;visual analytics solution;medical experts;artificial intelligence scientists;iterative design process;newly improved RNN-based model;RNN-based model;visual analytic researchers;interactive visual analytic tool;Machine learning;Medical diagnostic imaging;Task analysis;Predictive models;Computational modeling;Visual analytics;Data models;Interactive Artificial Intelligence;XAI (Explainable Artificial Intelligence);Interpretable Deep Learning;Healthcare},
 month = {Jan},
 number = {1},
 pages = {299-309},
 title = {RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records},
 volume = {25},
 year = {2019}
}

@article{8466590,
 abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
 author = {A. {Adadi} and M. {Berrada}},
 doi = {10.1109/ACCESS.2018.2870052},
 issn = {2169-3536},
 journal = {IEEE Access},
 keywords = {artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
 month = {},
 number = {},
 pages = {52138-52160},
 title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
 volume = {6},
 year = {2018}
}

@inproceedings{8468758,
 abstract = {Machine learning technology has been greatly developed in the last decade, which makes artificial intelligence reach a revolutionary breakthrough and lets us really perceive the potential of artificial intelligence in changing human life. In order to improve the understanding and application ability of artificial intelligence, carrying out the corresponding machine learning course is of significance for the students during the undergraduate period. This paper probes into the teaching content, teaching form and other aspects of the undergraduate machine learning course based on this issue and proposes a teaching method driven by application scenarios to guide the undergraduate students to understand the development, current situation and frontier technology of machine learning. In the experimental design, the students' theoretical knowledge is fully considered, the practical questions are simplified, and the students' ability to think and solve problems is also raised, so as to lay a theoretical and practical basis for further study of machine learning.},
 author = {W. {Sun} and X. {Gao}},
 booktitle = {2018 13th International Conference on Computer Science Education (ICCSE)},
 doi = {10.1109/ICCSE.2018.8468758},
 issn = {2473-9464},
 keywords = {artificial intelligence;computer aided instruction;computer science education;educational courses;further education;learning (artificial intelligence);teaching;artificial intelligence era;machine learning technology;undergraduate period;undergraduate students;undergraduate machine learning course;machine learning course;teaching content;teaching form;Machine learning;Machine learning algorithms;Classification algorithms;Prediction algorithms;Education;Decision trees;artificial intelligence;machine learning;undergraduate},
 month = {Aug},
 number = {},
 pages = {1-5},
 title = {The Construction of Undergraduate Machine Learning Course in the Artificial Intelligence Era},
 volume = {},
 year = {2018}
}

@inproceedings{8489172,
 abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for “algorithmic fairness” also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
 author = {X. {Liu} and X. {Wang} and S. {Matwin}},
 booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
 doi = {10.1109/IJCNN.2018.8489172},
 issn = {2161-4407},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);pattern classification;pattern clustering;random processes;meta-learning;model interpretability;CNN-INTE;clustering algorithm;meta-level training data;base learning algorithms;meta-level test data;machine learning approaches;interpretable deep convolutional neural networks;MNIST image dataset;random forest;deep CNN model;Prediction algorithms;Machine learning;Machine learning algorithms;Predictive models;Computational modeling;Training data;Visualization;interpretability;Meta-learning;deep learning;Convolutional Neural Network;TensorFlow;big data},
 month = {July},
 number = {},
 pages = {1-9},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 volume = {},
 year = {2018}
}

@inproceedings{8490433,
 abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
 author = {J. {Zhu} and A. {Liapis} and S. {Risi} and R. {Bidarra} and G. M. {Youngblood}},
 booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
 doi = {10.1109/CIG.2018.8490433},
 issn = {2325-4289},
 keywords = {computer games;human computer interaction;learning (artificial intelligence);game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework;Games;Task analysis;Machine learning;Neurons;Visualization;Tools;explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design},
 month = {Aug},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},
 volume = {},
 year = {2018}
}

@inproceedings{8490530,
 abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
 author = {A. {Holzinger}},
 booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (DISA)},
 doi = {10.1109/DISA.2018.8490530},
 issn = {},
 keywords = {learning (artificial intelligence);ontologies (artificial intelligence);probability;statistical machine learning methods;AI winter;deep learning approaches;big quality data;computational power;engineering effort;ontologies;knowledge representations;glass-box approaches;mathematical principles;artificial intelligence;logical approaches;probabilistic learning methods;rule-based approaches;contextual explanatory models;context-adaptive procedures;high-dimensional vectors;Machine learning;Data mining;Data visualization;Uncertainty;Games;Cognitive science},
 month = {Aug},
 number = {},
 pages = {55-66},
 title = {From Machine Learning to Explainable AI},
 volume = {},
 year = {2018}
}

@inproceedings{8491501,
 abstract = {To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future-i.e., new-data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.},
 author = {B. {Murray} and M. A. {Islam} and A. J. {Pinar} and T. C. {Havens} and D. T. {Anderson} and G. {Scott}},
 booktitle = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 doi = {10.1109/FUZZ-IEEE.2018.8491501},
 issn = {},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);optimisation;remote sensing;sensor fusion;data-driven optimization;Choquet integral;fusion solution;parametric nonlinear aggregation function;data-centric XAI tools;XAI-ChI methods;explainable AI;heterogeneous deep convolutional neural networks;remote sensing;Frequency modulation;Indexes;Remote sensing;Optimization;Artificial intelligence;Electronic mail;Convolutional neural networks;Choquet Integral;Fuzzy Integral;Explainable AI;Machine Learning},
 month = {July},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Understanding Decisions and Data-Driven Optimization of the Choquet Integral},
 volume = {},
 year = {2018}
}

@article{8494828,
 abstract = {Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and “what if”-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.},
 author = {H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
 doi = {10.1109/TVCG.2018.2865044},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
 month = {Jan},
 number = {1},
 pages = {353-363},
 title = {Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
 volume = {25},
 year = {2019}
}

@inproceedings{8519384,
 abstract = {Identifying the aspect for a given target is an important issue in synthetic aperture radar (SAR) image interpretation. A new SAR target aspect identification method based on machine learning theory is proposed in this paper. First, the aspect angles of the SAR target are discretized, and the spatial relationships of the neighborhoods of the SAR target samples are established. Then an optimal linear mapping is solved based on the proposed subspace aspect discriminant analysis. The samples will be projected into a low-dimensional space and be of a better aspect identifiability than in their original space. Finally, the projected samples are fed into a multilayer neural network, and the aspects of the SAR targets will be indicated. Experimental results have shown the superiority of the proposed method based on the moving and stationary target acquisition and recognition (MSTAR) data set.},
 author = {J. {Pei} and Y. {Huang} and W. {Huo} and Y. {Zhang} and J. {Yang}},
 booktitle = {IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium},
 doi = {10.1109/IGARSS.2018.8519384},
 issn = {2153-7003},
 keywords = {image sampling;learning (artificial intelligence);multilayer perceptrons;radar computing;radar imaging;radar target recognition;synthetic aperture radar;optimal linear mapping;low-dimensional space;moving-and-stationary target acquisition-and-recognition data set;MSTAR data set;multilayer neural network;aspect angles;machine learning theory;SAR target aspect identification method;synthetic aperture radar image interpretation;SAR image;stationary target acquisition;projected samples;aspect identifiability;subspace aspect discriminant analysis;SAR target samples;Synthetic aperture radar;Multi-layer neural network;Estimation;Training;Neurons;Machine learning;Synthetic aperture radar;target aspect identification;machine learning;multi-layer neural network},
 month = {July},
 number = {},
 pages = {2310-2313},
 title = {Target Aspect Identification in SAR Image: A Machine Learning Approach},
 volume = {},
 year = {2018}
}

@inproceedings{8538416,
 abstract = {In the age of knowledge, Natural Language Processing (NLP) express its demand by a huge range of utilization. Previously NLP was dealing with statically data. Contemporary time NLP is doing considerably with the corpus, lexicon database, pattern reorganization. Considering Deep Learning (DL) method recognize artificial Neural Network (NN) to nonlinear process, NLP tools become increasingly accurate and efficient that begin a debacle. Multi-Layer Neural Network obtaining the importance of the NLP for its capability including standard speed and resolute output. Hierarchical designs of data operate recurring processing layers to learn and with this arrangement of DL methods manage several practices. In this paper, this resumed striving to reach a review of the tools and the necessary methodology to present a clear understanding of the association of NLP and DL for truly understand in the training. Efficiency and execution both are improved in NLP by Part of speech tagging (POST), Morphological Analysis, Named Entity Recognition (NER), Semantic Role Labeling (SRL), Syntactic Parsing, and Coreference resolution. Artificial Neural Networks (ANN), Time Delay Neural Networks (TDNN), Recurrent Neural Network (RNN), Convolution Neural Networks (CNN), and Long-Short-Term-Memory (LSTM) dealings among Dense Vector (DV), Windows Approach (WA), and Multitask learning (MTL) as a characteristic of Deep Learning. After statically methods, when DL communicate the influence of NLP, the individual form of the NLP process and DL rule collaboration was started a fundamental connection.},
 author = {S. A. {Fahad} and A. E. {Yahya}},
 booktitle = {2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
 doi = {10.1109/ICSCEE.2018.8538416},
 issn = {},
 keywords = {learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;natural language processing;contemporary time NLP;Considering Deep;(DL) method;artificial Neural Network;nonlinear process;NLP tools;MultiLayer Neural Network;processing layers;DL methods;Artificial Neural Networks;Time Delay Neural Networks;Recurrent Neural Network;Convolution Neural Networks;deep learning;NLP process;Natural language processing;Artificial neural networks;Tagging;Semantics;Task analysis;-Deep Learning;Natural language processing;Deep nural Network;Multitask Learning},
 month = {July},
 number = {},
 pages = {1-4},
 title = {Inflectional Review of Deep Learning on Natural Language Processing},
 volume = {},
 year = {2018}
}

@article{8540793,
 abstract = {This technology manager's note piece identifies the major components in the artificial intelligence (AI) business ecosystem and discusses several implications for managers. Specifically, it emphasizes on the designing of AI user scenarios, data acquisition for AI, and building the AI ecosystem.},
 author = {X. I. {Quan} and J. {Sanderson}},
 doi = {10.1109/EMR.2018.2882430},
 issn = {0360-8581},
 journal = {IEEE Engineering Management Review},
 keywords = {artificial intelligence;business data processing;competitive intelligence;data acquisition;technology management;artificial intelligence business ecosystem;technology manager;AI user scenarios;AI ecosystem;data acquisition;Business;Ecosystems;Machine learning;Medical services;Buildings;Artificial intelligence;business ecosystem;technology management},
 month = {Fourthquarter},
 number = {4},
 pages = {22-25},
 title = {Understanding the Artificial Intelligence Business Ecosystem},
 volume = {46},
 year = {2018}
}

@inproceedings{8551093,
 abstract = {Natural Language (NL) is an essential part of ourlife. Humans use language for communication. NL is a prevailing tool used by the humans to convey the information. Natural Language Understanding (NLU) is a major challenge in Natural Language Processing (NLP). NLP is a part of Artificial Intelligence (AI). NLP provides a significant tool for communication. It attempts to produces noise free data and conversion of noise to text. NLU is having different levels. This paper presents the issue with respect to one of the level such as syntax analysis. To provide a solution for syntax analysis, dynamic fuzzy parser is designed and implemented to parse the English input sentences. Traditional approach of parsing is enhanced by applying fuzzy logic. This helps to know the syntactic correctness of the sentence. Penns tree bank parts of speech tags are used for the Parts of Speech Tagger (POS). POS tagger assigns the parts of speech tags for the input English sentence. Then these tags of the words are parsed using the grammar rules. Finally the result is displayed to represent the number of words parsed in a sentence with its associated fuzzy membership value. This parser produces Precision value of 1(100%), Recall value of 0.92 (92%) and F-measure value of 0.9583 for the sample of 50 correct and 50 incorrect sentences.},
 author = {S. G. {Kanakaraddi} and S. S. {Nandval}},
 booktitle = {2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)},
 doi = {10.1109/ICCTCT.2018.8551093},
 issn = {},
 keywords = {artificial intelligence;computational linguistics;context-free grammars;fuzzy logic;natural language processing;text analysis;syntax analysis;dynamic fuzzy parser;natural language;recall value;precision value;F-measure value;grammar rules;parts of speech tagger;Penns tree bank parts of speech tags;parse English input sentences;artificial intelligence;natural language processing;natural language understanding;fuzzy membership value;POS tagger;NLP;NLU;fuzzy max-min technique;fuzzy logic;Grammar;Syntactics;Natural language processing;Conferences;Market research;Artificial intelligence;POS;FCFG;NL;NLU;NLP;POS;AI;CFG},
 month = {March},
 number = {},
 pages = {1-5},
 title = {Dynamic Fuzzy Parser to Parse English Sentence Using POS Tagger and Fuzzy Max-Min Technique},
 volume = {},
 year = {2018}
}

@inproceedings{8588744,
 abstract = {We address the interpretability of convolutional neural networks (CNNs) for predicting a geo-location from an image. In a pilot experiment we classify images of Pittsburgh vs Tokyo and visualize the learned CNN filters. We found that varying the CNN architecture leads to variating in the visualized filters. This calls for further investigation of the effective parameters on the interpretability of CNNs.},
 author = {S. {Khademi} and X. {Shi} and T. {Mager} and R. {Siebes} and C. {Hein} and V. {de Boer} and J. {van Gemert}},
 booktitle = {2018 IEEE 14th International Conference on e-Science (e-Science)},
 doi = {10.1109/eScience.2018.00125},
 issn = {},
 keywords = {feedforward neural nets;learning (artificial intelligence);neural net architecture;sight-seeing;eyes;deep neural networks;interpretability;convolutional neural networks;CNNs;geo-location;Tokyo;learned CNN filters;CNN architecture;visualized filters;Visualization;Computer architecture;Neural networks;Image recognition;Conferences;Computer science;Intelligent systems;convolutional neural network (CNN);interpretability;place recognition;visualization;classification},
 month = {Oct},
 number = {},
 pages = {407-408},
 title = {Sight-Seeing in the Eyes of Deep Neural Networks},
 volume = {},
 year = {2018}
}

@inproceedings{8591457,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
 author = {D. L. {Marino} and C. S. {Wickramasinghe} and M. {Manic}},
 booktitle = {IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society},
 doi = {10.1109/IECON.2018.8591457},
 issn = {2577-1647},
 keywords = {learning (artificial intelligence);multilayer perceptrons;neural nets;pattern classification;security of data;adversarial approach;explainable AI;cyber-security applications;adversarial machine learning;multilayer perceptron classifiers;machine learning techniques;deep neural networks;data-driven intrusion detection systems;IDSs;Machine learning;Intrusion detection;Mathematical model;Visualization;Estimation;Adversarial Machine Learning;Adversarial samples;Explainable AI;cyber-security},
 month = {Oct},
 number = {},
 pages = {3237-3243},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection Systems},
 volume = {},
 year = {2018}
}

@inproceedings{8613997,
 abstract = {Personalized Healthcare (PH) is a new patientoriented healthcare approach which expects to improve the traditional healthcare system. The focus of this new advancement is the patient data collected from patient Electronic health records (EHR), Internet of Things (IoT) sensor devices, wearables and mobile devices, web-based information and social media. PH applies Artificial Intelligence (AI) techniques to the collected dataset to improve disease progression technique, disease prediction, patient selfmanagement and clinical intervention. Machine learning techniques are widely used in this regard to develop analytic models. These models are integrated into different healthcare service applications and clinical decision support systems. These models mainly analyse the collected data from sensor devices and other sources to identify behavioral patterns and clinical conditions of the patient. For example, these models analyse the collected data to identify the patient's improvements, habits and anomaly in daily routine, changes in sleeping and mobility, eating, drinking and digestive pattern. Based on those patterns the healthcare applications and the clinical decision support systems recommend lifestyle advice, special treatment and care plans for the patient. The doctors and caregivers can also be engaged in the care plan process to validate lifestyle advice. However, there are many uncertainties and a grey area when it comes to applying machine learning in this context. Clinical, behaviour and lifestyle data in nature are very sensitive. There could be different types of biased involved in the process of data collection and interpretation. The training data model could have an older version of the dataset. All these could lead to an incorrect decision from the system without the user's knowledge. In this paper, some of the standards of the ML models reported in the recent research trends, identify the reliability issues and propose improvements.},
 author = {F. {Ahamed} and F. {Farid}},
 booktitle = {2018 International Conference on Machine Learning and Data Engineering (iCMLDE)},
 doi = {10.1109/iCMLDE.2018.00014},
 issn = {},
 keywords = {data acquisition;decision support systems;diseases;electronic health records;health care;Internet of Things;learning (artificial intelligence);patient treatment;healthcare service applications;patient self-management;artificial intelligence techniques;Web-based information;Internet of Things sensor devices;electronic health records;healthcare system;electronic health records;ML models;training data model;data collection;lifestyle data;healthcare applications;clinical conditions;behavioral patterns;clinical decision support systems;machine learning techniques;clinical intervention;disease prediction;disease progression technique;social media;mobile devices;patient data;personalized healthcare;Machine learning;Internet of Things;Hospitals;Monitoring;Sleep apnea;Personalized Healthcare;Internet of Things;Machine Learning},
 month = {Dec},
 number = {},
 pages = {19-21},
 title = {Applying Internet of Things and Machine-Learning for Personalized Healthcare: Issues and Challenges},
 volume = {},
 year = {2018}
}

@inproceedings{8614007,
 abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
 author = {H. {Yanagimto} and K. {Hashimoto} and M. {Okada}},
 booktitle = {2018 International Conference on Machine Learning and Data Engineering (iCMLDE)},
 doi = {10.1109/iCMLDE.2018.00024},
 issn = {},
 keywords = {convolutional neural nets;learning (artificial intelligence);neural net architecture;sentiment analysis;neural network architecture;self-attention mechanism;Amazon reviews;GCNN;sentient analysis;attention visualization;natural language processing;gated convolutional neural networks;deep learning;Logic gates;Convolutional neural networks;Kernel;Sentiment analysis;Task analysis;Deep learning;Natural language processing;Gated CNN;Sentiment analysis;the self-attention mechanism},
 month = {Dec},
 number = {},
 pages = {77-82},
 title = {Attention Visualization of Gated Convolutional Neural Networks with Self Attention in Sentiment Analysis},
 volume = {},
 year = {2018}
}

@inproceedings{8614130,
 abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
 author = {S. {Ghanta} and S. {Subramanian} and S. {Sundararaman} and L. {Khermosh} and V. {Sridhar} and D. {Arteaga} and Q. {Luo} and D. {Das} and N. {Talagala}},
 booktitle = {2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 doi = {10.1109/ICMLA.2018.00105},
 issn = {},
 keywords = {learning (artificial intelligence);production engineering computing;production scenario;interpretability techniques;production ML reproducibility solution;production viability;reproducability;production machine learning applications;legal industry requirements;human understandable machine learning;Production;Pipelines;Predictive models;Machine learning;Training;Data models;Load modeling;reproducability;explainability;interpretability;systems;tracking},
 month = {Dec},
 number = {},
 pages = {658-664},
 title = {Interpretability and Reproducability in Production Machine Learning Applications},
 volume = {},
 year = {2018}
}

@inproceedings{8621470,
 abstract = {Development of chromosome conformation capture methods boosted progress in the study of the spatial organization of chromatin. Accumulation of large amounts of experimental data provides an opportunity to apply machine learning methods to examine the connection between epigenetics and the three-dimensional structure of chromatin. The aim of this study was to predict the characteristics of the chromatin structure, namely the transitional gamma, from ChIP-Seq experimental data by means of machine learning methods, and also to reveal the properties of epigenetic data influencing prediction. The neural network and the loss function designed for the prediction task are shown to perform with a sufficiently high accuracy. In addition, the genomic size of the chromatin context required for improving the quality of the prediction was assessed. Several neural network visualization techniques were tested as a means for improving interpretability of network, showing the possibility for using visualization to study interrelations in epigenetic data relevant for three-dimensional chromatin structure. To sum up, a close relationship between epigenetic factors and the structure of chromatin has been confirmed.},
 author = {S. {Starikov} and E. {Khrameeva} and M. {Gelfand}},
 booktitle = {2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
 doi = {10.1109/BIBM.2018.8621470},
 issn = {},
 keywords = {biology computing;data visualisation;genetics;genomics;learning (artificial intelligence);neural nets;chromosome conformation capture methods;ChIP-Seq experimental data;machine learning methods;epigenetic data influencing prediction;neural network visualization techniques;three-dimensional chromatin structure;chromatin spatial structure characteristics;Machine learning;Bioinformatics;Neural networks;Data visualization;Conferences;Biomedical engineering;Life sciences;Hi-C;machine learning;neural networks;ChIP-Seq},
 month = {Dec},
 number = {},
 pages = {2489-2489},
 title = {Prediction of chromatin spatial structure characteristics using machine learning methods},
 volume = {},
 year = {2018}
}

@inproceedings{8622073,
 abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
 author = {R. {Chhatwal} and P. {Gronvall} and N. {Huber-Fliflet} and R. {Keeling} and J. {Zhang} and H. {Zhao}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData.2018.8622073},
 issn = {},
 keywords = {law administration;pattern classification;supervised learning;text analysis;responsive documents;explainable AI system;typical legal document review scenarios;responsive snippets;text classification;explainable predictive coding methods;data-focused engagements;technology-based techniques;machine learning researchers;document classification;supervised learning task;electronically-stored documents;Predictive coding;Law;Predictive models;Text categorization;Machine learning;machine learning;text categorization;explainable AI;predictive coding;explainable predictive coding;legal document review},
 month = {Dec},
 number = {},
 pages = {1905-1911},
 title = {Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding},
 volume = {},
 year = {2018}
}

@inproceedings{8622433,
 abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
 author = {B. {Kovalerchuk} and N. {Neuhaus}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData.2018.8622433},
 issn = {},
 keywords = {learning (artificial intelligence);pattern classification;domain-explainable ML models;dominance classifier and predictor algorithm;DCP algorithm;unintelligible black box models;interpretable machine learning;interpretable mathematical forms;noninterpretable models;interpretable methods;human-understandable machine learning models;complex multidimensional relations;human perceptual limitations;white box models;interpretable grey box models;Classification algorithms;Prediction algorithms;Machine learning;Mathematical model;Machine learning algorithms;Computational modeling;Neural networks;machine learning;explainability;interpretability;accuracy;classifier;visualization;visual model;dominant intervals},
 month = {Dec},
 number = {},
 pages = {4940-4947},
 title = {Toward Efficient Automation of Interpretable Machine Learning},
 volume = {},
 year = {2018}
}

@inproceedings{8622439,
 abstract = {Explaining recommendations helps users to make more accurate and effective decisions and improves system credibility and transparency. Current explainable recommender systems tend to provide fixed statements such as "customers who purchased this item also purchased....". This explanation is generated only on the basis of the purchase history of similar customers, so it does not include the preferences of customers who have purchased the item or a description of the item. Since user-generated reviews generally contain information about the reviewer's preferences and a description of the item, such reviews typically have more effect on purchase decisions. Therefore, using reviews to explain recommendations should be more useful than providing only a fixed statement explanation. Aiming to create a system that provides personalized explanations for recommendations, we have developed a recurrent neural network model that uses multicriteria evaluation data to generate reviews.},
 author = {T. {Suzuki} and S. {Oyama} and M. {Kurihara}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData.2018.8622439},
 issn = {},
 keywords = {information filtering;information filters;recommender systems;recurrent neural nets;explainable recommendations;explainable recommender systems;review text;personalized explanations;fixed statement explanation;purchase decisions;reviewer;user-generated reviews;purchase history;fixed statements;transparency;system credibility;multicriteria evaluation data;Decoding;Data models;Recommender systems;Mathematical model;Computational modeling;History;Recurrent neural networks;explainable recommendation;text generation;RNN;recommender systems},
 month = {Dec},
 number = {},
 pages = {3549-3551},
 title = {Toward Explainable Recommendations: Generating Review Text from Multicriteria Evaluation Data},
 volume = {},
 year = {2018}
}

@article{8653995,
 abstract = {With the rapid development of deep learning models, their performances in various tasks are improved, while meanwhile their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In the paper, we propose a novel Multi-scale INTerpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.},
 author = {X. {Cui} and D. {Wang} and Z. J. {Wang}},
 doi = {10.1109/TMM.2019.2902099},
 issn = {1520-9210},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Visualization;Computational modeling;Analytical models;Feature extraction;Perturbation methods;Image segmentation;Heating systems;Model interpretability;multi-scale interpretation;convolutional neural networks;model-agnostic},
 month = {},
 number = {},
 pages = {1-1},
 title = {Multi-scale Interpretation Model for Convolutional Neural Networks: Building Trust based on Hierarchical Interpretation},
 volume = {},
 year = {2019}
}

@inproceedings{8679150,
 abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
 author = {D. {Kim} and W. {Lim} and M. {Hong} and H. {Kim}},
 booktitle = {2019 IEEE International Conference on Big Data and Smart Computing (BigComp)},
 doi = {10.1109/BIGCOMP.2019.8679150},
 issn = {2375-9356},
 keywords = {feature extraction;image classification;learning (artificial intelligence);natural language processing;neural nets;deep neural network;interpretable transfer learning;high-quality data;complex model;pretrained model;interpretability;transfer learning task;interpretable features;feature extraction layer;Korean characters classification;Feature extraction;Task analysis;Training;Data models;Convolution;Computational modeling;Neural networks;Interpretability;Transfer Learning;Machine Learning},
 month = {Feb},
 number = {},
 pages = {1-4},
 title = {The Structure of Deep Neural Network for Interpretable Transfer Learning},
 volume = {},
 year = {2019}
}

@inproceedings{8679370,
 abstract = {Facial expression is the most powerful and natural non-verbal emotional communication method. Facial Expression Recognition(FER) has significance in machine learning tasks. Deep Learning models perform well in FER tasks, but it doesn't provide any justification for its decisions. Based on the hypothesis that facial expression is a combination of facial muscle movements, we find that Facial Action Coding Units(AUs) and Emotion label have a relationship in CK+ Dataset. In this paper, we propose a model which utilises AUs to explain Convolutional Neural Network(CNN) model's classification results. The CNN model is trained with CK+ Dataset and classifies emotion based on extracted features. Explanation model classifies the multiple AUs with the extracted features and emotion classes from the CNN model. Our experiment shows that with only features and emotion classes obtained from the CNN model, Explanation model generates AUs very well.},
 author = {S. {Kim} and H. {Kim}},
 booktitle = {2019 IEEE International Conference on Big Data and Smart Computing (BigComp)},
 doi = {10.1109/BIGCOMP.2019.8679370},
 issn = {2375-9356},
 keywords = {convolutional neural nets;emotion recognition;face recognition;feature extraction;learning (artificial intelligence);nonverbal emotional communication method;machine learning tasks;Deep Learning models;FER tasks;facial muscle movements;CK+ Dataset;CNN model;emotion classes;facial expression recognition;convolutional neural network model;facial action coding units;deep explanation model;facial action coding unit;Hidden Markov models;Gold;Feature extraction;Face recognition;Deep learning;Computational modeling;Task analysis;Explanation Model;Facial Expression Recognition;Deep learning;Justification;Facial Action Coding System},
 month = {Feb},
 number = {},
 pages = {1-4},
 title = {Deep Explanation Model for Facial Expression Recognition Through Facial Action Coding Unit},
 volume = {},
 year = {2019}
}

@inproceedings{953778,
 abstract = {Document image understanding denotes the recognition of semantically relevant components in the layout extracted from a document image. This recognition process is based on some visual models, whose manual specification can be a highly demanding task. In order to automatically acquire these models, we propose the application of machine learning techniques. Problems raised by possible dependencies between concepts to be learned are illustrated and solved with a computational strategy based on the separate-and-parallel-conquer search. The approach is tested on a set of real multi-page documents processed by the system WISDOM++. New results confirm the validity of the proposed strategy and show some limits of the learning system used in this work.},
 author = {D. {Malerba} and F. {Esposito} and F. A. {Lisi} and O. {Altamura}},
 booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
 doi = {10.1109/ICDAR.2001.953778},
 issn = {},
 keywords = {document image processing;optical character recognition;learning (artificial intelligence);divide and conquer methods;search problems;document image understanding;logical component dependence discovery;document image recognition;visual models;machine learning;computational strategy;separate-and-parallel-conquer search;multi-page documents;WISDOM system;OCR;Image recognition;Text analysis;Image analysis;Optical character recognition software;XML;System testing;Publishing;Image databases;Digital images;Optical devices},
 month = {Sep.},
 number = {},
 pages = {174-178},
 title = {Automated discovery of dependencies between logical components in document image understanding},
 volume = {},
 year = {2001}
}

@inproceedings{953860,
 abstract = {We propose a geometric method for document image processing. This research focuses on document understanding and classification by applying the Winnow algorithm, an online machine learning method. This application makes the document image processing more flexible with various kind of documents since the meaningful knowledge can be extracted from training examples and the model for document type can be updated when there is a new example. This research aims to analyze and classify scientific papers. We conduct the experiments on documents from the proceedings of various conferences to show the performance of the proposed method. The experimental results are compared with the WISDOM++ system and also show the advantages of using the online machine learning method.},
 author = {C. {Nattee} and M. {Numao}},
 booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
 doi = {10.1109/ICDAR.2001.953860},
 issn = {},
 keywords = {document image processing;image retrieval;pattern classification;real-time systems;learning systems;document image processing;document understanding;pattern classification;geometric method;Winnow algorithm;machine learning;scientific papers;image retrieval;real time systems;Machine learning;Machine learning algorithms;Learning systems;Document image processing;Text analysis;Information analysis;Logic;Computer science;Electronic mail;Application software},
 month = {Sep.},
 number = {},
 pages = {602-606},
 title = {Geometric method for document understanding and classification using online machine learning},
 volume = {},
 year = {2001}
}

@incollection{aakurInherentExplainabilityPattern2018,
 abstract = {The ability of artificial intelligence systems to offer explanations for its decisions is central to building user confidence and structuring smart human-machine interactions. Expressing the rationale behind such a system's output is an important aspect of human-machine interaction as AI continues to be prominent in general, everyday use-cases. In this paper, we introduce a novel framework integrating Grenander's pattern theory structures to produce inherently explainable, symbolic representations for activity interpretations. These representations provide semantically rich and coherent interpretations of video activity using connected structures of detected (grounded) concepts, such as objects and actions, that are bound by semantics through background concepts not directly observed, i.e. contextualization cues. We use contextualization cues to establish semantic relationships among concepts to infer a deeper interpretation of events than what can be directly sensed. We propose the use of six questions that can be used to gain insight into the models ability to justify its decision and enhance its ability to interact with humans. The six questions are designed to (1) build an understanding of how the model is able to infer interpretations, (2) enable us to walk through its decision-making process, and (3) understand its drawbacks and possibly address them. We demonstrate the viability of this idea on video data using a dialog model that uses interpretations to generate explanations grounded in both video data and semantics.},
 address = {Cham},
 author = {Aakur, Sathyanarayanan N. and {de Souza}, Fillipe D. M. and Sarkar, Sudeep},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 doi = {10.1007/978-3-319-98131-4_11},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Activity interpretation,ConceptNet,Explainability,Semantics},
 language = {en},
 pages = {277-299},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {On the {{Inherent Explainability}} of {{Pattern Theory}}-{{Based Video Event Interpretations}}},
 year = {2018}
}

@inproceedings{Abdollahi:2017:UEC:3109859.3109913,
 acmid = {3109913},
 address = {New York, NY, USA},
 author = {Abdollahi, Behnoush and Nasraoui, Olfa},
 booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
 doi = {10.1145/3109859.3109913},
 isbn = {978-1-4503-4652-8},
 keywords = {explanations, interpretable models, latent factor models, matrix factorization, recommender systems},
 location = {Como, Italy},
 numpages = {5},
 pages = {79--83},
 publisher = {ACM},
 series = {RecSys '17},
 title = {Using Explainability for Constrained Matrix Factorization},
 url = {http://doi.acm.org/10.1145/3109859.3109913},
 year = {2017}
}

@inproceedings{Abreu:2009:RSF:1529282.1529374,
 acmid = {1529374},
 address = {New York, NY, USA},
 author = {Abreu, Rui and Mayer, Wolfgang and Stumptner, Markus and van Gemund, Arjan J. C.},
 booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
 doi = {10.1145/1529282.1529374},
 isbn = {978-1-60558-166-8},
 keywords = {abstract interpretation, fault localization, program spectra},
 location = {Honolulu, Hawaii},
 numpages = {6},
 pages = {409--414},
 publisher = {ACM},
 series = {SAC '09},
 title = {Refining Spectrum-based Fault Localization Rankings},
 url = {http://doi.acm.org/10.1145/1529282.1529374},
 year = {2009}
}

@article{Abstracts2016Society2016,
 doi = {10.1007/s11606-016-3657-7},
 file = {/home/tim/Zotero/storage/R9S8V92J/2016 - Abstracts from the 2016 Society of General Interna.pdf;/home/tim/Zotero/storage/SSWJIBRH/2016 - Abstracts from the 2016 Society of General Interna.pdf},
 issn = {1525-1497},
 journal = {Journal of General Internal Medicine},
 language = {en},
 month = {May},
 number = {2},
 pages = {85-922},
 title = {Abstracts from the 2016 {{Society}} of {{General Internal Medicine Annual Meeting}}},
 volume = {31},
 year = {2016}
}

@article{Abstracts2017Society2017,
 doi = {10.1007/s11606-017-4028-8},
 file = {/home/tim/Zotero/storage/DF6QGEYW/2017 - Abstracts from the 2017 Society of General Interna.pdf;/home/tim/Zotero/storage/PXA8F9U6/2017 - Abstracts from the 2017 Society of General Interna.pdf},
 issn = {1525-1497},
 journal = {Journal of General Internal Medicine},
 language = {en},
 month = {April},
 number = {2},
 pages = {83-808},
 title = {Abstracts from the 2017 {{Society}} of {{General Internal Medicine Annual Meeting}}},
 volume = {32},
 year = {2017}
}

@article{Abstracts36thAnnual2013,
 doi = {10.1007/s11606-013-2436-y},
 file = {/home/tim/Zotero/storage/NP4WH8EI/2013 - Abstracts from the 36th Annual Meeting of the Soci.pdf},
 issn = {1525-1497},
 journal = {Journal of General Internal Medicine},
 language = {en},
 month = {June},
 number = {1},
 pages = {1-489},
 title = {Abstracts from the 36th {{Annual Meeting}} of the {{Society}} of {{General Internal Medicine}}},
 volume = {28},
 year = {2013}
}

@incollection{AbstractsDocumentsThis1997,
 address = {Boston, MA},
 booktitle = {Political {{Science Abstracts}}: 1996 {{Annual Supplement}};{{Volume}} 1},
 doi = {10.1007/978-1-4615-5971-9_1},
 isbn = {978-1-4615-5971-9},
 language = {en},
 pages = {1-817},
 publisher = {{Springer US}},
 title = {Abstracts of {{Documents}} in {{This Supplement}}},
 year = {1997}
}

@article{AbstractsScientificPapers1999,
 doi = {10.1007/BF03323585},
 issn = {1432-1084},
 journal = {European Radiology},
 keywords = {Magnetic Resonance Angiography,Magnetic Resonance Imaging,Spiral Compute Tomography,Takayasu Arteritis,Transjugular Intrahepatic Portosystemic Shunt},
 language = {en},
 month = {March},
 number = {1},
 pages = {S1-S362},
 title = {Abstracts {{Scientific Papers Honorary Lectures Categorical Courses Workshops State}}-of-the-{{Art Symposia}}},
 volume = {9},
 year = {1999}
}

@incollection{aggarwalMachineLearningShallow2018,
 abstract = {Conventional machine learning often uses optimization and gradient-descent methods for learning parameterized models. Examples of such models include linear regression, support vector machines, logistic regression, dimensionality reduction, and matrix factorization. Neural networks are also parameterized models that are learned with continuous optimization methods.},
 address = {Cham},
 author = {Aggarwal, Charu C.},
 booktitle = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
 doi = {10.1007/978-3-319-94463-0_2},
 editor = {Aggarwal, Charu C.},
 isbn = {978-3-319-94463-0},
 language = {en},
 pages = {53-104},
 publisher = {{Springer International Publishing}},
 title = {Machine {{Learning}} with {{Shallow Neural Networks}}},
 year = {2018}
}

@incollection{aggarwalModelBasedCollaborativeFiltering2016,
 abstract = {The neighborhood-based methods of the previous chapter can be viewed as generalizations of k-nearest neighbor classifiers, which are commonly used in machine learning.},
 address = {Cham},
 author = {Aggarwal, Charu C.},
 booktitle = {Recommender {{Systems}}: {{The Textbook}}},
 doi = {10.1007/978-3-319-29659-3_3},
 editor = {Aggarwal, Charu C.},
 isbn = {978-3-319-29659-3},
 language = {en},
 pages = {71-138},
 publisher = {{Springer International Publishing}},
 title = {Model-{{Based Collaborative Filtering}}},
 year = {2016}
}

@inproceedings{alonsoZadehComputingWords2019,
 abstract = {The European Commission has identified Artificial Intelligence (AI) as the ``most strategic technology of the 21st century'' [7].},
 author = {Alonso, Jose M.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
 isbn = {978-3-030-12544-8},
 keywords = {Cointension,Computing with perceptions,Computing with words,Explainable AI,Fuzzy Logic,Interpretable fuzzy systems},
 language = {en},
 pages = {244-248},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {From {{Zadeh}}'s {{Computing}} with {{Words Towards eXplainable Artificial Intelligence}}},
 year = {2019}
}

@inproceedings{Amir:2018:ASS:3237383.3237877,
 acmid = {3237877},
 address = {Richland, SC},
 author = {Amir, Ofra and Doshi-Velez, Finale and Sarne, David},
 booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
 keywords = {explainable ai, strategy summarization},
 location = {Stockholm, Sweden},
 numpages = {5},
 pages = {1203--1207},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 series = {AAMAS '18},
 title = {Agent Strategy Summarization},
 url = {http://dl.acm.org/citation.cfm?id=3237383.3237877},
 year = {2018}
}

@inproceedings{Amir:2018:HSA:3237383.3237869,
 acmid = {3237869},
 address = {Richland, SC},
 author = {Amir, Dan and Amir, Ofra},
 booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
 keywords = {explainable ai, strategy summarization},
 location = {Stockholm, Sweden},
 numpages = {9},
 pages = {1168--1176},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 series = {AAMAS '18},
 title = {HIGHLIGHTS: Summarizing Agent Behavior to People},
 url = {http://dl.acm.org/citation.cfm?id=3237383.3237869},
 year = {2018}
}

@article{AnnualCongressEuropean2018,
 doi = {10.1007/s00259-018-4148-3},
 issn = {1619-7089},
 journal = {European Journal of Nuclear Medicine and Molecular Imaging},
 language = {en},
 month = {October},
 number = {1},
 pages = {1-844},
 title = {Annual {{Congress}} of the {{European Association}} of {{Nuclear Medicine October}} 13 \textendash{} 17, 2018 {{D\"usseldorf}}, {{Germany}}},
 volume = {45},
 year = {2018}
}

@incollection{azizMachineLearningAI2019,
 abstract = {We explore how machine learning and artificial intelligence (AI) solutions are transforming risk management. A non-technical overview is first given of the main machine learning and AI techniques of benefit to risk management. Then a review is provided, using current practice and empirical evidence, of the application of these techniques to the risk management fields of credit risk, market risk, operational risk, and compliance (`RegTech'). We conclude with some thoughts on current limitations and views on how the field is likely to develop in the short- to medium-term. Overall, we present an optimistic picture of the role of machine learning and AI in risk management, but note some practical limitations around suitable data management policies, transparency, and lack of necessary skillsets within firms.},
 address = {Cham},
 author = {Aziz, Saqib and Dowling, Michael},
 booktitle = {Disrupting {{Finance}}: {{FinTech}} and {{Strategy}} in the 21st {{Century}}},
 doi = {10.1007/978-3-030-02330-0_3},
 editor = {Lynn, Theo and Mooney, John G. and Rosati, Pierangelo and Cummins, Mark},
 file = {/home/tim/Zotero/storage/EY8T5IEC/Aziz and Dowling - 2019 - Machine Learning and AI for Risk Management.pdf},
 isbn = {978-3-030-02330-0},
 keywords = {AI,Credit risk,Machine learning,Market risk,Operational risk,RegTech,Risk management},
 language = {en},
 pages = {33-50},
 publisher = {{Springer International Publishing}},
 series = {Palgrave {{Studies}} in {{Digital Business}} \& {{Enabling Technologies}}},
 title = {Machine {{Learning}} and {{AI}} for {{Risk Management}}},
 year = {2019}
}

@inproceedings{Balachandran:2009:IRC:1645953.1646227,
 acmid = {1646227},
 address = {New York, NY, USA},
 author = {Balachandran, Vipin and P, Deepak and Khemani, Deepak},
 booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
 doi = {10.1145/1645953.1646227},
 isbn = {978-1-60558-512-3},
 keywords = {interpretable clustering},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {1773--1776},
 publisher = {ACM},
 series = {CIKM '09},
 title = {Interpretable and Reconfigurable Clustering of Document Datasets by Deriving Word-based Rules},
 url = {http://doi.acm.org/10.1145/1645953.1646227},
 year = {2009}
}

@article{balachandranInterpretableReconfigurableClustering2012,
 abstract = {Clusters of text documents output by clustering algorithms are often hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clusters and outline the problem of generating clusterings with interpretable and reconfigurable cluster models. We develop two clustering algorithms toward the outlined goal of building interpretable and reconfigurable cluster models. They generate clusters with associated rules that are composed of conditions on word occurrences or nonoccurrences. The proposed approaches vary in the complexity of the format of the rules; RGC employs disjunctions and conjunctions in rule generation whereas RGC-D rules are simple disjunctions of conditions signifying presence of various words. In both the cases, each cluster is comprised of precisely the set of documents that satisfy the corresponding rule. Rules of the latter kind are easy to interpret, whereas the former leads to more accurate clustering. We show that our approaches outperform the unsupervised decision tree approach for rule-generating clustering and also an approach we provide for generating interpretable models for general clusterings, both by significant margins. We empirically show that the purity and f-measure losses to achieve interpretability can be as little as 3 and 5\%, respectively using the algorithms presented herein.},
 author = {Balachandran, Vipin and {Deepak P} and Khemani, Deepak},
 doi = {10.1007/s10115-011-0446-9},
 file = {/home/tim/Zotero/storage/N4JXINRM/Balachandran et al. - 2012 - Interpretable and reconfigurable clustering of doc.pdf},
 issn = {0219-3116},
 journal = {Knowledge and Information Systems},
 keywords = {Data clustering,Interpretability,Text clustering},
 language = {en},
 month = {September},
 number = {3},
 pages = {475-503},
 title = {Interpretable and Reconfigurable Clustering of Document Datasets by Deriving Word-Based Rules},
 volume = {32},
 year = {2012}
}

@inproceedings{Baral:2018:RRA:3209219.3209237,
 acmid = {3209237},
 address = {New York, NY, USA},
 author = {Baral, Ramesh and Zhu, XiaoLong and Iyengar, S. S. and Li, Tao},
 booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
 doi = {10.1145/3209219.3209237},
 isbn = {978-1-4503-5589-6},
 keywords = {explainable recommendation, information retrieval, social networks},
 location = {Singapore, Singapore},
 numpages = {10},
 pages = {23--32},
 publisher = {ACM},
 series = {UMAP '18},
 title = {ReEL: Review Aware Explanation of Location Recommendation},
 url = {http://doi.acm.org/10.1145/3209219.3209237},
 year = {2018}
}

@inproceedings{Bashar:2014:IDP:2682647.2682753,
 acmid = {2682753},
 address = {Washington, DC, USA},
 author = {Bashar, Md Abul and Li, Yuefeng and Shen, Yan and Albathan, Mubarak},
 booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
 doi = {10.1109/WI-IAT.2014.67},
 isbn = {978-1-4799-4143-8},
 keywords = {Pattern Interpretation, Information Mismatch and Overload, Ontology-based Mining, Text Mining, Semantic Web},
 numpages = {6},
 pages = {432--437},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '14},
 title = {Interpreting Discovered Patterns in Terms of Ontology Concepts},
 url = {http://dx.doi.org/10.1109/WI-IAT.2014.67},
 year = {2014}
}

@inproceedings{Bellini:2018:KAE:3270323.3270327,
 acmid = {3270327},
 address = {New York, NY, USA},
 author = {Bellini, Vito and Schiavone, Angelo and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
 booktitle = {Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems},
 doi = {10.1145/3270323.3270327},
 isbn = {978-1-4503-6617-5},
 keywords = {Autoencoder Neural Networks, Deep Learning, Explainable Models, Explanation, Recommender Systems},
 location = {Vancouver, BC, Canada},
 numpages = {8},
 pages = {24--31},
 publisher = {ACM},
 series = {DLRS 2018},
 title = {Knowledge-aware Autoencoders for Explainable Recommender Systems},
 url = {http://doi.acm.org/10.1145/3270323.3270327},
 year = {2018}
}

@article{bench-caponHistoryAILaw2012,
 abstract = {We provide a retrospective of 25 years of the International Conference on AI and Law, which was first held in 1987. Fifty papers have been selected from the thirteen conferences and each of them is described in a short subsection individually written by one of the 24 authors. These subsections attempt to place the paper discussed in the context of the development of AI and Law, while often offering some personal reactions and reflections. As a whole, the subsections build into a history of the last quarter century of the field, and provide some insights into where it has come from, where it is now, and where it might go.},
 author = {{Bench-Capon}, Trevor and Araszkiewicz, Micha\l{} and Ashley, Kevin and Atkinson, Katie and Bex, Floris and Borges, Filipe and Bourcier, Daniele and Bourgine, Paul and Conrad, Jack G. and Francesconi, Enrico and Gordon, Thomas F. and Governatori, Guido and Leidner, Jochen L. and Lewis, David D. and Loui, Ronald P. and McCarty, L. Thorne and Prakken, Henry and Schilder, Frank and Schweighofer, Erich and Thompson, Paul and Tyrrell, Alex and Verheij, Bart and Walton, Douglas N. and Wyner, Adam Z.},
 doi = {10.1007/s10506-012-9131-x},
 file = {/home/tim/Zotero/storage/9RUMRANY/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf;/home/tim/Zotero/storage/XXTKI56R/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf},
 issn = {1572-8382},
 journal = {Artificial Intelligence and Law},
 keywords = {Artificial intelligence and law,Legal informatics,Models of legal reasoning},
 language = {en},
 month = {September},
 number = {3},
 pages = {215-319},
 shorttitle = {A History of {{AI}} and {{Law}} in 50 Papers},
 title = {A History of {{AI}} and {{Law}} in 50 Papers: 25~Years of the International Conference on {{AI}} and {{Law}}},
 volume = {20},
 year = {2012}
}

@inproceedings{berzinsInnovationsNaturalLanguage2008,
 abstract = {This paper evaluates the potential contributions of natural language processing to requirements engineering. We present a selective history of the relationship between requirements engineering (RE) and natural-language processing (NLP), and briefly summarize relevant recent trends in NLP. The paper outlines basic issues in RE and how they relate to interactions between a NLP front end and system-development processes. We suggest some improvements to NLP that may be possible in the context of RE and conclude with an assessment of what should be done to improve likelihood of practical impact in this direction.},
 author = {Berzins, Valdis and Martell, Craig and {Luqi} and Adams, Paige},
 booktitle = {Innovations for {{Requirement Analysis}}. {{From Stakeholders}}' {{Needs}} to {{Formal Designs}}},
 editor = {Paech, Barbara and Martell, Craig},
 isbn = {978-3-540-89778-1},
 keywords = {Ambiguity,Domain-Specific Methods,Gaps,Natural Language,Requirements},
 language = {en},
 pages = {125-146},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Innovations in {{Natural Language Document Processing}} for {{Requirements Engineering}}},
 year = {2008}
}

@article{bharadhwajExplanationsTemporalRecommendations2018,
 abstract = {Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 doi = {10.1007/s13218-018-0560-x},
 file = {/home/tim/Zotero/storage/4J8DH8MG/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf},
 issn = {1610-1987},
 journal = {KI - K\"unstliche Intelligenz},
 keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
 language = {en},
 month = {November},
 number = {4},
 pages = {267-272},
 title = {Explanations for {{Temporal Recommendations}}},
 volume = {32},
 year = {2018}
}

@article{Biecek:2018:DEC:3291125.3309646,
 acmid = {3309646},
 author = {Biecek, Przemys\law},
 issn = {1532-4435},
 issue_date = {January 2018},
 journal = {J. Mach. Learn. Res.},
 keywords = {explainable artificial intelligence, interpretable machine learning, model visualization, predictive modelling},
 month = {January},
 number = {1},
 numpages = {5},
 pages = {3245--3249},
 publisher = {JMLR.org},
 title = {DALEX: Explainers for Complex Predictive Models in R},
 url = {http://dl.acm.org/citation.cfm?id=3291125.3309646},
 volume = {19},
 year = {2018}
}

@inproceedings{biermannNaturalLanguageProgramming1983,
 abstract = {A procedural semantics system is described for English imperative sentences in natural language programming. Issues related to the handling of dialog focus, noun group resolution, quantifier processing, and imperative verb execution are discussed. Sequences of imperative sentences may be assembled to build natural language programs and techniques are given for processing such programs. The final sections include a discussion of related research and a brief overview of the field.},
 author = {Biermann, Alan W.},
 booktitle = {Computer {{Program Synthesis Methodologies}}},
 editor = {Biermann, Alan W. and Guiho, G\'erard},
 isbn = {978-94-009-7019-9},
 keywords = {Focus Mechanism,Head Noun,Natural Language,Naval Postgraduate School,Procedural Representation},
 language = {en},
 pages = {335-368},
 publisher = {{Springer Netherlands}},
 series = {{{NATO Advanced Study Institutes Series}}},
 title = {Natural {{Language Programming}}},
 year = {1983}
}

@inproceedings{Blank:2018:DLC:3159450.3162370,
 acmid = {3162370},
 address = {New York, NY, USA},
 author = {Blank, Douglas and Meeden, Lisa and Marshall, Jim},
 booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
 doi = {10.1145/3159450.3162370},
 isbn = {978-1-4503-5103-4},
 keywords = {artificial intelligence, artificial neural networks, deep learning, python},
 location = {Baltimore, Maryland, USA},
 numpages = {1},
 pages = {1055--1055},
 publisher = {ACM},
 series = {SIGCSE '18},
 title = {Deep Learning in the Classroom: (Abstract Only)},
 url = {http://doi.acm.org/10.1145/3159450.3162370},
 year = {2018}
}

@inproceedings{Bock:2018:VNN:3281505.3281605,
 acmid = {3281605},
 address = {New York, NY, USA},
 articleno = {132},
 author = {Bock, Marcel and Schreiber, Andreas},
 booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
 doi = {10.1145/3281505.3281605},
 isbn = {978-1-4503-6086-9},
 keywords = {deep learning, explainable ai, neural networks, visualization},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {132:1--132:2},
 publisher = {ACM},
 series = {VRST '18},
 title = {Visualization of Neural Networks in Virtual Reality Using Unreal Engine},
 url = {http://doi.acm.org/10.1145/3281505.3281605},
 year = {2018}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
 abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
 author = {Bratko, I.},
 booktitle = {Learning, {{Networks}} and {{Statistics}}},
 editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
 isbn = {978-3-7091-2668-4},
 keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
 language = {en},
 pages = {163-177},
 publisher = {{Springer Vienna}},
 series = {International {{Centre}} for {{Mechanical Sciences}}},
 shorttitle = {Machine {{Learning}}},
 title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
 year = {1997}
}

@inproceedings{brideDependableExplainableMachine2018,
 abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
 author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
 booktitle = {Formal {{Methods}} and {{Software Engineering}}},
 editor = {Sun, Jing and Sun, Meng},
 isbn = {978-3-030-02450-5},
 language = {en},
 pages = {412-416},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
 year = {2018}
}

@inproceedings{Brown:2018:RNN:3217871.3217872,
 acmid = {3217872},
 address = {New York, NY, USA},
 articleno = {1},
 author = {Brown, Andy and Tuor, Aaron and Hutchinson, Brian and Nichols, Nicole},
 booktitle = {Proceedings of the First Workshop on Machine Learning for Computing Systems},
 doi = {10.1145/3217871.3217872},
 isbn = {978-1-4503-5865-1},
 keywords = {Anomaly detection, Attention, Interpretable Machine Learning, Online Training, Recurrent Neural Networks, System Log Analysis},
 location = {Tempe, AZ, USA},
 numpages = {8},
 pages = {1:1--1:8},
 publisher = {ACM},
 series = {MLCS'18},
 title = {Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection},
 url = {http://doi.acm.org/10.1145/3217871.3217872},
 year = {2018}
}

@incollection{browneCriticalChallengesVisual2018,
 abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
 address = {Cham},
 author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 doi = {10.1007/978-3-319-90403-0_7},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {119-136},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
 year = {2018}
}

@article{Bultan:1999:MCS:325478.325480,
 acmid = {325480},
 address = {New York, NY, USA},
 author = {Bultan, Tevfik and Gerber, Richard and Pugh, William},
 doi = {10.1145/325478.325480},
 issn = {0164-0925},
 issue_date = {July 1999},
 journal = {ACM Trans. Program. Lang. Syst.},
 keywords = {Presburger arithmetic, abstract interpretation, symbolic model checking},
 month = {July},
 number = {4},
 numpages = {43},
 pages = {747--789},
 publisher = {ACM},
 title = {Model-checking Concurrent Systems with Unbounded Integer Variables: Symbolic Representations, Approximations, and Experimental Results},
 url = {http://doi.acm.org/10.1145/325478.325480},
 volume = {21},
 year = {1999}
}

@article{butz12thBiannualConference2014,
 author = {Butz, Martin V.},
 doi = {10.1007/s10339-014-0632-2},
 issn = {1612-4790},
 journal = {Cognitive Processing},
 keywords = {Head Noun,Lateralized Readiness Potential,Mental Rotation,Relative Clause,SNARC Effect},
 language = {en},
 month = {September},
 number = {1},
 pages = {1-158},
 title = {12th {{Biannual}} Conference of the {{German}} Cognitive Science Society ({{Gesellschaft}} F\"ur {{Kognitionswissenschaft}})},
 volume = {15},
 year = {2014}
}

@inproceedings{Cai:2019:EEE:3301275.3302289,
 acmid = {3302289},
 address = {New York, NY, USA},
 author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302289},
 isbn = {978-1-4503-6272-6},
 keywords = {example-based explanations, explainable AI, human-AI interaction, machine learning},
 location = {Marina del Ray, California},
 numpages = {5},
 pages = {258--262},
 publisher = {ACM},
 series = {IUI '19},
 title = {The Effects of Example-based Explanations in a Machine Learning Interface},
 url = {http://doi.acm.org/10.1145/3301275.3302289},
 year = {2019}
}

@inproceedings{Card:2019:DWA:3287560.3287595,
 acmid = {3287595},
 address = {New York, NY, USA},
 author = {Card, Dallas and Zhang, Michael and Smith, Noah A.},
 booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
 doi = {10.1145/3287560.3287595},
 isbn = {978-1-4503-6125-5},
 keywords = {conformal methods, interpretability credibility},
 location = {Atlanta, GA, USA},
 numpages = {10},
 pages = {369--378},
 publisher = {ACM},
 series = {FAT* '19},
 title = {Deep Weighted Averaging Classifiers},
 url = {http://doi.acm.org/10.1145/3287560.3287595},
 year = {2019}
}

@inproceedings{carringtonMeasuresModelInterpretability2018,
 abstract = {The literature lacks definitions for quantitative measures of model interpretability for automatic model selection to achieve high accuracy and interpretability, hence we define inherent model interpretability. We extend the work of Lipton et al. and Liu et al. from qualitative and subjective concepts of model interpretability to objective criteria and quantitative measures. We also develop another new measure called simplicity of sensitivity and illustrate prior, initial and posterior measurement. Measures are tested and validated with some measures recommended for use. It is demonstrated that high accuracy and high interpretability are jointly achievable with little to no sacrifice in either.},
 author = {Carrington, Andr\'e and Fieguth, Paul and Chen, Helen},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Kernels,Model interpretability,Model transparency,Support vector machines},
 language = {en},
 pages = {329-349},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measures of {{Model Interpretability}} for {{Model Selection}}},
 year = {2018}
}

@article{CARS2016Computer2016,
 doi = {10.1007/s11548-016-1412-5},
 issn = {1861-6429},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 language = {en},
 month = {June},
 number = {1},
 pages = {1-286},
 title = {{{CARS}} 2016\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 30th {{International Congress}} and {{Exhibition Heidelberg}}, {{Germany}}, {{June}} 21\textendash{}25, 2016},
 volume = {11},
 year = {2016}
}

@article{CARS2017Computer2017,
 doi = {10.1007/s11548-017-1588-3},
 file = {/home/tim/Zotero/storage/9MHAY3SY/2017 - CARS 2017—Computer Assisted Radiology and Surgery .pdf},
 issn = {1861-6429},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 language = {en},
 month = {June},
 number = {1},
 pages = {1-286},
 title = {{{CARS}} 2017\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 31st {{International Congress}} and {{Exhibition Barcelona}}, {{Spain}}, {{June}} 20\textendash{}24, 2017},
 volume = {12},
 year = {2017}
}

@article{CARS2018Computer2018,
 doi = {10.1007/s11548-018-1766-y},
 file = {/home/tim/Zotero/storage/UDZ8MQ7K/2018 - CARS 2018—Computer Assisted Radiology and Surgery .pdf},
 issn = {1861-6429},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 language = {en},
 month = {June},
 number = {1},
 pages = {1-273},
 title = {{{CARS}} 2018\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 32nd {{International Congress}} and {{Exhibition Berlin}}, {{Germany}}, {{June}} 20\textendash{}23, 2018},
 volume = {13},
 year = {2018}
}

@inproceedings{Chen:2018:NAR:3178876.3186070,
 acmid = {3186070},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
 booktitle = {Proceedings of the 2018 World Wide Web Conference},
 doi = {10.1145/3178876.3186070},
 isbn = {978-1-4503-5639-8},
 keywords = {explainable recommendation, neural attention network, recommender systems, review usefulness},
 location = {Lyon, France},
 numpages = {10},
 pages = {1583--1592},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '18},
 title = {Neural Attentional Rating Regression with Review-level Explanations},
 url = {https://doi.org/10.1145/3178876.3186070},
 year = {2018}
}

@inproceedings{Chen:2018:PET:3180308.3180362,
 acmid = {3180362},
 address = {New York, NY, USA},
 articleno = {53},
 author = {Chen, Mei-Ling and Wang, Hao-Chuan},
 booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/3180308.3180362},
 isbn = {978-1-4503-5571-1},
 keywords = {Conversational agents, explainable intelligent user interfaces, mental models},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {53:1--53:2},
 publisher = {ACM},
 series = {IUI '18 Companion},
 title = {How Personal Experience and Technical Knowledge Affect Using Conversational Agents},
 url = {http://doi.acm.org/10.1145/3180308.3180362},
 year = {2018}
}

@article{Cheng:2019:MER:3306215.3291060,
 acmid = {3291060},
 address = {New York, NY, USA},
 articleno = {16},
 author = {Cheng, Zhiyong and Chang, Xiaojun and Zhu, Lei and Kanjirathinkal, Rose C. and Kankanhalli, Mohan},
 doi = {10.1145/3291060},
 issn = {1046-8188},
 issue_date = {March 2019},
 journal = {ACM Trans. Inf. Syst.},
 keywords = {Aspect, explainable recommendation, latent factor model, multi-modal, rating prediction},
 month = {January},
 number = {2},
 numpages = {28},
 pages = {16:1--16:28},
 publisher = {ACM},
 title = {MMALFM: Explainable Recommendation by Leveraging Reviews and Images},
 url = {http://doi.acm.org/10.1145/3291060},
 volume = {37},
 year = {2019}
}

@inproceedings{Chu:2018:ECI:3219819.3220063,
 acmid = {3220063},
 address = {New York, NY, USA},
 author = {Chu, Lingyang and Hu, Xia and Hu, Juhua and Wang, Lanjun and Pei, Jian},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 doi = {10.1145/3219819.3220063},
 isbn = {978-1-4503-5552-0},
 keywords = {closed form, deep neural network, exact and consistent interpretation},
 location = {London, United Kingdom},
 numpages = {10},
 pages = {1244--1253},
 publisher = {ACM},
 series = {KDD '18},
 title = {Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution},
 url = {http://doi.acm.org/10.1145/3219819.3220063},
 year = {2018}
}

@inproceedings{Costa:2018:AGN:3180308.3180366,
 acmid = {3180366},
 address = {New York, NY, USA},
 articleno = {57},
 author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
 booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
 doi = {10.1145/3180308.3180366},
 isbn = {978-1-4503-5571-1},
 keywords = {Explainability, Explanations, Natural Language Generation, Neural Network, Recommender systems},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {57:1--57:2},
 publisher = {ACM},
 series = {IUI '18 Companion},
 title = {Automatic Generation of Natural Language Explanations},
 url = {http://doi.acm.org/10.1145/3180308.3180366},
 year = {2018}
}

@inproceedings{Croitoru:2010:GEN:1838206.1838404,
 acmid = {1838404},
 address = {Richland, SC},
 author = {Croitoru, Madalina and Oren, Nir and Miles, Simon and Luck, Michael},
 booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
 isbn = {978-0-9826571-1-9},
 keywords = {conceptual graphs, norms},
 location = {Toronto, Canada},
 numpages = {2},
 pages = {1405--1406},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 series = {AAMAS '10},
 title = {Graphically Explaining Norms},
 url = {http://dl.acm.org/citation.cfm?id=1838206.1838404},
 year = {2010}
}

@incollection{curuksuPrinciplesDataScience2018,
 abstract = {This chapter covers advanced analytics principles and applications. Let us first back up on our objectives and progress so far. In Chap. 6, we defined the key concepts underlying the mathematical science of data analysis. The discussion was structured in two categories: descriptive and inferential statistics. In the context of a data science project, these two categories may be referred to as unsupervised and supervised modeling respectively. These two categories are ubiquitous because the objective of a data science project is always (bear with me please) to better understand some data or else to predict something. Chapter 7 thus again follows this binary structure, although some topics (e.g. computer simulation, Sect. 7.3) may be used to collect and understand data, forecast events, or both.},
 address = {Cham},
 author = {Curuksu, Jeremy David},
 booktitle = {Data {{Driven}}: {{An Introduction}} to {{Management Consulting}} in the 21st {{Century}}},
 doi = {10.1007/978-3-319-70229-2_7},
 editor = {Curuksu, Jeremy David},
 isbn = {978-3-319-70229-2},
 language = {en},
 pages = {87-127},
 publisher = {{Springer International Publishing}},
 series = {Management for {{Professionals}}},
 shorttitle = {Principles of {{Data Science}}},
 title = {Principles of {{Data Science}}: {{Advanced}}},
 year = {2018}
}

@inproceedings{Dalmia:2018:TIN:3184558.3191523,
 acmid = {3191523},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
 booktitle = {Companion Proceedings of the The Web Conference 2018},
 doi = {10.1145/3184558.3191523},
 isbn = {978-1-4503-5640-4},
 keywords = {graph representation, model interpretability, neural networks},
 location = {Lyon, France},
 numpages = {8},
 pages = {945--952},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '18},
 title = {Towards Interpretation of Node Embeddings},
 url = {https://doi.org/10.1145/3184558.3191523},
 year = {2018}
}

@inproceedings{deGraaf:2018:ERS:3173386.3173568,
 acmid = {3173568},
 address = {New York, NY, USA},
 author = {de Graaf, Maartje M.A. and Malle, Bertram F. and Dragan, Anca and Ziemke, Tom},
 booktitle = {Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
 doi = {10.1145/3173386.3173568},
 isbn = {978-1-4503-5615-2},
 keywords = {behavior explanation, explainable robotics, intentionality, theory of mind, transparency, trust calibration.},
 location = {Chicago, IL, USA},
 numpages = {2},
 pages = {387--388},
 publisher = {ACM},
 series = {HRI '18},
 title = {Explainable Robotic Systems},
 url = {http://doi.acm.org/10.1145/3173386.3173568},
 year = {2018}
}

@incollection{dengEpilogueFrontiersNLP2018,
 abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
 address = {Singapore},
 author = {Deng, Li and Liu, Yang},
 booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
 doi = {10.1007/978-981-10-5209-5_11},
 editor = {Deng, Li and Liu, Yang},
 isbn = {978-981-10-5209-5},
 language = {en},
 pages = {309-326},
 publisher = {{Springer Singapore}},
 shorttitle = {Epilogue},
 title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
 year = {2018}
}

@incollection{dengJointIntroductionNatural2018,
 abstract = {In this chapter, we set up the fundamental framework for the book. We first provide an introduction to the basics of natural language processing (NLP) as an integral part of artificial intelligence. We then survey the historical development of NLP, spanning over five decades, in terms of three waves. The first two waves arose as rationalism and empiricism, paving ways to the current deep learning wave. The key pillars underlying the deep learning revolution for NLP consist of (1) distributed representations of linguistic entities via embedding, (2) semantic generalization due to the embedding, (3) long-span deep sequence modeling of natural language, (4) hierarchical networks effective for representing linguistic levels from low to high, and (5) end-to-end deep learning methods to jointly solve many NLP tasks. After the survey, several key limitations of current deep learning technology for NLP are analyzed. This analysis leads to five research directions for future advances in NLP.},
 address = {Singapore},
 author = {Deng, Li and Liu, Yang},
 booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
 doi = {10.1007/978-981-10-5209-5_1},
 editor = {Deng, Li and Liu, Yang},
 isbn = {978-981-10-5209-5},
 language = {en},
 pages = {1-22},
 publisher = {{Springer Singapore}},
 title = {A {{Joint Introduction}} to {{Natural Language Processing}} and to {{Deep Learning}}},
 year = {2018}
}

@inproceedings{Dominguez:2019:EEA:3301275.3302274,
 acmid = {3302274},
 address = {New York, NY, USA},
 author = {Dominguez, Vicente and Messina, Pablo and Donoso-Guzm\'{a}n, Ivania and Parra, Denis},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302274},
 isbn = {978-1-4503-6272-6},
 keywords = {art, explainable AI, visual recommender systems},
 location = {Marina del Ray, California},
 numpages = {9},
 pages = {408--416},
 publisher = {ACM},
 series = {IUI '19},
 title = {The Effect of Explanations and Algorithmic Accuracy on Visual Recommender Systems of Artistic Images},
 url = {http://doi.acm.org/10.1145/3301275.3302274},
 year = {2019}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
 abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
 address = {Cham},
 author = {{Doshi-Velez}, Finale and Kim, Been},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 doi = {10.1007/978-3-319-98131-4_1},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Accountability,Interpretability,Machine learning,Transparency},
 language = {en},
 pages = {3-17},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
 year = {2018}
}

@article{EANM152015,
 doi = {10.1007/s00259-015-3198-z},
 issn = {1619-7089},
 journal = {European Journal of Nuclear Medicine and Molecular Imaging},
 language = {en},
 month = {October},
 number = {1},
 pages = {1-924},
 title = {{{EANM}}'15},
 volume = {42},
 year = {2015}
}

@article{ECR2005Scientific2005,
 doi = {10.1007/s10406-005-0100-2},
 issn = {1613-3757},
 journal = {European Radiology Supplements},
 keywords = {Public Health,Scientific Programme},
 language = {en},
 month = {March},
 number = {1},
 pages = {1-688},
 title = {{{ECR}} 2005 \textendash{} {{Scientific Programme}} \textendash{} {{Abstracts}}},
 volume = {15},
 year = {2005}
}

@inproceedings{Ehsan:2018:RNM:3278721.3278736,
 acmid = {3278736},
 address = {New York, NY, USA},
 author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 doi = {10.1145/3278721.3278736},
 isbn = {978-1-4503-6012-8},
 keywords = {ai rationalization, artificial intelligence, explainable ai, interpretability, machine learning, transparency, user perception},
 location = {New Orleans, LA, USA},
 numpages = {7},
 pages = {81--87},
 publisher = {ACM},
 series = {AIES '18},
 title = {Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations},
 url = {http://doi.acm.org/10.1145/3278721.3278736},
 year = {2018}
}

@inproceedings{Ehsan:2019:ARG:3301275.3302316,
 acmid = {3302316},
 address = {New York, NY, USA},
 author = {Ehsan, Upol and Tambwekar, Pradyumna and Chan, Larry and Harrison, Brent and Riedl, Mark O.},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302316},
 isbn = {978-1-4503-6272-6},
 keywords = {algorithmic decision-making, algorithmic explanation, artificial intelligence, explainable AI, interpretability, machine learning, rationale generation, transparency, user perception},
 location = {Marina del Ray, California},
 numpages = {12},
 pages = {263--274},
 publisher = {ACM},
 series = {IUI '19},
 title = {Automated Rationale Generation: A Technique for Explainable AI and Its Effects on Human Perceptions},
 url = {http://doi.acm.org/10.1145/3301275.3302316},
 year = {2019}
}

@incollection{elmiedanyArtificialIntelligence2019,
 abstract = {Artificial intelligence can be defined as computer systems which have been designed to interact with the world through abilities (e.g. visual perception and speech recognition) and intelligent behaviours (e.g. evaluating the available information and then taking the most sensible action to achieve a defined aim) that we would think of as principally humans. Initially, research has focused on letting software do things better, in which computers have always been doing better, such as the analysis of large datasets. However, the use of artificial intelligence in our day-to-day life has increased exponentially. Data forms the basis for the development of artificial intelligent software systems that will not only collect information but is able to learn, understand and interpret information, adapt its behaviour, plan, conclude, solve problems, think abstract, come up with ideas and understand and interpret language. Thanks to AI, a smart phone can detect cancer and a smart watch can detect a stroke. Machine learning is infiltrating and optimizing nearly every aspect of medicine from the way 911 emergency services are dispatched to assisting doctors during surgery. People can even quit smoking or kick opiate addiction with the help of AI. AI scientists are currently developing new approaches in machine learning, computer modelling and probability statistics to improve decision-making processes and are using decision theory and neuroscience to drive the progress of more effective healthcare and education as well as economics. This chapter will discuss the science of AI and explore the importance of big data and AI strategies. It will expand to discuss AI and medicine as well as medical education. It will conclude with discussion of AI and education as well as the future of artificial intelligence.},
 address = {Cham},
 author = {El Miedany, Yasser},
 booktitle = {Rheumatology {{Teaching}}: {{The Art}} and {{Science}} of {{Medical Education}}},
 doi = {10.1007/978-3-319-98213-7_18},
 editor = {El Miedany, Yasser},
 isbn = {978-3-319-98213-7},
 keywords = {Artificial intelligence,Big data,Medical education,Science of artificial intelligence,Virtual reality in education},
 language = {en},
 pages = {347-378},
 publisher = {{Springer International Publishing}},
 title = {Artificial {{Intelligence}}},
 year = {2019}
}

@inproceedings{fabra-boludaModellingMachineLearning2018,
 abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
 author = {{Fabra-Boluda}, Ra\"ul and Ferri, C\`esar and {Hern\'andez-Orallo}, Jos\'e and {Mart\'inez-Plumed}, Fernando and {Ram\'irez-Quintana}, M. Jos\'e},
 booktitle = {Philosophy and {{Theory}} of {{Artificial Intelligence}} 2017},
 editor = {M\"uller, Vincent C.},
 isbn = {978-3-319-96448-5},
 language = {en},
 pages = {175-186},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Applied Philosophy}}, {{Epistemology}} and {{Rational Ethics}}},
 title = {Modelling {{Machine Learning Models}}},
 year = {2018}
}

@inproceedings{Farsal:2018:DLO:3289402.3289538,
 acmid = {3289538},
 address = {New York, NY, USA},
 articleno = {38},
 author = {Farsal, Wissal and Anter, Samir and Ramdani, Mohammed},
 booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
 doi = {10.1145/3289402.3289538},
 isbn = {978-1-4503-6462-1},
 keywords = {Artificial Intelligence, Deep Learning, Neural Networks},
 location = {Rabat, Morocco},
 numpages = {6},
 pages = {38:1--38:6},
 publisher = {ACM},
 series = {SITA'18},
 title = {Deep Learning: An Overview},
 url = {http://doi.acm.org/10.1145/3289402.3289538},
 year = {2018}
}

@inproceedings{Feng:2018:IPE:3206025.3206048,
 acmid = {3206048},
 address = {New York, NY, USA},
 author = {Feng, Zunlei and Yu, Zhenyun and Yang, Yezhou and Jing, Yongcheng and Jiang, Junxiao and Song, Mingli},
 booktitle = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},
 doi = {10.1145/3206025.3206048},
 isbn = {978-1-4503-5046-4},
 keywords = {adversarial, embedding, interpretable, outfit composition},
 location = {Yokohama, Japan},
 numpages = {9},
 pages = {143--151},
 publisher = {ACM},
 series = {ICMR '18},
 title = {Interpretable Partitioned Embedding for Customized Multi-item Fashion Outfit Composition},
 url = {http://doi.acm.org/10.1145/3206025.3206048},
 year = {2018}
}

@inproceedings{Fernandez:2017:MCC:3132515.3132520,
 acmid = {3132520},
 address = {New York, NY, USA},
 author = {Fernandez, Delia and Woodward, Alejandro and Campos, Victor and Giro-i-Nieto, Xavier and Jou, Brendan and Chang, Shih-Fu},
 booktitle = {Proceedings of the Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes},
 doi = {10.1145/3132515.3132520},
 isbn = {978-1-4503-5509-4},
 keywords = {adjective noun pairs, affective computing, compound concepts, convolutional neural networks, interpretable models},
 location = {Mountain View, California, USA},
 numpages = {9},
 pages = {61--69},
 publisher = {ACM},
 series = {MUSA2 '17},
 title = {More Cat Than Cute?: Interpretable Prediction of Adjective-Noun Pairs},
 url = {http://doi.acm.org/10.1145/3132515.3132520},
 year = {2017}
}

@inproceedings{Gad-Elrab:2019:EFE:3289600.3290996,
 acmid = {3290996},
 address = {New York, NY, USA},
 author = {Gad-Elrab, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/3289600.3290996},
 isbn = {978-1-4503-5940-5},
 keywords = {explainable evidence, fact-checking, knowledge graph, reasoning},
 location = {Melbourne VIC, Australia},
 numpages = {9},
 pages = {87--95},
 publisher = {ACM},
 series = {WSDM '19},
 title = {ExFaKT: A Framework for Explaining Facts over Knowledge Graphs and Text},
 url = {http://doi.acm.org/10.1145/3289600.3290996},
 year = {2019}
}

@incollection{galitskyDevelopingConversationalNatural2019,
 abstract = {In this Chapter we focus on a problem of a natural language access to a database, well-known and highly desired to be solved. We start with the modern approaches based on deep learning and analyze lessons learned from unusable database access systems. This chapter can serve as a brief introduction to neural networks for learning logic representations. Then a number of hybrid approaches are presented and their strong points are analyzed. Finally, we describe our approach that relies on parsing, thesaurus and disambiguation via chatbot communication mode. The conclusion is that a reliable and flexible database access via NL needs to employ a broad spectrum of linguistic, knowledge representation and learning techniques. We conclude this chapter by surveying the general technology trends related to NL2SQL, observing how AI and ML are seeping into virtually everything and represent a major battleground for technology providers.},
 address = {Cham},
 author = {Galitsky, Boris},
 booktitle = {Developing {{Enterprise Chatbots}}: {{Learning Linguistic Structures}}},
 doi = {10.1007/978-3-030-04299-8_4},
 editor = {Galitsky, Boris},
 isbn = {978-3-030-04299-8},
 language = {en},
 pages = {85-120},
 publisher = {{Springer International Publishing}},
 title = {Developing {{Conversational Natural Language Interface}} to a {{Database}}},
 year = {2019}
}

@inproceedings{Gaura:2003:UAT:944868.944890,
 acmid = {944890},
 address = {New York, NY, USA},
 author = {Gaura, Elena I. and Newman, Robert M.},
 booktitle = {Proceedings of the 21st Annual International Conference on Documentation},
 doi = {10.1145/944868.944890},
 isbn = {1-58113-696-X},
 keywords = {artificial intelligence, artificial neural networks, hypermedia},
 location = {San Francisco, CA, USA},
 numpages = {5},
 pages = {100--104},
 publisher = {ACM},
 series = {SIGDOC '03},
 title = {Using AI Techniques to Aid Hypermedia Design},
 url = {http://doi.acm.org/10.1145/944868.944890},
 year = {2003}
}

@inproceedings{Ghazimatin:2019:FFU:3289600.3290990,
 acmid = {3290990},
 address = {New York, NY, USA},
 author = {Ghazimatin, Azin and Saha Roy, Rishiraj and Weikum, Gerhard},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/3289600.3290990},
 isbn = {978-1-4503-5940-5},
 keywords = {explanation paths, interpretability, social feeds, user actions},
 location = {Melbourne VIC, Australia},
 numpages = {9},
 pages = {240--248},
 publisher = {ACM},
 series = {WSDM '19},
 title = {FAIRY: A Framework for Understanding Relationships Between Users' Actions and Their Social Feeds},
 url = {http://doi.acm.org/10.1145/3289600.3290990},
 year = {2019}
}

@inproceedings{Gilpin:2018:RPC:3173386.3176994,
 acmid = {3176994},
 address = {New York, NY, USA},
 author = {Gilpin, Leilani H. and Zaman, Cagri and Olson, Danielle and Yuan, Ben Z.},
 booktitle = {Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
 doi = {10.1145/3173386.3176994},
 isbn = {978-1-4503-5615-2},
 keywords = {commonsense reasoning, explainable ai, explainable robotic systems, virtual reality},
 location = {Chicago, IL, USA},
 numpages = {2},
 pages = {115--116},
 publisher = {ACM},
 series = {HRI '18},
 title = {Reasonable Perception: Connecting Vision and Language Systems for Validating Scene Descriptions},
 url = {http://doi.acm.org/10.1145/3173386.3176994},
 year = {2018}
}

@inproceedings{goebelExplainableAINew2018,
 abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
 author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning},
 language = {en},
 pages = {295-303},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Explainable {{AI}}},
 title = {Explainable {{AI}}: {{The New}} 42?},
 year = {2018}
}

@article{Goel:1991:MMT:122344.122358,
 acmid = {122358},
 address = {New York, NY, USA},
 author = {Goel, Ashok K. and Eiselt, Kurt P.},
 doi = {10.1145/122344.122358},
 issn = {0163-5719},
 issue_date = {Aug. 1991},
 journal = {SIGART Bull.},
 month = {July},
 number = {4},
 numpages = {4},
 pages = {75--78},
 publisher = {ACM},
 title = {Mental Models, Text Interpretation, and Knowledge Acquisition},
 url = {http://doi.acm.org/10.1145/122344.122358},
 volume = {2},
 year = {1991}
}

@inproceedings{Green:2009:GTS:1639714.1639768,
 acmid = {1639768},
 address = {New York, NY, USA},
 author = {Green, Stephen J. and Lamere, Paul and Alexander, Jeffrey and Maillet, Fran\c{c}ois and Kirk, Susanna and Holt, Jessica and Bourque, Jackie and Mak, Xiao-Wen},
 booktitle = {Proceedings of the Third ACM Conference on Recommender Systems},
 doi = {10.1145/1639714.1639768},
 isbn = {978-1-60558-435-5},
 keywords = {explainable recommender, steerable recommender},
 location = {New York, New York, USA},
 numpages = {4},
 pages = {281--284},
 publisher = {ACM},
 series = {RecSys '09},
 title = {Generating Transparent, Steerable Recommendations from Textual Descriptions of Items},
 url = {http://doi.acm.org/10.1145/1639714.1639768},
 year = {2009}
}

@article{grillHealthExploringComplexity2016,
 author = {Grill, Eva and M\"uller, Martin and Mansmann, Ulrich},
 doi = {10.1007/s10654-016-0183-1},
 file = {/home/tim/Zotero/storage/PB6D7S2L/Grill et al. - 2016 - Health—exploring complexity an interdisciplinary .pdf},
 issn = {1573-7284},
 journal = {European Journal of Epidemiology},
 language = {en},
 month = {August},
 number = {1},
 pages = {1-239},
 shorttitle = {Health\textemdash{}Exploring Complexity},
 title = {Health\textemdash{}Exploring Complexity: An Interdisciplinary Systems Approach {{HEC2016}}},
 volume = {31},
 year = {2016}
}

@article{guhaInterpretationInterpretabilityQuantitative2008,
 abstract = {The goal of a quantitative structure\textendash{}activity relationship (QSAR) model is to encode the relationship between molecular structure and biological activity or physical property. Based on this encoding, such models can be used for predictive purposes. Assuming the use of relevant and meaningful descriptors, and a statistically significant model, extraction of the encoded structure\textendash{}activity relationships (SARs) can provide insight into what makes a molecule active or inactive. Such analyses by QSAR models are useful in a number of scenarios, such as suggesting structural modifications to enhance activity, explanation of outliers and exploratory analysis of novel SARs. In this paper we discuss the need for interpretation and an overview of the factors that affect interpretability of QSAR models. We then describe interpretation protocols for different types of models, highlighting the different types of interpretations, ranging from very broad, global, trends to very specific, case-by-case, descriptions of the SAR, using examples from the training set. Finally, we discuss a number of case studies where workers have provide some form of interpretation of a QSAR model.},
 author = {Guha, Rajarshi},
 doi = {10.1007/s10822-008-9240-5},
 issn = {1573-4951},
 journal = {Journal of Computer-Aided Molecular Design},
 keywords = {Interpretation,Linear regression,Neural network,Partial least squares (PLS),Quantitative structure–activity relationship (QSAR)},
 language = {en},
 month = {December},
 number = {12},
 pages = {857-871},
 title = {On the Interpretation and Interpretability of Quantitative Structure\textendash{}Activity Relationship Models},
 volume = {22},
 year = {2008}
}

@article{Guidotti:2018:SME:3271482.3236009,
 acmid = {3236009},
 address = {New York, NY, USA},
 articleno = {93},
 author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
 doi = {10.1145/3236009},
 issn = {0360-0300},
 issue_date = {January 2019},
 journal = {ACM Comput. Surv.},
 keywords = {Open the black box, explanations, interpretability, transparent models},
 month = {August},
 number = {5},
 numpages = {42},
 pages = {93:1--93:42},
 publisher = {ACM},
 title = {A Survey of Methods for Explaining Black Box Models},
 url = {http://doi.acm.org/10.1145/3236009},
 volume = {51},
 year = {2018}
}

@inproceedings{Guo:2018:LED:3243734.3243792,
 acmid = {3243792},
 address = {New York, NY, USA},
 author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
 booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
 doi = {10.1145/3243734.3243792},
 isbn = {978-1-4503-5693-0},
 keywords = {binary analysis, deep recurrent neural networks, explainable AI},
 location = {Toronto, Canada},
 numpages = {16},
 pages = {364--379},
 publisher = {ACM},
 series = {CCS '18},
 title = {LEMNA: Explaining Deep Learning Based Security Applications},
 url = {http://doi.acm.org/10.1145/3243734.3243792},
 year = {2018}
}

@inproceedings{Ha:2018:DEA:3183654.3183683,
 acmid = {3183683},
 address = {New York, NY, USA},
 articleno = {14},
 author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
 booktitle = {Proceedings of the Technology, Mind, and Society},
 doi = {10.1145/3183654.3183683},
 isbn = {978-1-4503-5420-2},
 keywords = {Anthropomorphism, Attribution theory, Explainability, User perception},
 location = {Washington, DC, USA},
 numpages = {1},
 pages = {14:1--14:1},
 publisher = {ACM},
 series = {TechMindSociety '18},
 title = {Designing Explainability of an Artificial Intelligence System},
 url = {http://doi.acm.org/10.1145/3183654.3183683},
 year = {2018}
}

@inproceedings{Haddouchi:2018:AIC:3289402.3289549,
 acmid = {3289549},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Haddouchi, Maissae and Berrado, Abdelaziz},
 booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
 doi = {10.1145/3289402.3289549},
 isbn = {978-1-4503-6462-1},
 keywords = {Interpretability, ML, measures, scoring},
 location = {Rabat, Morocco},
 numpages = {6},
 pages = {49:1--49:6},
 publisher = {ACM},
 series = {SITA'18},
 title = {Assessing Interpretation Capacity in Machine Learning: A Critical Review},
 url = {http://doi.acm.org/10.1145/3289402.3289549},
 year = {2018}
}

@inproceedings{Hayes:2017:IRC:2909824.3020233,
 acmid = {3020233},
 address = {New York, NY, USA},
 author = {Hayes, Bradley and Shah, Julie A.},
 booktitle = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
 doi = {10.1145/2909824.3020233},
 isbn = {978-1-4503-4336-7},
 keywords = {human-robot collaboration, human-robot interaction, human-robot teaming, interpretable machine learning},
 location = {Vienna, Austria},
 numpages = {10},
 pages = {303--312},
 publisher = {ACM},
 series = {HRI '17},
 title = {Improving Robot Controller Transparency Through Autonomous Policy Explanation},
 url = {http://doi.acm.org/10.1145/2909824.3020233},
 year = {2017}
}

@inproceedings{Hicks:2018:CRA:3204949.3208113,
 acmid = {3208113},
 address = {New York, NY, USA},
 author = {Hicks, Steven Alexander and Pogorelov, Konstantin and de Lange, Thomas and Lux, Mathias and Jeppsson, Mattis and Randel, Kristin Ranheim and Eskeland, Sigrun and Halvorsen, P{\aa}l and Riegler, Michael},
 booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
 doi = {10.1145/3204949.3208113},
 isbn = {978-1-4503-5192-8},
 keywords = {automatic disease detection, deep learning, interpretable neural networks, medical documentation},
 location = {Amsterdam, Netherlands},
 numpages = {4},
 pages = {490--493},
 publisher = {ACM},
 series = {MMSys '18},
 title = {Comprehensible Reasoning and Automated Reporting of Medical Examinations Based on Deep Learning Analysis},
 url = {http://doi.acm.org/10.1145/3204949.3208113},
 year = {2018}
}

@inproceedings{Hicks:2018:MAR:3204949.3208129,
 acmid = {3208129},
 address = {New York, NY, USA},
 author = {Hicks, Steven Alexander and Eskeland, Sigrun and Lux, Mathias and de Lange, Thomas and Randel, Kristin Ranheim and Jeppsson, Mattis and Pogorelov, Konstantin and Halvorsen, P{\aa}l and Riegler, Michael},
 booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
 doi = {10.1145/3204949.3208129},
 isbn = {978-1-4503-5192-8},
 keywords = {automatic disease detection, deep learning, interpretable neural networks, medical documentation},
 location = {Amsterdam, Netherlands},
 numpages = {6},
 pages = {369--374},
 publisher = {ACM},
 series = {MMSys '18},
 title = {Mimir: An Automatic Reporting and Reasoning System for Deep Learning Based Analysis in the Medical Domain},
 url = {http://doi.acm.org/10.1145/3204949.3208129},
 year = {2018}
}

@inproceedings{Holcomb:2018:ODA:3206157.3206174,
 acmid = {3206174},
 address = {New York, NY, USA},
 author = {Holcomb, Sean D. and Porter, William K. and Ault, Shaun V. and Mao, Guifen and Wang, Jin},
 booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
 doi = {10.1145/3206157.3206174},
 isbn = {978-1-4503-6358-7},
 keywords = {AI, AlphaGo Zero, Deep Learning, Deep Mind, Neural Networks, Reinforcement Learning},
 location = {Honolulu, HI, USA},
 numpages = {5},
 pages = {67--71},
 publisher = {ACM},
 series = {ICBDE '18},
 title = {Overview on DeepMind and Its AlphaGo Zero AI},
 url = {http://doi.acm.org/10.1145/3206157.3206174},
 year = {2018}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
 abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
 author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainable AI,Knowledge extraction,Machine learning,Privacy},
 language = {en},
 pages = {1-8},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
 title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
 year = {2018}
}

@incollection{holzingerLectureMultimediaData2014,
 abstract = {At the end of this sixth lecture you:},
 address = {Cham},
 author = {Holzinger, Andreas},
 booktitle = {Biomedical {{Informatics}}: {{Discovering Knowledge}} in {{Big Data}}},
 doi = {10.1007/978-3-319-04528-3_6},
 editor = {Holzinger, Andreas},
 isbn = {978-3-319-04528-3},
 language = {en},
 pages = {251-298},
 publisher = {{Springer International Publishing}},
 title = {Lecture 6 {{Multimedia Data Mining}} and {{Knowledge Discovery}}},
 year = {2014}
}

@inproceedings{holzingerMachineLearningKnowledge2017,
 abstract = {During the last decade pathology has benefited from the rapid progress of image digitizing technologies, which led to the development of scanners, capable to produce so-called Whole Slide images (WSI) which can be explored by a pathologist on a computer screen comparable to the conventional microscope and can be used for diagnostics, research, archiving and also education and training. Digital pathology is not just the transformation of the classical microscopic analysis of histological slides by pathologists to just a digital visualization. It is a disruptive innovation that will dramatically change medical work-flows in the coming years and help to foster personalized medicine. Really powerful gets a pathologist if she/he is augmented by machine learning, e.g. by support vector machines, random forests and deep learning. The ultimate benefit of digital pathology is to enable to learn, to extract knowledge and to make predictions from a combination of heterogenous data, i.e. the histological image, the patient history and the *omics data. These challenges call for integrated/integrative machine learning approach fostering transparency, trust, acceptance and the ability to explain step-by-step why a decision has been made.},
 author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and M\"uller, Heimo and Reihs, Robert and Zatloukal, Kurt},
 booktitle = {Towards {{Integrative Machine Learning}} and {{Knowledge Extraction}}},
 editor = {Holzinger, Andreas and Goebel, Randy and Ferri, Massimo and Palade, Vasile},
 isbn = {978-3-319-69775-8},
 keywords = {Data integration,Deep learning,Digital pathology,Integrative machine learning,Transfer learning},
 language = {en},
 pages = {13-50},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Machine {{Learning}} and {{Knowledge Extraction}} in {{Digital Pathology Needs}} an {{Integrative Approach}}},
 year = {2017}
}

@incollection{hoschkeSelforganizingSensingSystem2008,
 address = {London},
 author = {Hoschke, N. and Lewis, C. J. and Price, D. C. and Scott, D. A. and Gerasimov, V. and Wang, P.},
 booktitle = {Advances in {{Applied Self}}-Organizing {{Systems}}},
 doi = {10.1007/978-1-84628-982-8_4},
 editor = {Prokopenko, Mikhail},
 isbn = {978-1-84628-982-8},
 keywords = {Cellular Automaton,Genetic Programming,Mobile Agent,Multiagent System},
 language = {en},
 pages = {51-76},
 publisher = {{Springer London}},
 series = {Advanced {{Information}} and {{Knowledge Processing}}},
 title = {A {{Self}}-Organizing {{Sensing System}} for {{Structural Health Monitoring}} of {{Aerospace Vehicles}}},
 year = {2008}
}

@inproceedings{Hotta:2008:NGT:1486927.1487030,
 acmid = {1487030},
 address = {Washington, DC, USA},
 author = {Hotta, Hajime and Hagiwara, Masafumi},
 booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
 doi = {10.1109/WIIAT.2008.141},
 isbn = {978-0-7695-3496-1},
 keywords = {neural network, visualization, geographic},
 numpages = {7},
 pages = {817--823},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '08},
 title = {A Neural-Network-Based Geographic Tendency Visualization},
 url = {http://dx.doi.org/10.1109/WIIAT.2008.141},
 year = {2008}
}

@article{http://arxiv.org/abs/1401.5390v1,
 abstract = {Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization.},
 author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
 journal = {arxiv},
 month = {1},
 title = {Learning to Win by Reading Manuals in a Monte-Carlo Framework},
 url = {http://arxiv.org/pdf/1401.5390v1},
 year = {2014}
}

@article{http://arxiv.org/abs/1602.04938v3,
 abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 journal = {arxiv},
 month = {2},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {http://arxiv.org/pdf/1602.04938v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1604.00289v3,
 abstract = {Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.},
 author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
 journal = {arxiv},
 month = {4},
 title = {Building Machines That Learn and Think Like People},
 url = {http://arxiv.org/pdf/1604.00289v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.03490v3,
 abstract = {Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.},
 author = {Lipton, Zachary C.},
 journal = {arxiv},
 month = {6},
 title = {The Mythos of Model Interpretability},
 url = {http://arxiv.org/pdf/1606.03490v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.05386v1,
 abstract = {Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 journal = {arxiv},
 month = {6},
 title = {Model-Agnostic Interpretability of Machine Learning},
 url = {http://arxiv.org/pdf/1606.05386v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1608.08974v2,
 abstract = {Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.},
 author = {Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
 journal = {arxiv},
 month = {8},
 title = {Towards Transparent AI Systems: Interpreting Visual Question Answering
Models},
 url = {http://arxiv.org/pdf/1608.08974v2},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07270v1,
 abstract = {Understanding neural networks is becoming increasingly important. Over the
last few years different types of visualisation and explanation methods have
been proposed. However, none of them explicitly considered the behaviour in the
presence of noise and distracting elements. In this work, we will show how
noise and distracting dimensions can influence the result of an explanation
model. This gives a new theoretical insights to aid selection of the most
appropriate explanation model within the deep-Taylor decomposition framework.},
 author = {Kindermans, Pieter-Jan and Schütt, Kristof and Müller, Klaus-Robert and Dähne, Sven},
 journal = {arxiv},
 month = {11},
 title = {Investigating the influence of noise and distractors on the
interpretation of neural networks},
 url = {http://arxiv.org/pdf/1611.07270v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07567v1,
 abstract = {Complex problems may require sophisticated, non-linear learning methods such
as kernel machines or deep neural networks to achieve state of the art
prediction accuracies. However, high prediction accuracies are not the only
objective to consider when solving problems using machine learning. Instead,
particular scientific applications require some explanation of the learned
prediction function. Unfortunately, most methods do not come with out of the
box straight forward interpretation. Even linear prediction functions are not
straight forward to explain if features exhibit complex correlation structure.
In this paper, we propose the Measure of Feature Importance (MFI). MFI is
general and can be applied to any arbitrary learning machine (including kernel
machines and deep learning). MFI is intrinsically non-linear and can detect
features that by itself are inconspicuous and only impact the prediction
function through their interaction with other features. Lastly, MFI can be used
for both --- model-based feature importance and instance-based feature
importance (i.e, measuring the importance of a feature for a particular data
point).},
 author = {Vidovic, Marina M. -C. and Görnitz, Nico and Müller, Klaus-Robert and Kloft, Marius},
 journal = {arxiv},
 month = {11},
 title = {Feature Importance Measure for Non-linear Learning Algorithms},
 url = {http://arxiv.org/pdf/1611.07567v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07634v1,
 abstract = {State of the art machine learning algorithms are highly optimized to provide
the optimal prediction possible, naturally resulting in complex models. While
these models often outperform simpler more interpretable models by order of
magnitudes, in terms of understanding the way the model functions, we are often
facing a "black box".
In this paper we suggest a simple method to interpret the behavior of any
predictive model, both for regression and classification. Given a particular
model, the information required to interpret it can be obtained by studying the
partial derivatives of the model with respect to the input. We exemplify this
insight by interpreting convolutional and multi-layer neural networks in the
field of natural language processing.},
 author = {Hechtlinger, Yotam},
 journal = {arxiv},
 month = {11},
 title = {Interpretation of Prediction Models Using the Input Gradient},
 url = {http://arxiv.org/pdf/1611.07634v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1612.07843v1,
 abstract = {Text documents can be described by a number of abstract concepts such as
semantic category, writing style, or sentiment. Machine learning (ML) models
have been trained to automatically map documents to these abstract concepts,
allowing to annotate very large text collections, more than could be processed
by a human in a lifetime. Besides predicting the text's category very
accurately, it is also highly desirable to understand how and why the
categorization process takes place. In this paper, we demonstrate that such
understanding can be achieved by tracing the classification decision back to
individual words using layer-wise relevance propagation (LRP), a recently
developed technique for explaining predictions of complex non-linear
classifiers. We train two word-based ML models, a convolutional neural network
(CNN) and a bag-of-words SVM classifier, on a topic categorization task and
adapt the LRP method to decompose the predictions of these models onto words.
Resulting scores indicate how much individual words contribute to the overall
classification decision. This enables one to distill relevant information from
text documents without an explicit semantic information extraction step. We
further use the word-wise relevance scores for generating novel vector-based
document representations which capture semantic information. Based on these
document vectors, we introduce a measure of model explanatory power and show
that, although the SVM and CNN models perform similarly in terms of
classification accuracy, the latter exhibits a higher level of explainability
which makes it more comprehensible for humans and potentially more useful for
other applications.},
 author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {12},
 title = {"What is Relevant in a Text Document?": An Interpretable Machine
Learning Approach},
 url = {http://arxiv.org/pdf/1612.07843v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1702.08635v1,
 abstract = {Machine learning is essentially the sciences of playing with data. An
adaptive data selection strategy, enabling to dynamically choose different data
at various training stages, can reach a more effective model in a more
efficient way. In this paper, we propose a deep reinforcement learning
framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter}
(\textbf{NDF}), to explore automatic and adaptive data selection in the
training process. In particular, NDF takes advantage of a deep neural network
to adaptively select and filter important data instances from a sequential
stream of training data, such that the future accumulative reward (e.g., the
convergence speed) is maximized. In contrast to previous studies in data
selection that is mainly based on heuristic strategies, NDF is quite generic
and thus can be widely suitable for many machine learning tasks. Taking neural
network training with stochastic gradient descent (SGD) as an example,
comprehensive experiments with respect to various neural network modeling
(e.g., multi-layer perceptron networks, convolutional neural networks and
recurrent neural networks) and several applications (e.g., image classification
and text understanding) demonstrate that NDF powered SGD can achieve comparable
accuracy with standard SGD process by using less data and fewer iterations.},
 author = {Fan, Yang and Tian, Fei and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
 journal = {arxiv},
 month = {2},
 title = {Learning What Data to Learn},
 url = {http://arxiv.org/pdf/1702.08635v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1703.06914v2,
 abstract = {In the modern era, each Internet user leaves enormous amounts of auxiliary
digital residuals (footprints) by using a variety of on-line services. All this
data is already collected and stored for many years. In recent works, it was
demonstrated that it's possible to apply simple machine learning methods to
analyze collected digital footprints and to create psycho-demographic profiles
of individuals. However, while these works clearly demonstrated the
applicability of machine learning methods for such an analysis, created simple
prediction models still lacks accuracy necessary to be successfully applied for
practical needs. We have assumed that using advanced deep machine learning
methods may considerably increase the accuracy of predictions. We started with
simple machine learning methods to estimate basic prediction performance and
moved further by applying advanced methods based on shallow and deep neural
networks. Then we compared prediction power of studied models and made
conclusions about its performance. Finally, we made hypotheses how prediction
accuracy can be further improved. As result of this work, we provide full
source code used in the experiments for all interested researchers and
practitioners in corresponding GitHub repository. We believe that applying deep
machine learning for psycho-demographic profiling may have an enormous impact
on the society (for good or worse) and provides means for Artificial
Intelligence (AI) systems to better understand humans by creating their
psychological profiles. Thus AI agents may achieve the human-like ability to
participate in conversation (communication) flow by anticipating human
opponents' reactions, expectations, and behavior.},
 author = {Omelianenko, Iaroslav},
 journal = {arxiv},
 month = {3},
 title = {Applying Deep Machine Learning for psycho-demographic profiling of
Internet users using O.C.E.A.N. model of personality},
 url = {http://arxiv.org/pdf/1703.06914v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1704.03296v3,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks "look" in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.},
 author = {Fong, Ruth and Vedaldi, Andrea},
 journal = {arxiv},
 month = {4},
 title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
 url = {http://arxiv.org/pdf/1704.03296v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1705.06824v2,
 abstract = {Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
"CNN Inception + Gate" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.},
 author = {Wang, Zhengyang and Ji, Shuiwang},
 journal = {arxiv},
 month = {5},
 title = {Learning Convolutional Text Representations for Visual Question
Answering},
 url = {http://arxiv.org/pdf/1705.06824v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07206v2,
 abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.},
 author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {6},
 title = {Explaining Recurrent Neural Network Predictions in Sentiment Analysis},
 url = {http://arxiv.org/pdf/1706.07206v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07979v1,
 abstract = {This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.},
 author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 journal = {arxiv},
 month = {6},
 title = {Methods for Interpreting and Understanding Deep Neural Networks},
 url = {http://arxiv.org/pdf/1706.07979v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1707.09641v2,
 abstract = {The predictive power of neural networks often costs model interpretability.
Several techniques have been developed for explaining model outputs in terms of
input features; however, it is difficult to translate such interpretations into
actionable insight. Here, we propose a framework to analyze predictions in
terms of the model's internal features by inspecting information flow through
the network. Given a trained network and a test image, we select neurons by two
metrics, both measured over a set of images created by perturbations to the
input image: (1) magnitude of the correlation between the neuron activation and
the network output and (2) precision of the neuron activation. We show that the
former metric selects neurons that exert large influence over the network
output while the latter metric selects neurons that activate on generalizable
features. By comparing the sets of neurons selected by these two metrics, our
framework suggests a way to investigate the internal attention mechanisms of
convolutional neural networks.},
 author = {Lengerich, Benjamin J. and Konam, Sandeep and Xing, Eric P. and Rosenthal, Stephanie and Veloso, Manuela},
 journal = {arxiv},
 month = {7},
 title = {Towards Visual Explanations for Convolutional Neural Networks via Input
Resampling},
 url = {http://arxiv.org/pdf/1707.09641v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1708.04988v1,
 abstract = {We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.},
 author = {Assya, Trofimov and Sebastien, Lemieux and Claude, Perreault},
 journal = {arxiv},
 month = {8},
 title = {Warp: a method for neural network interpretability applied to gene
expression profiles},
 url = {http://arxiv.org/pdf/1708.04988v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1708.08296v1,
 abstract = {With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.},
 author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
 journal = {arxiv},
 month = {8},
 title = {Explainable Artificial Intelligence: Understanding, Visualizing and
Interpreting Deep Learning Models},
 url = {http://arxiv.org/pdf/1708.08296v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.04806v2,
 abstract = {Deep neural networks are widely used for classification. These deep models
often suffer from a lack of interpretability -- they are particularly difficult
to understand because of their non-linear nature. As a result, neural networks
are often treated as "black box" models, and in the past, have been trained
purely to optimize the accuracy of predictions. In this work, we create a novel
network architecture for deep learning that naturally explains its own
reasoning for each prediction. This architecture contains an autoencoder and a
special prototype layer, where each unit of that layer stores a weight vector
that resembles an encoded training input. The encoder of the autoencoder allows
us to do comparisons within the latent space, while the decoder allows us to
visualize the learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be similar to at least
one encoded input, a term that encourages every encoded input to be close to at
least one prototype, and a term that encourages faithful reconstruction by the
autoencoder. The distances computed in the prototype layer are used as part of
the classification process. Since the prototypes are learned during training,
the learned network naturally comes with explanations for each prediction, and
the explanations are loyal to what the network actually computes.},
 author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
 journal = {arxiv},
 month = {10},
 title = {Deep Learning for Case-Based Reasoning through Prototypes: A Neural
Network that Explains Its Predictions},
 url = {http://arxiv.org/pdf/1710.04806v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.09511v2,
 abstract = {Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.},
 author = {Barratt, Shane},
 journal = {arxiv},
 month = {10},
 title = {InterpNET: Neural Introspection for Interpretable Deep Learning},
 url = {http://arxiv.org/pdf/1710.09511v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.10777v1,
 abstract = {Recurrent neural networks (RNNs) have been successfully applied to various
natural language processing (NLP) tasks and achieved better results than
conventional methods. However, the lack of understanding of the mechanisms
behind their effectiveness limits further improvements on their architectures.
In this paper, we present a visual analytics method for understanding and
comparing RNN models for NLP tasks. We propose a technique to explain the
function of individual hidden state units based on their expected response to
input texts. We then co-cluster hidden state units and words based on the
expected response and visualize co-clustering results as memory chips and word
clouds to provide more structured knowledge on RNNs' hidden states. We also
propose a glyph-based sequence visualization based on aggregate information to
analyze the behavior of an RNN's hidden state at the sentence-level. The
usability and effectiveness of our method are demonstrated through case studies
and reviews from domain experts.},
 author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
 journal = {arxiv},
 month = {10},
 title = {Understanding Hidden Memories of Recurrent Neural Networks},
 url = {http://arxiv.org/pdf/1710.10777v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.10967v3,
 abstract = {Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy
network" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its "reinforcement-learning value
network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.},
 author = {Igami, Mitsuru},
 journal = {arxiv},
 month = {10},
 title = {Artificial Intelligence as Structural Estimation: Economic
Interpretations of Deep Blue, Bonanza, and AlphaGo},
 url = {http://arxiv.org/pdf/1710.10967v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.00404v1,
 abstract = {As data-driven methods rise in popularity in materials science applications,
a key question is how these machine learning models can be used to understand
microstructure. Given the importance of process-structure-property relations
throughout materials science, it seems logical that models that can leverage
microstructural data would be more capable of predicting property information.
While there have been some recent attempts to use convolutional neural networks
to understand microstructural images, these early studies have focused only on
which featurizations yield the highest machine learning model accuracy for a
single data set. This paper explores the use of convolutional neural networks
for classifying microstructure with a more holistic set of objectives in mind:
generalization between data sets, number of features required, and
interpretability.},
 author = {Ling, Julia and Hutchinson, Maxwell and Antono, Erin and DeCost, Brian and Holm, Elizabeth A. and Meredig, Bryce},
 journal = {arxiv},
 month = {11},
 title = {Building Data-driven Models with Microstructural Images: Generalization
and Interpretability},
 url = {http://arxiv.org/pdf/1711.00404v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.06431v2,
 abstract = {We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.},
 author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
 journal = {arxiv},
 month = {11},
 title = {Using KL-divergence to focus Deep Visual Explanation},
 url = {http://arxiv.org/pdf/1711.06431v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.09482v2,
 abstract = {The practical impact of deep learning on complex supervised learning problems
has been significant, so much so that almost every Artificial Intelligence
problem, or at least a portion thereof, has been somehow recast as a deep
learning problem. The applications appeal is significant, but this appeal is
increasingly challenged by what some call the challenge of explainability, or
more generally the more traditional challenge of debuggability: if the outcomes
of a deep learning process produce unexpected results (e.g., less than expected
performance of a classifier), then there is little available in the way of
theories or tools to help investigate the potential causes of such unexpected
behavior, especially when this behavior could impact people's lives. We
describe a preliminary framework to help address this issue, which we call
"deep visual explanation" (DVE). "Deep," because it is the development and
performance of deep neural network models that we want to understand. "Visual,"
because we believe that the most rapid insight into a complex multi-dimensional
model is provided by appropriate visualization techniques, and "Explanation,"
because in the spectrum from instrumentation by inserting print statements to
the abductive inference of explanatory hypotheses, we believe that the key to
understanding deep learning relies on the identification and exposure of
hypotheses about the performance behavior of a learned deep model. In the
exposition of our preliminary framework, we use relatively straightforward
image classification examples and a variety of choices on initial configuration
of a deep model building scenario. By careful but not complicated
instrumentation, we expose classification outcomes of deep models using
visualization, and also show initial results for one potential application of
interpretability.},
 author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
 journal = {arxiv},
 month = {11},
 title = {An Introduction to Deep Visual Explanation},
 url = {http://arxiv.org/pdf/1711.09482v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.02034v2,
 abstract = {Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2vec, a deep RNN
that automatically learns features from SMILES to predict chemical properties,
without the need for additional explicit feature engineering. Using Bayesian
optimization methods to tune the network architecture, we show that an
optimized SMILES2vec model can serve as a general-purpose neural network for
predicting distinct chemical properties including toxicity, activity,
solubility and solvation energy, while also outperforming contemporary MLP
neural networks that uses engineered features. Furthermore, we demonstrate
proof-of-concept of interpretability by developing an explanation mask that
localizes on the most important characters used in making a prediction. When
tested on the solubility dataset, it identified specific parts of a chemical
that is consistent with established first-principles knowledge with an accuracy
of 88%. Our work demonstrates that neural networks can learn technically
accurate chemical concept and provide state-of-the-art accuracy, making
interpretable deep neural networks a useful tool of relevance to the chemical
industry.},
 author = {Goh, Garrett B. and Hodas, Nathan O. and Siegel, Charles and Vishnu, Abhinav},
 journal = {arxiv},
 month = {12},
 title = {SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for
Predicting Chemical Properties},
 url = {http://arxiv.org/pdf/1712.02034v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.06302v3,
 abstract = {Interpretation and explanation of deep models is critical towards wide
adoption of systems that rely on them. In this paper, we propose a novel scheme
for both interpretation as well as explanation in which, given a pretrained
model, we automatically identify internal features relevant for the set of
classes considered by the model, without relying on additional annotations. We
interpret the model through average visualizations of this reduced set of
features. Then, at test time, we explain the network prediction by accompanying
the predicted class label with supporting visualizations derived from the
identified features. In addition, we propose a method to address the artifacts
introduced by stridded operations in deconvNet-based visualizations. Moreover,
we introduce an8Flower, a dataset specifically designed for objective
quantitative evaluation of methods for visual explanation.Experiments on the
MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces
detailed explanations with good coverage of relevant features of the classes of
interest},
 author = {Oramas, Jose and Wang, Kaili and Tuytelaars, Tinne},
 journal = {arxiv},
 month = {12},
 title = {Visual Explanation by Interpretation: Improving Visual Feedback
Capabilities of Deep Neural Networks},
 url = {http://arxiv.org/pdf/1712.06302v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.08107v1,
 abstract = {Deep neural network models have been proven to be very successful in image
classification tasks, also for medical diagnosis, but their main concern is its
lack of interpretability. They use to work as intuition machines with high
statistical confidence but unable to give interpretable explanations about the
reported results. The vast amount of parameters of these models make difficult
to infer a rationale interpretation from them. In this paper we present a
diabetic retinopathy interpretable classifier able to classify retine images
into the different levels of disease severity and of explaining its results by
assigning a score for every point in the hidden and input space, evaluating its
contribution to the final classification in a linear way. The generated visual
maps can be interpreted by an expert in order to compare its own knowledge with
the interpretation given by the model.},
 author = {Torre, Jordi de la and Valls, Aida and Puig, Domenec},
 journal = {arxiv},
 month = {12},
 title = {A Deep Learning Interpretable Classifier for Diabetic Retinopathy
Disease Grading},
 url = {http://arxiv.org/pdf/1712.08107v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
 abstract = {In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.},
 author = {Mohseni, Sina and Ragan, Eric D.},
 journal = {arxiv},
 month = {1},
 title = {A Human-Grounded Evaluation Benchmark for Local Explanations of Machine
Learning},
 url = {http://arxiv.org/pdf/1801.05075v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.06889v3,
 abstract = {Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W's and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.},
 author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
 journal = {arxiv},
 month = {1},
 title = {Visual Analytics in Deep Learning: An Interrogative Survey for the Next
Frontiers},
 url = {http://arxiv.org/pdf/1801.06889v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.09808v1,
 abstract = {Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we
study such linear explanations produced either post-hoc by a few recent methods
or generated along with predictions with contextual explanation networks
(CENs). We focus on two questions: (i) whether linear explanations are always
consistent or can be misleading, and (ii) when integrated into the prediction
process, whether and how explanations affect the performance of the model. Our
analysis sheds more light on certain properties of explanations produced by
different methods and suggests that learning models that explain and predict
jointly is often advantageous.},
 author = {Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric P.},
 journal = {arxiv},
 month = {1},
 title = {The Intriguing Properties of Model Explanations},
 url = {http://arxiv.org/pdf/1801.09808v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00541v1,
 abstract = {Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.},
 author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
 journal = {arxiv},
 month = {2},
 title = {Causal Learning and Explanation of Deep Neural Networks via Autoencoded
Activations},
 url = {http://arxiv.org/pdf/1802.00541v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00560v2,
 abstract = {Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.},
 author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
 journal = {arxiv},
 month = {2},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 url = {http://arxiv.org/pdf/1802.00560v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00614v2,
 abstract = {This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles' heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.},
 author = {Zhang, Quanshi and Zhu, Song-Chun},
 journal = {arxiv},
 month = {2},
 title = {Visual Interpretability for Deep Learning: a Survey},
 url = {http://arxiv.org/pdf/1802.00614v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.03043v1,
 abstract = {With the popularity of deep learning (DL), artificial intelligence (AI) has
been applied in many areas of human life. Neural network or artificial neural
network (NN), the main technique behind DL, has been extensively studied to
facilitate computer vision and natural language recognition. However, the more
we rely on information technology, the more vulnerable we are. That is,
malicious NNs could bring huge threat in the so-called coming AI era. In this
paper, for the first time in the literature, we propose a novel approach to
design and insert powerful neural-level trojans or PoTrojan in pre-trained NN
models. Most of the time, PoTrojans remain inactive, not affecting the normal
functions of their host NN models. PoTrojans could only be triggered in very
rare conditions. Once activated, however, the PoTrojans could cause the host NN
models to malfunction, either falsely predicting or classifying, which is a
significant threat to human society of the AI era. We would explain the
principles of PoTrojans and the easiness of designing and inserting them in
pre-trained deep learning models. PoTrojans doesn't modify the existing
architecture or parameters of the pre-trained models, without re-training.
Hence, the proposed method is very efficient.},
 author = {Zou, Minhui and Shi, Yang and Wang, Chengliang and Li, Fangyu and Song, WenZhan and Wang, Yu},
 journal = {arxiv},
 month = {2},
 title = {PoTrojan: powerful neural-level trojan designs in deep learning models},
 url = {http://arxiv.org/pdf/1802.03043v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.07384v2,
 abstract = {We present a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU activations
to change its output. We argue that such a correction is a useful way to
provide feedback to a user when the network's output is different from a
desired output. Our algorithm generates such a correction by solving a series
of linear constraint satisfaction problems. The technique is evaluated on three
neural network models: one predicting whether an applicant will pay a mortgage,
one predicting whether a first-order theorem can be proved efficiently by a
solver using certain heuristics, and the final one judging whether a drawing is
an accurate rendition of a canonical drawing of a cat.},
 author = {Zhang, Xin and Solar-Lezama, Armando and Singh, Rishabh},
 journal = {arxiv},
 month = {2},
 title = {Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
Corrections},
 url = {http://arxiv.org/pdf/1802.07384v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.04263v3,
 abstract = {Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research.},
 author = {Weld, Daniel S. and Bansal, Gagan},
 journal = {arxiv},
 month = {3},
 title = {The Challenge of Crafting Intelligible Intelligence},
 url = {http://arxiv.org/pdf/1803.04263v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
 abstract = {Issues regarding explainable AI involve four components: users, laws &
regulations, explanations and algorithms. Together these components provide a
context in which explanation methods can be evaluated regarding their adequacy.
The goal of this chapter is to bridge the gap between expert users and lay
users. Different kinds of users are identified and their concerns revealed,
relevant statements from the General Data Protection Regulation are analyzed in
the context of Deep Neural Networks (DNNs), a taxonomy for the classification
of existing explanation methods is introduced, and finally, the various classes
of explanation methods are analyzed to verify if user concerns are justified.
Overall, it is clear that (visual) explanations can be given about various
aspects of the influence of the input on the output. However, it is noted that
explanation methods or interfaces for lay users are missing and we speculate
which criteria these methods / interfaces should satisfy. Finally it is noted
that two important concerns are difficult to address with explanation methods:
the concern about bias in datasets that leads to biased DNNs, as well as the
suspicion about unfair outcomes.},
 author = {Ras, Gabrielle and Gerven, Marcel van and Haselager, Pim},
 journal = {arxiv},
 month = {3},
 title = {Explanation Methods in Deep Learning: Users, Values, Concerns and
Challenges},
 url = {http://arxiv.org/pdf/1803.07517v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1804.02527v1,
 abstract = {Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.},
 author = {Choo, Jaegul and Liu, Shixia},
 journal = {arxiv},
 month = {4},
 title = {Visual Analytics for Explainable Deep Learning},
 url = {http://arxiv.org/pdf/1804.02527v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1805.07468v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., explaining knowledge representations hidden in middle conv-layers of the
CNN. Given feature maps of a certain conv-layer of the CNN, the explainer
performs like an auto-encoder, which first disentangles the feature maps into
object-part features and then inverts object-part features back to features of
higher conv-layers of the CNN. More specifically, the explainer contains
interpretable conv-layers, where each filter disentangles the representation of
a specific object part from chaotic input feature maps. As a paraphrase of CNN
features, the disentangled representations of object parts help people
understand the logic inside the CNN. We also learn the explainer to use
object-part features to reconstruct features of higher CNN layers, in order to
minimize loss of information during the feature disentanglement. More
crucially, we learn the explainer via network distillation without using any
annotations of sample labels, object parts, or textures for supervision. We
have applied our method to different types of CNNs for evaluation, and
explainers have significantly boosted the interpretability of CNN features.},
 author = {Zhang, Quanshi and Yang, Yu and Liu, Yuchen and Wu, Ying Nian and Zhu, Song-Chun},
 journal = {arxiv},
 month = {5},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks},
 url = {http://arxiv.org/pdf/1805.07468v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.00069v3,
 abstract = {There has recently been a surge of work in explanatory artificial
intelligence (XAI). This research area tackles the important problem that
complex machines and algorithms often cannot provide insights into their
behavior and thought processes. XAI allows users and parts of the internal
system to be more transparent, providing explanations of their decisions in
some level of detail. These explanations are important to ensure algorithmic
fairness, identify potential bias/problems in the training data, and to ensure
that the algorithms perform as expected. However, explanations produced by
these systems is neither standardized nor systematically assessed. In an effort
to create best practices and identify open challenges, we provide our
definition of explainability and show how it can be used to classify existing
literature. We discuss why current approaches to explanatory methods especially
for deep neural networks are insufficient. Finally, based on our survey, we
conclude with suggested future research directions for explanatory artificial
intelligence.},
 author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 journal = {arxiv},
 month = {5},
 title = {Explaining Explanations: An Overview of Interpretability of Machine
Learning},
 url = {http://arxiv.org/pdf/1806.00069v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.05337v2,
 abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance
due to their ability to learn complex, non-linear relationships between
variables. However, the inability to effectively visualize these relationships
has led to DNNs being characterized as black boxes and consequently limited
their applications. To ameliorate this problem, we introduce the use of
hierarchical interpretations to explain DNN predictions through our proposed
method, agglomerative contextual decomposition (ACD). Given a prediction from a
trained DNN, ACD produces a hierarchical clustering of the input features,
along with the contribution of each cluster to the final prediction. This
hierarchy is optimized to identify clusters of features that the DNN learned
are predictive. Using examples from Stanford Sentiment Treebank and ImageNet,
we show that ACD is effective at diagnosing incorrect predictions and
identifying dataset bias. Through human experiments, we demonstrate that ACD
enables users both to identify the more accurate of two DNNs and to better
trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to
adversarial perturbations, implying that it captures fundamental aspects of the
input and ignores spurious noise.},
 author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
 journal = {arxiv},
 month = {6},
 title = {Hierarchical interpretations for neural network predictions},
 url = {http://arxiv.org/pdf/1806.05337v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07470v1,
 abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI
(XAI) construct explanations based on the importance of features in
classification tasks. However, in a high-dimensional feature space this
approach may become unfeasible without restraining the set of important
features. We propose to utilize the human tendency to ask questions like "Why
this output (the fact) instead of that output (the foil)?" to reduce the number
of features to those that play a main role in the asked contrast. Our proposed
method utilizes locally trained one-versus-all decision trees to identify the
disjoint set of rules that causes the tree to classify data points as the foil
and not as the fact. In this study we illustrate this approach on three
benchmark classification tasks.},
 author = {Waa, Jasper van der and Robeer, Marcel and Diggelen, Jurriaan van and Brinkhuis, Matthieu and Neerincx, Mark},
 journal = {arxiv},
 month = {6},
 title = {Contrastive Explanations with Local Foil Trees},
 url = {http://arxiv.org/pdf/1806.07470v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07538v2,
 abstract = {Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.},
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 journal = {arxiv},
 month = {6},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {http://arxiv.org/pdf/1806.07538v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.09809v1,
 abstract = {Natural language explanations of deep neural network decisions provide an
intuitive way for a AI agent to articulate a reasoning process. Current textual
explanations learn to discuss class discriminative features in an image.
However, it is also helpful to understand which attributes might change a
classification decision if present in an image (e.g., "This is not a Scarlet
Tanager because it does not have black wings.") We call such textual
explanations counterfactual explanations, and propose an intuitive method to
generate counterfactual explanations by inspecting which evidence in an input
is missing, but might contribute to a different classification decision if
present in the image. To demonstrate our method we consider a fine-grained
image classification task in which we take as input an image and a
counterfactual class and output text which explains why the image does not
belong to a counterfactual class. We then analyze our generated counterfactual
explanations both qualitatively and quantitatively using proposed automatic
metrics.},
 author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
 journal = {arxiv},
 month = {6},
 title = {Generating Counterfactual Explanations with Natural Language},
 url = {http://arxiv.org/pdf/1806.09809v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
 abstract = {Interpretability methods should be both meaningful to a human and correctly
explain model behavior. In this work, we propose a benchmark to evaluate the
latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative
accuracy of interpretability methods that estimate feature importance in deep
neural networks. We evaluate commonly used interpretability methods and a set
of recently proposed ensemble-based derivative approaches. Our results across
several large-scale image classification datasets are consistent and
thought-provoking -- we find that the formal methods we consider produce
estimates that are less accurate or on par with a random designation of feature
importance. However, certain derivative approaches that ensemble these
estimates far outperform such a random guess. The manner of ensembling remains
critical, we show that some approaches do no better than the underlying method
but carry a far higher computational burden.},
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 journal = {arxiv},
 month = {6},
 title = {Evaluating Feature Importance Estimates},
 url = {http://arxiv.org/pdf/1806.10758v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.03418v1,
 abstract = {Interpretability of deep neural networks is a recently emerging area of
machine learning research targeting a better understanding of how models
perform feature selection and derive their classification decisions. In this
paper, two neural network architectures are trained on spectrogram and raw
waveform data for audio classification tasks on a newly created audio dataset
and layer-wise relevance propagation (LRP), a previously proposed
interpretability method, is applied to investigate the models' feature
selection and decision making. It is demonstrated that the networks are highly
reliant on feature marked as relevant by LRP through systematic manipulation of
the input data. Our results show that by making deep audio classifiers
interpretable, one can analyze and compare the properties and strategies of
different models beyond classification accuracy, which potentially opens up new
ways for model improvements.},
 author = {Becker, Sören and Ackermann, Marcel and Lapuschkin, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {7},
 title = {Interpreting and Explaining Deep Neural Networks for Classification of
Audio Signals},
 url = {http://arxiv.org/pdf/1807.03418v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.04178v1,
 abstract = {The Defense Advanced Research Projects Agency (DARPA) recently launched the
Explainable Artificial Intelligence (XAI) program that aims to create a suite
of new AI techniques that enable end users to understand, appropriately trust,
and effectively manage the emerging generation of AI systems.
In this paper, inspired by DARPA's XAI program, we propose a new paradigm in
security research: Explainable Security (XSec). We discuss the ``Six Ws'' of
XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and
complex characteristics: XSec involves several different stakeholders (i.e.,
the system's developers, analysts, users and attackers) and is multi-faceted by
nature (as it requires reasoning about system model, threat model and
properties of security, privacy and trust as well as about concrete attacks,
vulnerabilities and countermeasures). We define a roadmap for XSec that
identifies several possible research directions.},
 author = {Viganò, Luca and Magazzeni, Daniele},
 journal = {arxiv},
 month = {7},
 title = {Explainable Security},
 url = {http://arxiv.org/pdf/1807.04178v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 journal = {arxiv},
 month = {7},
 title = {Explanations for Temporal Recommendations},
 url = {http://arxiv.org/pdf/1807.06161v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06978v1,
 abstract = {An important task for a recommender system to provide interpretable
explanations for the user. This is important for the credibility of the system.
Current interpretable recommender systems tend to focus on certain features
known to be important to the user and offer their explanations in a structured
form. It is well known that user generated reviews and feedback from reviewers
have strong leverage over the users' decisions. On the other hand, recent text
generation works have been shown to generate text of similar quality to human
written text, and we aim to show that generated text can be successfully used
to explain recommendations.
In this paper, we propose a framework consisting of popular review-oriented
generation models aiming to create personalised explanations for
recommendations. The interpretations are generated at both character and word
levels. We build a dataset containing reviewers' feedback from the Amazon books
review dataset. Our cross-domain experiments are designed to bridge from
natural language processing to the recommender system domain. Besides language
model evaluation methods, we employ DeepCoNN, a novel review-oriented
recommender system using a deep neural network, to evaluate the recommendation
performance of generated reviews by root mean square error (RMSE). We
demonstrate that the synthetic personalised reviews have better recommendation
performance than human written reviews. To our knowledge, this presents the
first machine-generated natural language explanations for rating prediction.},
 author = {Ouyang, Sixun and Lawlor, Aonghus and Costa, Felipe and Dolog, Peter},
 journal = {arxiv},
 month = {7},
 title = {Improving Explainable Recommendations with Synthetic Reviews},
 url = {http://arxiv.org/pdf/1807.06978v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.07404v1,
 abstract = {Predictive geometric models deliver excellent results for many Machine
Learning use cases. Despite their undoubted performance, neural predictive
algorithms can show unexpected degrees of instability and variance,
particularly when applied to large datasets. We present an approach to measure
changes in geometric models with respect to both output consistency and
topological stability. Considering the example of a recommender system using
word2vec, we analyze the influence of single data points, approximation methods
and parameter settings. Our findings can help to stabilize models where needed
and to detect differences in informational value of data points on a large
scale.},
 author = {Regneri, Michaela and Hoffmann, Malte and Kost, Jurij and Pietsch, Niklas and Schulz, Timo and Stamm, Sabine},
 journal = {arxiv},
 month = {7},
 title = {Analyzing Hypersensitive AI: Instability in Corporate-Scale Machine
Learning},
 url = {http://arxiv.org/pdf/1807.07404v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.01591v1,
 abstract = {Recurrent neural networks (RNNs) are temporal networks and cumulative in
nature that have shown promising results in various natural language processing
tasks. Despite their success, it still remains a challenge to understand their
hidden behavior. In this work, we analyze and interpret the cumulative nature
of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation
(LISA) for explaining decisions and detecting the most likely (i.e., saliency)
patterns that the network relies on while decision making. We demonstrate (1)
LISA: "How an RNN accumulates or builds semantics during its sequential
processing for a given text example and expected response" (2) Example2pattern:
"How the saliency patterns look like for each category in the data according to
the network in decision making". We analyse the sensitiveness of RNNs about
different inputs to check the increase or decrease in prediction scores and
further extract the saliency patterns learned by the network. We employ two
relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to
explain RNN predictions via the LISA and example2pattern.},
 author = {Gupta, Pankaj and Schütze, Hinrich},
 journal = {arxiv},
 month = {8},
 title = {LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse
Semantic Accumulation and Example to Pattern Transformation},
 url = {http://arxiv.org/pdf/1808.01591v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
 abstract = {PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.},
 author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
 journal = {arxiv},
 month = {8},
 title = {Learning Explanations from Language Data},
 url = {http://arxiv.org/pdf/1808.04127v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.05054v1,
 abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.},
 author = {Honegger, Milo},
 journal = {arxiv},
 month = {8},
 title = {Shedding Light on Black Box Machine Learning Algorithms: Development of
an Axiomatic Framework to Assess the Quality of Methods that Explain
Individual Predictions},
 url = {http://arxiv.org/pdf/1808.05054v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.07292v2,
 abstract = {In this paper, we study two challenging problems. The first one is how to
implement \textit{k}-means in the neural network, which enjoys efficient
training based on the stochastic algorithm. The second one is how to enhance
the interpretability of network design for clustering. To solve the problems,
we propose a neural network which is a novel formulation of the vanilla
$k$-means objective. Our contribution is in twofold. From the view of neural
networks, the proposed \textit{k}-meansNet is with explicit interpretability in
neural processing. We could understand not only why the network structure is
presented like itself but also why it could perform data clustering. Such an
interpretable neural network remarkably differs from the existing works that
usually employ visualization technique to explain the result of the neural
network. From the view of \textit{k}-means, three highly desired properties are
achieved, i.e. robustness to initialization, the capability of handling new
coming data, and provable convergence. Extensive experimental studies show that
our method achieves promising performance comparing with 12 clustering methods
on some challenging datasets.},
 author = {Peng, Xi and Tsang, Ivor W. and Zhou, Joey Tianyi and Zhu, Hongyuan},
 journal = {arxiv},
 month = {8},
 title = {k-meansNet: When k-means Meets Differentiable Programming},
 url = {http://arxiv.org/pdf/1808.07292v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09551v1,
 abstract = {Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .},
 author = {Godin, Fréderic and Demuynck, Kris and Dambre, Joni and Neve, Wesley De and Demeester, Thomas},
 journal = {arxiv},
 month = {8},
 title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
They Discover Linguistic Rules?},
 url = {http://arxiv.org/pdf/1808.09551v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09744v1,
 abstract = {Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network's performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.},
 author = {Sushil, Madhumita and Šuster, Simon and Daelemans, Walter},
 journal = {arxiv},
 month = {8},
 title = {Rule induction for global explanation of trained models},
 url = {http://arxiv.org/pdf/1808.09744v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1809.02479v1,
 abstract = {Recently machine learning is being applied to almost every data domain one of
which is Question Answering Systems (QAS). A typical Question Answering System
is fairly an information retrieval system, which matches documents or text and
retrieve the most accurate one. The idea of open domain question answering
system put forth, involves convolutional neural network text classifiers. The
Classification model presented in this paper is multi-class text classifier.
The neural network classifier can be trained on large dataset. We report series
of experiments conducted on Convolution Neural Network (CNN) by training it on
two different datasets. Neural network model is trained on top of word
embedding. Softmax layer is applied to calculate loss and mapping of
semantically related words. Gathered results can help justify the fact that
proposed hypothetical QAS is feasible. We further propose a method to integrate
Convolutional Neural Network Classifier to an open domain question answering
system. The idea of Open domain will be further explained, but the generality
of it indicates to the system of domain specific trainable models, thus making
it an open domain.},
 author = {Amin, Muhammad Zain and Nadeem, Noman},
 journal = {arxiv},
 month = {9},
 title = {Convolutional Neural Network: Text Classification Model for Open Domain
Question Answering System},
 url = {http://arxiv.org/pdf/1809.02479v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1809.08037v1,
 abstract = {We present an analysis into the inner workings of Convolutional Neural
Networks (CNNs) for processing text. CNNs used for computer vision can be
interpreted by projecting filters into image space, but for discrete sequence
inputs CNNs remain a mystery. We aim to understand the method by which the
networks process and classify text. We examine common hypotheses to this
problem: that filters, accompanied by global max-pooling, serve as ngram
detectors. We show that filters may capture several different semantic classes
of ngrams by using different activation patterns, and that global max-pooling
induces behavior which separates important ngrams from the rest. Finally, we
show practical use cases derived from our findings in the form of model
interpretability (explaining a trained model by deriving a concrete identity
for each filter, bridging the gap between visualization tools in vision tasks
and NLP) and prediction interpretability (explaining predictions).},
 author = {Jacovi, Alon and Shalom, Oren Sar and Goldberg, Yoav},
 journal = {arxiv},
 month = {9},
 title = {Understanding Convolutional Neural Networks for Text Classification},
 url = {http://arxiv.org/pdf/1809.08037v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00024v1,
 abstract = {Establishing unique identities for both humans and end systems has been an
active research problem in the security community, giving rise to innovative
machine learning-based authentication techniques. Although such techniques
offer an automated method to establish identity, they have not been vetted
against sophisticated attacks that target their core machine learning
technique. This paper demonstrates that mimicking the unique signatures
generated by host fingerprinting and biometric authentication systems is
possible. We expose the ineffectiveness of underlying machine learning
classification models by constructing a blind attack based around the query
synthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an
attack in under 130 queries on a state-of-the-art face authentication system,
and under 100 queries on a host authentication system. We examine how these
attacks can be defended against and explore their limitations. XAI provides an
effective means for adversaries to infer decision boundaries and provides a new
way forward in constructing attacks against systems using machine learning
models for authentication.},
 author = {Garcia, Washington and Choi, Joseph I. and Adari, Suman K. and Jha, Somesh and Butler, Kevin R. B.},
 journal = {arxiv},
 month = {9},
 title = {Explainable Black-Box Attacks Against Model-based Authentication},
 url = {http://arxiv.org/pdf/1810.00024v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
 abstract = {Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).},
 author = {Ross, Andrew Slavin},
 journal = {arxiv},
 month = {9},
 title = {Training Machine Learning Models by Regularizing their Explanations},
 url = {http://arxiv.org/pdf/1810.00869v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
 abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian
predictive models by projecting the information in the predictive distribution
locally to a simpler, interpretable explanation model. The proposed approach
combines the recent Local Interpretable Model-agnostic Explanations (LIME)
method with ideas from Bayesian projection predictive variable selection
methods. The information theoretic basis helps in navigating the trade-off
between explanation fidelity and complexity. We demonstrate the method in
explaining MNIST digit classifications made by a Bayesian deep convolutional
neural network.},
 author = {Peltola, Tomi},
 journal = {arxiv},
 month = {10},
 title = {Local Interpretable Model-agnostic Explanations of Bayesian Predictive
Models via Kullback-Leibler Projections},
 url = {http://arxiv.org/pdf/1810.02678v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.09312v1,
 abstract = {Convolutional neural networks have been successfully applied to various NLP
tasks. However, it is not obvious whether they model different linguistic
patterns such as negation, intensification, and clause compositionality to help
the decision-making process. In this paper, we apply visualization techniques
to observe how the model can capture different linguistic features and how
these features can affect the performance of the model. Later on, we try to
identify the model errors and their sources. We believe that interpreting CNNs
is the first step to understand the underlying semantic features which can
raise awareness to further improve the performance and explainability of CNN
models.},
 author = {Koupaee, Mahnaz and Wang, William Yang},
 journal = {arxiv},
 month = {10},
 title = {Analyzing and Interpreting Convolutional Neural Networks in NLP},
 url = {http://arxiv.org/pdf/1810.09312v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13192v4,
 abstract = {The developments of deep neural networks (DNN) in recent years have ushered a
brand new era of artificial intelligence. DNNs are proved to be excellent in
solving very complex problems, e.g., visual recognition and text understanding,
to the extent of competing with or even surpassing people. Despite inspiring
and encouraging success of DNNs, thorough theoretical analyses still lack to
unravel the mystery of their magics. The design of DNN structure is dominated
by empirical results in terms of network depth, number of neurons and
activations. A few of remarkable works published recently in an attempt to
interpret DNNs have established the first glimpses of their internal
mechanisms. Nevertheless, research on exploring how DNNs operate is still at
the initial stage with plenty of room for refinement. In this paper, we extend
precedent research on neural networks with piecewise linear activations (PLNN)
concerning linear regions bounds. We present (i) the exact maximal number of
linear regions for single layer PLNNs; (ii) a upper bound for multi-layer
PLNNs; and (iii) a tighter upper bound for the maximal number of liner regions
on rectifier networks. The derived bounds also indirectly explain why deep
models are more powerful than shallow counterparts, and how non-linearity of
activation functions impacts on expressiveness of networks.},
 author = {Hu, Qiang and Zhang, Hao},
 journal = {arxiv},
 month = {10},
 title = {Nearly-tight bounds on linear regions of piecewise linear neural
networks},
 url = {http://arxiv.org/pdf/1810.13192v4},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13373v1,
 abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.},
 author = {Barrett, David G. T. and Morcos, Ari S. and Macke, Jakob H.},
 journal = {arxiv},
 month = {10},
 title = {Analyzing biological and artificial neural networks: challenges with
opportunities for synergy?},
 url = {http://arxiv.org/pdf/1810.13373v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13425v2,
 abstract = {Techniques for understanding the functioning of complex machine learning
models are becoming increasingly popular, not only to improve the validation
process, but also to extract new insights about the data via exploratory
analysis. Though a large class of such tools currently exists, most assume that
predictions are point estimates and use a sensitivity analysis of these
estimates to interpret the model. Using lightweight probabilistic networks we
show how including prediction uncertainties in the sensitivity analysis leads
to: (i) more robust and generalizable models; and (ii) a new approach for model
interpretation through uncertainty decomposition. In particular, we introduce a
new regularization that takes both the mean and variance of a prediction into
account and demonstrate that the resulting networks provide improved
generalization to unseen data. Furthermore, we propose a new technique to
explain prediction uncertainties through uncertainties in the input domain,
thus providing new ways to validate and interpret deep learning models.},
 author = {Thiagarajan, Jayaraman J. and Kim, Irene and Anirudh, Rushil and Bremer, Peer-Timo},
 journal = {arxiv},
 month = {10},
 title = {Understanding Deep Neural Networks through Input Uncertainties},
 url = {http://arxiv.org/pdf/1810.13425v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.00196v1,
 abstract = {Building explainable systems is a critical problem in the field of Natural
Language Processing (NLP), since most machine learning models provide no
explanations for the predictions. Existing approaches for explainable machine
learning systems tend to focus on interpreting the outputs or the connections
between inputs and outputs. However, the fine-grained information is often
ignored, and the systems do not explicitly generate the human-readable
explanations. To better alleviate this problem, we propose a novel generative
explanation framework that learns to make classification decisions and generate
fine-grained explanations at the same time. More specifically, we introduce the
explainable factor and the minimum risk training approach that learn to
generate more reasonable explanations. We construct two new datasets that
contain summaries, rating scores, and fine-grained reasons. We conduct
experiments on both datasets, comparing with several strong neural network
baseline systems. Experimental results show that our method surpasses all
baselines on both datasets, and is able to generate concise explanations at the
same time.},
 author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
 journal = {arxiv},
 month = {11},
 title = {Towards Explainable NLP: A Generative Explanation Framework for Text
Classification},
 url = {http://arxiv.org/pdf/1811.00196v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.02783v1,
 abstract = {We introduce a novel approach to feed-forward neural network interpretation
based on partitioning the space of sequences of neuron activations. In line
with this approach, we propose a model-specific interpretation method, called
YASENN. Our method inherits many advantages of model-agnostic distillation,
such as an ability to focus on the particular input region and to express an
explanation in terms of features different from those observed by a neural
network. Moreover, examination of distillation error makes the method
applicable to the problems with low tolerance to interpretation mistakes.
Technically, YASENN distills the network with an ensemble of layer-wise
gradient boosting decision trees and encodes the sequences of neuron
activations with leaf indices. The finite number of unique codes induces a
partitioning of the input space. Each partition may be described in a variety
of ways, including examination of an interpretable model (e.g. a logistic
regression or a decision tree) trained to discriminate between objects of those
partitions. Our experiments provide an intuition behind the method and
demonstrate revealed artifacts in neural network decision making.},
 author = {Zharov, Yaroslav and Korzhenkov, Denis and Shvechikov, Pavel and Tuzhilin, Alexander},
 journal = {arxiv},
 month = {11},
 title = {YASENN: Explaining Neural Networks via Partitioning Activation Sequences},
 url = {http://arxiv.org/pdf/1811.02783v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.06471v2,
 abstract = {Deep learning adoption in the financial services industry has been limited
due to a lack of model interpretability. However, several techniques have been
proposed to explain predictions made by a neural network. We provide an initial
investigation into these techniques for the assessment of credit risk with
neural networks.},
 author = {Modarres, Ceena and Ibrahim, Mark and Louie, Melissa and Paisley, John},
 journal = {arxiv},
 month = {11},
 title = {Towards Explainable Deep Learning for Credit Lending: A Case Study},
 url = {http://arxiv.org/pdf/1811.06471v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.07253v1,
 abstract = {Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.},
 author = {Xiao, Yijun and Wang, William Yang},
 journal = {arxiv},
 month = {11},
 title = {Quantifying Uncertainties in Natural Language Processing Tasks},
 url = {http://arxiv.org/pdf/1811.07253v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.08120v1,
 abstract = {Latent factor models (LFMs) such as matrix factorization achieve the
state-of-the-art performance among various Collaborative Filtering (CF)
approaches for recommendation. Despite the high recommendation accuracy of
LFMs, a critical issue to be resolved is the lack of explainability. Extensive
efforts have been made in the literature to incorporate explainability into
LFMs. However, they either rely on auxiliary information which may not be
available in practice, or fail to provide easy-to-understand explanations. In
this paper, we propose a fast influence analysis method named FIA, which
successfully enforces explicit neighbor-style explanations to LFMs with the
technique of influence functions stemmed from robust statistics. We first
describe how to employ influence functions to LFMs to deliver neighbor-style
explanations. Then we develop a novel influence computation algorithm for
matrix factorization with high efficiency. We further extend it to the more
general neural collaborative filtering and introduce an approximation algorithm
to accelerate influence analysis over neural network models. Experimental
results on real datasets demonstrate the correctness, efficiency and usefulness
of our proposed method.},
 author = {Cheng, Weiyu and Shen, Yanyan and Zhu, Yanmin and Huang, Linpeng},
 journal = {arxiv},
 month = {11},
 title = {Explaining Latent Factor Models for Recommendation with Influence
Functions},
 url = {http://arxiv.org/pdf/1811.08120v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.09725v1,
 abstract = {Deep learning is currently playing a crucial role toward higher levels of
artificial intelligence. This paradigm allows neural networks to learn complex
and abstract representations, that are progressively obtained by combining
simpler ones. Nevertheless, the internal "black-box" representations
automatically discovered by current neural architectures often suffer from a
lack of interpretability, making of primary interest the study of explainable
machine learning techniques. This paper summarizes our recent efforts to
develop a more interpretable neural model for directly processing speech from
the raw waveform. In particular, we propose SincNet, a novel Convolutional
Neural Network (CNN) that encourages the first layer to discover more
meaningful filters by exploiting parametrized sinc functions. In contrast to
standard CNNs, which learn all the elements of each filter, only low and high
cutoff frequencies of band-pass filters are directly learned from data. This
inductive bias offers a very compact way to derive a customized filter-bank
front-end, that only depends on some parameters with a clear physical meaning.
Our experiments, conducted on both speaker and speech recognition, show that
the proposed architecture converges faster, performs better, and is more
interpretable than standard CNNs.},
 author = {Ravanelli, Mirco and Bengio, Yoshua},
 journal = {arxiv},
 month = {11},
 title = {Interpretable Convolutional Filters with SincNet},
 url = {http://arxiv.org/pdf/1811.09725v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.10799v1,
 abstract = {Recent efforts in Machine Learning (ML) interpretability have focused on
creating methods for explaining black-box ML models. However, these methods
rely on the assumption that simple approximations, such as linear models or
decision-trees, are inherently human-interpretable, which has not been
empirically tested. Additionally, past efforts have focused exclusively on
comprehension, neglecting to explore the trust component necessary to convince
non-technical experts, such as clinicians, to utilize ML models in practice. In
this paper, we posit that reinforcement learning (RL) can be used to learn what
is interpretable to different users and, consequently, build their trust in ML
models. To validate this idea, we first train a neural network to provide risk
assessments for heart failure patients. We then design a RL-based clinical
decision-support system (DSS) around the neural network model, which can learn
from its interactions with users. We conduct an experiment involving a diverse
set of clinicians from multiple institutions in three different countries. Our
results demonstrate that ML experts cannot accurately predict which system
outputs will maximize clinicians' confidence in the underlying neural network
model, and suggest additional findings that have broad implications to the
future of research into ML interpretability and the use of ML in medicine.},
 author = {Lahav, Owen and Mastronarde, Nicholas and Schaar, Mihaela van der},
 journal = {arxiv},
 month = {11},
 title = {What is Interpretable? Using Machine Learning to Design Interpretable
Decision-Support Systems},
 url = {http://arxiv.org/pdf/1811.10799v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11705v1,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g.
Deep Neural Networks) in cyber-security applications, most of these models are
perceived as a black-box for the user. Adversarial machine learning offers an
approach to increase our understanding of these models. In this paper we
present an approach to generate explanations for incorrect classifications made
by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is
used to find the minimum modifications (of the input features) required to
correctly classify a given set of misclassified samples. The magnitude of such
modifications is used to visualize the most relevant features that explain the
reason for the misclassification. The presented methodology generated
satisfactory explanations that describe the reasoning behind the
mis-classifications, with descriptions that match expert knowledge. The
advantages of the presented methodology are: 1) applicable to any classifier
with defined gradients. 2) does not require any modification of the classifier
model. 3) can be extended to perform further diagnosis (e.g. vulnerability
assessment) and gain further understanding of the system. Experimental
evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and
Multilayer perceptron classifiers. The results are shown using intuitive
visualizations in order to improve the interpretability of the results.},
 author = {Marino, Daniel L. and Wickramasinghe, Chathurika S. and Manic, Milos},
 journal = {arxiv},
 month = {11},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection
Systems},
 url = {http://arxiv.org/pdf/1811.11705v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
 abstract = {The need for interpretable and accountable intelligent system gets sensible
as artificial intelligence plays more role in human life. Explainable
artificial intelligence systems can be a solution by self-explaining the
reasoning behind the decisions and predictions of the intelligent system.
Researchers from different disciplines work together to define, design and
evaluate interpretable intelligent systems for the user. Our work supports the
different evaluation goals in interpretable machine learning research by a
thorough review of evaluation methodologies used in machine-explanation
research across the fields of human-computer interaction, visual analytics, and
machine learning. We present a 2D categorization of interpretable machine
learning evaluation methods and show a mapping between user groups and
evaluation measures. Further, we address the essential factors and steps for a
right evaluation plan by proposing a nested model for design and evaluation of
explainable artificial intelligence systems.},
 author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
 journal = {arxiv},
 month = {11},
 title = {A Survey of Evaluation Methods and Measures for Interpretable Machine
Learning},
 url = {http://arxiv.org/pdf/1811.11839v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.12615v1,
 abstract = {We propose a possible solution to a public challenge posed by the Fair Isaac
Corporation (FICO), which is to provide an explainable model for credit risk
assessment. Rather than present a black box model and explain it afterwards, we
provide a globally interpretable model that is as accurate as other neural
networks. Our "two-layer additive risk model" is decomposable into subscales,
where each node in the second layer represents a meaningful subscale, and all
of the nonlinearities are transparent. We provide three types of explanations
that are simpler than, but consistent with, the global model. One of these
explanation methods involves solving a minimum set cover problem to find
high-support globally-consistent explanations. We present a new online
visualization tool to allow users to explore the global model and its
explanations.},
 author = {Chen, Chaofan and Lin, Kangcheng and Rudin, Cynthia and Shaposhnik, Yaron and Wang, Sijia and Wang, Tong},
 journal = {arxiv},
 month = {11},
 title = {An Interpretable Model with Globally Consistent Explanations for Credit
Risk},
 url = {http://arxiv.org/pdf/1811.12615v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
 abstract = {Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque "black boxes". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.},
 author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
 journal = {arxiv},
 month = {12},
 title = {Sensitivity based Neural Networks Explanations},
 url = {http://arxiv.org/pdf/1812.01029v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.04801v1,
 abstract = {Interactions such as double negation in sentences and scene interactions in
images are common forms of complex dependencies captured by state-of-the-art
machine learning models. We propose Mah\'e, a novel approach to provide
Model-agnostic hierarchical \'explanations of how powerful machine learning
models, such as deep neural networks, capture these interactions as either
dependent on or free of the context of data instances. Specifically, Mah\'e
provides context-dependent explanations by a novel local interpretation
algorithm that effectively captures any-order interactions, and obtains
context-free explanations through generalizing context-dependent interactions
to explain global behaviors. Experimental results show that Mah\'e obtains
improved local interaction interpretations over state-of-the-art methods and
successfully explains interactions that are context-free.},
 author = {Tsang, Michael and Sun, Youbang and Ren, Dongxu and Liu, Yan},
 journal = {arxiv},
 month = {12},
 title = {Can I trust you more? Model-Agnostic Hierarchical Explanations},
 url = {http://arxiv.org/pdf/1812.04801v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.07169v1,
 abstract = {This paper presents a method to explain the knowledge encoded in a
convolutional neural network (CNN) quantitatively and semantically. The
analysis of the specific rationale of each prediction made by the CNN presents
a key issue of understanding neural networks, but it is also of significant
practical values in certain applications. In this study, we propose to distill
knowledge from the CNN into an explainable additive model, so that we can use
the explainable model to provide a quantitative explanation for the CNN
prediction. We analyze the typical bias-interpreting problem of the explainable
model and develop prior losses to guide the learning of the explainable
additive model. Experimental results have demonstrated the effectiveness of our
method.},
 author = {Chen, Runjin and Chen, Hao and Huang, Ge and Ren, Jie and Zhang, Quanshi},
 journal = {arxiv},
 month = {12},
 title = {Explaining Neural Networks Semantically and Quantitatively},
 url = {http://arxiv.org/pdf/1812.07169v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.10537v2,
 abstract = {In the present paper, a method of defining the industrial process parameters
for a new product using machine learning algorithms will be presented. The
study will describe how to go from the product characteristics till the
prediction of the suitable machine parameters to produce a good quality of this
product, and this is based on an historical training dataset of similar
products with their respective process parameters. In the first part of our
study, we will focus on the ultrasonic welding process definition, welding
parameters and on how it operate. While in second part, we present the design
and implementation of the prediction models such multiple linear regression,
support vector regression, and we compare them to an artificial neural networks
algorithm. In the following part, we present a new application of Convolutional
Neural Networks (CNN) to the industrial process parameters prediction. In
addition, we will propose the generalization approach of our CNN to any
prediction problem of industrial process parameters. Finally the results of the
four methods will be interpreted and discussed.},
 author = {Khdoudi, Abdelmoula and Masrour, Tawfik},
 journal = {arxiv},
 month = {12},
 title = {Prediction of Industrial Process Parameters using Artificial
Intelligence Algorithms},
 url = {http://arxiv.org/pdf/1812.10537v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1901.03838v1,
 abstract = {Prediction accuracy and model explainability are the two most important
objectives when developing machine learning algorithms to solve real-world
problems. The neural networks are known to possess good prediction performance,
but lack of sufficient model explainability. In this paper, we propose to
enhance the explainability of neural networks through the following
architecture constraints: a) sparse additive subnetworks; b) orthogonal
projection pursuit; and c) smooth function approximation. It leads to a sparse,
orthogonal and smooth explainable neural network (SOSxNN). The multiple
parameters in the SOSxNN model are simultaneously estimated by a modified
mini-batch gradient descent algorithm based on the backpropagation technique
for calculating the derivatives and the Cayley transform for preserving the
projection orthogonality. The hyperparameters controlling the sparse and smooth
constraints are optimized by the grid search. Through simulation studies, we
compare the SOSxNN method to several benchmark methods including least absolute
shrinkage and selection operator, support vector machine, random forest, and
multi-layer perceptron. It is shown that proposed model keeps the flexibility
of pursuing prediction accuracy while attaining the improved interpretability,
which can be therefore used as a promising surrogate model for complex model
approximation. Finally, the real data example from the Lending Club is employed
as a showcase of the SOSxNN application.},
 author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
 journal = {arxiv},
 month = {1},
 title = {Enhancing Explainability of Neural Networks through Architecture
Constraints},
 url = {http://arxiv.org/pdf/1901.03838v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.06560v1,
 abstract = {There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don't explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs' use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.},
 author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
 journal = {arxiv},
 month = {1},
 title = {Explaining Explanations to Society},
 url = {http://arxiv.org/pdf/1901.06560v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.07538v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., the explainer uses interpretable visual concepts to explain features in
middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the
explainer performs like an auto-encoder, which decomposes the feature maps into
object-part features. The object-part features are learned to reconstruct CNN
features without much loss of information. We can consider the disentangled
representations of object parts a paraphrase of CNN features, which help people
understand the knowledge encoded by the CNN. More crucially, we learn the
explainer via knowledge distillation without using any annotations of object
parts or textures for supervision. In experiments, our method was widely used
to interpret features of different benchmark CNNs, and explainers significantly
boosted the feature interpretability without hurting the discrimination power
of the CNNs.},
 author = {Zhang, Quanshi and Yang, Yu and Wu, Ying Nian},
 journal = {arxiv},
 month = {1},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks
(extended abstract)},
 url = {http://arxiv.org/pdf/1901.07538v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.08547v1,
 abstract = {Transfer learning which aims at utilizing knowledge learned from one problem
(source domain) to solve another different but related problem (target domain)
has attracted wide research attentions. However, the current transfer learning
methods are mostly uninterpretable, especially to people without ML expertise.
In this extended abstract, we brief introduce two knowledge graph (KG) based
frameworks towards human understandable transfer learning explanation. The
first one explains the transferability of features learned by Convolutional
Neural Network (CNN) from one domain to another through pre-training and
fine-tuning, while the second justifies the model of a target domain predicted
by models from multiple source domains in zero-shot learning (ZSL). Both
methods utilize KG and its reasoning capability to provide rich and human
understandable explanations to the transfer procedure.},
 author = {Geng, Yuxia and Chen, Jiaoyan and Jimenez-Ruiz, Ernesto and Chen, Huajun},
 journal = {arxiv},
 month = {1},
 title = {Human-centric Transfer Learning Explanation via Knowledge Graph
[Extended Abstract]},
 url = {http://arxiv.org/pdf/1901.08547v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09813v1,
 abstract = {Word embeddings generated by neural network methods such as word2vec (W2V)
are well known to exhibit seemingly linear behaviour, e.g. the embeddings of
analogy "woman is to queen as man is to king" approximately describe a
parallelogram. This property is particularly intriguing since the embeddings
are not trained to achieve it. Several explanations have been proposed, but
each introduces assumptions that do not hold in practice. We derive a
probabilistically grounded definition of paraphrasing and show it can be
re-interpreted as word transformation, a mathematical description of "$w_x$ is
to $w_y$". From these concepts we prove existence of the linear relationship
between W2V-type embeddings that underlies the analogical phenomenon, and
identify explicit error terms in the relationship.},
 author = {Allen, Carl and Hospedales, Timothy},
 journal = {arxiv},
 month = {1},
 title = {Analogies Explained: Towards Understanding Word Embeddings},
 url = {http://arxiv.org/pdf/1901.09813v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09839v1,
 abstract = {While the success of deep neural networks (DNNs) is well-established across a
variety of domains, our ability to explain and interpret these methods is
limited. Unlike previously proposed local methods which try to explain
particular classification decisions, we focus on global interpretability and
ask a universally applicable question: given a trained model, which features
are the most important? In the context of neural networks, a feature is rarely
important on its own, so our strategy is specifically designed to leverage
partial covariance structures and incorporate variable dependence into feature
ranking. Our methodological contributions in this paper are two-fold. First, we
propose an effect size analogue for DNNs that is appropriate for applications
with highly collinear predictors (ubiquitous in computer vision). Second, we
extend the recently proposed "RelATive cEntrality" (RATE) measure (Crawford et
al., 2019) to the Bayesian deep learning setting. RATE applies an information
theoretic criterion to the posterior distribution of effect sizes to assess
feature significance. We apply our framework to three broad application areas:
computer vision, natural language processing, and social science.},
 author = {Ish-Horowicz, Jonathan and Udwin, Dana and Flaxman, Seth and Filippi, Sarah and Crawford, Lorin},
 journal = {arxiv},
 month = {1},
 title = {Interpreting Deep Neural Networks Through Variable Importance},
 url = {http://arxiv.org/pdf/1901.09839v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02041v1,
 abstract = {We ask whether the neural network interpretation methods can be fooled via
adversarial model manipulation, which is defined as a model fine-tuning step
that aims to radically alter the explanations without hurting the accuracy of
the original model. By incorporating the interpretation results directly in the
regularization term of the objective function for fine-tuning, we show that the
state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled
with our model manipulation. We propose two types of fooling, passive and
active, and demonstrate such foolings generalize well to the entire validation
set as well as transfer to other interpretation methods. Our results are
validated by both visually showing the fooled explanations and reporting
quantitative metrics that measure the deviations from the original
explanations. We claim that the stability of neural network interpretation
method with respect to our adversarial model manipulation is an important
criterion to check for developing robust and reliable neural network
interpretation method.},
 author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
 journal = {arxiv},
 month = {2},
 title = {Fooling Neural Network Interpretations via Adversarial Model
Manipulation},
 url = {http://arxiv.org/pdf/1902.02041v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02384v1,
 abstract = {A barrier to the wider adoption of neural networks is their lack of
interpretability. While local explanation methods exist for one prediction,
most global attributions still reduce neural network decisions to a single set
of features. In response, we present an approach for generating global
attributions called GAM, which explains the landscape of neural network
predictions across subpopulations. GAM augments global explanations with the
proportion of samples that each attribution best explains and specifies which
samples are described by each attribution. Global explanations also have
tunable granularity to detect more or fewer subpopulations. We demonstrate that
GAM's global explanations 1) yield the known feature importances of simulated
data, 2) match feature weights of interpretable statistical models on real
data, and 3) are intuitive to practitioners through user studies. With more
transparent predictions, GAM can help ensure neural network decisions are
generated for the right reasons.},
 author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
 journal = {arxiv},
 month = {2},
 title = {Global Explanations of Neural Networks: Mapping the Landscape of
Predictions},
 url = {http://arxiv.org/pdf/1902.02384v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02497v1,
 abstract = {With the widespread applications of deep convolutional neural networks
(DCNNs), it becomes increasingly important for DCNNs not only to make accurate
predictions but also to explain how they make their decisions. In this work, we
propose a CHannel-wise disentangled InterPretation (CHIP) model to give the
visual interpretation to the predictions of DCNNs. The proposed model distills
the class-discriminative importance of channels in networks by utilizing the
sparse regularization. Here, we first introduce the network perturbation
technique to learn the model. The proposed model is capable to not only distill
the global perspective knowledge from networks but also present the
class-discriminative visual interpretation for specific predictions of
networks. It is noteworthy that the proposed model is able to interpret
different layers of networks without re-training. By combining the distilled
interpretation knowledge in different layers, we further propose the Refined
CHIP visual interpretation that is both high-resolution and
class-discriminative. Experimental results on the standard dataset demonstrate
that the proposed model provides promising visual interpretation for the
predictions of networks in image classification task compared with existing
visual interpretation methods. Besides, the proposed method outperforms related
approaches in the application of ILSVRC 2015 weakly-supervised localization
task.},
 author = {Cui, Xinrui and Wang, Dan and Wang, Z. Jane},
 journal = {arxiv},
 month = {2},
 title = {CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional
Neural Networks},
 url = {http://arxiv.org/pdf/1902.02497v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.03380v2,
 abstract = {Discovering and exploiting the causality in deep neural networks (DNNs) are
crucial challenges for understanding and reasoning causal effects (CE) on an
explainable visual model. "Intervention" has been widely used for recognizing a
causal relation ontologically. In this paper, we propose a causal inference
framework for visual reasoning via do-calculus. To study the intervention
effects on pixel-level feature(s) for causal reasoning, we introduce pixel-wise
masking and adversarial perturbation. In our framework, CE is calculated using
features in a latent space and perturbed prediction from a DNN-based model. We
further provide a first look into the characteristics of discovered CE of
adversarially perturbed images generated by gradient-based methods.
Experimental results show that CE is a competitive and robust index for
understanding DNNs when compared with conventional methods such as
class-activation mappings (CAMs) on the ChestX-ray 14 dataset for
human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds
promises for detecting adversarial examples as it possesses distinct
characteristics in the presence of adversarial perturbations.},
 author = {Yang, Chao-Han Huck and Liu, Yi-Chieh and Chen, Pin-Yu and Ma, Xiaoli and Tsai, Yi-Chang James},
 journal = {arxiv},
 month = {2},
 title = {When Causal Intervention Meets Image Masking and Adversarial
Perturbation for Deep Neural Networks},
 url = {http://arxiv.org/pdf/1902.03380v2},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
 abstract = {Despite a growing literature on explaining neural networks, no consensus has
been reached on how to explain a neural network decision or how to evaluate an
explanation. In fact, most works rely on manually assessing the explanation to
evaluate the quality of a method. This injects uncertainty in the explanation
process along several dimensions: Which explanation method to apply? Who should
we ask to evaluate it and which criteria should be used for the evaluation? Our
contributions in this paper are twofold. First, we investigate schemes to
combine explanation methods and reduce model uncertainty to obtain a single
aggregated explanation. Our findings show that the aggregation is more robust,
well-aligned with human explanations and can attribute relevance to a broader
set of features (completeness). Second, we propose a novel way of evaluating
explanation methods that circumvents the need for manual evaluation and is not
reliant on the alignment of neural networks and humans decision processes.},
 author = {Rieger, Laura and Hansen, Lars Kai},
 journal = {arxiv},
 month = {3},
 title = {Aggregating explainability methods for neural networks stabilizes
explanations},
 url = {http://arxiv.org/pdf/1903.00519v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.10246v1,
 abstract = {We review computational and robotics models of early language learning and
development. We first explain why and how these models are used to understand
better how children learn language. We argue that they provide concrete
theories of language learning as a complex dynamic system, complementing
traditional methods in psychology and linguistics. We review different modeling
formalisms, grounded in techniques from machine learning and artificial
intelligence such as Bayesian and neural network approaches. We then discuss
their role in understanding several key mechanisms of language development:
cross-situational statistical learning, embodiment, situated social
interaction, intrinsically motivated learning, and cultural evolution. We
conclude by discussing future challenges for research, including modeling of
large-scale empirical data about language acquisition in real-world
environments.
Keywords: Early language learning, Computational and robotic models, machine
learning, development, embodiment, social interaction, intrinsic motivation,
self-organization, dynamical systems, complexity.},
 author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
 journal = {arxiv},
 month = {3},
 title = {Computational and Robotic Models of Early Language Development: A Review},
 url = {http://arxiv.org/pdf/1903.10246v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
 abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently.
Explainability is being presented as a remedy for lack of trust in model
predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise
instance level interpretability for any complex machine learning model. But how
certain are these explanations? Can we rely on additive explanations for
non-additive models? In this paper, we examine the behavior of model explainers
under the presence of interactions. We define two sources of uncertainty, model
level uncertainty, and explanation level uncertainty. We show that adding
interactions reduces explanation level uncertainty. We introduce a new method
iBreakDown that generates non-additive explanations with local interaction.},
 author = {Gosiewska, Alicja and Biecek, Przemyslaw},
 journal = {arxiv},
 month = {3},
 title = {iBreakDown: Uncertainty of Model Explanations for Non-additive
Predictive Models},
 url = {http://arxiv.org/pdf/1903.11420v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.02323v2,
 abstract = {Deep learning is increasingly used in decision-making tasks. However,
understanding how neural networks produce final predictions remains a
fundamental challenge. Existing work on interpreting neural network predictions
for images often focuses on explaining predictions for single images or
neurons. As predictions are often computed based off of millions of weights
that are optimized over millions of images, such explanations can easily miss a
bigger picture. We present Summit, the first interactive system that scalably
and systematically summarizes and visualizes what features a deep learning
model has learned and how those features interact to make predictions. Summit
introduces two new scalable summarization techniques: (1) activation
aggregation discovers important neurons, and (2) neuron-influence aggregation
identifies relationships among such neurons. Summit combines these techniques
to create the novel attribution graph that reveals and summarizes crucial
neuron associations and substructures that contribute to a model's outcomes.
Summit scales to large data, such as the ImageNet dataset with 1.2M images, and
leverages neural network feature visualization and dataset examples to help
users distill large, complex neural network models into compact, interactive
visualizations. We present neural network exploration scenarios where Summit
helps us discover multiple surprising insights into a state-of-the-art image
classifier's learned representations and informs future neural network
architecture design. The Summit visualization runs in modern web browsers and
is open-sourced.},
 author = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng},
 journal = {arxiv},
 month = {4},
 title = {Summit: Scaling Deep Learning Interpretability by Visualizing Activation
and Attribution Summarizations},
 url = {http://arxiv.org/pdf/1904.02323v2},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.04063v1,
 abstract = {The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques
specifically developed for analyzing and understanding the inner-workings and
representations acquired by neural models of language. Approaches included:
systematic manipulation of input to neural networks and investigating the
impact on their performance, testing whether interpretable knowledge can be
decoded from intermediate representations acquired by neural networks,
proposing modifications to neural network architectures to make their knowledge
state or generated output more explainable, and examining the performance of
networks on simplified or formal languages. Here we review a number of
representative studies in each category.},
 author = {Alishahi, Afra and Chrupała, Grzegorz and Linzen, Tal},
 journal = {arxiv},
 month = {4},
 title = {Analyzing and Interpreting Neural Networks for NLP: A Report on the
First BlackboxNLP Workshop},
 url = {http://arxiv.org/pdf/1904.04063v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.05488v1,
 abstract = {Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved
test accuracy for all models it has been applied to.},
 author = {Tao, Sean},
 journal = {arxiv},
 month = {4},
 title = {Deep Neural Network Ensembles},
 url = {http://arxiv.org/pdf/1904.05488v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.08939v1,
 abstract = {A neuroscience method to understanding the brain is to find and study the
preferred stimuli that highly activate an individual cell or groups of cells.
Recent advances in machine learning enable a family of methods to synthesize
preferred stimuli that cause a neuron in an artificial or biological brain to
fire strongly. Those methods are known as Activation Maximization (AM) or
Feature Visualization via Optimization. In this chapter, we (1) review existing
AM techniques in the literature; (2) discuss a probabilistic interpretation for
AM; and (3) review the applications of AM in debugging and explaining networks.},
 author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
 journal = {arxiv},
 month = {4},
 title = {Understanding Neural Networks via Feature Visualization: A survey},
 url = {http://arxiv.org/pdf/1904.08939v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.09273v1,
 abstract = {By their nature, the composition of black box models is opaque. This makes
the ability to generate explanations for the response to stimuli challenging.
The importance of explaining black box models has become increasingly important
given the prevalence of AI and ML systems and the need to build legal and
regulatory frameworks around them. Such explanations can also increase trust in
these uncertain systems. In our paper we present RICE, a method for generating
explanations of the behaviour of black box models by (1) probing a model to
extract model output examples using sensitivity analysis; (2) applying
CNPInduce, a method for inductive logic program synthesis, to generate logic
programs based on critical input-output pairs; and (3) interpreting the target
program as a human-readable explanation. We demonstrate the application of our
method by generating explanations of an artificial neural network trained to
follow simple traffic rules in a hypothetical self-driving car simulation. We
conclude with a discussion on the scalability and usability of our approach and
its potential applications to explanation-critical scenarios.},
 author = {Paçacı, Görkem and Johnson, David and McKeever, Steve and Hamfelt, Andreas},
 journal = {arxiv},
 month = {4},
 title = {"Why did you do that?": Explaining black box models with Inductive
Synthesis},
 url = {http://arxiv.org/pdf/1904.09273v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1905.00122v1,
 abstract = {Converting malware into images followed by vision-based deep learning
algorithms has shown superior threat detection efficacy compared with classical
machine learning algorithms. When malware are visualized as images,
visual-based interpretation schemes can also be applied to extract insights of
why individual samples are classified as malicious. In this work, via two case
studies of dynamic malware classification, we extend the local interpretable
model-agnostic explanation algorithm to explain image-based dynamic malware
classification and examine its interpretation fidelity. For both case studies,
we first train deep learning models via transfer learning on malware images,
demonstrate high classification effectiveness, apply an explanation method on
the images, and correlate the results back to the samples to validate whether
the algorithmic insights are consistent with security domain expertise. In our
first case study, the interpretation framework identifies indirect calls that
uniquely characterize the underlying exploit behavior of a malware family. In
our second case study, the interpretation framework extracts insightful
information such as cryptography-related APIs when applied on images created
from API existence, but generate ambiguous interpretation on images created
from API sequences and frequencies. Our findings indicate that current
image-based interpretation techniques are promising for explaining vision-based
malware classification. We continue to develop image-based interpretation
schemes specifically for security applications.},
 author = {Chen, Li and Yagemann, Carter and Downing, Evan},
 journal = {arxiv},
 month = {4},
 title = {To believe or not to believe: Validating explanation fidelity for
dynamic malware analysis},
 url = {http://arxiv.org/pdf/1905.00122v1},
 year = {2019}
}

@inproceedings{Hu:2017:IWI:3132847.3133198,
 acmid = {3133198},
 address = {New York, NY, USA},
 author = {Hu, Xia and Ji, Shuiwang},
 booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
 doi = {10.1145/3132847.3133198},
 isbn = {978-1-4503-4918-5},
 keywords = {data mining, deep models, interpretability, machine learning, shallow models},
 location = {Singapore, Singapore},
 numpages = {2},
 pages = {2565--2566},
 publisher = {ACM},
 series = {CIKM '17},
 title = {IDM 2017: Workshop on Interpretable Data Mining -- Bridging the Gap Between Shallow and Deep Models},
 url = {http://doi.acm.org/10.1145/3132847.3133198},
 year = {2017}
}

@inproceedings{huExplainableNeuralComputation2018,
 abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
 author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
 booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
 editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
 isbn = {978-3-030-01234-2},
 keywords = {Interpretable reasoning,Neural module networks,Visual question answering},
 language = {en},
 pages = {55-71},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
 year = {2018}
}

@article{Israelsen:2019:XAY:3303862.3267338,
 acmid = {3267338},
 address = {New York, NY, USA},
 articleno = {113},
 author = {Israelsen, Brett W. and Ahmed, Nisar R.},
 doi = {10.1145/3267338},
 issn = {0360-0300},
 issue_date = {February 2019},
 journal = {ACM Comput. Surv.},
 keywords = {Human-computer trust, accountability, algorithmic assurances, explainable artificial intelligence, fairness, interpretable machine learning, transparency},
 month = {January},
 number = {6},
 numpages = {37},
 pages = {113:1--113:37},
 publisher = {ACM},
 title = {\&\#x201C;Dave...I Can Assure You ...That It\&\#x2019;s Going to Be All Right ...\&\#x201D; A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships},
 url = {http://doi.acm.org/10.1145/3267338},
 volume = {51},
 year = {2019}
}

@article{itoGINNGradientInterpretable2018,
 abstract = {This study aims to visualize financial documents in such a way that even nonexperts can understand the sentiments contained therein. To achieve this, we propose a novel text visualization method using an interpretable neural network (NN) architecture, called a gradient interpretable NN (GINN). A GINN can visualize a market sentiment score from an entire financial document and the sentiment gradient scores in both word and concept units. Moreover, the GINN can visualize important concepts given in various sentence contexts. Such visualization helps nonexperts easily understand financial documents. We theoretically analyze the validity of the GINN and experimentally demonstrate the validity of text visualization produced by the GINN using real financial texts.},
 author = {Ito, Tomoki and Sakaji, Hiroki and Izumi, Kiyoshi and Tsubouchi, Kota and Yamashita, Tatsuo},
 doi = {10.1007/s41060-018-0160-8},
 issn = {2364-4168},
 journal = {International Journal of Data Science and Analytics},
 keywords = {Interpretable neural network,Support system,Text mining},
 language = {en},
 month = {December},
 shorttitle = {{{GINN}}},
 title = {{{GINN}}: Gradient Interpretable Neural Networks for Visualizing Financial Texts},
 year = {2018}
}

@inproceedings{itoTextVisualizingNeuralNetwork2018,
 abstract = {This study aims to visualize financial documents to swiftly obtain market sentiment information from these documents and determine the reason for which sentiment decisions are made. This type of visualization is considered helpful for nonexperts to easily understand technical documents such as financial reports. To achieve this, we propose a novel interpretable neural network (NN) architecture called gradient interpretable NN (GINN). GINN can visualize both the market sentiment score from a whole financial document and the sentiment gradient scores in concept units. We experimentally demonstrate the validity of text visualization produced by GINN using a real textual dataset.},
 author = {Ito, Tomoki and Sakaji, Hiroki and Tsubouchi, Kota and Izumi, Kiyoshi and Yamashita, Tatsuo},
 booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
 editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
 isbn = {978-3-319-93040-4},
 keywords = {Interpretable neural network,Support system,Text mining},
 language = {en},
 pages = {247-259},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Text-{{Visualizing Neural Network Model}}},
 title = {Text-{{Visualizing Neural Network Model}}: {{Understanding Online Financial Textual Data}}},
 year = {2018}
}

@incollection{ivancevicIntroductionHumanComputational2007,
 address = {Berlin, Heidelberg},
 booktitle = {Computational {{Mind}}: {{A Complex Dynamics Perspective}}},
 doi = {10.1007/978-3-540-71561-0_1},
 editor = {Ivancevic, Vladimir G. and Ivancevic, Tijana T.},
 isbn = {978-3-540-71561-0},
 keywords = {Adaptive Resonance Theory,Cellular Automaton,Horn Clause,Human Mind},
 language = {en},
 pages = {1-269},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 shorttitle = {Introduction},
 title = {Introduction: {{Human}} and {{Computational Mind}}},
 year = {2007}
}

@inproceedings{Iyer:2018:TED:3278721.3278776,
 acmid = {3278776},
 address = {New York, NY, USA},
 author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 doi = {10.1145/3278721.3278776},
 isbn = {978-1-4503-6012-8},
 keywords = {deep reinforcement learning, explainable ai, human factors, human-ai interaction, system transparency},
 location = {New Orleans, LA, USA},
 numpages = {7},
 pages = {144--150},
 publisher = {ACM},
 series = {AIES '18},
 title = {Transparency and Explanation in Deep Reinforcement Learning Neural Networks},
 url = {http://doi.acm.org/10.1145/3278721.3278776},
 year = {2018}
}

@inproceedings{Jacobson:2018:VNN:3243250.3243266,
 acmid = {3243266},
 address = {New York, NY, USA},
 author = {Jacobson, Victor and Li, J. Jenny and Tapia, Kevin and Morreale, Patricia},
 booktitle = {Proceedings of the International Conference on Pattern Recognition and Artificial Intelligence},
 doi = {10.1145/3243250.3243266},
 isbn = {978-1-4503-6482-9},
 keywords = {CNN, Neural networks, RNN, pattern recognition},
 location = {Union, NJ, USA},
 numpages = {5},
 pages = {18--22},
 publisher = {ACM},
 series = {PRAI 2018},
 title = {Visualizing Neural Networks for Pattern Recognition},
 url = {http://doi.acm.org/10.1145/3243250.3243266},
 year = {2018}
}

@incollection{jinSimultaneousGenerationAccurate2006,
 abstract = {Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.},
 address = {Berlin, Heidelberg},
 author = {Jin, Yaochu and Sendhoff, Bernhard and K\"orner, Edgar},
 booktitle = {Multi-{{Objective Machine Learning}}},
 doi = {10.1007/3-540-33019-4_13},
 editor = {Jin, Yaochu},
 isbn = {978-3-540-33019-6},
 keywords = {Hide Neuron,Mean Square Error,Multiobjective Optimization,Neural Network,Pareto Front},
 language = {en},
 pages = {291-312},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Simultaneous {{Generation}} of {{Accurate}} and {{Interpretable Neural Network Classifiers}}},
 year = {2006}
}

@incollection{kashyapPracticalConceptsMachine2017,
 abstract = {This is an important chapter because it discusses the basic and practical concepts of machine learning (ML). I did not take the academic book style to explain these concepts. I have directed my thoughts and energy to provide you with the concepts that are useful during practical decision making. Hence, while explaining the concepts, terminologies, and technical details, I use examples and case studies that are be helpful in extracting relevant insight from the chapter.},
 address = {Berkeley, CA},
 author = {Kashyap, Patanjali},
 booktitle = {Machine {{Learning}} for {{Decision Makers}}: {{Cognitive Computing Fundamentals}} for {{Better Decision Making}}},
 doi = {10.1007/978-1-4842-2988-0_2},
 editor = {Kashyap, Patanjali},
 isbn = {978-1-4842-2988-0},
 language = {en},
 pages = {35-90},
 publisher = {{Apress}},
 title = {The {{Practical Concepts}} of {{Machine Learning}}},
 year = {2017}
}

@inproceedings{Kasneci:2016:LLW:2983323.2983746,
 acmid = {2983746},
 address = {New York, NY, USA},
 author = {Kasneci, Gjergji and Gottron, Thomas},
 booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
 doi = {10.1145/2983323.2983746},
 isbn = {978-1-4503-4073-1},
 keywords = {artificial neural networks, contribution, explanation, input variables, linear weighting scheme},
 location = {Indianapolis, Indiana, USA},
 numpages = {10},
 pages = {45--54},
 publisher = {ACM},
 series = {CIKM '16},
 title = {LICON: A Linear Weighting Scheme for the Contribution ofInput Variables in Deep Artificial Neural Networks},
 url = {http://doi.acm.org/10.1145/2983323.2983746},
 year = {2016}
}

@incollection{kayaMultimodalPersonalityTrait2018,
 abstract = {Automatic analysis of job interview screening decisions is useful for establishing the nature of biases that may play a role in such decisions. In particular, assessment of apparent personality gives insights into the first impressions evoked by a candidate. Such analysis tools can be used for training purposes, if they can be configured to provide appropriate and clear feedback. In this chapter, we describe a multimodal system that analyzes a short video of a job candidate, producing apparent personality scores and a prediction about whether the candidate will be invited for a further job interview or not. This system provides a visual and textual explanation about its decision, and was ranked first in the ChaLearn 2017 Job Candidate Screening Competition. We discuss the application scenario and the considerations from a broad perspective.},
 address = {Cham},
 author = {Kaya, Heysem and Salah, Albert Ali},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 doi = {10.1007/978-3-319-98131-4_10},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Explainable machine learning,Job candidate screening,Multimodal affective computing,Personality trait analysis},
 language = {en},
 pages = {255-275},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {Multimodal {{Personality Trait Analysis}} for {{Explainable Modeling}} of {{Job Interview Decisions}}},
 year = {2018}
}

@incollection{kimExplainableDeepDriving2018,
 abstract = {Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable\textemdash{}they should provide easy-to-interpret rationales for their behavior\textemdash{}so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here, we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolutional network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 h of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network highlights interpretable features that are used by humans while driving, and causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.},
 address = {Cham},
 author = {Kim, Jinkyu and Canny, John},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 doi = {10.1007/978-3-319-98131-4_8},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Explainable AI,Self-driving vehicles,Visual attention},
 language = {en},
 pages = {173-193},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {Explainable {{Deep Driving}} by {{Visualizing Causal Attention}}},
 year = {2018}
}

@incollection{kochGroupCognitionCollaborative2018,
 abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
 address = {Cham},
 author = {Koch, Janin and Oulasvirta, Antti},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 doi = {10.1007/978-3-319-90403-0_15},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {293-312},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Group {{Cognition}} and {{Collaborative AI}}},
 year = {2018}
}

@article{kotsiantisMachineLearningReview2006,
 abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy\textemdash{}ensembles of classifiers.},
 author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.},
 doi = {10.1007/s10462-007-9052-3},
 issn = {1573-7462},
 journal = {Artificial Intelligence Review},
 keywords = {Classifiers,Data mining techniques,Intelligent data analysis,Learning algorithms},
 language = {en},
 month = {November},
 number = {3},
 pages = {159-190},
 shorttitle = {Machine Learning},
 title = {Machine Learning: A Review of Classification and Combining Techniques},
 volume = {26},
 year = {2006}
}

@inproceedings{Kouki:2019:PEH:3301275.3302306,
 acmid = {3302306},
 address = {New York, NY, USA},
 author = {Kouki, Pigi and Schaffer, James and Pujara, Jay and O'Donovan, John and Getoor, Lise},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302306},
 isbn = {978-1-4503-6272-6},
 keywords = {explainable artificial intelligence, explainable intelligent user interfaces, explainable recommender systems, hybrid recommender systems},
 location = {Marina del Ray, California},
 numpages = {12},
 pages = {379--390},
 publisher = {ACM},
 series = {IUI '19},
 title = {Personalized Explanations for Hybrid Recommender Systems},
 url = {http://doi.acm.org/10.1145/3301275.3302306},
 year = {2019}
}

@inproceedings{Kuo:2008:FEA:1486927.1486968,
 acmid = {1486968},
 address = {Washington, DC, USA},
 author = {Kuo, Yen-Ting and Sonenberg, Liz and Lonie, Andrew},
 booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
 doi = {10.1109/WIIAT.2008.330},
 isbn = {978-0-7695-3496-1},
 keywords = {explanation generation, pattern interpretation},
 numpages = {4},
 pages = {48--51},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '08},
 title = {Finding Explanations for Assisting Pattern Interpretation},
 url = {http://dx.doi.org/10.1109/WIIAT.2008.330},
 year = {2008}
}

@article{kuwajimaImprovingTransparencyDeep2019,
 abstract = {Deep learning techniques are rapidly advanced recently and becoming a necessity component for widespread systems. However, the inference process of deep learning is black box and is not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of (1) structural feature analysis: lists of the features contributing to inference process, (2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and (3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75\% workers answer that input data and result of our feature analysis are consistent, and 70\% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provides suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.},
 author = {Kuwajima, Hiroshi and Tanaka, Masayuki and Okutomi, Masatoshi},
 doi = {10.1007/s13748-019-00179-x},
 issn = {2192-6360},
 journal = {Progress in Artificial Intelligence},
 keywords = {Black box,Deep neural network,Explainable AI,Transparency,Visual attribute,Visualization},
 language = {en},
 month = {April},
 title = {Improving Transparency of Deep Neural Inference Process},
 year = {2019}
}

@article{Lapuschkin:2016:LTA:2946645.3007067,
 acmid = {3007067},
 author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and M\"{u}ller, Klaus-Robert and Samek, Wojciech},
 issn = {1532-4435},
 issue_date = {January 2016},
 journal = {J. Mach. Learn. Res.},
 keywords = {artificial neural networks, computer vision, deep learning, explaining classifiers, layer-wise relevance propagation},
 month = {January},
 number = {1},
 numpages = {5},
 pages = {3938--3942},
 publisher = {JMLR.org},
 title = {The LRP Toolbox for Artificial Neural Networks},
 url = {http://dl.acm.org/citation.cfm?id=2946645.3007067},
 volume = {17},
 year = {2016}
}

@inproceedings{laugelComparisonBasedInverseClassification2018,
 abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
 author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
 booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
 editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
 isbn = {978-3-319-91473-2},
 keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
 language = {en},
 pages = {100-111},
 publisher = {{Springer International Publishing}},
 series = {Communications in {{Computer}} and {{Information Science}}},
 title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}}},
 year = {2018}
}

@incollection{liaoMiningHumanInterpretable2006,
 abstract = {This chapter focuses on one particular class of data mining methodologies that expresses the mined knowledge in the form of fuzzy If-Then rules or fuzzy decision trees that can be easily understood by a human. Past studies on generating fuzzy If-Then rules (mostly from exemplar crisp data and a few from exemplar fuzzy data) are grouped into six major categories: grid partitioning, fuzzy clustering, genetic algorithms, neural networks, hybrid methods, and others. The representative method in each category is detailed. The latest improvements and advancements in each category are also reviewed. Similarly, past studies on generating fuzzy decision trees (from exemplar nominal and/or numeric data as well as from exemplar fuzzy data) are surveyed. The essence of each method is presented. Moreover, we discuss selected studies that address most of the necessary conditions for a fuzzy model to be interpretable and highlight areas for future studies. To give an idea of where fuzzy modeling methods have been applied, major application areas are also summarized.},
 address = {Boston, MA},
 author = {Liao, T. Warren},
 booktitle = {Data {{Mining}} and {{Knowledge Discovery Approaches Based}} on {{Rule Induction Techniques}}},
 doi = {10.1007/0-387-34296-6_15},
 editor = {Triantaphyllou, Evangelos and Felici, Giovanni},
 isbn = {978-0-387-34296-2},
 keywords = {Data mining,Fuzzy clustering,Fuzzy decision trees,Fuzzy If-Then rules,Fuzzy modeling,Fuzzy-neural networks,Genetic algorithms,Neural networks},
 language = {en},
 pages = {495-550},
 publisher = {{Springer US}},
 series = {Massive {{Computing}}},
 shorttitle = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}},
 title = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}: {{An Overview}}},
 year = {2006}
}

@inproceedings{Lim:2019:EES:3308557.3313112,
 acmid = {3313112},
 address = {New York, NY, USA},
 author = {Lim, Brian and Sarkar, Advait and Smith-Renner, Alison and Stumpf, Simone},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
 doi = {10.1145/3308557.3313112},
 isbn = {978-1-4503-6673-1},
 keywords = {explanations, intelligent systems, intelligibility, machine learning, transparency, visualizations},
 location = {Marina del Ray, California},
 numpages = {2},
 pages = {125--126},
 publisher = {ACM},
 series = {IUI '19},
 title = {ExSS: Explainable Smart Systems 2019},
 url = {http://doi.acm.org/10.1145/3308557.3313112},
 year = {2019}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
 abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
 author = {Lisboa, P. J. G.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
 isbn = {978-3-319-03200-9},
 keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
 language = {en},
 pages = {15-21},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
 year = {2013}
}

@inproceedings{Liu:2018:ADM:3219819.3220027,
 acmid = {3220027},
 address = {New York, NY, USA},
 author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 doi = {10.1145/3219819.3220027},
 isbn = {978-1-4503-5552-0},
 keywords = {adversarial detection, machine learning interpretation, spammer detection},
 location = {London, United Kingdom},
 numpages = {9},
 pages = {1803--1811},
 publisher = {ACM},
 series = {KDD '18},
 title = {Adversarial Detection with Model Interpretation},
 url = {http://doi.acm.org/10.1145/3219819.3220027},
 year = {2018}
}

@inproceedings{Liu:2018:INE:3219819.3220001,
 acmid = {3220001},
 address = {New York, NY, USA},
 author = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 doi = {10.1145/3219819.3220001},
 isbn = {978-1-4503-5552-0},
 keywords = {machine learning interpretation, network embedding, taxonomy},
 location = {London, United Kingdom},
 numpages = {9},
 pages = {1812--1820},
 publisher = {ACM},
 series = {KDD '18},
 title = {On Interpretation of Network Embedding via Taxonomy Induction},
 url = {http://doi.acm.org/10.1145/3219819.3220001},
 year = {2018}
}

@inproceedings{Liu:2019:RIS:3289600.3290960,
 acmid = {3290960},
 address = {New York, NY, USA},
 author = {Liu, Ninghao and Du, Mengnan and Hu, Xia},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/3289600.3290960},
 isbn = {978-1-4503-5940-5},
 keywords = {interpretation, recommender systems, representation learning},
 location = {Melbourne VIC, Australia},
 numpages = {9},
 pages = {60--68},
 publisher = {ACM},
 series = {WSDM '19},
 title = {Representation Interpretation with Spatial Encoding and Multimodal Analytics},
 url = {http://doi.acm.org/10.1145/3289600.3290960},
 year = {2019}
}

@incollection{liuInterpretabilityComputationalModels2016,
 abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
 address = {Cham},
 author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
 booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
 doi = {10.1007/978-3-319-30319-2_9},
 editor = {Pedrycz, Witold and Chen, Shyi-Ming},
 isbn = {978-3-319-30319-2},
 keywords = {Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Machine learning,Rule based networks,Rule based systems,Sentiment prediction},
 language = {en},
 pages = {199-220},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
 year = {2016}
}

@incollection{luceBasicsArtificialIntelligence2019,
 abstract = {Fashion not only provides functional purpose, but captures mysterious and elusive aspects of being human. Fashion expresses and invokes human emotion and creativity. How we look and sometimes even how we feel is intertwined in this industry. Fashion has always been forward looking, grabbing onto new technologies as they arise. Artificial intelligence is no exception, and it's moving as quickly as fashion does.},
 address = {Berkeley, CA},
 author = {Luce, Leanne},
 booktitle = {Artificial {{Intelligence}} for {{Fashion}}: {{How AI}} Is {{Revolutionizing}} the {{Fashion Industry}}},
 doi = {10.1007/978-1-4842-3931-5_1},
 editor = {Luce, Leanne},
 isbn = {978-1-4842-3931-5},
 language = {en},
 pages = {3-18},
 publisher = {{Apress}},
 title = {Basics of {{Artificial Intelligence}}},
 year = {2019}
}

@incollection{lughoferModelExplanationInterpretation2018,
 abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
 address = {Cham},
 author = {Lughofer, Edwin},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 doi = {10.1007/978-3-319-90403-0_10},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {177-221},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
 year = {2018}
}

@inproceedings{maragoudakisMiningNaturalLanguage2008,
 abstract = {Learning a programming language is a painstaking process, as it requires knowledge of its syntax, apart from knowing the basic process of representing logical sequences to programming stages. This fact deteriorates the coding process and expels most users from programming. Particularly for novice users or persons with vision problems, learning of how to program and tracing the syntax errors could be improved dramatically by using the most natural of all interfaces, i.e. natural language. Towards this orientation, we suggest a wider framework for allowing programming using natural language. The framework can be easily extended to support different object-oriented programming languages such as C, C++, Visual Basic or Java. Our suggested model is named ``Language Oriented Basic'' and it concerns an intelligent interface that supports code creation, modification and control in Visual Basic. Users can use simple-structured Greek sentences in natural language and the system can output the corresponding syntactic tree. When users declare end of input, the system transforms the syntactic trees to source code. Throughout the whole interaction process, users can check the under-development code in order to verify its correspondence to their expectations. Due to the fact that using natural language can cause a great degree of ambiguity, Bayesian networks and learning from examples have been utilized as an attempt to reason on the most probable programming representation, given a natural language input sentence. In order to enhance the classifier, we propose a novel variation of Bayesian networks that favor the classification process. Experimental results have depicted precision and recall measures in a range of 73\% and 70\% respectively.},
 author = {Maragoudakis, Manolis and Cosmas, Nikolaos and Garbis, Aristogiannis},
 booktitle = {Advanced {{Data Mining}} and {{Applications}}},
 editor = {Tang, Changjie and Ling, Charles X. and Zhou, Xiaofang and Cercone, Nick J. and Li, Xue},
 isbn = {978-3-540-88192-6},
 keywords = {Bayesian Classifier,Bayesian Network,Conditional Independence Assumption,Conditional Probability Table,Natural Language},
 language = {en},
 pages = {15-26},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Mining {{Natural Language Programming Directives}} with {{Class}}-{{Oriented Bayesian Networks}}},
 year = {2008}
}

@inproceedings{Mei:2018:IGA:3209280.3229119,
 acmid = {3229119},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Mei, Jie and Jiang, Xiang and Islam, Aminul and Moh'd, Abidalrahman and Milios, Evangelos},
 booktitle = {Proceedings of the ACM Symposium on Document Engineering 2018},
 doi = {10.1145/3209280.3229119},
 isbn = {978-1-4503-5769-2},
 keywords = {Attention Mechanism, Neural Network, Self-Attention},
 location = {Halifax, NS, Canada},
 numpages = {4},
 pages = {49:1--49:4},
 publisher = {ACM},
 series = {DocEng '18},
 title = {Integrating Global Attention for Pairwise Text Comparison},
 url = {http://doi.acm.org/10.1145/3209280.3229119},
 year = {2018}
}

@incollection{miradiKnowledgeDiscoveryData2009,
 abstract = {The main goal of this study was to discover knowledge from data about Porous Asphalt Concrete (PAC) roads to achieve a better understanding of the behavior of them and via this understanding improve pavement quality and enhance its lifespan. The knowledge discovery process includes five steps, being understanding the problem, understanding the data, data preparation, data mining (modeling), and the interpretation/evaluation of the results of the models. At the moment, almost 75\% of the Dutch motorways network has a PAC top layer. The main damage of PAC is raveling, which is when the top layer of the road loses stones. The SHRP-NL databases provided ten years of material property data from PAC roads. The data for climate and traffic were obtained from databases of the Royal Dutch Meteorological Institute (KNMI) and the Ministry of Transport and Water Management, respectively. Due to the low number of data points (74 data points), an extensive variable selection was performed using eight different methods to determine the four or five most influential input variables and consequently reduce the input dimension. These methods were decision trees, genetic polynomial, artificial neural network, rough set theory, correlation based variable selection with bidirectional and genetic search, wrappers of neural network with genetic search, and relief ranking filter. The modeling step resulted in 8 intelligent models which were developed using two prediction techniques, being artificial neural networks and support vector machines and two rule-based techniques, being decision trees and rough set theory. Taking the low number of data points into account, the prediction models showed a good performance (R2 = 0.95). The rule based models were transparent and easy to interpret but performed less.},
 address = {Berlin, Heidelberg},
 author = {Miradi, Maryam and Molenaar, Andre A. A. and {van de Ven}, Martin F. C.},
 booktitle = {Intelligent and {{Soft Computing}} in {{Infrastructure Systems Engineering}}: {{Recent Advances}}},
 doi = {10.1007/978-3-642-04586-8_5},
 editor = {Gopalakrishnan, Kasthurirangan and Ceylan, Halil and {Attoh-Okine}, Nii O.},
 isbn = {978-3-642-04586-8},
 keywords = {Artificial Neural Network,Data Mining,Knowledge Discovery,Support Vector Machine,Test Section},
 language = {en},
 pages = {107-176},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Knowledge {{Discovery}} and {{Data Mining Using Artificial Intelligence}} to {{Unravel Porous Asphalt Concrete}} in the {{Netherlands}}},
 year = {2009}
}

@inproceedings{Model:1980:MVI:800087.802805,
 acmid = {802805},
 address = {New York, NY, USA},
 author = {Model, Mitchell L},
 booktitle = {Proceedings of the 1980 ACM Conference on LISP and Functional Programming},
 doi = {10.1145/800087.802805},
 location = {Stanford University, California, USA},
 numpages = {8},
 pages = {188--195},
 publisher = {ACM},
 series = {LFP '80},
 title = {Multiprocessing via Intercommunicating LISP Systems},
 url = {http://doi.acm.org/10.1145/800087.802805},
 year = {1980}
}

@article{moriBalancingTradeoffAccuracy2019,
 abstract = {ContextClassification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required.ObjectiveThe aim of this research is to propose a new classification model, called superposed naive Bayes (SNB), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation.MethodIn order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets.ResultsA trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method (SNB) can produce a balanced output that satisfies both accuracy and interpretability criteria.ConclusionsSNB offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability.},
 author = {Mori, Toshiki and Uchihira, Naoshi},
 doi = {10.1007/s10664-018-9638-1},
 issn = {1573-7616},
 journal = {Empirical Software Engineering},
 keywords = {Ensemble learning,Interpretability,Model approximation,Naive Bayes classifier,Predictive accuracy,Software defect prediction,Trade-off analysis,Weights of evidence},
 language = {en},
 month = {April},
 number = {2},
 pages = {779-825},
 title = {Balancing the Trade-off between Accuracy and Interpretability in Software Defect Prediction},
 volume = {24},
 year = {2019}
}

@article{Mulkers:1994:LDA:174662.174664,
 acmid = {174664},
 address = {New York, NY, USA},
 author = {Mulkers, Anne and Winsborough, William and Bruynooghe, Maurice},
 doi = {10.1145/174662.174664},
 issn = {0164-0925},
 issue_date = {March 1994},
 journal = {ACM Trans. Program. Lang. Syst.},
 keywords = {Prolog, abstract interpretation, compile-time garbage collection, liveness, program analysis},
 month = {March},
 number = {2},
 numpages = {54},
 pages = {205--258},
 publisher = {ACM},
 title = {Live-structure Dataflow Analysis for Prolog},
 url = {http://doi.acm.org/10.1145/174662.174664},
 volume = {16},
 year = {1994}
}

@incollection{neukartReverseEngineeringMind2017,
 abstract = {Within this chapter all the requirements for reverse engineering the mind based on the knowledge imparted in the previous chapters will be discussed, and open questions attempted to be solved. A suitable theory of mind that on one side may not be the whole truth from a philosophical point of view, but serves as a valid foundation from an engineering point of view on the other side is introduced. Furthermore, as I indicated more than once, I am of the opinion that both quantum physics as well as self-organization occupy the most important roles in how our brain works and lets us experience conscious content and again, it is required to plunge into the information theoretical approach to quantum physics, quantum computer science.},
 address = {Wiesbaden},
 author = {Neukart, Florian},
 booktitle = {Reverse {{Engineering}} the {{Mind}}: {{Consciously Acting Machines}} and {{Accelerated Evolution}}},
 doi = {10.1007/978-3-658-16176-7_10},
 editor = {Neukart, Florian},
 isbn = {978-3-658-16176-7},
 keywords = {Artificial Neural Network,Hide Markov Model,Quantum Computer,Reverse Engineering,Semantic Network},
 language = {en},
 pages = {237-354},
 publisher = {{Springer Fachmedien Wiesbaden}},
 series = {{{AutoUni}} \textendash{} {{Schriftenreihe}}},
 title = {Reverse Engineering the Mind},
 year = {2017}
}

@incollection{ngomoIntroductionLinkedData2014,
 abstract = {With Linked Data, a very pragmatic approach towards achieving the vision of the Semantic Web has gained some traction in the last years. The term Linked Data refers to a set of best practices for publishing and interlinking structured data on the Web. While many standards, methods and technologies developed within by the Semantic Web community are applicable for Linked Data, there are also a number of specific characteristics of Linked Data, which have to be considered. In this article we introduce the main concepts of Linked Data. We present an overview of the Linked Data life-cycle and discuss individual approaches as well as the state-of-the-art with regard to extraction, authoring, linking, enrichment as well as quality of Linked Data. We conclude the chapter with a discussion of issues, limitations and further research and development challenges of Linked Data. This article is an updated version of a similar lecture given at Reasoning Web Summer School 2013.},
 address = {Cham},
 author = {Ngomo, Axel-Cyrille Ngonga and Auer, S\"oren and Lehmann, Jens and Zaveri, Amrapali},
 booktitle = {Reasoning {{Web}}. {{Reasoning}} on the {{Web}} in the {{Big Data Era}}: 10th {{International Summer School}} 2014, {{Athens}}, {{Greece}}, {{September}} 8-13, 2014. {{Proceedings}}},
 doi = {10.1007/978-3-319-10587-1_1},
 editor = {Koubarakis, Manolis and Stamou, Giorgos and Stoilos, Giorgos and Horrocks, Ian and Kolaitis, Phokion and Lausen, Georg and Weikum, Gerhard},
 file = {/home/tim/Zotero/storage/6M2EVAFK/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf;/home/tim/Zotero/storage/X9CPENNW/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf},
 isbn = {978-3-319-10587-1},
 keywords = {Inductive Logic Programming,Link Data,Link Open Data,Resource Description Framework,SPARQL Query},
 language = {en},
 pages = {1-99},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Introduction to {{Linked Data}} and {{Its Lifecycle}} on the {{Web}}},
 year = {2014}
}

@article{nguyenMachineLearningDeep2019,
 abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
 author = {Nguyen, Giang and Dlugolinsky, Stefan and Bob\'ak, Martin and Tran, Viet and L\'opez Garc\'ia, \'Alvaro and Heredia, Ignacio and Mal\'ik, Peter and Hluch\'y, Ladislav},
 doi = {10.1007/s10462-018-09679-z},
 file = {/home/tim/Zotero/storage/U6RYW6DN/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and .pdf},
 issn = {1573-7462},
 journal = {Artificial Intelligence Review},
 keywords = {Artificial Intelligence software,Deep Learning,Graphics processing unit (GPU),Intensive computing,Large-scale data mining,Machine Learning,Parallel processing},
 language = {en},
 month = {January},
 shorttitle = {Machine {{Learning}} and {{Deep Learning}} Frameworks and Libraries for Large-Scale Data Mining},
 title = {Machine {{Learning}} and {{Deep Learning}} Frameworks and Libraries for Large-Scale Data Mining: A Survey},
 year = {2019}
}

@incollection{nissanNarrativesFormalismComputational2014,
 abstract = {We recapitulate four decades of computational processing of narratives. Vladimir Propp's work in the 1920s paved the way to both the structuralists' approach to the folktale and to narratives in general, and the story grammars approach to automate story-processing. In the latter domain, grammar-driven processing was overtaken by goal-driven processing, but there has been a comeback of story grammars, in combination with other devices. Propp's concern was with Russian folktales, and some story-generation programs are relevant indeed for folktale studies: such is the case of the programs TALE-SPIN and Joseph, which reportedly generated fables; MINSTREL generated Arthurian tales. Sometimes, bugs reveal more than proper functioning does, about the actual underlying model. Automated story processing, within artificial intelligence, showed important results since the late 1970s. After slowing down during the 1990s, since the turn of the century the field resurged, especially in the perspective of virtual environments and interactive narratives, also benefiting from the popularity of computer models of the emotions.},
 address = {Berlin, Heidelberg},
 author = {Nissan, Ephraim},
 booktitle = {Language, {{Culture}}, {{Computation}}. {{Computing}} of the {{Humanities}}, {{Law}}, and {{Narratives}}: {{Essays Dedicated}} to {{Yaacov Choueka}} on the {{Occasion}} of {{His}} 75th {{Birthday}}, {{Part II}}},
 doi = {10.1007/978-3-642-45324-3_11},
 editor = {Dershowitz, Nachum and Nissan, Ephraim},
 isbn = {978-3-642-45324-3},
 keywords = {Belief Revision,Computational Linguistics,Computational Tool,Computer Science Department,Natural Language Processing},
 language = {en},
 pages = {270-393},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Narratives, {{Formalism}}, {{Computational Tools}}, and {{Nonlinearity}}},
 year = {2014}
}

@inproceedings{Nobrega:2019:TER:3297280.3297443,
 acmid = {3297443},
 address = {New York, NY, USA},
 author = {N\'{o}brega, Caio and Marinho, Leandro},
 booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
 doi = {10.1145/3297280.3297443},
 isbn = {978-1-4503-5933-7},
 keywords = {explanations, factorization machines, recommender systems, transparency},
 location = {Limassol, Cyprus},
 numpages = {8},
 pages = {1671--1678},
 publisher = {ACM},
 series = {SAC '19},
 title = {Towards Explaining Recommendations Through Local Surrogate Models},
 url = {http://doi.acm.org/10.1145/3297280.3297443},
 year = {2019}
}

@article{NUTSBOLTSBEHAVIORAL2017,
 doi = {10.1007/s12160-017-9903-3},
 issn = {1532-4796},
 journal = {Annals of Behavioral Medicine},
 language = {en},
 month = {March},
 number = {1},
 pages = {1-2867},
 shorttitle = {{{THE}} ``{{NUTS AND BOLTS}}'' {{OF BEHAVIORAL INTERVENTION DEVELOPMENT}}},
 title = {{{THE}} ``{{NUTS AND BOLTS}}'' {{OF BEHAVIORAL INTERVENTION DEVELOPMENT}}: {{STUDY DESIGNS}}, {{METHODS AND FUNDING OPPORTUNITIES}}},
 volume = {51},
 year = {2017}
}

@inproceedings{Nyawira:2018:UNP:3219104.3229285,
 acmid = {3229285},
 address = {New York, NY, USA},
 articleno = {65},
 author = {Nyaw\~{i}ra, Ishtar and Bushman, Kristi and Qian, Iris and Zhang, Annie},
 booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
 doi = {10.1145/3219104.3229285},
 isbn = {978-1-4503-6446-1},
 keywords = {Artificial Intelligence, Biomedical Image Processing, Convolutional Neural Networks, Deep Learning, Machine Learning},
 location = {Pittsburgh, PA, USA},
 numpages = {8},
 pages = {65:1--65:8},
 publisher = {ACM},
 series = {PEARC '18},
 title = {Understanding Neural Pathways in Zebrafish Through Deep Learning and High Resolution Electron Microscope Data},
 url = {http://doi.acm.org/10.1145/3219104.3229285},
 year = {2018}
}

@inproceedings{oitaReverseEngineeringCreativity2020,
 abstract = {In the field of AI the ultimate goal is to achieve generic intelligence, also called ``true AI'', but which depends on the successful enablement of imagination and creativity in artificial agents. To address this problem, this paper presents a novel deep learning framework for creativity, called INNGenuity. Pursuing an interdisciplinary implementation of creativity conditions, INNGenuity aims at the resolution of the various flaws of current AI learning architectures, which stem from the opacity of their models. Inspired by the neuroanatomy of the brain during creative cognition, the proposed framework's hybrid architecture blends both symbolic and connectionist AI, inline with Minsky's ``society of mind''. At its core, semantic gates are designed to facilitate an input/output flow of semantic structures and enable the usage of aligning mechanisms between neural activation clusters and semantic graphs. Having as goal alignment maximization, such a system would enable interpretability through the creation of labeled patterns of computation, and propose unaligned but relevant computation patterns as novel and useful, therefore creative.},
 author = {Oita, Marilena},
 booktitle = {Advances in {{Information}} and {{Communication}}},
 editor = {Arai, Kohei and Bhatia, Rahul},
 isbn = {978-3-030-12385-7},
 keywords = {Creativity,Imagination,Interpretability,Knowledge,Neural architecture,Neural networks,Semantic networks},
 language = {en},
 pages = {235-247},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
 title = {Reverse {{Engineering Creativity}} into {{Interpretable Neural Networks}}},
 year = {2020}
}

@inproceedings{otteSafeInterpretableMachine2013,
 abstract = {When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.},
 author = {Otte, Clemens},
 booktitle = {Computational {{Intelligence}} in {{Intelligent Data Analysis}}},
 editor = {Moewes, Christian and N\"urnberger, Andreas},
 isbn = {978-3-642-32378-2},
 keywords = {Input Space,Methodological Review,Multivariate Adaptive Regression Spline,Symbolic Model,Symbolic Regression},
 language = {en},
 pages = {111-122},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 shorttitle = {Safe and {{Interpretable Machine Learning}}},
 title = {Safe and {{Interpretable Machine Learning}}: {{A~Methodological Review}}},
 year = {2013}
}

@incollection{pace-siggeWhereCorpusLinguistics2018,
 abstract = {This chapter will provide a platform to showcase the more recent developments that have grown out of the early laid groundwork. The latest theories in the field of linguistics will be presented, based on empirical data taken from naturally occurring language. In particular, the lexical priming theory will be introduced as a way to explain structures of language that corpus linguists have uncovered. Furthermore, the chapter will discuss the development of increasingly sophisticated algorithms that also deal with the use of language. Here, the focus will be on key achievements in the 1980s by IBM which created a solid foundation for applications that are now widely used in mobile and desktop devices\textemdash{}namely ``assistants'' like Amazon's Echo, Apple's SIRI or Google's (and Android's) Google Go.},
 address = {Cham},
 author = {{Pace-Sigge}, Michael},
 booktitle = {Spreading {{Activation}}, {{Lexical Priming}} and the {{Semantic Web}}: {{Early Psycholinguistic Theories}}, {{Corpus Linguistics}} and {{AI Applications}}},
 doi = {10.1007/978-3-319-90719-2_3},
 editor = {{Pace-Sigge}, Michael},
 isbn = {978-3-319-90719-2},
 keywords = {Digital translators,Hoey,Lexical priming,LSTM,N-gram model,Norvig,Quillian},
 language = {en},
 pages = {29-82},
 publisher = {{Springer International Publishing}},
 title = {Where {{Corpus Linguistics}} and {{Artificial Intelligence}} ({{AI}}) {{Meet}}},
 year = {2018}
}

@inproceedings{Palmirani:2011:FMS:2018358.2018385,
 acmid = {2018385},
 address = {New York, NY, USA},
 author = {Palmirani, Monica and Ceci, Marcello and Radicioni, Daniele and Mazzei, Alessandro},
 booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Law},
 doi = {10.1145/2018358.2018385},
 isbn = {978-1-4503-0755-0},
 keywords = {FrameNet, NLP, legal knowledge modelling, semantic interpretation},
 location = {Pittsburgh, Pennsylvania},
 numpages = {5},
 pages = {189--193},
 publisher = {ACM},
 series = {ICAIL '11},
 title = {FrameNet Model of the Suspension of Norms},
 url = {http://doi.acm.org/10.1145/2018358.2018385},
 year = {2011}
}

@incollection{panesarMachineLearningAlgorithms2019,
 abstract = {You do not need a background in algebra and statistics to get started in machine learning. However, be under no illusions, mathematics is a huge part of machine learning. Math is key to understanding how the algorithm works and why coding a machine learning project from scratch is a great way to improve your mathematical and statistical skills. Not understanding the underlying principles behind an algorithm can lead to a limited understanding of methods or adopting limited interpretations of algorithms. If nothing else, it is useful to understand the mathematical principles that algorithms are based on and thus understand best which machine learning techniques are most appropriate.},
 address = {Berkeley, CA},
 author = {Panesar, Arjun},
 booktitle = {Machine {{Learning}} and {{AI}} for {{Healthcare}}       : {{Big Data}} for {{Improved Health Outcomes}}},
 doi = {10.1007/978-1-4842-3799-1_4},
 editor = {Panesar, Arjun},
 isbn = {978-1-4842-3799-1},
 language = {en},
 pages = {119-188},
 publisher = {{Apress}},
 title = {Machine {{Learning Algorithms}}},
 year = {2019}
}

@inproceedings{Pastor:2019:EBB:3297280.3297328,
 acmid = {3297328},
 address = {New York, NY, USA},
 author = {Pastor, Eliana and Baralis, Elena},
 booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
 doi = {10.1145/3297280.3297328},
 isbn = {978-1-4503-5933-7},
 keywords = {interpretability, local model, prediction explanation},
 location = {Limassol, Cyprus},
 numpages = {8},
 pages = {510--517},
 publisher = {ACM},
 series = {SAC '19},
 title = {Explaining Black Box Models by Means of Local Rules},
 url = {http://doi.acm.org/10.1145/3297280.3297328},
 year = {2019}
}

@inproceedings{pomarlanMeaningfulClusteringsRecurrent2018,
 abstract = {Recurrent neural networks have found applications in NLP, but their operation is difficult to interpret. A state automaton that approximates the network would be more interpretable, but for this one needs a method to group network activation states by their behavior. In this paper we propose such a method, and compare it to an existing dimensionality reduction and clustering approach. Our method is better able to group together neural states of similar behavior.},
 author = {Pomarlan, Mihai and Bateman, John},
 booktitle = {Mining {{Intelligence}} and {{Knowledge Exploration}}},
 editor = {Groza, Adrian and Prasath, Rajendra},
 isbn = {978-3-030-05918-7},
 keywords = {Interpretability,Natural language processing,Recurrent neural networks},
 language = {en},
 pages = {11-20},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Meaningful {{Clusterings}} of {{Recurrent Neural Network Activations}} for {{NLP}}},
 year = {2018}
}

@inproceedings{pomarlanUnderstandingNLPNeural2018,
 abstract = {Recurrent neural networks have proven useful in natural language processing. For example, they can be trained to predict, and even generate plausible text with few or no spelling and syntax errors. However, it is not clear what grammar a network has learned, or how it keeps track of the syntactic structure of its input. In this paper, we present a new method to extract a finite state machine from a recurrent neural network. A FSM is in principle a more interpretable representation of a grammar than a neural net would be, however the extracted FSMs for realistic neural networks will also be large. Therefore, we also look at ways to group the states and paths through the extracted FSM so as to get a smaller, easier to understand model of the neural network. To illustrate our methods, we use them to investigate how a neural network learns noun-verb agreement from a simple grammar where relative clauses may appear between noun and verb.},
 author = {Pomarlan, Mihai and Bateman, John},
 booktitle = {{{KI}} 2018: {{Advances}} in {{Artificial Intelligence}}},
 editor = {Trollmann, Frank and Turhan, Anni-Yasmin},
 isbn = {978-3-030-00111-7},
 keywords = {Interpretability,Natural language processing,Recurrent neural networks},
 language = {en},
 pages = {284-296},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Understanding {{NLP Neural Networks}} by the {{Texts They Generate}}},
 year = {2018}
}

@inproceedings{Popat:2017:TLE:3041021.3055133,
 acmid = {3055133},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Popat, Kashyap and Mukherjee, Subhabrata and Str\"{o}tgen, Jannik and Weikum, Gerhard},
 booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
 doi = {10.1145/3041021.3055133},
 isbn = {978-1-4503-4914-7},
 keywords = {credibility analysis, rumor and hoax detection, text mining},
 location = {Perth, Australia},
 numpages = {10},
 pages = {1003--1012},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '17 Companion},
 title = {Where the Truth Lies: Explaining the Credibility of Emerging Claims on the Web and Social Media},
 url = {https://doi.org/10.1145/3041021.3055133},
 year = {2017}
}

@article{Posters2009,
 doi = {10.1007/s12603-009-0095-9},
 issn = {1760-4788},
 journal = {JNHA - The Journal of Nutrition, Health and Aging},
 language = {en},
 month = {June},
 number = {1},
 pages = {210-723},
 title = {Posters},
 volume = {13},
 year = {2009}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
 abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
 author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
 booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
 editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
 isbn = {978-3-319-71746-3},
 language = {en},
 pages = {167-180},
 publisher = {{Springer International Publishing}},
 series = {Communications in {{Computer}} and {{Information Science}}},
 shorttitle = {Interpretable {{Probabilistic Embeddings}}},
 title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
 year = {2018}
}

@inproceedings{Pynadath:2018:CBR:3237383.3237923,
 acmid = {3237923},
 address = {Richland, SC},
 author = {Pynadath, David V. and Wang, Ning and Rovira, Ericka and Barnes, Michael J.},
 booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
 keywords = {affect recognition, explainable ai, human-agent teams, trust},
 location = {Stockholm, Sweden},
 numpages = {9},
 pages = {1495--1503},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 series = {AAMAS '18},
 title = {Clustering Behavior to Recognize Subjective Beliefs in Human-Agent Teams},
 url = {http://dl.acm.org/citation.cfm?id=3237383.3237923},
 year = {2018}
}

@article{qureshiEVEExplainableVector2018,
 abstract = {We present an unsupervised explainable vector embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a concept using human-readable labels, thereby it is readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed model, we consider its usefulness in three fundamental tasks: 1) intruder detection\textemdash{}to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster\textemdash{}to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first\textemdash{}to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.},
 author = {Qureshi, M. Atif and Greene, Derek},
 doi = {10.1007/s10844-018-0511-x},
 file = {/home/tim/Zotero/storage/9BHZ5UXW/Qureshi and Greene - 2018 - EVE explainable vector based embedding technique .pdf},
 issn = {1573-7675},
 journal = {Journal of Intelligent Information Systems},
 keywords = {Distributional semantics,Unsupervised learning,Wikipedia},
 language = {en},
 month = {June},
 shorttitle = {{{EVE}}},
 title = {{{EVE}}: Explainable Vector Based Embedding Technique Using {{Wikipedia}}},
 year = {2018}
}

@inproceedings{Ribeiro:2016:WIT:2939672.2939778,
 acmid = {2939778},
 address = {New York, NY, USA},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 doi = {10.1145/2939672.2939778},
 isbn = {978-1-4503-4232-2},
 keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
 location = {San Francisco, California, USA},
 numpages = {10},
 pages = {1135--1144},
 publisher = {ACM},
 series = {KDD '16},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {http://doi.acm.org/10.1145/2939672.2939778},
 year = {2016}
}

@inproceedings{Sang:2018:DLI:3240508.3241472,
 acmid = {3241472},
 address = {New York, NY, USA},
 author = {Sang, Jitao},
 booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
 doi = {10.1145/3240508.3241472},
 isbn = {978-1-4503-5665-7},
 keywords = {deep learning, interpretable machine learning},
 location = {Seoul, Republic of Korea},
 numpages = {3},
 pages = {2098--2100},
 publisher = {ACM},
 series = {MM '18},
 title = {Deep Learning Interpretation},
 url = {http://doi.acm.org/10.1145/3240508.3241472},
 year = {2018}
}

@incollection{sarkarAnalyzingMovieReviews2018,
 abstract = {In this chapter, we continue with our focus on case-study oriented chapters, where we will focus on specific real-world problems and scenarios and how we can use Machine Learning to solve them. We will cover aspects pertaining to natural language processing (NLP), text analytics, and Machine Learning in this chapter. The problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents. Sentiment analysis is perhaps one of the most popular applications of natural language processing and text analytics with a vast number of websites, books and tutorials on this subject. Typically sentiment analysis seems to work best on subjective text, where people express opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.},
 address = {Berkeley, CA},
 author = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
 booktitle = {Practical {{Machine Learning}} with {{Python}}: {{A Problem}}-{{Solver}}'s {{Guide}} to {{Building Real}}-{{World Intelligent Systems}}},
 doi = {10.1007/978-1-4842-3207-1_7},
 editor = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
 isbn = {978-1-4842-3207-1},
 language = {en},
 pages = {331-372},
 publisher = {{Apress}},
 title = {Analyzing {{Movie Reviews Sentiment}}},
 year = {2018}
}

@incollection{sarkarMachineLearningBasics2018,
 abstract = {The idea of making intelligent, sentient, and self-aware machines is not something that suddenly came into existence in the last few years. In fact a lot of lore from Greek mythology talks about intelligent machines and inventions having self-awareness and intelligence of their own. The origins and the evolution of the computer have been really revolutionary over a period of several centuries, starting from the basic Abacus and its descendant the slide rule in the 17th Century to the first general purpose computer designed by Charles Babbage in the 1800s. In fact, once computers started evolving with the invention of the Analytical Engine by Babbage and the first computer program, which was written by Ada Lovelace in 1842, people started wondering and contemplating that could there be a time when computers or machines truly become intelligent and start thinking for themselves. In fact, the renowned computer scientist, Alan Turing, was highly influential in the development of theoretical computer science, algorithms, and formal language and addressed concepts like artificial intelligence and Machine Learning as early as the 1950s. This brief insight into the evolution of making machines learn is just to give you an idea of something that has been out there since centuries but has recently started gaining a lot of attention and focus.},
 address = {Berkeley, CA},
 author = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
 booktitle = {Practical {{Machine Learning}} with {{Python}}: {{A Problem}}-{{Solver}}'s {{Guide}} to {{Building Real}}-{{World Intelligent Systems}}},
 doi = {10.1007/978-1-4842-3207-1_1},
 editor = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
 isbn = {978-1-4842-3207-1},
 language = {en},
 pages = {3-65},
 publisher = {{Apress}},
 title = {Machine {{Learning Basics}}},
 year = {2018}
}

@incollection{schetininAdvancedFeatureRecognition2007,
 address = {Berlin, Heidelberg},
 author = {Schetinin, V. and Zharkova, Valentina and Brazhnikov, A. and Zharkov, S. I. and Salerno, Emanuele and Bedini, Luigi and Kuruoglu, Ercan E. and Tonazzini, Anna and Zazula, Damjan and Cigale, Boris and Yoshida, Hiroyuki},
 booktitle = {Artificial {{Intelligence}} in {{Recognition}} and {{Classification}} of {{Astrophysical}} and {{Medical Images}}},
 doi = {10.1007/978-3-540-47518-7_4},
 editor = {Zharkova, Valentina and Jain, Lakhmi C.},
 isbn = {978-3-540-47518-7},
 keywords = {Cellular Neural Network,Compute Tomography Colonography,Independent Component Analysis,Source Separation},
 language = {en},
 pages = {151-338},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Advanced {{Feature Recognition}} and {{Classification Using Artificial Intelligence Paradigms}}},
 year = {2007}
}

@article{schubbachJudgingMachinesPhilosophical2019,
 abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
 author = {Schubbach, Arno},
 doi = {10.1007/s11229-019-02167-z},
 issn = {1573-0964},
 journal = {Synthese},
 keywords = {Algorithm,Artificial intelligence,Computation,Deep learning,Explanation,Judgment,Justification,Kant,Machine learning},
 language = {en},
 month = {March},
 shorttitle = {Judging Machines},
 title = {Judging Machines: Philosophical Aspects of Deep Learning},
 year = {2019}
}

@inproceedings{Schuessler:2019:MEC:3290607.3312823,
 acmid = {3312823},
 address = {New York, NY, USA},
 articleno = {LBW2810},
 author = {Schuessler, Martin and Wei\ss, Philipp},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 doi = {10.1145/3290607.3312823},
 isbn = {978-1-4503-5971-9},
 keywords = {deep neural networks, explanations, image classification, interpretable machine learning},
 location = {Glasgow, Scotland Uk},
 numpages = {6},
 pages = {LBW2810:1--LBW2810:6},
 publisher = {ACM},
 series = {CHI EA '19},
 title = {Minimalistic Explanations: Capturing the Essence of Decisions},
 url = {http://doi.acm.org/10.1145/3290607.3312823},
 year = {2019}
}

@article{SCIENTIFICABSTRACTS2018,
 doi = {10.1007/s11606-018-4413-y},
 file = {/home/tim/Zotero/storage/E8QM5V3M/2018 - SCIENTIFIC ABSTRACTS.pdf},
 issn = {1525-1497},
 journal = {Journal of General Internal Medicine},
 language = {en},
 month = {April},
 number = {2},
 pages = {83-840},
 title = {{{SCIENTIFIC ABSTRACTS}}},
 volume = {33},
 year = {2018}
}

@article{ScientificProgrammeAbstracts2003,
 doi = {10.1007/BF03323651},
 issn = {1432-1084},
 journal = {European Radiology},
 language = {en},
 month = {February},
 number = {1},
 pages = {93-589},
 title = {Scientific {{Programme}} \textemdash{} {{Abstracts}}},
 volume = {13},
 year = {2003}
}

@inproceedings{Seo:2017:ICN:3109859.3109890,
 acmid = {3109890},
 address = {New York, NY, USA},
 author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
 booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
 doi = {10.1145/3109859.3109890},
 isbn = {978-1-4503-4652-8},
 keywords = {attention model, convolutional neural network, deep learning for recommender systems},
 location = {Como, Italy},
 numpages = {9},
 pages = {297--305},
 publisher = {ACM},
 series = {RecSys '17},
 title = {Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction},
 url = {http://doi.acm.org/10.1145/3109859.3109890},
 year = {2017}
}

@inproceedings{Sha:2017:IPC:3107411.3107445,
 acmid = {3107445},
 address = {New York, NY, USA},
 author = {Sha, Ying and Wang, May D.},
 booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
 doi = {10.1145/3107411.3107445},
 isbn = {978-1-4503-4722-8},
 keywords = {attention, deep learning, electronic health records, health care, interpretability, recurrent neural networks, visualization},
 location = {Boston, Massachusetts, USA},
 numpages = {8},
 pages = {233--240},
 publisher = {ACM},
 series = {ACM-BCB '17},
 title = {Interpretable Predictions of Clinical Outcomes with An Attention-based Recurrent Neural Network},
 url = {http://doi.acm.org/10.1145/3107411.3107445},
 year = {2017}
}

@article{Shao:2013:ICS:2461912.2462003,
 acmid = {2462003},
 address = {New York, NY, USA},
 articleno = {56},
 author = {Shao, Tianjia and Li, Wilmot and Zhou, Kun and Xu, Weiwei and Guo, Baining and Mitra, Niloy J.},
 doi = {10.1145/2461912.2462003},
 issn = {0730-0301},
 issue_date = {July 2013},
 journal = {ACM Trans. Graph.},
 keywords = {NPR, concept sketch, part relations, product design, shape analysis},
 month = {July},
 number = {4},
 numpages = {10},
 pages = {56:1--56:10},
 publisher = {ACM},
 title = {Interpreting Concept Sketches},
 url = {http://doi.acm.org/10.1145/2461912.2462003},
 volume = {32},
 year = {2013}
}

@article{sharpee25thAnnualComputational2016,
 abstract = {Table of contentsA1 Functional advantages of cell-type heterogeneity in neural circuitsTatyana O. SharpeeA2 Mesoscopic modeling of propagating waves in visual cortexAlain DestexheA3 Dynamics and biomarkers of mental disordersMitsuo KawatoF1 Precise recruitment of spiking output at theta frequencies requires dendritic h-channels in multi-compartment models of oriens-lacunosum/moleculare hippocampal interneuronsVladislav Sekuli\'c, Frances K. SkinnerF2 Kernel methods in reconstruction of current sources from extracellular potentials for single cells and the whole brainsDaniel K. W\'ojcik, Chaitanya Chintaluri, Dorottya Cserp\'an, Zolt\'an Somogyv\'ariF3 The synchronized periods depend on intracellular transcriptional repression mechanisms in circadian clocks.Jae Kyoung Kim, Zachary P. Kilpatrick, Matthew R. Bennett, Kresimir Josi\'cO1 Assessing irregularity and coordination of spiking-bursting rhythms in central pattern generatorsIrene Elices, David Arroyo, Rafael Levi, Francisco B. Rodriguez, Pablo VaronaO2 Regulation of top-down processing by cortically-projecting parvalbumin positive neurons in basal forebrainEunjin Hwang, Bowon Kim, Hio-Been Han, Tae Kim, James T. McKenna, Ritchie E. Brown, Robert W. McCarley, Jee Hyun ChoiO3 Modeling auditory stream segregation, build-up and bistabilityJames Rankin, Pamela Osborn Popp, John RinzelO4 Strong competition between tonotopic neural ensembles explains pitch-related dynamics of auditory cortex evoked fieldsAlejandro Tabas, Andr\'e Rupp, Emili Balaguer-BallesterO5 A simple model of retinal response to multi-electrode stimulationMatias I. Maturana, David B. Grayden, Shaun L. Cloherty, Tatiana Kameneva, Michael R. Ibbotson, Hamish MeffinO6 Noise correlations in V4 area correlate with behavioral performance in visual discrimination taskVeronika Koren, Timm Lochmann, Valentin Dragoi, Klaus ObermayerO7 Input-location dependent gain modulation in cerebellar nucleus neuronsMaria Psarrou, Maria Schilstra, Neil Davey, Benjamin Torben-Nielsen, Volker SteuberO8 Analytic solution of cable energy function for cortical axons and dendritesHuiwen Ju, Jiao Yu, Michael L. Hines, Liang Chen, Yuguo YuO9 C. elegans interactome: interactive visualization of Caenorhabditis elegans worm neuronal networkJimin Kim, Will Leahy, Eli ShlizermanO10 Is the model any good? Objective criteria for computational neuroscience model selectionJustas Birgiolas, Richard C. Gerkin, Sharon M. CrookO11 Cooperation and competition of gamma oscillation mechanismsAtthaphon Viriyopase, Raoul-Martin Memmesheimer, Stan GielenO12 A discrete structure of the brain wavesYuri Dabaghian, Justin DeVito, Luca PerottiO13 Direction-specific silencing of the Drosophila gaze stabilization systemAnmo J. Kim, Lisa M. Fenk, Cheng Lyu, Gaby MaimonO14 What does the fruit fly think about values? A model of olfactory associative learningChang Zhao, Yves Widmer, Simon Sprecher,Walter SennO15 Effects of ionic diffusion on power spectra of local field potentials (LFP)Geir Halnes, Tuomo M\"aki-Marttunen, Daniel Keller, Klas H. Pettersen,Ole A. Andreassen, Gaute T. EinevollO16 Large-scale cortical models towards understanding relationship between brain structure abnormalities and cognitive deficitsYasunori YamadaO17 Spatial coarse-graining the brain: origin of minicolumnsMoira L. Steyn-Ross, D. Alistair Steyn-RossO18 Modeling large-scale cortical networks with laminar structureJorge F. Mejias, John D. Murray, Henry Kennedy, Xiao-Jing WangO19 Information filtering by partial synchronous spikes in a neural populationAlexandra Kruscha, Jan Grewe, Jan Benda, Benjamin LindnerO20 Decoding context-dependent olfactory valence in Drosophila Laurent Badel, Kazumi Ohta, Yoshiko Tsuchimoto, Hokto KazamaP1 Neural network as a scale-free network: the role of a hubB. KahngP2 Hemodynamic responses to emotions and decisions using near-infrared spectroscopy optical imagingNicoladie D. TamP3 Phase space analysis of hemodynamic responses to intentional movement directions using functional near-infrared spectroscopy (fNIRS) optical imaging techniqueNicoladie D.Tam, Luca Pollonini, George ZouridakisP4 Modeling jamming avoidance of weakly electric fishJaehyun Soh, DaeEun KimP5 Synergy and redundancy of retinal ganglion cells in predictionMinsu Yoo, S. E. PalmerP6 A neural field model with a third dimension representing cortical depthViviana Culmone, Ingo BojakP7 Network analysis of a probabilistic connectivity model of the Xenopus tadpole spinal cordAndrea Ferrario, Robert Merrison-Hort, Roman BorisyukP8 The recognition dynamics in the brainChang Sub KimP9 Multivariate spike train analysis using a positive definite kernelTaro TezukaP10 Synchronization of burst periods may govern slow brain dynamics during general anesthesiaPangyu JooP11 The ionic basis of heterogeneity affects stochastic synchronyYoung-Ah Rho, Shawn D. Burton, G. Bard Ermentrout, Jaeseung Jeong, Nathaniel N. UrbanP12 Circular statistics of noise in spike trains with a periodic componentPetr MarsalekP14 Representations of directions in EEG-BCI using Gaussian readoutsHoon-Hee Kim, Seok-hyun Moon, Do-won Lee, Sung-beom Lee, Ji-yong Lee, Jaeseung JeongP15 Action selection and reinforcement learning in basal ganglia during reaching movementsYaroslav I. Molkov, Khaldoun Hamade, Wondimu Teka, William H. Barnett, Taegyo Kim, Sergey Markin, Ilya A. RybakP17 Axon guidance: modeling axonal growth in T-Junction assayCsaba Forro, Harald Dermutz, L\'aszl\'o Demk\'o, J\'anos V\"or\"osP19 Transient cell assembly networks encode persistent spatial memoriesYuri Dabaghian, Andrey BabichevP20 Theory of population coupling and applications to describe high order correlations in large populations of interacting neuronsHaiping HuangP21 Design of biologically-realistic simulations for motor controlSergio Verduzco-FloresP22 Towards understanding the functional impact of the behavioural variability of neuronsFilipa Dos Santos, Peter AndrasP23 Different oscillatory dynamics underlying gamma entrainment deficits in schizophreniaChristoph Metzner, Achim Schweikard, Bartosz ZurowskiP24 Memory recall and spike frequency adaptationJames P. Roach, Leonard M. Sander, Michal R. ZochowskiP25 Stability of neural networks and memory consolidation preferentially occur near criticalityQuinton M. Skilling, Nicolette Ognjanovski, Sara J. Aton, Michal ZochowskiP26 Stochastic Oscillation in Self-Organized Critical States of Small Systems: Sensitive Resting State in Neural SystemsSheng-Jun Wang, Guang Ouyang, Jing Guang, Mingsha Zhang, K. Y. Michael Wong, Changsong ZhouP27 Neurofield: a C++ library for fast simulation of 2D neural field modelsPeter A. Robinson, Paula Sanz-Leon, Peter M. Drysdale, Felix Fung, Romesh G. Abeysuriya, Chris J. Rennie, Xuelong ZhaoP28 Action-based grounding: Beyond encoding/decoding in neural codeYoonsuck Choe, Huei-Fang YangP29 Neural computation in a dynamical system with multiple time scalesYuanyuan Mi, Xiaohan Lin, Si WuP30 Maximum entropy models for 3D layouts of orientation selectivityJoscha Liedtke, Manuel Schottdorf, Fred WolfP31 A behavioral assay for probing computations underlying curiosity in rodentsYoriko Yamamura, Jeffery R. WickensP32 Using statistical sampling to balance error function contributions to optimization of conductance-based modelsTimothy Rumbell, Julia Ramsey, Amy Reyes, Danel Dragulji\'c, Patrick R. Hof, Jennifer Luebke, Christina M. WeaverP33 Exploration and implementation of a self-growing and self-organizing neuron network building algorithmHu He, Xu Yang, Hailin Ma, Zhiheng Xu, Yuzhe WangP34 Disrupted resting state brain network in obese subjects: a data-driven graph theory analysisKwangyeol Baek, Laurel S. Morris, Prantik Kundu, Valerie VoonP35 Dynamics of cooperative excitatory and inhibitory plasticityEverton J. Agnes, Tim P. VogelsP36 Frequency-dependent oscillatory signal gating in feed-forward networks of integrate-and-fire neuronsWilliam F. Podlaski, Tim P. VogelsP37 Phenomenological neural model for adaptation of neurons in area ITMartin Giese, Pradeep Kuravi, Rufin VogelsP38 ICGenealogy: towards a common topology of neuronal ion channel function and genealogy in model and experimentAlexander Seeholzer, William Podlaski, Rajnish Ranjan, Tim VogelsP39 Temporal input discrimination from the interaction between dynamic synapses and neural subthreshold oscillationsJoaquin J. Torres, Fabiano Baroni, Roberto Latorre, Pablo VaronaP40 Different roles for transient and sustained activity during active visual processingBart Gips, Eric Lowet, Mark J. Roberts, Peter de Weerd, Ole Jensen, Jan van der EerdenP41 Scale-free functional networks of 2D Ising model are highly robust against structural defects: neuroscience implicationsAbdorreza Goodarzinick, Mohammad D. Niry, Alireza ValizadehP42 High frequency neuron can facilitate propagation of signal in neural networksAref Pariz, Shervin S. Parsi, Alireza ValizadehP43 Investigating the effect of Alzheimer's disease related amyloidopathy on gamma oscillations in the CA1 region of the hippocampusJulia M. Warburton, Lucia Marucci, Francesco Tamagnini, Jon Brown, Krasimira Tsaneva-AtanasovaP44 Long-tailed distributions of inhibitory and excitatory weights in a balanced network with eSTDP and iSTDPFlorence I. Kleberg, Jochen TrieschP45 Simulation of EMG recording from hand muscle due to TMS of motor cortexBahar Moezzi, Nicolangelo Iannella, Natalie Schaworonkow, Lukas Plogmacher, Mitchell R. Goldsworthy, Brenton Hordacre, Mark D. McDonnell, Michael C. Ridding, Jochen TrieschP46 Structure and dynamics of axon network formed in primary cell cultureMartin Zapotocky, Daniel Smit, Coralie Fouquet, Alain TrembleauP47 Efficient signal processing and sampling in random networks that generate variabilitySakyasingha Dasgupta, Isao Nishikawa, Kazuyuki Aihara, Taro ToyoizumiP48 Modeling the effect of riluzole on bursting in respiratory neural networksDaniel T. Robb, Nick Mellen, Natalia ToporikovaP49 Mapping relaxation training using effective connectivity analysisRongxiang Tang, Yi-Yuan TangP50 Modeling neuron oscillation of implicit sequence learningGuangsheng Liang, Seth A. Kiser, James H. Howard, Jr., Yi-Yuan TangP51 The role of cerebellar short-term synaptic plasticity in the pathology and medication of downbeat nystagmusJulia Goncharenko, Neil Davey, Maria Schilstra, Volker SteuberP52 Nonlinear response of noisy neuronsSergej O. Voronenko, Benjamin LindnerP53 Behavioral embedding suggests multiple chaotic dimensions underlie C. elegans locomotionTosif Ahamed, Greg StephensP54 Fast and scalable spike sorting for large and dense multi-electrodes recordingsPierre Yger, Baptiste Lefebvre, Giulia Lia Beatrice Spampinato, Elric Esposito, Marcel Stimberg et Olivier MarreP55 Sufficient sampling rates for fast hand motion trackingHansol Choi, Min-Ho SongP56 Linear readout of object manifoldsSueYeon Chung, Dan D. Lee, Haim SompolinskyP57 Differentiating models of intrinsic bursting and rhythm generation of the respiratory pre-B\"otzinger complex using phase response curvesRyan S. Phillips, Jeffrey SmithP58 The effect of inhibitory cell network interactions during theta rhythms on extracellular field potentials in CA1 hippocampusAlexandra Pierri Chatzikalymniou, Katie Ferguson, Frances K. SkinnerP59 Expansion recoding through sparse sampling in the cerebellar input layer speeds learningN. Alex Cayco Gajic, Claudia Clopath, R. Angus SilverP60 A set of curated cortical models at multiple scales on Open Source BrainPadraig Gleeson, Boris Marin, Sadra Sadeh, Adrian Quintana, Matteo Cantarelli, Salvador Dura-Bernal, William W. Lytton, Andrew Davison, R. Angus SilverP61 A synaptic story of dynamical information encoding in neural adaptationLuozheng Li, Wenhao Zhang, Yuanyuan Mi, Dahui Wang, Si WuP62 Physical modeling of rule-observant rodent behaviorYoungjo Song, Sol Park, Ilhwan Choi, Jaeseung Jeong, Hee-sup ShinP64 Predictive coding in area V4 and prefrontal cortex explains dynamic discrimination of partially occluded shapesHannah Choi, Anitha Pasupathy, Eric Shea-BrownP65 Stability of FORCE learning on spiking and rate-based networksDongsung Huh, Terrence J. SejnowskiP66 Stabilising STDP in striatal neurons for reliable fast state recognition in noisy environmentsSimon M. Vogt, Arvind Kumar, Robert SchmidtP67 Electrodiffusion in one- and two-compartment neuron models for characterizing cellular effects of electrical stimulationStephen Van Wert, Steven J. SchiffP68 STDP improves speech recognition capabilities in spiking recurrent circuits parameterized via differential evolution Markov Chain Monte CarloRichard Veale, Matthias ScheutzP69 Bidirectional transformation between dominant cortical neural activities and phase difference distributionsSang Wan LeeP70 Maturation of sensory networks through homeostatic structural plasticityJ\'ulia Gallinaro, Stefan RotterP71 Corticothalamic dynamics: structure, number of solutions and stability of steady-state solutions in the space of synaptic couplingsPaula Sanz-Leon, Peter A. RobinsonP72 Optogenetic versus electrical stimulation of the parkinsonian basal ganglia. Computational studyLeonid L. Rubchinsky, Chung Ching Cheung, Shivakeshavan Ratnadurai-GiridharanP73 Exact spike-timing distribution reveals higher-order interactions of neuronsSafura Rashid Shomali, Majid Nili Ahmadabadi, Hideaki Shimazaki, S. Nader RasuliP74 Neural mechanism of visual perceptual learning using a multi-layered neural networkXiaochen Zhao, Malte J. RaschP75 Inferring collective spiking dynamics from mostly unobserved systemsJens Wilting, Viola PriesemannP76 How to infer distributions in the brain from subsampled observationsAnna Levina, Viola PriesemannP77 Influences of embedding and estimation strategies on the inferred memory of single spiking neuronsLucas Rudelt, Joseph T. Lizier, Viola PriesemannP78 A nearest-neighbours based estimator for transfer entropy between spike trainsJoseph T. Lizier, Richard E. Spinney, Mikail Rubinov, Michael Wibral, Viola PriesemannP79 Active learning of psychometric functions with multinomial logistic modelsJi Hyun Bak, Jonathan PillowP81 Inferring low-dimensional network dynamics with variational latent Gaussian processYuan Zaho, Il Memming ParkP82 Computational investigation of energy landscapes in the resting state subcortical brain networkJiyoung Kang, Hae-Jeong ParkP83 Local repulsive interaction between retinal ganglion cells can generate a consistent spatial periodicity of orientation mapJaeson Jang, Se-Bum PaikP84 Phase duration of bistable perception reveals intrinsic time scale of perceptual decision under noisy conditionWoochul Choi, Se-Bum PaikP85 Feedforward convergence between retina and primary visual cortex can determine the structure of orientation mapChangju Lee, Jaeson Jang, Se-Bum PaikP86 Computational method classifying neural network activity patterns for imaging dataMin Song, Hyeonsu Lee, Se-Bum PaikP87 Symmetry of spike-timing-dependent-plasticity kernels regulates volatility of memoryYoungjin Park, Woochul Choi, Se-Bum PaikP88 Effects of time-periodic coupling strength on the first-spike latency dynamics of a scale-free network of stochastic Hodgkin-Huxley neuronsErgin Yilmaz, Veli Baysal, Mahmut OzerP89 Spectral properties of spiking responses in V1 and V4 change within the trial and are highly relevant for behavioral performanceVeronika Koren, Klaus ObermayerP90 Methods for building accurate models of individual neuronsDaniel Saska, Thomas NowotnyP91 A full size mathematical model of the early olfactory system of honeybeesHo Ka Chan, Alan Diamond, Thomas NowotnyP92 Stimulation-induced tuning of ongoing oscillations in spiking neural networksChristoph S. Herrmann, Micah M. Murray, Silvio Ionta, Axel Hutt, J\'er\'emie LefebvreP93 Decision-specific sequences of neural activity in balanced random networks driven by structured sensory inputPhilipp Weidel, Renato Duarte, Abigail MorrisonP94 Modulation of tuning induced by abrupt reduction of SST cell activityJung H. Lee, Ramakrishnan Iyer, Stefan MihalasP95 The functional role of VIP cell activation during locomotionJung H. Lee, Ramakrishnan Iyer, Christof Koch, Stefan MihalasP96 Stochastic inference with spiking neural networksMihai A. Petrovici, Luziwei Leng, Oliver Breitwieser, David St\"ockel, Ilja Bytschok, Roman Martel, Johannes Bill, Johannes Schemmel, Karlheinz MeierP97 Modeling orientation-selective electrical stimulation with retinal prosthesesTimothy B. Esler, Anthony N. Burkitt, David B. Grayden, Robert R. Kerr, Bahman Tahayori, Hamish MeffinP98 Ion channel noise can explain firing correlation in auditory nervesBahar Moezzi, Nicolangelo Iannella, Mark D. McDonnellP99 Limits of temporal encoding of thalamocortical inputs in a neocortical microcircuitMax Nolte, Michael W. Reimann, Eilif Muller, Henry MarkramP100 On the representation of arm reaching movements: a computational modelAntonio Parziale, Rosa Senatore, Angelo MarcelliP101 A computational model for investigating the role of cerebellum in acquisition and retention of motor behaviorRosa Senatore, Antonio Parziale, Angelo MarcelliP102 The emergence of semantic categories from a large-scale brain network of semantic knowledgeK. Skiker, M. MaoueneP103 Multiscale modeling of M1 multitarget pharmacotherapy for dystoniaSamuel A. Neymotin, Salvador Dura-Bernal, Alexandra Seidenstein, Peter Lakatos, Terence D. Sanger, William W. LyttonP104 Effect of network size on computational capacitySalvador Dura-Bernal, Rosemary J. Menzies, Campbell McLauchlan, Sacha J. van Albada, David J. Kedziora, Samuel Neymotin, William W. Lytton, Cliff C. KerrP105 NetPyNE: a Python package for NEURON to facilitate development and parallel simulation of biological neuronal networksSalvador Dura-Bernal, Benjamin A. Suter, Samuel A. Neymotin, Cliff C. Kerr, Adrian Quintana, Padraig Gleeson, Gordon M. G. Shepherd, William W. LyttonP107 Inter-areal and inter-regional inhomogeneity in co-axial anisotropy of Cortical Point Spread in human visual areasJuhyoung Ryu, Sang-Hun LeeP108 Two bayesian quanta of uncertainty explain the temporal dynamics of cortical activity in the non-sensory areas during bistable perceptionJoonwon Lee, Sang-Hun LeeP109 Optimal and suboptimal integration of sensory and value information in perceptual decision makingHyang Jung Lee, Sang-Hun LeeP110 A Bayesian algorithm for phoneme Perception and its neural implementationDaeseob Lim, Sang-Hun LeeP111 Complexity of EEG signals is reduced during unconsciousness induced by ketamine and propofolJisung Wang, Heonsoo LeeP112 Self-organized criticality of neural avalanche in a neural model on complex networksNam Jung, Le Anh Quang, Seung Eun Maeng, Tae Ho Lee, Jae Woo LeeP113 Dynamic alterations in connection topology of the hippocampal network during ictal-like epileptiform activity in an in vitro rat modelChang-hyun Park, Sora Ahn, Jangsup Moon, Yun Seo Choi, Juhee Kim, Sang Beom Jun, Seungjun Lee, Hyang Woon LeeP114 Computational model to replicate seizure suppression effect by electrical stimulationSora Ahn, Sumin Jo, Eunji Jun, Suin Yu, Hyang Woon Lee, Sang Beom Jun, Seungjun LeeP115 Identifying excitatory and inhibitory synapses in neuronal networks from spike trains using sorted local transfer entropyFelix Goetze, Pik-Yin LaiP116 Neural network model for obstacle avoidance based on neuromorphic computational model of boundary vector cell and head direction cellSeonghyun Kim, Jeehyun KwagP117 Dynamic gating of spike pattern propagation by Hebbian and anti-Hebbian spike timing-dependent plasticity in excitatory feedforward network modelHyun Jae Jang, Jeehyun KwagP118 Inferring characteristics of input correlations of cells exhibiting up-down state transitions in the rat striatumMarko Filipovi\'c, Ramon Reig, Ad Aertsen, Gilad Silberberg, Arvind KumarP119 Graph properties of the functional connected brain under the influence of Alzheimer's diseaseClaudia Bachmann, Simone Buttler, Heidi Jacobs, Kim Dillen, Gereon R. Fink, Juraj Kukolja, Abigail MorrisonP120 Learning sparse representations in the olfactory bulbDaniel Kepple, Hamza Giaffar, Dima Rinberg, Steven Shea, Alex KoulakovP121 Functional classification of homologous basal-ganglia networksJyotika Bahuguna,Tom Tetzlaff, Abigail Morrison, Arvind Kumar, Jeanette Hellgren KotaleskiP122 Short term memory based on multistabilityTim Kunze, Andre Peterson, Thomas Kn\"oscheP123 A physiologically plausible, computationally efficient model and simulation software for mammalian motor unitsMinjung Kim, Hojeong KimP125 Decoding laser-induced somatosensory information from EEGJi Sung Park, Ji Won Yeon, Sung-Phil KimP126 Phase synchronization of alpha activity for EEG-based personal authenticationJae-Hwan Kang, Chungho Lee, Sung-Phil KimP129 Investigating phase-lags in sEEG data using spatially distributed time delays in a large-scale brain network modelAndreas Spiegler, Spase Petkoski, Matias J. Palva, Viktor K. JirsaP130 Epileptic seizures in the unfolding of a codimension-3 singularityMaria L. Saggio, Silvan F. Siep, Andreas Spiegler, William C. Stacey, Christophe Bernard, Viktor K. JirsaP131 Incremental dimensional exploratory reasoning under multi-dimensional environmentOh-hyeon Choung, Yong JeongP132 A low-cost model of eye movements and memory in personal visual cognitionYong-il Lee, Jaeseung JeongP133 Complex network analysis of structural connectome of autism spectrum disorder patientsSu Hyun Kim, Mir Jeong, Jaeseung JeongP134 Cognitive motives and the neural correlates underlying human social information transmission, gossipJeungmin Lee, Jaehyung Kwon, Jerald D. Kralik, Jaeseung JeongP135 EEG hyperscanning detects neural oscillation for the social interaction during the economic decision-makingJaehwan Jahng, Dong-Uk Hwang, Jaeseung JeongP136 Detecting purchase decision based on hyperfrontality of the EEGJae-Hyung Kwon, Sang-Min Park, Jaeseung JeongP137 Vulnerability-based critical neurons, synapses, and pathways in the Caenorhabditis elegans connectomeSeongkyun Kim, Hyoungkyu Kim, Jerald D. Kralik, Jaeseung JeongP138 Motif analysis reveals functionally asymmetrical neurons in C. elegans Pyeong Soo Kim, Seongkyun Kim, Hyoungkyu Kim, Jaeseung JeongP139 Computational approach to preference-based serial decision dynamics: do temporal discounting and working memory affect it?Sangsup Yoon, Jaehyung Kwon, Sewoong Lim, Jaeseung JeongP141 Social stress induced neural network reconfiguration affects decision making and learning in zebrafishChoongseok Park, Thomas Miller, Katie Clements, Sungwoo Ahn, Eoon Hye Ji, Fadi A. IssaP142 Descriptive, generative, and hybrid approaches for neural connectivity inference from neural activity dataJeongHun Baek, Shigeyuki Oba, Junichiro Yoshimoto, Kenji Doya, Shin IshiiP145 Divergent-convergent synaptic connectivities accelerate coding in multilayered sensory systemsThiago S. Mosqueiro, Martin F. Strube-Bloss, Brian Smith, Ramon HuertaP146 Swinging networksMichal Hadrava, Jaroslav HlinkaP147 Inferring dynamically relevant motifs from oscillatory stimuli: challenges, pitfalls, and solutionsHannah Bos, Moritz HeliasP148 Spatiotemporal mapping of brain network dynamics during cognitive tasks using magnetoencephalography and deep learningCharles M. Welzig, Zachary J. HarperP149 Multiscale complexity analysis for the segmentation of MRI imagesWon Sup Kim, In-Seob Shin, Hyeon-Man Baek, Seung Kee HanP150 A neuro-computational model of emotional attentionRen\'e Richter, Julien Vitay, Frederick Beuth, Fred H. HamkerP151 Multi-site delayed feedback stimulation in parkinsonian networksKelly Toppin, Yixin GuoP152 Bistability in Hodgkin\textendash{}Huxley-type equationsTatiana Kameneva, Hamish Meffin, Anthony N. Burkitt, David B. GraydenP153 Phase changes in postsynaptic spiking due to synaptic connectivity and short term plasticity: mathematical analysis of frequency dependencyMark D. McDonnell, Bruce P. GrahamP154 Quantifying resilience patterns in brain networks: the importance of directionalityPenelope J. Kale, Leonardo L. GolloP155 Dynamics of rate-model networks with separate excitatory and inhibitory populationsMerav Stern, L. F. AbbottP156 A model for multi-stable dynamics in action recognition modulated by integration of silhouette and shading cuesLeonid A. Fedorov, Martin A. GieseP157 Spiking model for the interaction between action recognition and action executionMohammad Hovaidi Ardestani, Martin GieseP158 Surprise-modulated belief update: how to learn within changing environments?Mohammad Javad Faraji, Kerstin Preuschoff, Wulfram GerstnerP159 A fast, stochastic and adaptive model of auditory nerve responses to cochlear implant stimulationMargriet J. van Gendt, Jeroen J. Briaire, Randy K. Kalkman, Johan H. M. FrijnsP160 Quantitative comparison of graph theoretical measures of simulated and empirical functional brain networksWon Hee Lee, Sophia FrangouP161 Determining discriminative properties of fMRI signals in schizophrenia using highly comparative time-series analysisBen D. Fulcher, Patricia H. P. Tran, Alex FornitoP162 Emergence of narrowband LFP oscillations from completely asynchronous activity during seizures and high-frequency oscillationsStephen V. Gliske, William C. Stacey, Eugene Lim, Katherine A. Holman, Christian G. FinkP163 Neuronal diversity in structure and function: cross-validation of anatomical and physiological classification of retinal ganglion cells in the mouseJinseop S. Kim, Shang Mu, Kevin L. Briggman, H. Sebastian Seung, the EyeWirersP164 Analysis and modelling of transient firing rate changes in area MT in response to rapid stimulus feature changesDetlef Wegener, Lisa Bohnenkamp, Udo A. ErnstP165 Step-wise model fitting accounting for high-resolution spatial measurements: construction of a layer V pyramidal cell model with reduced morphologyTuomo M\"aki-Marttunen, Geir Halnes, Anna Devor, Christoph Metzner, Anders M. Dale, Ole A. Andreassen, Gaute T. EinevollP166 Contributions of schizophrenia-associated genes to neuron firing and cardiac pacemaking: a polygenic modeling approachTuomo M\"aki-Marttunen, Glenn T. Lines, Andy Edwards, Aslak Tveito, Anders M. Dale, Gaute T. Einevoll, Ole A. AndreassenP167 Local field potentials in a 4 \texttimes{} 4 mm2 multi-layered network modelEspen Hagen, Johanna Senk, Sacha J. van Albada, Markus DiesmannP168 A spiking network model explains multi-scale properties of cortical dynamicsMaximilian Schmidt, Rembrandt Bakker, Kelly Shen, Gleb Bezgin, Claus-Christian Hilgetag, Markus Diesmann, Sacha Jennifer van AlbadaP169 Using joint weight-delay spike-timing dependent plasticity to find polychronous neuronal groupsHaoqi Sun, Olga Sourina, Guang-Bin Huang, Felix Klanner, Cornelia DenkP170 Tensor decomposition reveals RSNs in simulated resting state fMRIKatharina Glomb, Adri\'an Ponce-Alvarez, Matthieu Gilson, Petra Ritter, Gustavo DecoP171 Getting in the groove: testing a new model-based method for comparing task-evoked vs resting-state activity in fMRI data on music listeningMatthieu Gilson, Maria AG Witek, Eric F. Clarke, Mads Hansen, Mikkel Wallentin, Gustavo Deco, Morten L. Kringelbach, Peter VuustP172 STochastic engine for pathway simulation (STEPS) on massively parallel processorsGuido Klingbeil, Erik De SchutterP173 Toolkit support for complex parallel spatial stochastic reaction\textendash{}diffusion simulation in STEPSWeiliang Chen, Erik De SchutterP174 Modeling the generation and propagation of Purkinje cell dendritic spikes caused by parallel fiber synaptic inputYunliang Zang, Erik De SchutterP175 Dendritic morphology determines how dendrites are organized into functional subunitsSungho Hong, Akira Takashima, Erik De SchutterP176 A model of Ca2+/calmodulin-dependent protein kinase II activity in long term depression at Purkinje cellsCriseida Zamora, Andrew R. Gallimore, Erik De SchutterP177 Reward-modulated learning of population-encoded vectors for insect-like navigation in embodied agentsDennis Goldschmidt, Poramate Manoonpong, Sakyasingha DasguptaP178 Data-driven neural models part II: connectivity patterns of human seizuresPhilippa J. Karoly, Dean R. Freestone, Daniel Soundry, Levin Kuhlmann, Liam Paninski, Mark CookP179 Data-driven neural models part I: state and parameter estimationDean R. Freestone, Philippa J. Karoly, Daniel Soundry, Levin Kuhlmann, Mark CookP180 Spectral and spatial information processing in human auditory streamingJaejin Lee, Yonatan I. Fishman, Yale E. CohenP181 A tuning curve for the global effects of local perturbations in neural activity: Mapping the systems-level susceptibility of the brainLeonardo L. Gollo, James A. Roberts, Luca CocchiP182 Diverse homeostatic responses to visual deprivation mediated by neural ensemblesYann Sweeney, Claudia ClopathP183 Opto-EEG: a novel method for investigating functional connectome in mouse brain based on optogenetics and high density electroencephalographySoohyun Lee, Woo-Sung Jung, Jee Hyun ChoiP184 Biphasic responses of frontal gamma network to repetitive sleep deprivation during REM sleepBowon Kim, Youngsoo Kim, Eunjin Hwang, Jee Hyun ChoiP185 Brain-state correlate and cortical connectivity for frontal gamma oscillations in top-down fashion assessed by auditory steady-state responseYounginha Jung, Eunjin Hwang, Yoon-Kyu Song, Jee Hyun ChoiP186 Neural field model of localized orientation selective activation in V1James Rankin, Fr\'ed\'eric ChavaneP187 An oscillatory network model of Head direction and Grid cells using locomotor inputsKarthik Soman, Vignesh Muralidharan, V. Srinivasa ChakravarthyP188 A computational model of hippocampus inspired by the functional architecture of basal gangliaKarthik Soman, Vignesh Muralidharan, V. Srinivasa ChakravarthyP189 A computational architecture to model the microanatomy of the striatum and its functional propertiesSabyasachi Shivkumar, Vignesh Muralidharan, V. Srinivasa ChakravarthyP190 A scalable cortico-basal ganglia model to understand the neural dynamics of targeted reachingVignesh Muralidharan, Alekhya Mandali, B. Pragathi Priyadharsini, Hima Mehta, V. Srinivasa ChakravarthyP191 Emergence of radial orientation selectivity from synaptic plasticityCatherine E. Davey, David B. Grayden, Anthony N. BurkittP192 How do hidden units shape effective connections between neurons?Braden A. W. Brinkman, Tyler Kekona, Fred Rieke, Eric Shea-Brown, Michael BuiceP193 Characterization of neural firing in the presence of astrocyte-synapse signalingMaurizio De Pitt\`a, Hugues Berry, Nicolas BrunelP194 Metastability of spatiotemporal patterns in a large-scale network model of brain dynamicsJames A. Roberts, Leonardo L. Gollo, Michael BreakspearP195 Comparison of three methods to quantify detection and discrimination capacity estimated from neural population recordingsGary Marsat, Jordan Drew, Phillip D. Chapman, Kevin C. Daly, Samual P. BradleyP196 Quantifying the constraints for independent evoked and spontaneous NMDA receptor mediated synaptic transmission at individual synapsesSat Byul Seo, Jianzhong Su, Ege T. Kavalali, Justin BlackwellP199 Gamma oscillation via adaptive exponential integrate-and-fire neuronsLieJune Shiau, Laure Buhry, Kanishka BasnayakeP200 Visual face representations during memory retrieval compared to perceptionSue-Hyun Lee, Brandon A. Levy, Chris I. BakerP201 Top-down modulation of sequential activity within packets modeled using avalanche dynamicsTimoth\'ee Leleu, Kazuyuki AiharaQ28 An auto-encoder network realizes sparse features under the influence of desynchronized vascular dynamicsRyan T. Philips, Karishma Chhabria, V. Srinivasa Chakravarthy},
 author = {Sharpee, Tatyana O. and Destexhe, Alain and Kawato, Mitsuo and Sekuli\'c, Vladislav and Skinner, Frances K. and W\'ojcik, Daniel K. and Chintaluri, Chaitanya and Cserp\'an, Dorottya and Somogyv\'ari, Zolt\'an and Kim, Jae Kyoung and Kilpatrick, Zachary P. and Bennett, Matthew R. and Josi\'c, Kresimir and Elices, Irene and Arroyo, David and Levi, Rafael and Rodriguez, Francisco B. and Varona, Pablo and Hwang, Eunjin and Kim, Bowon and Han, Hio-Been and Kim, Tae and McKenna, James T. and Brown, Ritchie E. and McCarley, Robert W. and Choi, Jee Hyun and Rankin, James and Popp, Pamela Osborn and Rinzel, John and Tabas, Alejandro and Rupp, Andr\'e and {Balaguer-Ballester}, Emili and Maturana, Matias I. and Grayden, David B. and Cloherty, Shaun L. and Kameneva, Tatiana and Ibbotson, Michael R. and Meffin, Hamish and Koren, Veronika and Lochmann, Timm and Dragoi, Valentin and Obermayer, Klaus and Psarrou, Maria and Schilstra, Maria and Davey, Neil and {Torben-Nielsen}, Benjamin and Steuber, Volker and Ju, Huiwen and Yu, Jiao and Hines, Michael L. and Chen, Liang and Yu, Yuguo and Kim, Jimin and Leahy, Will and Shlizerman, Eli and Birgiolas, Justas and Gerkin, Richard C. and Crook, Sharon M. and Viriyopase, Atthaphon and Memmesheimer, Raoul-Martin and Gielen, Stan and Dabaghian, Yuri and DeVito, Justin and Perotti, Luca and Kim, Anmo J. and Fenk, Lisa M. and Cheng, Cheng and Maimon, Gaby and Zhao, Chang and Widmer, Yves and Sprecher, Simon and Senn, Walter and Halnes, Geir and {M\"aki-Marttunen}, Tuomo and Keller, Daniel and Pettersen, Klas H. and Andreassen, Ole A. and Einevoll, Gaute T. and Yamada, Yasunori and {Steyn-Ross}, Moira L. and {Alistair Steyn-Ross}, D. and Mejias, Jorge F. and Murray, John D. and Kennedy, Henry and Wang, Xiao-Jing and Kruscha, Alexandra and Grewe, Jan and Benda, Jan and Lindner, Benjamin and Badel, Laurent and Ohta, Kazumi and Tsuchimoto, Yoshiko and Kazama, Hokto and Kahng, B. and Tam, Nicoladie D. and Pollonini, Luca and Zouridakis, George and Soh, Jaehyun and Kim, DaeEun and Yoo, Minsu and Palmer, S. E. and Culmone, Viviana and Bojak, Ingo and Ferrario, Andrea and {Merrison-Hort}, Robert and Borisyuk, Roman and Kim, Chang Sub and Tezuka, Taro and Joo, Pangyu and Rho, Young-Ah and Burton, Shawn D. and Bard Ermentrout, G. and Jeong, Jaeseung and Urban, Nathaniel N. and Marsalek, Petr and Kim, Hoon-Hee and Moon, Seok-hyun and Lee, Do-won and Lee, Sung-beom and Lee, Ji-yong and Molkov, Yaroslav I. and Hamade, Khaldoun and Teka, Wondimu and Barnett, William H. and Kim, Taegyo and Markin, Sergey and Rybak, Ilya A. and Forro, Csaba and Dermutz, Harald and Demk\'o, L\'aszl\'o and V\"or\"os, J\'anos and Babichev, Andrey and Huang, Haiping and {Verduzco-Flores}, Sergio and Dos Santos, Filipa and Andras, Peter and Metzner, Christoph and Schweikard, Achim and Zurowski, Bartosz and Roach, James P. and Sander, Leonard M. and Zochowski, Michal R. and Skilling, Quinton M. and Ognjanovski, Nicolette and Aton, Sara J. and Zochowski, Michal and Wang, Sheng-Jun and Ouyang, Guang and Guang, Jing and Zhang, Mingsha and Michael Wong, K. Y. and Zhou, Changsong and Robinson, Peter A. and {Sanz-Leon}, Paula and Drysdale, Peter M. and Fung, Felix and Abeysuriya, Romesh G. and Rennie, Chris J. and Zhao, Xuelong and Choe, Yoonsuck and Yang, Huei-Fang and Mi, Yuanyuan and Lin, Xiaohan and Wu, Si and Liedtke, Joscha and Schottdorf, Manuel and Wolf, Fred and Yamamura, Yoriko and Wickens, Jeffery R. and Rumbell, Timothy and Ramsey, Julia and Reyes, Amy and Dragulji\'c, Danel and Hof, Patrick R. and Luebke, Jennifer and Weaver, Christina M. and He, Hu and Yang, Xu and Ma, Hailin and Xu, Zhiheng and Wang, Yuzhe and Baek, Kwangyeol and Morris, Laurel S. and Kundu, Prantik and Voon, Valerie and Agnes, Everton J. and Vogels, Tim P. and Podlaski, William F. and Giese, Martin and Kuravi, Pradeep and Vogels, Rufin and Seeholzer, Alexander and Podlaski, William and Ranjan, Rajnish and Vogels, Tim and Torres, Joaquin J. and Baroni, Fabiano and Latorre, Roberto and Gips, Bart and Lowet, Eric and Roberts, Mark J. and {de Weerd}, Peter and Jensen, Ole and {van der Eerden}, Jan and Goodarzinick, Abdorreza and Niry, Mohammad D. and Valizadeh, Alireza and Pariz, Aref and Parsi, Shervin S. and Warburton, Julia M. and Marucci, Lucia and Tamagnini, Francesco and Brown, Jon and {Tsaneva-Atanasova}, Krasimira and Kleberg, Florence I. and Triesch, Jochen and Moezzi, Bahar and Iannella, Nicolangelo and Schaworonkow, Natalie and Plogmacher, Lukas and Goldsworthy, Mitchell R. and Hordacre, Brenton and McDonnell, Mark D. and Ridding, Michael C. and Zapotocky, Martin and Smit, Daniel and Fouquet, Coralie and Trembleau, Alain and Dasgupta, Sakyasingha and Nishikawa, Isao and Aihara, Kazuyuki and Toyoizumi, Taro and Robb, Daniel T. and Mellen, Nick and Toporikova, Natalia and Tang, Rongxiang and Tang, Yi-Yuan and Liang, Guangsheng and Kiser, Seth A. and Howard, James H. and Goncharenko, Julia and Voronenko, Sergej O. and Ahamed, Tosif and Stephens, Greg and Yger, Pierre and Lefebvre, Baptiste and Spampinato, Giulia Lia Beatrice and Esposito, Elric and {et Olivier Marre}, Marcel Stimberg and Choi, Hansol and Song, Min-Ho and Chung, SueYeon and Lee, Dan D. and Sompolinsky, Haim and Phillips, Ryan S. and Smith, Jeffrey and Chatzikalymniou, Alexandra Pierri and Ferguson, Katie and Alex Cayco Gajic, N. and Clopath, Claudia and Angus Silver, R. and Gleeson, Padraig and Marin, Boris and Sadeh, Sadra and Quintana, Adrian and Cantarelli, Matteo and {Dura-Bernal}, Salvador and Lytton, William W. and Davison, Andrew and Li, Luozheng and Zhang, Wenhao and Wang, Dahui and Song, Youngjo and Park, Sol and Choi, Ilhwan and Shin, Hee-sup and Choi, Hannah and Pasupathy, Anitha and {Shea-Brown}, Eric and Huh, Dongsung and Sejnowski, Terrence J. and Vogt, Simon M. and Kumar, Arvind and Schmidt, Robert and Van Wert, Stephen and Schiff, Steven J. and Veale, Richard and Scheutz, Matthias and Lee, Sang Wan and Gallinaro, J\'ulia and Rotter, Stefan and Rubchinsky, Leonid L. and Cheung, Chung Ching and {Ratnadurai-Giridharan}, Shivakeshavan and Shomali, Safura Rashid and Ahmadabadi, Majid Nili and Shimazaki, Hideaki and Nader Rasuli, S. and Zhao, Xiaochen and Rasch, Malte J.},
 doi = {10.1186/s12868-016-0283-6},
 file = {/home/tim/Zotero/storage/XF8GJ5F7/Sharpee et al. - 2016 - 25th Annual Computational Neuroscience Meeting CN.pdf},
 issn = {1471-2202},
 journal = {BMC Neuroscience},
 language = {en},
 month = {August},
 number = {1},
 pages = {54},
 shorttitle = {25th {{Annual Computational Neuroscience Meeting}}},
 title = {25th {{Annual Computational Neuroscience Meeting}}: {{CNS}}-2016},
 volume = {17},
 year = {2016}
}

@article{Shin:2016:ITD:2946645.3007060,
 acmid = {3007060},
 author = {Shin, Hoo-Chang and Lu, Le and Kim, Lauren and Seff, Ari and Yao, Jianhua and Summers, Ronald M.},
 issn = {1532-4435},
 issue_date = {January 2016},
 journal = {J. Mach. Learn. Res.},
 keywords = {convolutional neural networks, deep learning, medical Imaging, natural language processing, topic models},
 month = {January},
 number = {1},
 numpages = {31},
 pages = {3729--3759},
 publisher = {JMLR.org},
 title = {Interleaved Text/Image Deep Mining on a Large-scale Radiology Database for Automated Image Interpretation},
 url = {http://dl.acm.org/citation.cfm?id=2946645.3007060},
 volume = {17},
 year = {2016}
}

@incollection{shoombuatongRevivalInterpretableQSAR2017,
 abstract = {Quantitative structure-activity relationship (QSAR) has been instrumental in aiding medicinal chemists and physical scientists in understanding how modification of substituents at different positions on a molecular structure exert its influence on the observed biological activity and physicochemical property, respectively. QSAR has received great attention owing to its predictive capability and as such efforts had been directed toward obtaining models with high prediction performance. However, to be useful QSAR models need to be informative and interpretable in which the underlying molecular features that contribute to the increase or decrease of the biological activity are revealed by the model. Thus, the aim of this chapter is to briefly review the general concepts of QSAR modeling, its development and discussions on key issues influencing and contributing to the interpretability of QSAR models.},
 address = {Cham},
 author = {Shoombuatong, Watshara and Prathipati, Philip and Owasirikul, Wiwat and Worachartcheewan, Apilak and Simeon, Saw and Anuwongcharoen, Nuttapat and Wikberg, Jarl E. S. and Nantasenamat, Chanin},
 booktitle = {Advances in {{QSAR Modeling}}: {{Applications}} in {{Pharmaceutical}}, {{Chemical}}, {{Food}}, {{Agricultural}} and {{Environmental Sciences}}},
 doi = {10.1007/978-3-319-56850-8_1},
 editor = {Roy, Kunal},
 isbn = {978-3-319-56850-8},
 keywords = {Cheminformatics,Chemogenomics,Data mining,Drug design,Drug discovery,Interpretable,Machine learning,Proteochemometrics,QSAR,QSPR,Quantitative structure-activity relationship,Quantitative structure-property relationship},
 language = {en},
 pages = {3-55},
 publisher = {{Springer International Publishing}},
 series = {Challenges and {{Advances}} in {{Computational Chemistry}} and {{Physics}}},
 title = {Towards the {{Revival}} of {{Interpretable QSAR Models}}},
 year = {2017}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
 abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
 author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
 booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
 editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
 isbn = {978-3-030-02628-8},
 keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
 language = {en},
 pages = {133-140},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
 year = {2018}
}

@inproceedings{Simmons:2009:LLA:1480945.1480949,
 acmid = {1480949},
 address = {New York, NY, USA},
 author = {Simmons, Robert J. and Pfenning, Frank},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
 doi = {10.1145/1480945.1480949},
 isbn = {978-1-60558-327-3},
 keywords = {abstract interpretation, bottom-up linear logic programming, operational semantics},
 location = {Savannah, GA, USA},
 numpages = {12},
 pages = {9--20},
 publisher = {ACM},
 series = {PEPM '09},
 title = {Linear Logical Approximations},
 url = {http://doi.acm.org/10.1145/1480945.1480949},
 year = {2009}
}

@inproceedings{Singh:2019:EES:3289600.3290620,
 acmid = {3290620},
 address = {New York, NY, USA},
 author = {Singh, Jaspreet and Anand, Avishek},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/3289600.3290620},
 isbn = {978-1-4503-5940-5},
 keywords = {explainable search, interpretability, neural ranking models},
 location = {Melbourne VIC, Australia},
 numpages = {4},
 pages = {770--773},
 publisher = {ACM},
 series = {WSDM '19},
 title = {EXS: Explainable Search Using Local Model Agnostic Interpretability},
 url = {http://doi.acm.org/10.1145/3289600.3290620},
 year = {2019}
}

@incollection{skienaMachineLearning2017,
 abstract = {For much of my career, I was highly suspicious of the importance of machine learning. I sat through many talks over the years, with grandiose claims and very meager results. But it is clear that the tide has turned. The most interesting work in computer science today revolves around machine learning, both powerful new algorithms and exciting new applications.},
 address = {Cham},
 author = {Skiena, Steven S.},
 booktitle = {The {{Data Science Design Manual}}},
 doi = {10.1007/978-3-319-55444-0_11},
 editor = {Skiena, Steven S.},
 isbn = {978-3-319-55444-0},
 language = {en},
 pages = {351-390},
 publisher = {{Springer International Publishing}},
 series = {Texts in {{Computer Science}}},
 title = {Machine {{Learning}}},
 year = {2017}
}

@inproceedings{Sklar:2018:ETA:3284432.3284470,
 acmid = {3284470},
 address = {New York, NY, USA},
 author = {Sklar, Elizabeth I. and Azhar, Mohammad Q.},
 booktitle = {Proceedings of the 6th International Conference on Human-Agent Interaction},
 doi = {10.1145/3284432.3284470},
 isbn = {978-1-4503-5953-5},
 keywords = {computational argumentation, explainable ai, human-robot interaction},
 location = {Southampton, United Kingdom},
 numpages = {9},
 pages = {277--285},
 publisher = {ACM},
 series = {HAI '18},
 title = {Explanation Through Argumentation},
 url = {http://doi.acm.org/10.1145/3284432.3284470},
 year = {2018}
}

@article{SPR20192019,
 doi = {10.1007/s00247-019-04376-7},
 file = {/home/tim/Zotero/storage/TX23AHNA/2019 - SPR 2019.pdf},
 issn = {1432-1998},
 journal = {Pediatric Radiology},
 language = {en},
 month = {April},
 number = {1},
 pages = {1-245},
 title = {{{SPR}} 2019},
 volume = {49},
 year = {2019}
}

@inproceedings{Sridhara:2010:TAG:1858996.1859006,
 acmid = {1859006},
 address = {New York, NY, USA},
 author = {Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and Vijay-Shanker, K.},
 booktitle = {Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
 doi = {10.1145/1858996.1859006},
 isbn = {978-1-4503-0116-9},
 keywords = {comment generation, method summarization, natural language program analysis},
 location = {Antwerp, Belgium},
 numpages = {10},
 pages = {43--52},
 publisher = {ACM},
 series = {ASE '10},
 title = {Towards Automatically Generating Summary Comments for Java Methods},
 url = {http://doi.acm.org/10.1145/1858996.1859006},
 year = {2010}
}

@inproceedings{St.Amant:2003:BEI:604045.604074,
 acmid = {604074},
 address = {New York, NY, USA},
 author = {St. Amant, Robert and Dinardo, Michael D. and Buckner, Nickie},
 booktitle = {Proceedings of the 8th International Conference on Intelligent User Interfaces},
 doi = {10.1145/604045.604074},
 isbn = {1-58113-586-6},
 keywords = {data mountain, efficiency, interpretability, navigation, user interface design},
 location = {Miami, Florida, USA},
 numpages = {8},
 pages = {181--188},
 publisher = {ACM},
 series = {IUI '03},
 title = {Balancing Efficiency and Interpretability in an Interactive Statistical Assistant},
 url = {http://doi.acm.org/10.1145/604045.604074},
 year = {2003}
}

@inproceedings{Strobel:2018:AAE:3278721.3278788,
 acmid = {3278788},
 address = {New York, NY, USA},
 author = {Strobel, Martin},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 doi = {10.1145/3278721.3278788},
 isbn = {978-1-4503-6012-8},
 keywords = {axiomatic approach, explainable machine learning},
 location = {New Orleans, LA, USA},
 numpages = {2},
 pages = {380--381},
 publisher = {ACM},
 series = {AIES '18},
 title = {An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract},
 url = {http://doi.acm.org/10.1145/3278721.3278788},
 year = {2018}
}

@inproceedings{Tamagnini:2017:IBC:3077257.3077260,
 acmid = {3077260},
 address = {New York, NY, USA},
 articleno = {6},
 author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
 booktitle = {Proceedings of the 2Nd Workshop on Human-In-the-Loop Data Analytics},
 doi = {10.1145/3077257.3077260},
 isbn = {978-1-4503-5029-7},
 keywords = {classification, explanation, machine learning, visual analytics},
 location = {Chicago, IL, USA},
 numpages = {6},
 pages = {6:1--6:6},
 publisher = {ACM},
 series = {HILDA'17},
 title = {Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations},
 url = {http://doi.acm.org/10.1145/3077257.3077260},
 year = {2017}
}

@inproceedings{Tan:2018:IAD:3278721.3278802,
 acmid = {3278802},
 address = {New York, NY, USA},
 author = {Tan, Sarah},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 doi = {10.1145/3278721.3278802},
 isbn = {978-1-4503-6012-8},
 keywords = {algorithmic fairness, black-box models, interpretability, model distillation, transparency},
 location = {New Orleans, LA, USA},
 numpages = {2},
 pages = {382--383},
 publisher = {ACM},
 series = {AIES '18},
 title = {Interpretable Approaches to Detect Bias in Black-Box Models},
 url = {http://doi.acm.org/10.1145/3278721.3278802},
 year = {2018}
}

@inproceedings{Tang:2018:EVM:3302425.3302476,
 acmid = {3302476},
 address = {New York, NY, USA},
 articleno = {52},
 author = {Tang, Haijing and Wang, Yiru and Yang, Xu},
 booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
 doi = {10.1145/3302425.3302476},
 isbn = {978-1-4503-6625-0},
 keywords = {Artificial Intelligence, Convolutional Neural Network, Machine Learning, Visualization},
 location = {Sanya, China},
 numpages = {5},
 pages = {52:1--52:5},
 publisher = {ACM},
 series = {ACAI 2018},
 title = {Evaluation of Visualization Methods' Effect on Convolutional Neural Networks Research},
 url = {http://doi.acm.org/10.1145/3302425.3302476},
 year = {2018}
}

@article{tienInternetThingsRealTime2017,
 abstract = {In several earlier papers, the author defined and detailed the concept of a servgood, which can be thought of as a physical good or product enveloped by a services-oriented layer that makes the good smarter or more adaptable and customizable for a particular use. Adding another layer of physical sensors could then enhance its smartness and intelligence, especially if it were to be connected with other servgoods\textemdash{}thus, constituting an Internet of Things (IoT) or servgoods. More importantly, real-time decision making is central to the Internet of Things; it is about decision informatics and embraces the advanced technologies of sensing (i.e., Big Data), processing (i.e., real-time analytics), reacting (i.e., real-time decision-making), and learning (i.e., deep learning). Indeed, real-time decision making (RTDM) is becoming an integral aspect of IoT and artificial intelligence (AI), including its improving abilities at voice and video recognition, speech and predictive synthesis, and language and social-media understanding. These three key and mutually supportive technologies\textemdash{}IoT, RTDM, and AI\textemdash{}are considered herein, including their progress to date.},
 author = {Tien, James M.},
 doi = {10.1007/s40745-017-0112-5},
 issn = {2198-5812},
 journal = {Annals of Data Science},
 keywords = {Artificial intelligence,Goods,Internet of things,Real-time decision making,Servgoods,Services},
 language = {en},
 month = {June},
 number = {2},
 pages = {149-178},
 title = {Internet of {{Things}}, {{Real}}-{{Time Decision Making}}, and {{Artificial Intelligence}}},
 volume = {4},
 year = {2017}
}

@inproceedings{Treanor:2012:MG:2282338.2282347,
 acmid = {2282347},
 address = {New York, NY, USA},
 author = {Treanor, Mike and Schweizer, Bobby and Bogost, Ian and Mateas, Michael},
 booktitle = {Proceedings of the International Conference on the Foundations of Digital Games},
 doi = {10.1145/2282338.2282347},
 isbn = {978-1-4503-1333-9},
 keywords = {game design, game interpretation, procedural rhetoric},
 location = {Raleigh, North Carolina},
 numpages = {8},
 pages = {18--25},
 publisher = {ACM},
 series = {FDG '12},
 title = {The Micro-rhetorics of Game-o-Matic},
 url = {http://doi.acm.org/10.1145/2282338.2282347},
 year = {2012}
}

@incollection{turnerControllingCreations2019,
 abstract = {Turner explains how in order to implement constraints into AI directly, we will need to address both moral and technical questions: Which norms should be chosen? How can these be implemented? Potential basic laws for robots include: a law of identification, requiring that AI makes its status clear; a law of explanation, requiring that at least some parts of AI's reasoning be divulged; a laws on avoiding bias; and a law setting out any limits to areas where AI can operate. Finally, a kill switch law might make it mandatory that AI systems include a mechanism for safely interrupting their processes or operations, either temporarily or permanently.},
 address = {Cham},
 author = {Turner, Jacob},
 booktitle = {Robot {{Rules}} : {{Regulating Artificial Intelligence}}},
 doi = {10.1007/978-3-319-96235-1_8},
 editor = {Turner, Jacob},
 isbn = {978-3-319-96235-1},
 language = {en},
 pages = {319-369},
 publisher = {{Springer International Publishing}},
 title = {Controlling the {{Creations}}},
 year = {2019}
}

@inproceedings{Vartak:2018:MSS:3183713.3196934,
 acmid = {3196934},
 address = {New York, NY, USA},
 author = {Vartak, Manasi and F. da Trindade, Joana M. and Madden, Samuel and Zaharia, Matei},
 booktitle = {Proceedings of the 2018 International Conference on Management of Data},
 doi = {10.1145/3183713.3196934},
 isbn = {978-1-4503-4703-7},
 keywords = {machine learning, model diagnosis, model interpretability, systems for machine learning},
 location = {Houston, TX, USA},
 numpages = {16},
 pages = {1285--1300},
 publisher = {ACM},
 series = {SIGMOD '18},
 title = {MISTIQUE: A System to Store and Query Model Intermediates for Model Diagnosis},
 url = {http://doi.acm.org/10.1145/3183713.3196934},
 year = {2018}
}

@inproceedings{Wang:2016:TCW:2906831.2906852,
 acmid = {2906852},
 address = {Piscataway, NJ, USA},
 author = {Wang, Ning and Pynadath, David V. and Hill, Susan G.},
 booktitle = {The Eleventh ACM/IEEE International Conference on Human Robot Interaction},
 isbn = {978-1-4673-8370-7},
 keywords = {explainable a.i., human-robot interaction, pomdp, trust},
 location = {Christchurch, New Zealand},
 numpages = {8},
 pages = {109--116},
 publisher = {IEEE Press},
 series = {HRI '16},
 title = {Trust Calibration Within a Human-Robot Team: Comparing Automatically Generated Explanations},
 url = {http://dl.acm.org/citation.cfm?id=2906831.2906852},
 year = {2016}
}

@inproceedings{Wang:2018:TTE:3178876.3186066,
 acmid = {3186066},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Wang, Xiang and He, Xiangnan and Feng, Fuli and Nie, Liqiang and Chua, Tat-Seng},
 booktitle = {Proceedings of the 2018 World Wide Web Conference},
 doi = {10.1145/3178876.3186066},
 isbn = {978-1-4503-5639-8},
 keywords = {embedding-based model, explainable recommendation, neural attention network, tree-based model},
 location = {Lyon, France},
 numpages = {10},
 pages = {1543--1552},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '18},
 title = {TEM: Tree-enhanced Embedding Model for Explainable Recommendation},
 url = {https://doi.org/10.1145/3178876.3186066},
 year = {2018}
}

@inproceedings{Wang:2019:DTU:3290605.3300831,
 acmid = {3300831},
 address = {New York, NY, USA},
 articleno = {601},
 author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 doi = {10.1145/3290605.3300831},
 isbn = {978-1-4503-5970-2},
 keywords = {clinical decision making, decision making, explainable artificial intelligence, explanations, intelligibility},
 location = {Glasgow, Scotland Uk},
 numpages = {15},
 pages = {601:1--601:15},
 publisher = {ACM},
 series = {CHI '19},
 title = {Designing Theory-Driven User-Centric Explainable AI},
 url = {http://doi.acm.org/10.1145/3290605.3300831},
 year = {2019}
}

@inproceedings{weberInvestigatingTextualCaseBased2018,
 abstract = {This paper demonstrates how case-based reasoning (CBR) can be used for an explainable artificial intelligence (XAI) approach to justify solutions produced by an opaque learning method (i.e., target method), particularly in the context of unstructured textual data. Our general hypothesis is twofold: (1) There exists patterns in the relationship between problems and solutions and there should be data or a body of knowledge that describes how problems and solutions relate; and (2) the identification, manipulation, and learning of such patterns through case features can help create and reuse explanations for solutions produced by the target method. When the target method relies on neural network architectures (e.g., deep learning), the resulting latent space (i.e., word embeddings) becomes useful for finding patterns and semantic relatedness in textual data. In the proposed approach, case problems are input-output pairs from the target method, and case solutions are explanations. We exemplify our approach by explaining recommended citations from Citeomatic - a multi-layer neural-network architecture from the Allen Institute for Artificial Intelligence. Citation analysis is the body of knowledge that describes how query documents (i.e., inputs) relate to recommended citations (i.e., outputs). We build cases and similarity assessment to learn features that represent patterns between problems and solutions that can lead to the reuse of corresponding explanations. The illustrative implementation we present becomes an explanation-augmented citation recommender that targets human-computer trust.},
 author = {Weber, Rosina O. and Johs, Adam J. and Li, Jianfei and Huang, Kent},
 booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
 editor = {Cox, Michael T. and Funk, Peter and Begum, Shahina},
 isbn = {978-3-030-01081-2},
 keywords = {Case-Based reasoning,Citation recommendation,Explainable artificial intelligence,Human-Computer trust,Semantic relatedness,Textual Case-Based reasoning,Word embeddings},
 language = {en},
 pages = {431-447},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Investigating {{Textual Case}}-{{Based XAI}}},
 year = {2018}
}

@inproceedings{Weisz:2019:BTS:3301275.3302290,
 acmid = {3302290},
 address = {New York, NY, USA},
 author = {Weisz, Justin D. and Jain, Mohit and Joshi, Narendra Nath and Johnson, James and Lange, Ingrid},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302290},
 isbn = {978-1-4503-6272-6},
 keywords = {conversational agents, explainable AI, mechanical turk},
 location = {Marina del Ray, California},
 numpages = {12},
 pages = {448--459},
 publisher = {ACM},
 series = {IUI '19},
 title = {BigBlueBot: Teaching Strategies for Successful Human-agent Interactions},
 url = {http://doi.acm.org/10.1145/3301275.3302290},
 year = {2019}
}

@article{wengMedicalSubdomainClassification2017,
 abstract = {BackgroundThe medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note.MethodsWe constructed the pipeline using the clinical NLP system, clinical Text Analysis and Knowledge Extraction System (cTAKES), the Unified Medical Language System (UMLS) Metathesaurus, Semantic Network, and learning algorithms to extract features from two datasets \textemdash{} clinical notes from Integrating Data for Analysis, Anonymization, and Sharing (iDASH) data repository (n = 431) and Massachusetts General Hospital (MGH) (n = 91,237), and built medical subdomain classifiers with different combinations of data representation methods and supervised learning algorithms. We evaluated the performance of classifiers and their portability across the two datasets.ResultsThe convolutional recurrent neural network with neural word embeddings trained-medical subdomain classifier yielded the best performance measurement on iDASH and MGH datasets with area under receiver operating characteristic curve (AUC) of 0.975 and 0.991, and F1 scores of 0.845 and 0.870, respectively. Considering better clinical interpretability, linear support vector machine-trained medical subdomain classifier using hybrid bag-of-words and clinically relevant UMLS concepts as the feature representation, with term frequency-inverse document frequency (tf-idf)-weighting, outperformed other shallow learning classifiers on iDASH and MGH datasets with AUC of 0.957 and 0.964, and F1 scores of 0.932 and 0.934 respectively. We trained classifiers on one dataset, applied to the other dataset and yielded the threshold of F1 score of 0.7 in classifiers for half of the medical subdomains we studied.ConclusionOur study shows that a supervised learning-based NLP approach is useful to develop medical subdomain classifiers. The deep learning algorithm with distributed word representation yields better performance yet shallow learning algorithms with the word and concept representation achieves comparable performance with better clinical interpretability. Portable classifiers may also be used across datasets from different institutions.},
 author = {Weng, Wei-Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
 doi = {10.1186/s12911-017-0556-8},
 file = {/home/tim/Zotero/storage/8NGVVGDF/Weng et al. - 2017 - Medical subdomain classification of clinical notes.pdf},
 issn = {1472-6947},
 journal = {BMC Medical Informatics and Decision Making},
 keywords = {Computer-assisted,Deep Learning,Distributed Representation,Machine Learning,Medical Decision Making,Natural Language Processing,Unified Medical Language System},
 language = {en},
 month = {December},
 number = {1},
 pages = {155},
 title = {Medical Subdomain Classification of Clinical Notes Using a Machine Learning-Based Natural Language Processing Approach},
 volume = {17},
 year = {2017}
}

@inproceedings{Wiegand:2019:IDY:3290607.3312817,
 acmid = {3312817},
 address = {New York, NY, USA},
 articleno = {LBW0163},
 author = {Wiegand, Gesa and Schmidmaier, Matthias and Weber, Thomas and Liu, Yuanting and Hussmann, Heinrich},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 doi = {10.1145/3290607.3312817},
 isbn = {978-1-4503-5971-9},
 keywords = {autonomous driving, explainability, mental model, situation awareness},
 location = {Glasgow, Scotland Uk},
 numpages = {6},
 pages = {LBW0163:1--LBW0163:6},
 publisher = {ACM},
 series = {CHI EA '19},
 title = {I Drive - You Trust: Explaining Driving Behavior Of Autonomous Cars},
 url = {http://doi.acm.org/10.1145/3290607.3312817},
 year = {2019}
}

@incollection{wodeckiArtificialIntelligenceMethods2019,
 abstract = {Artificial intelligence (AI) is a fascinating concept whose origins can be found in the mid-twentieth century. It is an interdisciplinary field, integrating the efforts of logicians, mathematicians, computer scientists, psychologists and, more recently, managers and ethicists. Developing dynamically in the dimension of methods as well as technology, on the one hand, raises many hopes; on the other hand, it raises many fears and controversies (compare e.g. Bostrom 2014), particularly among investors who are interested in ventures with high development potential, yet they are afraid to invest in projects they simply do not understand.},
 address = {Cham},
 author = {Wodecki, Andrzej},
 booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
 doi = {10.1007/978-3-319-91596-8_2},
 editor = {Wodecki, Andrzej},
 isbn = {978-3-319-91596-8},
 language = {en},
 pages = {71-132},
 publisher = {{Springer International Publishing}},
 title = {Artificial {{Intelligence Methods}} and {{Techniques}}},
 year = {2019}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
 abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
 address = {Cham},
 author = {Wodecki, Andrzej},
 booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
 doi = {10.1007/978-3-319-91596-8_3},
 editor = {Wodecki, Andrzej},
 isbn = {978-3-319-91596-8},
 language = {en},
 pages = {133-246},
 publisher = {{Springer International Publishing}},
 title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
 year = {2019}
}

@inproceedings{Wolf:2019:EST:3301275.3302317,
 acmid = {3302317},
 address = {New York, NY, USA},
 author = {Wolf, Christine T.},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 doi = {10.1145/3301275.3302317},
 isbn = {978-1-4503-6272-6},
 keywords = {XAI, aging-in-place, explainability scenarios, scenario-based design},
 location = {Marina del Ray, California},
 numpages = {6},
 pages = {252--257},
 publisher = {ACM},
 series = {IUI '19},
 title = {Explainability Scenarios: Towards Scenario-based XAI Design},
 url = {http://doi.acm.org/10.1145/3301275.3302317},
 year = {2019}
}

@inproceedings{Wu:2016:ERR:2872518.2889400,
 acmid = {2889400},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Wu, Chao-Yuan and Beutel, Alex and Ahmed, Amr and Smola, Alexander J.},
 booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
 doi = {10.1145/2872518.2889400},
 isbn = {978-1-4503-4144-8},
 keywords = {co-clustering, joint modeling, recommendation systems},
 location = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
 numpages = {2},
 pages = {127--128},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '16 Companion},
 title = {Explaining Reviews and Ratings with PACO: Poisson Additive Co-Clustering},
 url = {https://doi.org/10.1145/2872518.2889400},
 year = {2016}
}

@inproceedings{Wu:2018:SDN:3178876.3185995,
 acmid = {3185995},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Wu, Huijun and Wang, Chen and Yin, Jie and Lu, Kai and Zhu, Liming},
 booktitle = {Proceedings of the 2018 World Wide Web Conference},
 doi = {10.1145/3178876.3185995},
 isbn = {978-1-4503-5639-8},
 keywords = {decision boundary, deep neural networks, interpretability, model sharing},
 location = {Lyon, France},
 numpages = {10},
 pages = {177--186},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '18},
 title = {Sharing Deep Neural Network Models with Interpretation},
 url = {https://doi.org/10.1145/3178876.3185995},
 year = {2018}
}

@article{Wyner:2017:ESA:3122009.3153004,
 acmid = {3153004},
 author = {Wyner, Abraham J. and Olson, Matthew and Bleich, Justin and Mease, David},
 issn = {1532-4435},
 issue_date = {January 2017},
 journal = {J. Mach. Learn. Res.},
 keywords = {adaboost, classification, overfitting, random forests, tree-ensembles},
 month = {January},
 number = {1},
 numpages = {33},
 pages = {1558--1590},
 publisher = {JMLR.org},
 title = {Explaining the Success of Adaboost and Random Forests As Interpolating Classifiers},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3153004},
 volume = {18},
 year = {2017}
}

@inproceedings{Xiong:2017:ENA:3077136.3080809,
 acmid = {3080809},
 address = {New York, NY, USA},
 author = {Xiong, Chenyan and Dai, Zhuyun and Callan, Jamie and Liu, Zhiyuan and Power, Russell},
 booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 doi = {10.1145/3077136.3080809},
 isbn = {978-1-4503-5022-8},
 keywords = {embedding, kernel pooling, neural ir, ranking, relevance model},
 location = {Shinjuku, Tokyo, Japan},
 numpages = {10},
 pages = {55--64},
 publisher = {ACM},
 series = {SIGIR '17},
 title = {End-to-End Neural Ad-hoc Ranking with Kernel Pooling},
 url = {http://doi.acm.org/10.1145/3077136.3080809},
 year = {2017}
}

@inproceedings{Yang:2018:MNN:3194452.3194473,
 acmid = {3194473},
 address = {New York, NY, USA},
 author = {Yang, Shanliang and Sun, Qi and Zhou, Huyong and Gong, Zhengjie},
 booktitle = {Proceedings of the 2018 International Conference on Computing and Artificial Intelligence},
 doi = {10.1145/3194452.3194473},
 isbn = {978-1-4503-6419-5},
 keywords = {BiLSTM, BiLSTM-CNN, CNN, Neural network, Sentiment recognition, Word embedding},
 location = {Chengdu, China},
 numpages = {7},
 pages = {23--29},
 publisher = {ACM},
 series = {ICCAI 2018},
 title = {A Multi-Layer Neural Network Model Integrating BiLSTM and CNN for Chinese Sentiment Recognition},
 url = {http://doi.acm.org/10.1145/3194452.3194473},
 year = {2018}
}

@article{yankovskayaTradeoffSearchMethods2017,
 abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
 author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
 doi = {10.1134/S1054661817020134},
 issn = {1555-6212},
 journal = {Pattern Recognition and Image Analysis},
 keywords = {accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,machine learning,metaheuristic,pattern recognition,synergy},
 language = {en},
 month = {April},
 number = {2},
 pages = {243-265},
 title = {Tradeoff Search Methods between Interpretability and Accuracy of the Identification Fuzzy Systems Based on Rules},
 volume = {27},
 year = {2017}
}

@inproceedings{Zaheer:2019:UHS:3289600.3291036,
 acmid = {3291036},
 address = {New York, NY, USA},
 author = {Zaheer, Manzil and Ahmed, Amr and Wang, Yuan and Silva, Daniel and Najork, Marc and Wu, Yuchen and Sanan, Shibani and Chatterjee, Surojit},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 doi = {10.1145/3289600.3291036},
 isbn = {978-1-4503-5940-5},
 keywords = {sequence clustering, interpretable recurrent neural network, topic models},
 location = {Melbourne VIC, Australia},
 numpages = {9},
 pages = {186--194},
 publisher = {ACM},
 series = {WSDM '19},
 title = {Uncovering Hidden Structure in Sequence Data via Threading Recurrent Models},
 url = {http://doi.acm.org/10.1145/3289600.3291036},
 year = {2019}
}

@inproceedings{Zantedeschi:2017:EDA:3128572.3140449,
 acmid = {3140449},
 address = {New York, NY, USA},
 author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
 booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
 doi = {10.1145/3128572.3140449},
 isbn = {978-1-4503-5202-4},
 keywords = {adversarial learning, deep neural network, defenses, model security},
 location = {Dallas, Texas, USA},
 numpages = {11},
 pages = {39--49},
 publisher = {ACM},
 series = {AISec '17},
 title = {Efficient Defenses Against Adversarial Attacks},
 url = {http://doi.acm.org/10.1145/3128572.3140449},
 year = {2017}
}

@article{zerilliTransparencyAlgorithmicHuman2018,
 abstract = {We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems, show in what respects AI fares little worse or better and argue that at least some regulatory proposals for explainable AI could end up setting the bar higher than is necessary or indeed helpful. The demands of practical reason require the justification of action to be pitched at the level of practical reason. Decision tools that support or supplant practical reasoning should not be expected to aim higher than this. We cast this desideratum in terms of Daniel Dennett's theory of the ``intentional stance'' and argue that since the justification of action for human purposes takes the form of intentional stance explanation, the justification of algorithmic decisions should take the same form. In practice, this means that the sorts of explanations for algorithmic decisions that are analogous to intentional stance explanations should be preferred over ones that aim at the architectural innards of a decision tool.},
 author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
 doi = {10.1007/s13347-018-0330-6},
 issn = {2210-5441},
 journal = {Philosophy \& Technology},
 keywords = {Algorithmic decision-making,Explainable AI,Intentional stance,Transparency},
 language = {en},
 month = {September},
 shorttitle = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}},
 title = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}: {{Is There}} a {{Double Standard}}?},
 year = {2018}
}

@inproceedings{Zhang:2018:DAP:3301551.3301588,
 acmid = {3301588},
 address = {New York, NY, USA},
 author = {Zhang, Chiliang and Yang, Zhimou and Ye, Zuochang},
 booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
 doi = {10.1145/3301551.3301588},
 isbn = {978-1-4503-6629-8},
 keywords = {Adversarial Examples, Convolutional Neural Networks, Model Interpretation, Saliency},
 location = {Hong Kong, Hong Kong},
 numpages = {6},
 pages = {25--30},
 publisher = {ACM},
 series = {ICIT 2018},
 title = {Detecting Adversarial Perturbations with Salieny},
 url = {http://doi.acm.org/10.1145/3301551.3301588},
 year = {2018}
}

@inproceedings{Zhang:2018:SWE:3209978.3210193,
 acmid = {3210193},
 address = {New York, NY, USA},
 author = {Zhang, Yongfeng and Zhang, Yi and Zhang, Min},
 booktitle = {The 41st International ACM SIGIR Conference on Research \&\#38; Development in Information Retrieval},
 doi = {10.1145/3209978.3210193},
 isbn = {978-1-4503-5657-2},
 keywords = {explainable recommendation, explainable search, information retrieval, recommendation systems},
 location = {Ann Arbor, MI, USA},
 numpages = {3},
 pages = {1411--1413},
 publisher = {ACM},
 series = {SIGIR '18},
 title = {SIGIR 2018 Workshop on ExplainAble Recommendation and Search (EARS 2018)},
 url = {http://doi.acm.org/10.1145/3209978.3210193},
 year = {2018}
}

@article{Zhang:2019:DLM:3309769.3279952,
 acmid = {3279952},
 address = {New York, NY, USA},
 articleno = {2},
 author = {Zhang, Wei and Yao, Ting and Zhu, Shiai and Saddik, Abdulmotaleb El},
 doi = {10.1145/3279952},
 issn = {1551-6857},
 issue_date = {February 2019},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 keywords = {Multimedia analytics, deep learning, neural networks},
 month = {January},
 number = {1s},
 numpages = {26},
 pages = {2:1--2:26},
 publisher = {ACM},
 title = {Deep Learning\&\#x02013;Based Multimedia Analytics: A Review},
 url = {http://doi.acm.org/10.1145/3279952},
 volume = {15},
 year = {2019}
}

@inproceedings{zhangInterpretableNeuralModel2019,
 abstract = {Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.},
 author = {Zhang, Yin and Liu, Ninghao and Ji, Shuiwang and Caverlee, James and Hu, Xia},
 booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
 editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
 isbn = {978-3-030-16142-2},
 keywords = {Interpretation,Neural network,Stepwise Influence},
 language = {en},
 pages = {528-540},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {An {{Interpretable Neural Model}} with {{Interactive Stepwise Influence}}},
 year = {2019}
}

@article{zhangVisualInterpretabilityDeep2018,
 abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human\textendash{}computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
 author = {Zhang, Quan-shi and Zhu, Song-chun},
 doi = {10.1631/FITEE.1700808},
 file = {/home/tim/Zotero/storage/RDRDKZXC/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf},
 issn = {2095-9230},
 journal = {Frontiers of Information Technology \& Electronic Engineering},
 keywords = {Artificial intelligence,Deep learning,Interpretable model,TP391},
 language = {en},
 month = {January},
 number = {1},
 pages = {27-39},
 shorttitle = {Visual Interpretability for Deep Learning},
 title = {Visual Interpretability for Deep Learning: A Survey},
 volume = {19},
 year = {2018}
}

@inproceedings{Zheng:2018:DDL:3232565.3232569,
 acmid = {3232569},
 address = {New York, NY, USA},
 author = {Zheng, Ying and Liu, Ziyu and You, Xinyu and Xu, Yuedong and Jiang, Junchen},
 booktitle = {Proceedings of the 2Nd Asia-Pacific Workshop on Networking},
 doi = {10.1145/3232565.3232569},
 isbn = {978-1-4503-6395-2},
 keywords = {Interpretability, Neural networks, Resource allocation},
 location = {Beijing, China},
 numpages = {7},
 pages = {1--7},
 publisher = {ACM},
 series = {APNet '18},
 title = {Demystifying Deep Learning in Networking},
 url = {http://doi.acm.org/10.1145/3232565.3232569},
 year = {2018}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
 abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
 author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
 booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
 editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
 isbn = {978-3-030-04503-6},
 keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
 language = {en},
 pages = {295-308},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
 year = {2018}
}

@article{zhuangChallengesOpportunitiesBig2017,
 abstract = {In this paper, we review recent emerging theoretical and technological advances of artificial intelligence (AI) in the big data settings. We conclude that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI, as follows: from shallow computation to deep neural reasoning; from merely data-driven model to data-driven with structured logic rules models; from task-oriented (domain-specific) intelligence (adherence to explicit instructions) to artificial general intelligence in a general context (the capability to learn from experience). Motivated by such endeavors, the next generation of AI, namely AI 2.0, is positioned to reinvent computing itself, to transform big data into structured knowledge, and to enable better decision-making for our society.},
 author = {Zhuang, Yue-ting and Wu, Fei and Chen, Chun and Pan, Yun-he},
 doi = {10.1631/FITEE.1601883},
 file = {/home/tim/Zotero/storage/CXXGKFY7/Zhuang et al. - 2017 - Challenges and opportunities from big data to kno.pdf},
 issn = {2095-9230},
 journal = {Frontiers of Information Technology \& Electronic Engineering},
 keywords = {Artificial general intelligence,Big data,Cross media,Deep reasoning,Knowledge base population,TP391.4},
 language = {en},
 month = {January},
 number = {1},
 pages = {3-14},
 shorttitle = {Challenges and Opportunities},
 title = {Challenges and Opportunities: From Big Data to Knowledge in {{AI}} 2.0},
 volume = {18},
 year = {2017}
}

