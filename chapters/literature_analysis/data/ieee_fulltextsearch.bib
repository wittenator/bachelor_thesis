@INPROCEEDINGS{8614020,
author={D. {Howard} and M. A. {Edwards}},
booktitle={2018 International Conference on Machine Learning and Data Engineering (iCMLDE)},
title={Explainable A.I.: The Promise of Genetic Programming Multi-run Subtree Encapsulation},
year={2018},
volume={},
number={},
pages={158-159},
abstract={Deep Learning and other Artificial Neural Network based solutions are rarely transparent, and white-box solutions are often called for. This paper explains how Multirun Subtree Encapsulation can provide equivalent white box solutions to facilitate Explainable Artificial Intelligence.},
keywords={data encapsulation;genetic algorithms;learning (artificial intelligence);neural nets;trees (mathematics);white-box solutions;artificial neural network;explainable artificial intelligence;genetic programming;deep learning;multirun subtree encapsulation;explainable AI;Encapsulation;Databases;Genetic programming;Standards;Deep learning;Artificial neural networks;Explainable Artificial Intelligence;A.I.;Genetic Programming;Evolutionary Computation;modularization;Subtree Encapsulation;Automatically Defined Functions;Software Evolution;white box;black box;expression simplification;Deep Learning;Artificial Neural Networks;Multirun Subtree Encapsulation;subtree database},
doi={10.1109/iCMLDE.2018.00037},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5158992,
author={M. {Xue} and C. {Zhu}},
booktitle={2009 International Joint Conference on Artificial Intelligence},
title={A Study and Application on Machine Learning of Artificial Intellligence},
year={2009},
volume={},
number={},
pages={272-274},
abstract={This thesis elaborated the concept, significance and main strategy of machine learning as well as the basic structure of machine learning system. By combining several basic ideas of main strategies, great effort are laid on introducing several machine learning methods, such as Rote learning, Explanation-based learning, Learning from instruction, Learning by deduction, Learning by analogy and Inductive learning, etc. Meanwhile, comparison and analysis are made upon their respective advantages and limitations. At the end of the article, it proposes the research objective of machine learning and points out its development trend.Machine learning is a fundamental way that enable the computer to have the intelligence ; Its application which had been used mainly the method of induction and the synthesis, rather than the deduction has already reached many fields of Artificial Intelligence.},
keywords={learning (artificial intelligence);artificial intelligence;machine learning system;rote learning;explanation-based learning;learning from instruction;learning by deduction;learning by analogy;Inductive learning;Machine learning;Artificial intelligence;Learning systems;Application software;Humans;Computational modeling;Machine learning algorithms;Intelligent systems;Intelligent robots;Physiology;machine learning;AI;system structure;learning strategy;algorithm},
doi={10.1109/JCAI.2009.55},
ISSN={},
month={April},}
@INPROCEEDINGS{8419428,
author={M. A. {Ahmad} and A. {Teredesai} and C. {Eckert}},
booktitle={2018 IEEE International Conference on Healthcare Informatics (ICHI)},
title={Interpretable Machine Learning in Healthcare},
year={2018},
volume={},
number={},
pages={447-447},
abstract={This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.},
keywords={health care;learning (artificial intelligence);interpretable machine learning models;healthcare;interpretable machine learning algorithm;Machine learning;Machine learning algorithms;Prediction algorithms;Tutorials;Predictive models;Cancer;interpretable machine learning;explainable artificial intelligence},
doi={10.1109/ICHI.2018.00095},
ISSN={2575-2634},
month={June},}
@ARTICLE{4359183,
author={A. L. P. {Tay} and J. M. {Zurada} and L. {Wong} and J. {Xu}},
journal={IEEE Transactions on Neural Networks},
title={The Hierarchical Fast Learning Artificial Neural Network (HieFLANN)—An Autonomous Platform for Hierarchical Neural Network Construction},
year={2007},
volume={18},
number={6},
pages={1645-1657},
abstract={The hierarchical fast learning artificial neural network (HieFLANN) is a clustering NN that can be initialized using statistical properties of the data set. This provides the possibility of constructing the entire network autonomously with no manual intervention. This distinguishes it from many existing networks that, though hierarchically plausible, still require manual initialization processes. The unique system of hierarchical networks begins with a reduction of the high-dimensional feature space into smaller and manageable ones. This process involves using the K-iterations fast learning artificial neural network (KFLANN) to systematically cluster a square matrix containing the Mahalanobis distances (MDs) between data set features, into homogeneous feature subspaces (HFSs). The KFLANN is used for its heuristic network initialization capabilities on a given data set and requires no supervision. Through the recurring use of the KFLANN and a second stage involving canonical correlation analysis (CCA), the HieFLANN is developed. Experimental results on several standard benchmark data sets indicate that the autonomous determination of the HFS provides a viable avenue for feasible partitioning of feature subspaces. When coupled with the network transformation process, the HieFLANN yields results showing accuracies comparable with available methods. This provides a new platform by which data sets with high-dimensional feature spaces can be systematically resolved and trained autonomously, alleviating the effects of the curse of dimensionality.},
keywords={learning (artificial intelligence);neural net architecture;hierarchical fast learning artificial neural network;autonomous platform;hierarchical neural network construction;clustering neural network;high-dimensional feature space;K-iteration fast learning artificial neural network;square matrix;Mahalanobis distance;homogeneous feature subspace;heuristic network initialization;canonical correlation analysis;Artificial neural networks;Animal structures;Neural networks;Bioinformatics;Data analysis;Manufacturing;Collaborative work;Neurons;Canonical correlation analysis (CCA);data presentation sequence sensitivity (DPSS);hierarchical neural networks (NNs);homogeneous feature subspaces;$K$-iterations fast learning artificial neural network (KFLANN);Canonical correlation analysis (CCA);data presentation sequence sensitivity (DPSS);hierarchical neural networks (NNs);homogeneous feature subspaces;$K$-iterations fast learning artificial neural network (KFLANN)},
doi={10.1109/TNN.2007.900231},
ISSN={1045-9227},
month={Nov},}
@INPROCEEDINGS{5460760,
author={T. {Jayalskshmi} and A. {Santhakumaran}},
booktitle={2010 Second International Conference on Machine Learning and Computing},
title={Impact of Preprocessing for Diagnosis of Diabetes Mellitus Using Artificial Neural Networks},
year={2010},
volume={},
number={},
pages={109-112},
abstract={Medicine has always benefited from the technology. Artificial Neural Networks is currently the promising area of interest to solve medical problems. Diagnosis of diabetes is one of the most challenging problems in machine learning. This medical data set is seldom complete. Artificial neural networks require complete set of data for an accurate classification. The system explains how the pre-processing procedure and missing values influence the data set during the classification. The implemented system compares various missing value techniques and pre-processing techniques. Some combinations prove the real influence of these techniques. A classifier has applied to Pima Indian Diabetes dataset and the results were improved tremendously when using certain combination of preprocessing and missing value techniques. The experimental system achieves an excellent classification accuracy of 99% which is best than before.},
keywords={diseases;learning (artificial intelligence);medicine;neural nets;patient diagnosis;pattern classification;set theory;Diabetes Mellitus;artificial neural networks;diagnosis preprocessing;medicine;machine learning;data set;classification;Pima Indian Diabetes dataset;Diabetes;Artificial neural networks;Blood;Sugar;Neural networks;Machine learning;Medical diagnostic imaging;Educational institutions;Insulin;Noise level;Artificial Neural Networks;Back Propagation Method;Diabetes Mellitus;Missing Value Analysis;Pre Processing Methods},
doi={10.1109/ICMLC.2010.65},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8324372,
author={H. J. {Vishnukumar} and B. {Butting} and C. {Müller} and E. {Sax}},
booktitle={2017 Intelligent Systems Conference (IntelliSys)},
title={Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation},
year={2017},
volume={},
number={},
pages={714-721},
abstract={Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile & V-model, where test & validation (T&V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning & deep neural network (AI-core) for lab & real-world T&V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.},
keywords={automobiles;control engineering computing;driver information systems;learning (artificial intelligence);mobile robots;neural nets;program testing;road safety;software engineering;traffic engineering computing;real-world test;autonomous vehicle;efficient test;safe vehicles;intelligent autonomous vehicles;development cycle;AI-core;Advanced driver assistance system;autonomous driving functions;virtual tests;precise tests;high quality test;artificial intelligence core;machine learning;deep neural network;ADAS;T&V;Autonomous vehicles;Learning (artificial intelligence);Testing;Autonomous automobiles;Machine learning algorithms;Machine learning;Machine Learning;Artificial Intelligence;AI-core;Autonomous Vehicles;ADAS systems;Test and Validation;Simulation;High quality;Efficient},
doi={10.1109/IntelliSys.2017.8324372},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8679150,
author={D. {Kim} and W. {Lim} and M. {Hong} and H. {Kim}},
booktitle={2019 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={The Structure of Deep Neural Network for Interpretable Transfer Learning},
year={2019},
volume={},
number={},
pages={1-4},
abstract={Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
keywords={feature extraction;image classification;learning (artificial intelligence);natural language processing;neural nets;deep neural network;interpretable transfer learning;high-quality data;complex model;pretrained model;interpretability;transfer learning task;interpretable features;feature extraction layer;Korean characters classification;Feature extraction;Task analysis;Training;Data models;Convolution;Computational modeling;Neural networks;Interpretability;Transfer Learning;Machine Learning},
doi={10.1109/BIGCOMP.2019.8679150},
ISSN={2375-9356},
month={Feb},}
@ARTICLE{4359216,
author={K. {Huang} and H. {Yang} and I. {King} and M. R. {Lyu}},
journal={IEEE Transactions on Neural Networks},
title={Maxi–Min Margin Machine: Learning Large Margin Classifiers Locally and Globally},
year={2008},
volume={19},
number={2},
pages={260-272},
abstract={In this paper, we propose a novel large margin classifier, called the maxi-min margin machine (M4). This model learns the decision boundary both locally and globally. In comparison, other large margin classifiers construct separating hyperplanes only either locally or globally. For example, a state-of-the-art large margin classifier, the support vector machine (SVM), considers data only locally, while another significant model, the minimax probability machine (MPM), focuses on building the decision hyperplane exclusively based on the global information. As a major contribution, we show that SVM yields the same solution as M4when data satisfy certain conditions, and MPM can be regarded as a relaxation model of M4. Moreover, based on our proposed local and global view of data, another popular model, the linear discriminant analysis, can easily be interpreted and extended as well. We describe the M4model definition, provide a geometrical interpretation, present theoretical justifications, and propose a practical sequential conic programming method to solve the optimization problem. We also show how to exploit Mercer kernels to extend M4for nonlinear classifications. Furthermore, we perform a series of evaluations on both synthetic data sets and real-world benchmark data sets. Comparison with SVM and MPM demonstrates the advantages of our new model.},
keywords={boundary-value problems;computational geometry;learning (artificial intelligence);minimax techniques;nonlinear programming;pattern classification;probability;relaxation theory;support vector machines;maxi-min margin machine;large margin classifier learning;decision boundaries;minimax probability machine;relaxation model;linear discriminant analysis;geometrical interpretation;sequential conic programming method;optimization problem;Mercer kernels;nonlinear classification;support vector machines;Machine learning;Support vector machines;Support vector machine classification;Minimax techniques;Buildings;Linear discriminant analysis;Solid modeling;Optimization methods;Kernel;Performance evaluation;Classification;kernel methods;large margin;learning locally and globally;second-order cone programming;Classification;kernel methods;large margin;learning locally and globally;second-order cone programming;Algorithms;Artificial Intelligence;Cluster Analysis;Neural Networks (Computer);Pattern Recognition, Automated},
doi={10.1109/TNN.2007.905855},
ISSN={1045-9227},
month={Feb},}
@INPROCEEDINGS{5460721,
author={J. {Bellary} and B. {Peyakunta} and S. {Konetigari}},
booktitle={2010 Second International Conference on Machine Learning and Computing},
title={Hybrid Machine Learning Approach in Data Mining},
year={2010},
volume={},
number={},
pages={305-308},
abstract={In this paper we discuss various machine learning approaches used in mining of data. Further we distinguish between symbolic and sub-symbolic data mining methods. We also attempt to propose a hybrid method with the combination of Artificial Neural Network (ANN) and Cased Based Reasoning (CBR) in mining of data.},
keywords={case-based reasoning;data mining;learning (artificial intelligence);neural nets;machine learning;subsymbolic data mining;artificial neural network;cased based reasoning;Machine learning;Data mining;Artificial neural networks;Data analysis;Databases;Information systems;Learning systems;Classification tree analysis;Data visualization;Hospitals;Data Mining;Machine Learning;Artificial Neural Networ (ANN);Case Based Reasoning( CBR);Hybrid Approach},
doi={10.1109/ICMLC.2010.57},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7496034,
author={Z. {Cömert} and A. F. {Kocamaz} and S. {Güngör}},
booktitle={2016 24th Signal Processing and Communication Application Conference (SIU)},
title={Cardiotocography signals with artificial neural network and extreme learning machine},
year={2016},
volume={},
number={},
pages={1493-1496},
abstract={Cardiotocography (CTG) is a monitoring technique that is used routinely during pregnancy and labor to assess fetal well-being. CTG consists of two signals which are fetal heart rate (FHR) and uterine contraction (UC). Twenty-one features representing the characteristic of FHR have been used in this work. The features are obtained from a large dataset consisting of 2126 records in UCI Machine Learning Repository. The prominent features, such as baseline, the number of acceleration and deceleration patterns, and variability recommended by International Federation of Gynecology and Obstetrics (FIGO) have also taken into account during CTG analysis. The features were applied as the input to feedforward neural network (ANN) and Extreme Learning Machine (ELM) to classify FHR patterns in this study. FHR is recently divided into three classes as normal, suspicious and pathological. According to the results of this study, the accuracy of classification of ANN and ELM were obtained as 91.84% and 93.42%, respectively.},
keywords={cardiology;feedforward neural nets;learning (artificial intelligence);medical signal detection;medical signal processing;signal classification;cardiotocography signals classification;artificial neural network;extreme learning machine;monitoring technique;fetal heart rate;uterine contraction;UCI machine learning repository;deceleration patterns;acceleration patterns;Federation of Gynecology and Obstetrics;feedforward neural network;FHR patterns;Training;Artificial neural networks;Testing;Feature extraction;Fetal heart rate;Cardiography;Monitoring;Cardiotocography;fetal heart rate classification;feedforward neural network;extreme learning machine},
doi={10.1109/SIU.2016.7496034},
ISSN={},
month={May},}
@ARTICLE{774103,
author={I. A. {Taha} and J. {Ghosh}},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Symbolic interpretation of artificial neural networks},
year={1999},
volume={11},
number={3},
pages={448-463},
abstract={Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
keywords={symbol manipulation;knowledge representation;explanation;truth maintenance;feedforward neural nets;neural net architecture;knowledge based systems;learning (artificial intelligence);symbolic interpretation;hybrid intelligent systems;knowledge-based systems;artificial neural networks;domain knowledge representation;domain knowledge mapping;connectionist architecture;network training;rule extraction;explanation power;output decision validation;knowledge refinement;binary rules;feedforward networks;hidden layer;sigmoidal units;partial rules;embedded knowledge;adjustable detail level;rule evaluation technique;rule ordering;performance measures;iris data set;breast cancer data set;Artificial neural networks;Neural networks;Data mining;Military computing;Knowledge representation;Knowledge based systems;Fuzzy neural networks;Fuzzy sets;Computer networks;Intelligent systems},
doi={10.1109/69.774103},
ISSN={1041-4347},
month={May},}
@INPROCEEDINGS{5376285,
author={Q. {Yang} and M. {Li} and X. {Mu} and J. {Wang}},
booktitle={2009 International Conference on Artificial Intelligence and Computational Intelligence},
title={Application of Artificial Intelligence (AI) in Power Transformer Fault Diagnosis},
year={2009},
volume={4},
number={},
pages={442-445},
abstract={This paper introduces the new intelligence technology in the transformer fault diagnosis - artificial intelligence system (TFDAI). An artificial intelligence system design includes selection of input, network topology, synaptic connection weight, and output. TFDAI module structure, data processing and diagnostic techniques are described in detail. It consists of expert system (ES) and artificial neural network (ANN). This paper covers TFDAI developing and application. It states that artificial intelligence system is very useful tool for transformer early hidden faults achieves the possibility and accuracy of primary diagnosis.},
keywords={expert systems;fault diagnosis;network topology;neural nets;power engineering computing;power transformer protection;artificial intelligence system;power transformer fault diagnosis;network topology;synaptic connection weight;data processing;diagnostic techniques;expert system;artificial neural network;Artificial intelligence;Power transformers;Fault diagnosis;Computational intelligence;Power Transformer fault diagnosis;Expert System;Artificial neutral network;Artificial Intelligence},
doi={10.1109/AICI.2009.497},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8259629,
author={P. {Ongsulee}},
booktitle={2017 15th International Conference on ICT and Knowledge Engineering (ICT KE)},
title={Artificial intelligence, machine learning and deep learning},
year={2017},
volume={},
number={},
pages={1-6},
abstract={It is increasingly recognized that artificial intelligence has been touted as a new mobile. Because of the high volume of data that being generated by devices, sensors and social media users, the machine can learn to distinguish the pattern and makes a reasonably good prediction. This article will explore the use of machine learning and its methodologies. Furthermore, the field of deep learning which is being exploited in many leading IT providers will be clarified and discussed.},
keywords={artificial intelligence;learning (artificial intelligence);artificial intelligence;machine learning;social media users;reasonably good prediction;deep learning;Learning (artificial intelligence);Graphics processing units;Machine learning;Machine learning algorithms;Prediction algorithms;Algorithm design and analysis;Artificial Intelligence;machine learning;deep learning;artificial neural network},
doi={10.1109/ICTKE.2017.8259629},
ISSN={2157-099X},
month={Nov},}
@INPROCEEDINGS{8389957,
author={A. {Lasod} and D. {Soni}},
booktitle={2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
title={Efficiency enhancement of food recognition using artificial neural network},
year={2017},
volume={},
number={},
pages={2758-2762},
abstract={In this paper, we have a tendency to apply a artificial neural network (ANN) to the tasks of detective work and recognizing food pictures. Be- explanation for the wide diversity of styles of food, image recognition of food things is usually terribly difficulties. Be that as it may, deep learning has been indicated as of late to be a truly intense image recognition system, and ANN could be a dynamic way to deal with deep learning. We tend to connected ANN to the errands of food location and recognition through parameter change. We tend to made a dataset of the preeminent incessant food things in a publically available food-logging framework, and utilized it to recognition execution. ANN demonstrated fundamentally higher accuracy than did antiquated support-vector-machine-based routes with handmade choices. Furthermore, we tend to establish that the convolution bits demonstrate that shading commands the component extraction strategy. For food image discovery, ANN also indicated fundamentally higher accuracy than a customary procedure. Fundamentally higher accuracy than a customary strategy.},
keywords={image recognition;learning (artificial intelligence);neural nets;support vector machines;efficiency enhancement;food recognition;artificial neural network;deep learning;food location;publically available food-logging framework;food image discovery;image recognition system;ANN;support-vector-machine;food pictures recognization;handmade choices;convolution bits;Image recognition;Artificial neural networks;Biological neural networks;Feature extraction;Machine learning;Neurons;Artificial neural network;SVM;Food image recognition},
doi={10.1109/ICECDS.2017.8389957},
ISSN={},
month={Aug},}
@INPROCEEDINGS{1279325,
author={ and and and },
booktitle={International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003},
title={A multiple objective optimization based GA for designing interpretable and comprehensible neural network trees},
year={2003},
volume={1},
number={},
pages={518-521 Vol.1},
abstract={Neural network tree (NNTree) is a hybrid model for machine learning. The overall structure is a decision tree (DT), and each non-terminal node is an expert neural network (ENN). Generally speaking, NNTrees can achieve better performance than conventional DTs with fewer nodes, and the performance of the tree can be improved through incremental learning. In addition, the NNTrees can be interpreted in polynomial time if the number of inputs for each ENN is limited. In this paper, we propose a multiple objective optimization based genetic algorithm (MOO-GA) for designing interpretable and comprehensible NNTrees. The efficiency of the proposed algorithm is validated by experimental results.},
keywords={decision trees;neural nets;learning (artificial intelligence);genetic algorithms;multiple objective optimization;neural network trees;machine learning;decision tree;expert neural network;incremental learning;genetic algorithm;Design optimization;Neural networks;Algorithm design and analysis;Polynomials;Machine learning;Decision trees;Genetic algorithms;Machine learning algorithms;Helium;Humans},
doi={10.1109/ICNNSP.2003.1279325},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8489172,
author={X. {Liu} and X. {Wang} and S. {Matwin}},
booktitle={2018 International Joint Conference on Neural Networks (IJCNN)},
title={Interpretable Deep Convolutional Neural Networks via Meta-learning},
year={2018},
volume={},
number={},
pages={1-9},
abstract={Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for “algorithmic fairness” also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
keywords={convolution;feedforward neural nets;learning (artificial intelligence);pattern classification;pattern clustering;random processes;meta-learning;model interpretability;CNN-INTE;clustering algorithm;meta-level training data;base learning algorithms;meta-level test data;machine learning approaches;interpretable deep convolutional neural networks;MNIST image dataset;random forest;deep CNN model;Prediction algorithms;Machine learning;Machine learning algorithms;Predictive models;Computational modeling;Training data;Visualization;interpretability;Meta-learning;deep learning;Convolutional Neural Network;TensorFlow;big data},
doi={10.1109/IJCNN.2018.8489172},
ISSN={2161-4407},
month={July},}
@ARTICLE{6746640,
author={F. {Fernández-Navarro} and A. {Riccardi} and S. {Carloni}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Ordinal Neural Networks Without Iterative Tuning},
year={2014},
volume={25},
number={11},
pages={2075-2085},
abstract={Ordinal regression (OR) is an important branch of supervised learning in between the multiclass classification and regression. In this paper, the traditional classification scheme of neural network is adapted to learn ordinal ranks. The model proposed imposes monotonicity constraints on the weights connecting the hidden layer with the output layer. To do so, the weights are transcribed using padding variables. This reformulation leads to the so-called inequality constrained least squares (ICLS) problem. Its numerical solution can be obtained by several iterative methods, for example, trust region or line search algorithms. In this proposal, the optimum is determined analytically according to the closed-form solution of the ICLS problem estimated from the Karush-Kuhn-Tucker conditions. Furthermore, following the guidelines of the extreme learning machine framework, the weights connecting the input and the hidden layers are randomly generated, so the final model estimates all its parameters without iterative tuning. The model proposed achieves competitive performance compared with the state-of-the-art neural networks methods for OR.},
keywords={iterative methods;learning (artificial intelligence);least squares approximations;neural nets;pattern classification;ordinal neural networks;ordinal regression;supervised learning;multiclass classification;neural network;monotonicity constraints;padding variables;inequality constrained least square problem;iterative methods;trust region;line search algorithms;ICLS problem;Karush-Kuhn-Tucker conditions;extreme learning machine framework;Adaptation models;Encoding;Analytical models;Joining processes;Vectors;Biological neural networks;Extreme learning machine (ELM);neural networks;ordinal regression (OR).;Extreme learning machine (ELM);neural networks;ordinal regression (OR);Algorithms;Artificial Intelligence;Humans;Learning;Models, Statistical;Neural Networks (Computer);Time Factors},
doi={10.1109/TNNLS.2014.2304976},
ISSN={2162-237X},
month={Nov},}
@INPROCEEDINGS{8400040,
author={F. K. {Došilović} and M. {Brčić} and N. {Hlupić}},
booktitle={2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)},
title={Explainable artificial intelligence: A survey},
year={2018},
volume={},
number={},
pages={0210-0215},
abstract={In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
keywords={learning (artificial intelligence);interpretability;healthcare;finance;explainable artificial intelligence;XAI;recent developments;supervised learning;artificial general intelligence;datasets;computing power;machine learning systems;(super)human performance;image recognition;speech analysis;strategic game planning;state-of-the-art models;transparency;Predictive models;Machine learning;Support vector machines;Decision trees;Supervised learning;Optimization;explainable artificial intelligence;interpretability;explainability;comprehensibility},
doi={10.23919/MIPRO.2018.8400040},
ISSN={},
month={May},}
@ARTICLE{1402509,
author={M. {Boden} and J. {Hawkins}},
journal={IEEE Transactions on Neural Networks},
title={Improved access to sequential motifs: a note on the architectural bias of recurrent networks},
year={2005},
volume={16},
number={2},
pages={491-494},
abstract={For many biological sequence problems the available data occupies only sparse regions of the problem space. To use machine learning effectively for the analysis of sparse data we must employ architectures with an appropriate bias. By experimentation we show that the bias of recurrent neural networks-recently analyzed by Tino et al. and Hammer and Tino-offers superior access to motifs (sequential patterns) compared to the, in bioinformatics, standardly used feedforward neural networks.},
keywords={learning (artificial intelligence);recurrent neural nets;feedforward neural nets;pattern recognition;biology computing;biological sequence problem;machine learning;sparse data;recurrent neural network;sequential pattern;bioinformatics;feedforward neural network;Recurrent neural networks;Bioinformatics;Machine learning;Data analysis;Neural networks;Feedforward neural networks;History;Amino acids;Information technology;Australia;Architectural bias;biological sequence;bioinformatics;recurrent neural network;Computational Biology;Entropy;Neural Networks (Computer);Sequence Analysis},
doi={10.1109/TNN.2005.844086},
ISSN={1045-9227},
month={March},}
@ARTICLE{5337957,
author={H. {Chen} and X. {Yao}},
journal={IEEE Transactions on Neural Networks},
title={Regularized Negative Correlation Learning for Neural Network Ensembles},
year={2009},
volume={20},
number={12},
pages={1962-1979},
abstract={Negative correlation learning (NCL) is a neural network ensemble learning algorithm that introduces a correlation penalty term to the cost function of each individual network so that each neural network minimizes its mean square error (MSE) together with the correlation of the ensemble. This paper analyzes NCL and reveals that the training of NCL (when ¿ = 1) corresponds to training the entire ensemble as a single learning machine that only minimizes the MSE without regularization. This analysis explains the reason why NCL is prone to overfitting the noise in the training set. This paper also demonstrates that tuning the correlation parameter ¿ in NCL by cross validation cannot overcome the overfitting problem. The paper analyzes this problem and proposes the regularized negative correlation learning (RNCL) algorithm which incorporates an additional regularization term for the whole ensemble. RNCL decomposes the ensemble's training objectives, including MSE and regularization, into a set of sub-objectives, and each sub-objective is implemented by an individual neural network. In this paper, we also provide a Bayesian interpretation for RNCL and provide an automatic algorithm to optimize regularization parameters based on Bayesian inference. The RNCL formulation is applicable to any nonlinear estimator minimizing the MSE. The experiments on synthetic as well as real-world data sets demonstrate that RNCL achieves better performance than NCL, especially when the noise level is nontrivial in the data set.},
keywords={belief networks;inference mechanisms;learning (artificial intelligence);mean square error methods;neural nets;regularized negative correlation learning;neural network ensemble learning algorithm;mean square error;learning machine;Bayesian interpretation;Bayesian inference;Neural networks;Machine learning;Cost function;Mean square error methods;Bayesian methods;Algorithm design and analysis;Inference algorithms;Noise level;Computational intelligence;Application software;Ensembles;negative correlation learning (NCL);neural network ensembles;neural networks;probabilistic model;regularization;Algorithms;Automatic Data Processing;Bayes Theorem;Humans;Learning;Neural Networks (Computer);Nonlinear Dynamics;Signal Processing, Computer-Assisted},
doi={10.1109/TNN.2009.2034144},
ISSN={1045-9227},
month={Dec},}
@INPROCEEDINGS{8646474,
author={A. {Shahroudnejad} and P. {Afshar} and K. N. {Plataniotis} and A. {Mohammadi}},
booktitle={2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
title={IMPROVED EXPLAINABILITY OF CAPSULE NETWORKS: RELEVANCE PATH BY AGREEMENT},
year={2018},
volume={},
number={},
pages={549-553},
abstract={Recent advancements in signal processing domain have resulted in a surge of interest in deep neural networks (DNNs) due to their unprecedented performance and high accuracy for challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in medical applications), it is of paramount importance to understand, trust, and in one word "explain" the rational behind deep models' decisions. Generally, DNNs are considered as black-box systems, which do not provide any clue on their internal processing actions. Although some recent efforts have been initiated to explain behavior/decisions of deep networks, explainable artificial intelligence (XAI) domain is still in its infancy. In this regard, we consider capsule networks (referred to as CapsNets), which are novel deep structures; recently proposed as an alternative counterpart to convolutional neural networks (CNNs), and posed to change the future of machine intelligence. In this paper, we investigate and analyze structure and behavior of CapsNets and illustrate potential explainability properties of such networks. Furthermore, we show possibility of transforming deep architectures in to transparent networks via incorporation of capsules in different layers instead of convolution layers of the CNNs.},
keywords={convolutional neural nets;learning (artificial intelligence);signal processing;capsule networks;relevance path;signal processing domain;deep neural networks;unprecedented performance;significant engineering importance;deep learning architectures;critical decisions;human lives;medical applications;deep models;black-box systems;internal processing actions;deep networks;explainable artificial intelligence domain;CapsNets;deep structures;convolutional neural networks;potential explainability properties;deep architectures;transparent networks;DNN;XAI;CNN;Feature extraction;Predictive models;Neural networks;Machine learning;Training;Couplings;Computer architecture;Explainable Machine Learning;Capsule Networks;Deep Neural Networks;Convolutional Neural Networks},
doi={10.1109/GlobalSIP.2018.8646474},
ISSN={},
month={Nov},}
@ARTICLE{728352,
author={A. B. {Tickle} and R. {Andrews} and M. {Golea} and J. {Diederich}},
journal={IEEE Transactions on Neural Networks},
title={The truth will come to light: directions and challenges in extracting the knowledge embedded within trained artificial neural networks},
year={1998},
volume={9},
number={6},
pages={1057-1068},
abstract={To date, the preponderance of techniques for eliciting the knowledge embedded in trained artificial neural networks (ANN's) has focused primarily on extracting rule-based explanations from feedforward ANN's. The ADT taxonomy for categorizing such techniques was proposed in 1995 to provide a basis for the systematic comparison of the different approaches. This paper shows that not only is this taxonomy applicable to a cross section of current techniques for extracting rules from trained feedforward ANN's but also how the taxonomy can be adapted and extended to embrace a broader range of ANN types (e,g., recurrent neural networks) and explanation structures. In addition we identify some of the key research questions in extracting the knowledge embedded within ANN's including the need for the formulation of a consistent theoretical basis for what has been, until recently, a disparate collection of empirical results.},
keywords={feedforward neural nets;recurrent neural nets;knowledge acquisition;finite automata;explanation;feedforward neural networks;ADT taxonomy;rule extraction;knowledge acquisition;explanation structures;finite state automata;fuzzy neural networks;knowledge insertion;recurrent neural networks;rule refinement;Artificial neural networks;Taxonomy;Recurrent neural networks;Intelligent networks;Neural networks;Feedforward neural networks;Automata;Fuzzy neural networks;Pattern recognition;Function approximation},
doi={10.1109/72.728352},
ISSN={1045-9227},
month={Nov},}
@INPROCEEDINGS{5366651,
author={H. W. K. {Chia} and C. L. {Tan} and S. Y. {Sung}},
booktitle={2009 21st IEEE International Conference on Tools with Artificial Intelligence},
title={Probabilistic Neural Logic Network Learning: Taking Cues from Neuro-Cognitive Processes},
year={2009},
volume={},
number={},
pages={698-702},
abstract={This paper describes an attempt to devise a knowledge discovery model that is inspired from the two theoretical frameworks of selectionism and constructivism in human cognitive learning. The "selectionist" nature of human decision making indicates the use of an evolutionary paradigm for composing rudimentary neural network units, while the "constructivist" component takes the form of neural weight training during the learning process. We explore the possibility of amalgamating these two ideas into a neural learning system for the discovery of meaningful rules in the context of pattern discovery in data.},
keywords={cognitive systems;data mining;decision making;learning (artificial intelligence);neural nets;probabilistic neural logic network learning;neurocognitive processes;knowledge discovery model;selectionism;constructivism;human cognitive learning;human decision making;rudimentary neural network units;neural weight training;data pattern discovery;neural learning system;Probabilistic logic;Humans;Network address translation;Decision making;Artificial neural networks;Biological neural networks;Artificial intelligence;Learning;Computer networks;Drives},
doi={10.1109/ICTAI.2009.65},
ISSN={1082-3409},
month={Nov},}
@INPROCEEDINGS{8474593,
author={J. {Rathod} and V. {Wazhmode} and A. {Sodha} and P. {Bhavathankar}},
booktitle={2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)},
title={Diagnosis of skin diseases using Convolutional Neural Networks},
year={2018},
volume={},
number={},
pages={1048-1051},
abstract={Dermatology is one of the most unpredictable and difficult terrains to diagnose due its complexity. In the field of dermatology, many a times extensive tests are to be carried out so as to decide upon the skin condition the patient may be facing. The time may vary from practitioner to practitioner. This is also based on the experience of that person too. So, there is a need of a system which can diagnose the skin diseases without any of these constraints. We propose an automated image based system for recognition of skin diseases using machine learning classification. This system will utilize computational technique to analyze, process, and relegate the image data predicated on various features of the images. Skin images are filtered to remove unwanted noise and also process it for enhancement of the image. Feature extraction using complex techniques such as Convolutional Neural Network (CNN), classify the image based on the algorithm of softmax classifier and obtain the diagnosis report as an output. This system will give more accuracy and will generate results faster than the traditional method, making this application an efficient and dependable system for dermatological disease detection. Furthermore, this can also be used as a reliable real time teaching tool for medical students in the dermatology stream.},
keywords={convolution;diseases;feature extraction;feedforward neural nets;image classification;image enhancement;learning (artificial intelligence);medical image processing;skin;Convolutional Neural networks;skin condition;automated image based system;skin images;Convolutional Neural Network;dermatological disease detection;dermatology stream;skin disease diagnosis;skin disease recognition;feature extraction;image enhancement;CNN;image classification;machine learning classification;Skin;Diseases;Convolutional neural networks;Feature extraction;Conferences;Machine learning;Convolution;Dermatology;Image Processing;Computer Vision;Machine Learning;Artificial Intelligence;Neural Network;Deep Learning;Computational Intelligence;Automated Disease Diagnosis;Convolutional Neural Network},
doi={10.1109/ICECA.2018.8474593},
ISSN={},
month={March},}
@INPROCEEDINGS{7893155,
author={H. M. {Yao} and Y. W. {Qin} and L. J. {Jiang}},
booktitle={2016 IEEE Electrical Design of Advanced Packaging and Systems (EDAPS)},
title={Machine learning based MoM (ML-MoM) for parasitic capacitance extractions},
year={2016},
volume={},
number={},
pages={171-173},
abstract={This paper is a rethinking of the conventional method of moments (MoM) using the modern machine learning (ML) technology. By repositioning the MoM matrix and unknowns in an artificial neural network (ANN), the conventional linear algebra MoM solving is changed into a machine learning training process. The trained result is the solution. As an application, the parasitic capacitance extraction broadly needed by VLSI modeling is solved through the proposed new machine learning based method of moments (ML-MoM). The multiple linear regression (MLR) is employed to train the model. The computations are done on Amazon Web Service (AWS). Benchmarks demonstrated the interesting feasibility and efficiency of the proposed approach. According to our knowledge, this is the first MoM truly powered by machine learning methods. It opens enormous software and hardware resources for MoM and related algorithms that can be applied to signal integrity and power integrity simulations.},
keywords={integrated circuit modelling;learning (artificial intelligence);method of moments;neural nets;regression analysis;VLSI;machine learning training process;method of moments;ML-MoM;parasitic capacitance extractions;ML technology;MoM matrix;artificial neural network;ANN;linear algebra MoM solving;VLSI modeling;multiple linear regression;MLR;Amazon Web Service;AWS;software resources;hardware resources;signal integrity;power integrity simulations;Method of moments;Capacitance;Training;Machine learning algorithms;Computational modeling;Artificial neural networks;Software;Method of Moments;Machine Learning;Capacitance Extraction;Artificial Neural Network},
doi={10.1109/EDAPS.2016.7893155},
ISSN={2151-1233},
month={Dec},}
@INPROCEEDINGS{8622433,
author={B. {Kovalerchuk} and N. {Neuhaus}},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Toward Efficient Automation of Interpretable Machine Learning},
year={2018},
volume={},
number={},
pages={4940-4947},
abstract={Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
keywords={learning (artificial intelligence);pattern classification;domain-explainable ML models;dominance classifier and predictor algorithm;DCP algorithm;unintelligible black box models;interpretable machine learning;interpretable mathematical forms;noninterpretable models;interpretable methods;human-understandable machine learning models;complex multidimensional relations;human perceptual limitations;white box models;interpretable grey box models;Classification algorithms;Prediction algorithms;Machine learning;Mathematical model;Machine learning algorithms;Computational modeling;Neural networks;machine learning;explainability;interpretability;accuracy;classifier;visualization;visual model;dominant intervals},
doi={10.1109/BigData.2018.8622433},
ISSN={},
month={Dec},}
@INPROCEEDINGS{374427,
author={ and },
booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},
title={Hybrid neural network-driven reasoning approach to bankruptcy prediction: comparison with MDA, ACLS, and neural network},
year={1994},
volume={3},
number={},
pages={1787-1792 vol.3},
abstract={The objective of this paper is to propose a new neural network-based approach to bankruptcy prediction problem, named HYNEN (hybrid neural network-driven reasoning) model which is based on two types of neural networks: unsupervised and supervised neural network. Accordingly, it consists of two stages: 1) clustering neural network (CNN) stage, and 2) output neural network (ONN) stage. CNN categorizes input sample into an appropriate cluster, which is identical to finding a relevant rule to be fired in knowledge base. Then in the ONN stage, ONNs are built based on information about the clusters derived from CNN stage, and used to make a final decision: "bankrupt" or "non-bankrupt". CNN uses two types of unsupervised neural network models for pattern clustering, the self-organizing map and learning vector quantization, and then learns the clusters in a supervised manner. ONN utilizes a supervised neural network. We performed comparative experiments with Korean bankruptcy data using HYNEN, MDA (Multivariate Discriminant Analysis), and ACLS (Analog Concept Learning System) conventional neural network approach.<<ETX>>},
keywords={neural nets;learning (artificial intelligence);inference mechanisms;financial data processing;bankruptcy prediction;hybrid neural network-driven reasoning;supervised neural network;unsupervised neural network;clustering neural network;output neural network;self-organizing map;learning vector quantization;clusters;Neural networks;Cellular neural networks;Artificial neural networks;Predictive models;Statistical analysis;Artificial intelligence;Computer network management;Computer networks;Vector quantization;Performance analysis},
doi={10.1109/ICNN.1994.374427},
ISSN={},
month={June},}
@INPROCEEDINGS{236591,
author={J. {Genest}},
booktitle={Proceedings First International Conference on Artificial Intelligence Applications on Wall Street},
title={Building a banking system specification using machine learning},
year={1991},
volume={},
number={},
pages={263-268},
abstract={Transforming user requirements into software specification is a complex and demanding task. Artificial intelligence methods such as machine learning (ML) can assist in the software specification process by providing support to system designers. This paper presents an approach based on explanation-based learning (EBL), a ML technique in which a concept is learned by building an explanation. The approach is presented in the context of the system LISE (Learning in Software Engineering). LISE converts a user requirement for a software module into an operational module definition using EBL with an incomplete theory. An example where LISE is used to build the specification of a banking system is illustrated.<<ETX>>},
keywords={bank data processing;case-based reasoning;explanation;formal specification;learning (artificial intelligence);case-based reasoning;banking system specification;user requirements;machine learning;explanation-based learning;LISE;Learning in Software Engineering;Banking;Machine learning;Buildings;Software engineering;Mathematics;Artificial intelligence;Programming;Software design;Multilevel systems;Software libraries},
doi={10.1109/AIAWS.1991.236591},
ISSN={},
month={Oct},}
@ARTICLE{7369936,
author={J. G. {Wolff}},
journal={IEEE Access},
title={The SP Theory of Intelligence: Distinctive Features and Advantages},
year={2016},
volume={4},
number={},
pages={216-246},
abstract={This paper aims to highlight distinctive features of the SP theory of intelligence, realized in the SP computer model, and its apparent advantages compared with some AI-related alternatives. Perhaps most importantly, the theory simplifies and integrates observations and concepts in AI-related areas, and has potential to simplify and integrate of structures and processes in computing systems. Unlike most other AI-related theories, the SP theory is itself a theory of computing, which can be the basis for new architectures for computers. Fundamental in the theory is information compression via the matching and unification of patterns and, more specifically, via a concept of multiple alignment. The theory promotes transparency in the representation and processing of knowledge, and unsupervised learning of natural structures via information compression. It provides an interpretation of aspects of mathematics and an interpretation of phenomena in human perception and cognition. concepts in the theory may be realized in terms of neurons and their inter-connections (SP-neural). These features and advantages of the SP system are discussed in relation to AI-related alternatives: the concept of minimum length encoding and related concepts, how computational and energy efficiency in computing may be achieved, deep learning in neural networks, unified theories of cognition and related research, universal search, Bayesian networks and some other models for AI, IBM's Watson, solving problems associated with big data and in the development of intelligence in autonomous robots, pattern recognition and vision, the learning and processing of natural language, exact and inexact forms of reasoning, representation and processing of diverse forms of knowledge, and software engineering. In conclusion, the SP system can provide a firm foundation for the long-term development of AI and related areas, and at the same time, it may deliver useful results on relatively short timescales.},
keywords={knowledge representation;learning (artificial intelligence);neural nets;pattern matching;deep-learning;energy efficiency;computational efficiency;minimum length encoding;SP-neural;neuron interconnections;cognition;human perception;information compression;unsupervised learning;knowledge representation;knowledge processing;pattern unification;pattern matching;information compression;AI;SP computer model;SP intelligence theory;Artificial intelligence;Computational modeling;Information compression;Computer architecture;Unsupervised learning;Turing machines;Neural networks;Mathematics;Unsupervised learning;artificial intelligence;information compression;multiple alignment;perception;cognition;neural networks;deep learning;unsupervised learning;reasoning;mathematics;Artificial intelligence;information compression;multiple alignment;perception;cognition;neural networks;deep learning;unsupervised learning;reasoning;mathematics},
doi={10.1109/ACCESS.2015.2513822},
ISSN={2169-3536},
month={},}
@ARTICLE{4298139,
author={D. S. {Yeung} and W. W. Y. {Ng} and D. {Wang} and E. C. C. {Tsang} and X. {Wang}},
journal={IEEE Transactions on Neural Networks},
title={Localized Generalization Error Model and Its Application to Architecture Selection for Radial Basis Function Neural Network},
year={2007},
volume={18},
number={5},
pages={1294-1305},
abstract={The generalization error bounds found by current error models using the number of effective parameters of a classifier and the number of training samples are usually very loose. These bounds are intended for the entire input space. However, support vector machine (SVM), radial basis function neural network (RBFNN), and multilayer perceptron neural network (MLPNN) are local learning machines for solving problems and treat unseen samples near the training samples to be more important. In this paper, we propose a localized generalization error model which bounds from above the generalization error within a neighborhood of the training samples using stochastic sensitivity measure. It is then used to develop an architecture selection technique for a classifier with maximal coverage of unseen samples by specifying a generalization error threshold. Experiments using 17 University of California at Irvine (UCI) data sets show that, in comparison with cross validation (CV), sequential learning, and two other ad hoc methods, our technique consistently yields the best testing classification accuracy with fewer hidden neurons and less training time.},
keywords={generalisation (artificial intelligence);learning (artificial intelligence);multilayer perceptrons;pattern classification;radial basis function networks;support vector machines;localized generalization error model;architecture selection;radial basis function neural network;support vector machine;multilayer perceptron neural network;local learning machines;sequential learning;ad hoc methods;hidden neurons;Radial basis function networks;Support vector machines;Support vector machine classification;Multilayer perceptrons;Neural networks;Multi-layer neural network;Machine learning;Stochastic processes;Sequential analysis;Neurons;Localized generalization error;network architecture selection;radial basis function neural network (RBFNN);sensitivity measure;Algorithms;Computer Simulation;Models, Statistical;Neural Networks (Computer);Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity},
doi={10.1109/TNN.2007.894058},
ISSN={1045-9227},
month={Sep.},}
@ARTICLE{8474484,
author={F. {Khomh} and B. {Adams} and J. {Cheng} and M. {Fokaefs} and G. {Antoniol}},
journal={IEEE Software},
title={Software Engineering for Machine-Learning Applications: The Road Ahead},
year={2018},
volume={35},
number={5},
pages={81-84},
abstract={The First Symposium on Software Engineering for Machine Learning Applications (SEMLA) aimed to create a space in which machine learning (ML) and software engineering (SE) experts could come together to discuss challenges, new insights, and practical ideas regarding the engineering of ML and AI-based systems. Key challenges discussed included the accuracy of systems built using ML and AI models, the testing of those systems, industrial applications of AI, and the rift between the ML and SE communities. This article is part of a theme issue on software engineering's 50th anniversary.},
keywords={artificial intelligence;learning (artificial intelligence);software engineering;AI-based systems;ML model;software engineering for machine learning applications;SEMLA;industrial applications;Learning systems;Software engineering;Software systems;Software development;Machine learning;Artificial intelligence;First Symposium on Software Engineering for Machine Learning Applications;SEMLA;machine learning;artificial intelligence;AI;software engineering;SE;software development;Invited Content},
doi={10.1109/MS.2018.3571224},
ISSN={0740-7459},
month={Sep.},}
@INPROCEEDINGS{5362936,
author={H. {Wang} and C. {Ma} and L. {Zhou}},
booktitle={2009 International Conference on Information Engineering and Computer Science},
title={A Brief Review of Machine Learning and Its Application},
year={2009},
volume={},
number={},
pages={1-4},
abstract={With the popularization of information and the establishment of the databases in great number, and how to extract data from the useful information is the urgent problem to be solved. Machine learning is the core issue of artificial intelligence research, this paper introduces the definition of machine learning and its basic structure, and describes a variety of machine learning methods, including rote learning, inductive learning, analogy learning , explained learning, learning based on neural network and knowledge discovery and so on. This paper also brings foreword the objectives of machine learning, and points out the development trend of machine learning.},
keywords={data mining;database management systems;learning (artificial intelligence);neural nets;databases;artificial intelligence research;machine learning methods;rote learning;inductive learning;analogy learning;explained learning;neural network;knowledge discovery;Machine learning;Learning systems;Humans;Competitive intelligence;Data engineering;Databases;Data mining;Artificial intelligence;Artificial neural networks;Application software},
doi={10.1109/ICIECS.2009.5362936},
ISSN={2156-7379},
month={Dec},}
@INPROCEEDINGS{8491679,
author={R. {Chimatapu} and H. {Hagras} and A. {Starkey} and G. {Owusu}},
booktitle={2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
title={Interval Type-2 Fuzzy Logic Based Stacked Autoencoder Deep Neural Network For Generating Explainable AI Models in Workforce Optimization},
year={2018},
volume={},
number={},
pages={1-8},
abstract={In Utility based industries that employ a large mobile workforce, efficient utilization of field engineers is key to optimal service delivery. The utilization of the engineers can be improved by predicting the future performance of work areas by using machine learning tools such as Deep Neural Networks (DNNs).The dramatic success of DNNs has led to an explosion of its applications. However, the effectiveness of DNNs can be limited by the inability to explain how the models arrived at their predictions.In this paper, we present a novel Type-2 Fuzzy Logic System (FLS) whose inputs are preprocessed by a Stacked Autoencoder Neural Network to add some interpretability to a Deep Neural Network model. The proposed type-2 FLS will contain a small rule set with a small number of antecedents per rule to maximize the model's interpretability. We also present an algorithm which can be used to efficiently train the proposed model.We will compare the proposed model with a Standard Stacked Autoencoder Deep Neural Network, a Multi-Layer Perceptron (MLP) neural network and an Interval Type-2 Fuzzy Logic System.The results show that even though the Standard Stacked Autoencoder and MLP Neural Networks have better performance, they do not provide any insight into the reasoning behind the predictions. The Proposed model, on the other hand, provides better result than the standalone type-2 FLS and a comparable performance to the neural networks and provides a little bit of insight into the decision-making process. Without this insight, we cannot be sure why there is a drop in the performance and we need to further analyze the WA before we can take any decision. This leads to quicker decision making and potentially improving the efficiency of the engineers.},
keywords={decision making;fuzzy logic;fuzzy set theory;learning (artificial intelligence);multilayer perceptrons;optimisation;workforce optimization;mobile workforce;field engineers;optimal service delivery;machine learning tools;DNNs;standalone type-2 FLS;multilayer perceptron neural network;interval type-2 fuzzy logic system;MLP neural networks;utility based industries;explainable AI models;standard stacked autoencoder deep neural network;decision-making process;Fuzzy logic;Neural networks;Fuzzy sets;Optimization;Task analysis;Predictive models;Type-2 fuzzy logic;Big Bang - Big Crunch;Deep Neural Networks;Explainable Artificial Intelligence},
doi={10.1109/FUZZ-IEEE.2018.8491679},
ISSN={},
month={July},}
@INPROCEEDINGS{542794,
author={D. {Furundzic} and M. {Djordjevic} and A. {Bekic}},
booktitle={Proceedings of International Workshop on Neural Networks for Identification, Control, Robotics and Signal/Image Processing},
title={Artificial neural networks for early breast carcinoma detection},
year={1996},
volume={},
number={},
pages={355-359},
abstract={This paper shows a model designed for an efficient detection of patients with breast carcinoma (BC), using an artificial neural network (NN). A multilayer NN is used as a convenient modeling tool for the identification of a chosen set of risk factors and symptoms which characterizes the patients suffering from BC. We used a properly selected representative learning set of patterns referring to 60 patients for the BC identification process (the NN learning task). The other set of patterns referring to 140 patients was used as a test set for verification of the proposed model. The instructed NN successfully identified 198 test patterns, which means that identification accuracy was greater than 98%. Previously set hypothesis, which claimed that hyperspace of weight matrix enables an efficient separation of healthy group of patients from group suffering from BC, proved to be correct. Expert knowledge was used for proper selection of corresponding sets of risk factors and symptoms on one side and for selection of a representative set of learning patterns on the other side.},
keywords={medical diagnostic computing;patient diagnosis;feedforward neural nets;pattern recognition;learning (artificial intelligence);breast carcinoma detection;multilayer neural network;risk factors;symptoms;learning set;hyperspace;weight matrix;learning patterns;patient diagnosis;Artificial neural networks;Breast;Neural networks;Nonhomogeneous media;Diseases;Pattern recognition;Multi-layer neural network;Testing;Medical treatment;Predictive models},
doi={10.1109/NICRSP.1996.542794},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8538383,
author={N. A. {Al-Sammarraie} and Y. M. H. {Al-Mayali} and Y. A. {Baker El-Ebiary}},
booktitle={2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
title={Classification and diagnosis using back propagation Artificial Neural Networks (ANN)},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Artificial neural networks (ANN) consider classification as one of the most dynamic research and application areas. ANN is the branch of Artificial Intelligence (AI). The neural network was trained by back propagation algorithm [1]. A neural network represent a mathematical models of information processing, benefited from the human biological systems (i.e. a brain or nerve cell), where the neutral network can be trained and learned the same as a human brain does. The learning will be done by changing the weight during the training process and by using certain formula. One of the most known neural networks is the Back Propagation Network. This net has been used in variety of application areas. One of the, the classification of certain objects by known only a portion of information of the object to be classified. In this paper we shall use the back propagation network to classify the human blood groups, also we shall use the same program to be same analysis to find the best number of neurons in hidden layer that gives lower number of iteration.},
keywords={backpropagation;blood;learning (artificial intelligence);medical computing;neural nets;patient diagnosis;pattern classification;ANN;propagation algorithm;information processing;human biological systems;neutral network;human brain;training process;back propagation network;artificial intelligence;artificial neural networks;hidden layer;object classification;Backpropagation;Neurons;Blood;Biological neural networks;Training;Artificial neural networks;Cost function;Artificial neural networks (ANN);Back propagation network;Classification},
doi={10.1109/ICSCEE.2018.8538383},
ISSN={},
month={July},}
@ARTICLE{1603636,
author={G. {Costantini} and D. {Casali} and R. {Perfetti}},
journal={IEEE Transactions on Neural Networks},
title={Associative memory design for 256 gray-level images using a multilayer neural network},
year={2006},
volume={17},
number={2},
pages={519-522},
abstract={A design procedure is presented for neural associative memories storing gray-scale images. It is an evolution of a previous work based on the decomposition of the image with 2/sup L/ gray levels into L binary patterns, stored in L uncoupled neural networks. In this letter, an L-layer neural network is proposed with both intralayer and interlayer connections. The connections between different layers introduce interactions among all the neurons, increasing the recall performance with respect to the uncoupled case. In particular, the proposed network can store images with the commonly used number of 256 gray levels instead of 16, as in the previous approach.},
keywords={neural nets;content-addressable storage;neural associative memory design;gray-level images;multilayer neural network;2/sup L/ gray levels;L binary patterns;L uncoupled neural networks;Multi-layer neural network;Associative memory;Neural networks;Neurons;Biological neural networks;Image storage;Gray-scale;Pixel;Cellular neural networks;Quantization;Associative memories;brain-state-in-a-box (BSB) neural networks;gray-scale images;multilayer architectures;Algorithms;Artificial Intelligence;Colorimetry;Computer Graphics;Computer Simulation;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Theoretical;Neural Networks (Computer);Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Signal Processing, Computer-Assisted},
doi={10.1109/TNN.2005.863465},
ISSN={1045-9227},
month={March},}
@ARTICLE{1356017,
author={H. K. {Lam} and F. H. F. {Leung}},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
title={Digit and command interpretation for electronic book using neural network and genetic algorithm},
year={2004},
volume={34},
number={6},
pages={2273-2283},
abstract={This work presents the interpretation of digits and commands using a modified neural network and the genetic algorithm. The modified neural network exhibits a node-to-node relationship which enhances its learning and generalization abilities. A digit-and-command interpreter constructed by the modified neural networks is proposed to recognize handwritten digits and commands. A genetic algorithm is employed to train the parameters of the modified neural networks of the digit-and-command interpreter. The proposed digit-and-command interpreter is successfully realized in an electronic book. Simulation and experimental results will be presented to show the applicability and merits of the proposed approach.},
keywords={genetic algorithms;neural nets;learning (artificial intelligence);generalisation (artificial intelligence);electronic publishing;handwritten character recognition;digit interpretation;command interpretation;neural network;genetic algorithm;artificial intelligence learning;artificial intelligence generalization;electronic book;Electronic publishing;Neural networks;Genetic algorithms;Biological neural networks;Signal processing algorithms;Backpropagation algorithms;Error correction;Neurons;Books;Handwriting recognition;Digit and command interpretation;electronic book;genetic algorithm;neural networks;Algorithms;Artificial Intelligence;Automatic Data Processing;Books;Computer Graphics;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Neural Networks (Computer);Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Publishing;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;User-Computer Interface;Word Processing},
doi={10.1109/TSMCB.2004.834432},
ISSN={1083-4419},
month={Dec},}
@INPROCEEDINGS{7966169,
author={R. {Kamimura}},
booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
title={Potential layer-wise supervised learning for training multi-layered neural networks},
year={2017},
volume={},
number={},
pages={2568-2575},
abstract={The present paper tries to show that the greedy layer-wise supervised learning becomes effective enough to improve generalization and interpretation by the help of potential learning. It has been observed that unsupervised pre-training has a shortcoming of vanishing information as is the case of simple multi-layered network training. When the layer becomes higher, valuable information becomes smaller. Information through many different layers tends to diminish considerably and naturally from the information-theoretic point of view. For this, we use the layer-wise supervised training to prevent information from diminishing. The supervised learning has been said to be not good for pre-training for multi-layered neural networks. However, we have found that the new potential learning can be effectively used to extract valuable information through supervised pre-training. With the help of important components extracted by the potential learning, the supervised pre-training becomes effective for training multi-layered neural networks. We applied the method to two data sets, namely, an artificial and banknote data sets. In both cases, the potential learning proved to be effective in increasing generalization performance. In addition, we could show a possibility that final representation by this method could be clearly understood.},
keywords={generalisation (artificial intelligence);greedy algorithms;information theory;learning (artificial intelligence);neural nets;potential layerwise supervised learning;multilayered neural networks training;greedy layerwise supervised learning;generalization;interpretation;information theory;Neurons;Biological neural networks;Training;Machine learning;Supervised learning;Minimization},
doi={10.1109/IJCNN.2017.7966169},
ISSN={2161-4407},
month={May},}
@ARTICLE{7185326,
author={X. {Wang} and X. {Li} and V. C. M. {Leung}},
journal={IEEE Access},
title={Artificial Intelligence-Based Techniques for Emerging Heterogeneous Network: State of the Arts, Opportunities, and Challenges},
year={2015},
volume={3},
number={},
pages={1379-1391},
abstract={Recently, mobile networking systems have been designed with more complexity of infrastructure and higher diversity of associated devices and resources, as well as more dynamical formations of networks, due to the fast development of current Internet and mobile communication industry. In such emerging mobile heterogeneous networks (HetNets), there are a large number of technical challenges focusing on the efficient organization, management, maintenance, and optimization, over the complicated system resources. In particular, HetNets have attracted great interest from academia and industry in deploying more effective solutions based on artificial intelligence (AI) techniques, e.g., machine learning, bio-inspired algorithms, fuzzy neural network, and so on, because AI techniques can naturally handle the problems of large-scale complex systems, such as HetNets towards more intelligent and automatic-evolving ones. In this paper, we discuss the state-of-the-art AI-based techniques for evolving the smarter HetNets infrastructure and systems, focusing on the research issues of self-configuration, self-healing, and self-optimization, respectively. A detailed taxonomy of the related AI-based techniques of HetNets is also shown by discussing the pros and cons for various AI-based techniques for different problems in HetNets. Opening research issues and pending challenges are concluded as well, which can provide guidelines for future research work.},
keywords={fault tolerant computing;fuzzy neural nets;internetworking;learning (artificial intelligence);radio access networks;artificial intelligence-based techniques;heterogeneous network;mobile networking systems;infrastructure complexity;dynamical network formation;mobile heterogeneous networks;system resources;HetNets;AI techniques;machine learning;bio-inspired algorithms;fuzzy neural network;large-scale complex systems;self-configuration;self-healing;self-optimization;Mobile communication;Genetic algorithms;Ant colony optimization;Artificial intelligence;Heterogeneous networks;Biological system modeling;Complexity theory;Neural networks;Artificial Intelligence;Genetic Algorithms;Ant Colony Optimization;Self-Organization Networks;Heterogeneous Networks;Artificial intelligence;genetic algorithms;ant colony optimization;self-organization networks;heterogeneous networks},
doi={10.1109/ACCESS.2015.2467174},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8490433,
author={J. {Zhu} and A. {Liapis} and S. {Risi} and R. {Bidarra} and G. M. {Youngblood}},
booktitle={2018 IEEE Conference on Computational Intelligence and Games (CIG)},
title={Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},
year={2018},
volume={},
number={},
pages={1-8},
abstract={Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
keywords={computer games;human computer interaction;learning (artificial intelligence);game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework;Games;Task analysis;Machine learning;Neurons;Visualization;Tools;explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design},
doi={10.1109/CIG.2018.8490433},
ISSN={2325-4289},
month={Aug},}
@ARTICLE{6410433,
author={W. {Yeh}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={New Parameter-Free Simplified Swarm Optimization for Artificial Neural Network Training and its Application in the Prediction of Time Series},
year={2013},
volume={24},
number={4},
pages={661-665},
abstract={A new soft computing method called the parameter-free simplified swarm optimization (SSO)-based artificial neural network (ANN), or improved SSO for short, is proposed to adjust the weights in ANNs. The method is a modification of the SSO, and seeks to overcome some of the drawbacks of SSO. In the experiments, the iSSO is compared with five other famous soft computing methods, including the backpropagation algorithm, the genetic algorithm, the particle swarm optimization (PSO) algorithm, cooperative random learning PSO, and the SSO, and its performance is tested on five famous time-series benchmark data to adjust the weights of two ANN models (multilayer perceptron and single multiplicative neuron model). The experimental results demonstrate that iSSO is robust and more efficient than the other five algorithms.},
keywords={learning (artificial intelligence);multilayer perceptrons;particle swarm optimisation;time series;parameter-free simplified swarm optimization;artificial neural network training;time series prediction;soft computing method;multilayer perceptron;single multiplicative neuron model;Artificial neural networks;Time series analysis;Particle swarm optimization;Forecasting;Biological neural networks;Training;Predictive models;Artificial intelligence;evolutionary computation;machine learning;neural network;Artificial Intelligence;Computing Methodologies;Forecasting;Neural Networks (Computer);Time Factors},
doi={10.1109/TNNLS.2012.2232678},
ISSN={2162-237X},
month={April},}
@ARTICLE{1359749,
author={S. {Marinai} and M. {Gori} and G. {Soda}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Artificial neural networks for document analysis and recognition},
year={2005},
volume={27},
number={1},
pages={23-35},
abstract={Artificial neural networks have been extensively applied to document analysis and recognition. Most efforts have been devoted to the recognition of isolated handwritten and printed characters with widely recognized successful results. However, many other document processing tasks, like preprocessing, layout analysis, character segmentation, word recognition, and signature verification, have been effectively faced with very promising results. This paper surveys the most significant problems in the area of offline document image processing, where connectionist-based approaches have been applied. Similarities and differences between approaches belonging to different categories are discussed. A particular emphasis is given on the crucial role of prior knowledge for the conception of both appropriate architectures and learning algorithms. Finally, the paper provides a critical analysts on the reviewed approaches and depicts the most promising research guidelines in the field. In particular, a second generation of connectionist-based models are foreseen which are based on appropriate graphical representations of the learning environment.},
keywords={document image processing;recurrent neural nets;handwritten character recognition;image segmentation;handwriting recognition;learning (artificial intelligence);artificial neural networks;document image analysis;document image recognition;handwritten recognition;character recognition;layout analysis;character segmentation;word recognition;signature verification;offline document image processing;connectionist based approach;learning algorithms;document preprocessing;recurrent neural nets;graphical representations;Artificial neural networks;Text analysis;Character recognition;Handwriting recognition;Image analysis;Image recognition;Neural networks;Optical character recognition software;Image segmentation;Face recognition;Index Terms- Character segmentation;document image analysis and recognition;layout analysis;neural networks;preprocessing;recursive neural networks;word recognition.;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Neural Networks (Computer);Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;User-Computer Interface},
doi={10.1109/TPAMI.2005.4},
ISSN={0162-8828},
month={Jan},}
@ARTICLE{8466590,
author={A. {Adadi} and M. {Berrada}},
journal={IEEE Access},
title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
year={2018},
volume={6},
number={},
pages={52138-52160},
abstract={At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
keywords={artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
doi={10.1109/ACCESS.2018.2870052},
ISSN={2169-3536},
month={},}
@ARTICLE{6841041,
author={T. {Teng} and A. {Tan} and J. M. {Zurada}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Self-Organizing Neural Networks Integrating Domain Knowledge and Reinforcement Learning},
year={2015},
volume={26},
number={5},
pages={889-902},
abstract={The use of domain knowledge in learning systems is expected to improve learning efficiency and reduce model complexity. However, due to the incompatibility with knowledge structure of the learning systems and real-time exploratory nature of reinforcement learning (RL), domain knowledge cannot be inserted directly. In this paper, we show how self-organizing neural networks designed for online and incremental adaptation can integrate domain knowledge and RL. Specifically, symbol-based domain knowledge is translated into numeric patterns before inserting into the self-organizing neural networks. To ensure effective use of domain knowledge, we present an analysis of how the inserted knowledge is used by the self-organizing neural networks during RL. To this end, we propose a vigilance adaptation and greedy exploitation strategy to maximize exploitation of the inserted domain knowledge while retaining the plasticity of learning and using new knowledge. Our experimental results based on the pursuit-evasion and minefield navigation problem domains show that such self-organizing neural network can make effective use of domain knowledge to improve learning efficiency and reduce model complexity.},
keywords={computational complexity;greedy algorithms;learning (artificial intelligence);self-organising feature maps;self-organizing neural networks;domain knowledge;reinforcement learning;learning systems;learning efficiency;model complexity;RL;incremental adaptation;online adaptation;symbol-based domain knowledge;greedy exploitation strategy;minefield navigation problem;pursuit-evasion;Vectors;Knowledge engineering;Neural networks;Learning systems;Learning (artificial intelligence);Bayes methods;Training;Adaptive resonance theory (ART);domain knowledge;reinforcement learning (RL);self-organizing neural networks.;Adaptive resonance theory (ART);domain knowledge;reinforcement learning (RL);self-organizing neural networks;Algorithms;Cognition;Computer Simulation;Humans;Knowledge;Models, Theoretical;Neural Networks (Computer);Pattern Recognition, Automated;Reinforcement (Psychology)},
doi={10.1109/TNNLS.2014.2327636},
ISSN={2162-237X},
month={May},}
@INPROCEEDINGS{4469183,
author={M. {Uzak} and I. {Vertal'} and R. {Jaksa} and P. {Sincak}},
booktitle={2008 6th International Symposium on Applied Machine Intelligence and Informatics},
title={Reduction of Visual Information in Neural Network Learning Process Visualization},
year={2008},
volume={},
number={},
pages={279-284},
abstract={Visualization of the learning of neural network faces the problem of dealing with overwhelming amount of visual information. This paper describes the application of clustering methods for reduction of visual information in the response function visualization. When only clusters of neurons are visualized, instead of direct visualization of responses of all neurons in the network, the amount of visually presented information can be significantly reduced. This is useful for reducing user fatigue and also for minimizing the visualization equipment requirements. We show, that application of Kohonen network or growing neural gas with utility factor algorithm allows to visualize the learning of moderate-sized neural networks in real time. Comparison of both algorithms in this task is provided, also with performance analysis and example results of response function visualization.},
keywords={data visualisation;learning (artificial intelligence);neural nets;visual information;neural network learning process visualization;clustering methods;response function visualization;Kohonen network;utility factor algorithm;growing neural gas;Neural networks;Visualization;Neurons;Artificial neural networks;Humans;Learning;Artificial intelligence;Clustering methods;Two dimensional displays;Silicon compounds},
doi={10.1109/SAMI.2008.4469183},
ISSN={},
month={Jan},}
@INPROCEEDINGS{1224095,
author={ and C. {Hinde} and D. {Gillingwater}},
booktitle={Proceedings of the International Joint Conference on Neural Networks, 2003.},
title={A new method for explaining neural network reasoning},
year={2003},
volume={4},
number={},
pages={3256-3260 vol.4},
abstract={This paper presents a new method for explaining the reasoning results of a trained neural network. The method considers the most significant attribute first under the guidance of a relative strength of effect analysis and eliminates irrelevant points. Following the adaptive search in the dynamic state space, a set of relevant points are extracted and form the basis of the explanation of the neural network reasoning. Combining a relative strength of effect analysis with the relevant points, a case based explanation approach is put forward. As an illustration, an experiment with a small data set on the relationship between weather conditions and play decisions is presented to demonstrate the utility of the proposed approach.},
keywords={neural nets;learning (artificial intelligence);explanation;inference mechanisms;neural network reasoning;trained neural network;relative strength of effect analysis;adaptive search;dynamic state space;relevant point extraction;case based explanation approach;weather conditions;play decisions;Neural networks;Data mining;Knowledge representation;Computational intelligence;Intelligent structures;Computer science;State-space methods;Information analysis;Training data;Artificial neural networks},
doi={10.1109/IJCNN.2003.1224095},
ISSN={1098-7576},
month={July},}
@ARTICLE{6994284,
author={M. {Kusy} and R. {Zajdel}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Application of Reinforcement Learning Algorithms for the Adaptive Computation of the Smoothing Parameter for Probabilistic Neural Network},
year={2015},
volume={26},
number={9},
pages={2163-2175},
abstract={In this paper, we propose new methods for the choice and adaptation of the smoothing parameter of the probabilistic neural network (PNN). These methods are based on three reinforcement learning algorithms: Q(0)-learning, Q(λ)-learning, and stateless Q-learning. We regard three types of PNN classifiers: the model that uses single smoothing parameter for the whole network, the model that utilizes single smoothing parameter for each data attribute, and the model that possesses the matrix of smoothing parameters different for each data variable and data class. Reinforcement learning is applied as the method of finding such a value of the smoothing parameter, which ensures the maximization of the prediction ability. PNN models with smoothing parameters computed according to the proposed algorithms are tested on eight databases by calculating the test error with the use of the cross validation procedure. The results are compared with state-of-the-art methods for PNN training published in the literature up to date and, additionally, with PNN whose sigma is determined by means of the conjugate gradient approach. The results demonstrate that the proposed approaches can be used as alternative PNN training procedures.},
keywords={conjugate gradient methods;learning (artificial intelligence);neural nets;smoothing parameter;probabilistic neural network;reinforcement learning algorithms;Q(0)-learning;Q(λ)-learning;stateless Q-learning;PNN classifiers;data attribute;data variable;conjugate gradient approach;PNN training;Smoothing methods;Learning (artificial intelligence);Neurons;Training;Computational modeling;Optimization;Neural networks;Prediction ability;probabilistic neural network (PNN);reinforcement learning;smoothing parameter.;Prediction ability;probabilistic neural network (PNN);reinforcement learning;smoothing parameter;Algorithms;Humans;Models, Theoretical;Neural Networks (Computer);Probability;Reinforcement (Psychology)},
doi={10.1109/TNNLS.2014.2376703},
ISSN={2162-237X},
month={Sep.},}
@ARTICLE{5993545,
author={S. {Razavi} and B. A. {Tolson}},
journal={IEEE Transactions on Neural Networks},
title={A New Formulation for Feedforward Neural Networks},
year={2011},
volume={22},
number={10},
pages={1588-1598},
abstract={Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
keywords={feedforward neural nets;function approximation;generalisation (artificial intelligence);learning (artificial intelligence);optimisation;function approximation techniques;black box model;neural network functional geometry;geometrical interpretation;reformulated neural network;feedforward neural network;error response surface;learning ability;training method;derivative free optimization algorithm;generalization ability;ReNN approach;Biological neural networks;Training;Nickel;Optimization;Function approximation;Neurons;Feedforward neural networks;generalization;geometrical interpretation;internal behavior;measure of regularization;reformulated neural network;training;Algorithms;Artificial Intelligence;Feedback;Models, Neurological;Neural Networks (Computer);Software Design},
doi={10.1109/TNN.2011.2163169},
ISSN={1045-9227},
month={Oct},}
@INPROCEEDINGS{7861177,
author={M. C. {Ergene} and A. {Durdu} and H. {Cetin}},
booktitle={2016 8th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)},
title={Imitation and learning of human hand gesture tasks of the 3D printed robotic hand by using artificial neural networks},
year={2016},
volume={},
number={},
pages={1-6},
abstract={In this study social learning and skill acquisition of a robotic hand via teaching and imitation was aimed. The subject of Human-Robot collaboration, which includes the theme of this paper, is a common field of experiments in our age of technology. Many disabilities can be defeated or many other things, which a human being would not be able to do, can be done with the help of this technology. As an example, a robotic hand can be a light of hope of a person who does not have a hand or wants to hold an object remotely over the internet. So that in our paper it is explained how a robotic hand can learn via imitation. In the experiment a robotic hand, which was printed by a 3D printer, was used and controlled wirelessly by a computer that recognizes human hand gesture via image processing algorithms. The communication between the computer and the robot is provided with a Bluetooth module. First of all, the image processing algorithms such as filtering and background subtraction were applied to the frames of the camera and extracted the features. Secondly, the process of teaching and testing of Artificial Neural Networks (ANNs) was made for the recognition of the hand and the gestures. After that, recognized actions were imitated by the robotic-hand hardware. Eventually, the learning of the robot via imitation was achieved with some small errors and the results are given at the end of the paper.},
keywords={feature extraction;gesture recognition;human-robot interaction;manipulators;neural nets;human hand gesture tasks;3D printed robotic hand;artificial neural networks;human-robot collaboration;Internet;image processing algorithms;Bluetooth module;feature extraction;ANN;Neurons;Artificial neural networks;Image processing;Maximum likelihood detection;Nonlinear filters;Computer vision;component;image processing;human-robot interaction;artificial neural networks;skill acquisition;learning;imitation;robotic hand},
doi={10.1109/ECAI.2016.7861177},
ISSN={},
month={June},}
@INPROCEEDINGS{8452744,
author={S. {Leavy}},
booktitle={2018 IEEE/ACM 1st International Workshop on Gender Equality in Software Engineering (GE)},
title={Gender Bias in Artificial Intelligence: The Need for Diversity and Gender Theory in Machine Learning},
year={2018},
volume={},
number={},
pages={14-16},
abstract={Artificial intelligence is increasingly influencing the opinions and behaviour of people in everyday life. However, the over-representation of men in the design of these technologies could quietly undo decades of advances in gender equality. Over centuries, humans developed critical theory to inform decisions and avoid basing them solely on personal experience. However, machine intelligence learns primarily from observing data that it is presented with. While a machine's ability to process large volumes of data may address this in part, if that data is laden with stereotypical concepts of gender, the resulting application of the technology will perpetuate this bias. While some recent studies sought to remove bias from learned algorithms they largely ignore decades of research on how gender ideology is embedded in language. Awareness of this re-search and incorporating it into approaches to machine learning from text would help prevent the generation of biased algorithms. Leading thinkers in the emerging field addressing bias in artificial intelligence are also primarily female, suggesting that those who are potentially affected by bias are more likely to see, understand and attempt to resolve it. Gender balance in machine learning is therefore crucial to prevent algorithms from perpetuating gender ideologies that disadvantage women.},
keywords={gender issues;learning (artificial intelligence);gender ideology;machine learning;artificial intelligence;gender bias;gender theory;gender equality;machine intelligence;Machine learning algorithms;Machine learning;Training data;Linguistics;Media;Conferences;Gender Bias;Machine Learning;Text Analytics},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{1546920,
author={ and and },
booktitle={2005 IEEE/PES Transmission Distribution Conference Exposition: Asia and Pacific},
title={Development of an Artificial Intelligent Diagnosis System for Transformer Fault},
year={2005},
volume={},
number={},
pages={1-5},
abstract={This paper introduces artificial intelligence system method and describes the developing and application in transformer fault diagnosis. An artificial intelligent system (TFDAI) design includes selection of input, network topology, synaptic connection weights, two-passageway, and output. This paper introduces the new intelligence technology in the transformer fault diagnosis -TFDAI System. TFDAI based data processing and diagnostic techniques are described in detail. It consists of expert system (ES ), artificial neural network (ANN), guide-rule, two-passageway and their characteristics are presented. This paper mentions the two-passageway structure of the artificial intelligence system and with practical examples. ES and ANN are connected by the two-passageway effectively. It states that artificial intelligence system for transformer early hidden faults achieves the possibility and accuracy of primary diagnosis},
keywords={artificial intelligence;expert systems;fault diagnosis;neural nets;power engineering computing;power transformer testing;artificial intelligence;transformer fault diagnosis;network topology;synaptic connection;two-passageway;data processing;expert system;artificial neural network;guide-rule;Intelligent systems;Artificial intelligence;Fault diagnosis;Oil insulation;Power transformers;Power transformer insulation;Artificial neural networks;Dissolved gas analysis;Gas insulation;Petroleum;Expert system (ES);Artificial neural network (ANN);Artificial Intelligence (AI) Two-passageway;Transformer fault diagnosis},
doi={10.1109/TDC.2005.1546920},
ISSN={2160-8636},
month={Aug},}
@ARTICLE{6763063,
author={L. {Shao} and D. {Wu} and X. {Li}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Learning Deep and Wide: A Spectral Method for Learning Deep Networks},
year={2014},
volume={25},
number={12},
pages={2303-2308},
abstract={Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many computer vision-related tasks. We propose the multispectral neural networks (MSNN) to learn features from multicolumn deep neural networks and embed the penultimate hierarchical discriminative manifolds into a compact representation. The low-dimensional embedding explores the complementary property of different views wherein the distribution of each view is sufficiently smooth and hence achieves robustness, given few labeled training data. Our experiments show that spectrally embedding several deep neural networks can explore the optimum output from the multicolumn networks and consistently decrease the error rate compared with a single deep network.},
keywords={error statistics;learning (artificial intelligence);neural nets;spectral method;learning deep networks;building intelligent system;high-dimensional sensory data;computer vision-related task;multispectral neural networks;MSNN;multicolumn deep neural network;penultimate hierarchical discriminative manifolds;compact representation;low-dimensional embedding;deep neural networks;multicolumn networks;error rate;Feature extraction;Laplace equations;Training;Error analysis;Noise;Neural networks;Data models;Deep networks;multispectral embedding;representation learning.;Deep networks;multispectral embedding;representation learning;Artificial Intelligence;Databases, Factual;Humans;Neural Networks (Computer);Pattern Recognition, Visual},
doi={10.1109/TNNLS.2014.2308519},
ISSN={2162-237X},
month={Dec},}
@INPROCEEDINGS{287210,
author={ and and },
booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
title={A conceptual interpretation of spurious memories in the Hopfield-type neural network},
year={1992},
volume={1},
number={},
pages={21-26 vol.1},
abstract={It is shown that the spurious memories are represented as logical combinations of the learned memories. By assigning a conceptual interpretation to each learned memory, the spurious memories can be interpreted as novel conceptual knowledge created by the network. It is proposed that the generation of spurious memories be considered a primitive creativity that the simple network exhibits in high-level information processing.<<ETX>>},
keywords={Hebbian learning;Hopfield neural nets;knowledge representation;Hopfield-type neural network;spurious memories;learned memories;conceptual interpretation;conceptual knowledge;creativity;Intelligent networks;Neural networks;Hopfield neural networks;Neurons;Information processing;Samarium;Cities and towns;Artificial intelligence;Artificial neural networks;Art},
doi={10.1109/IJCNN.1992.287210},
ISSN={},
month={June},}
@INPROCEEDINGS{8388371,
author={W. {Pietrowski} and K. {Górny} and G. {Wiśniewski}},
booktitle={2018 International Interdisciplinary PhD Workshop (IIPhDW)},
title={Application of artificial neural network and OpenCL in spectral and wavelet analysis of phase current of LSPMS machine},
year={2018},
volume={},
number={},
pages={269-272},
abstract={The paper presents a parallel computing algorithm with its implementation in software for diagnostic of line start permanent magnet synchronous motor (LSPMSM). The software based on the developed algorithm, allows for analysis using a discrete Fourier transform (DFT) or a discrete wavelet transform (DWT). The elaborated software was tested using the phase current of the LSPMSM. In the case of wavelet analysis, the input signal refers to start-up of the motor supplied with symmetrical voltage, without external load, while steady-state waveforms were used for the DFT analysis. Moreover, the mentioned software has an implemented multi-layer perceptron neural network which can be used as decision element of the diagnostic system. In addition, the article brought closer the issues related to the structure and learning algorithms of artificial neural networks and OpenCL framework.},
keywords={discrete Fourier transforms;discrete wavelet transforms;learning (artificial intelligence);multilayer perceptrons;parallel algorithms;permanent magnet motors;synchronous motors;LSPMSM;phase current;wavelet analysis;input signal;steady-state waveforms;DFT analysis;multilayer perceptron neural network;diagnostic system;learning algorithms;artificial neural network;OpenCL framework;LSPMS machine;parallel computing algorithm;line start permanent magnet synchronous motor;discrete Fourier transform;discrete wavelet transform;software;spectral analysis;symmetrical voltage;decision element;DWT;Artificial neural networks;Kernel;Software algorithms;Graphics processing units;Wavelet analysis;Discrete Fourier transforms;Discrete Fourier transform;Discrete wavelet transforms;OpenCL;Parallel computing;Artificial neural networks;Fault analysis},
doi={10.1109/IIPHDW.2018.8388371},
ISSN={},
month={May},}
@ARTICLE{6808500,
author={M. F. {Mohammed} and C. P. {Lim}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={An Enhanced Fuzzy Min–Max Neural Network for Pattern Classification},
year={2015},
volume={26},
number={3},
pages={417-429},
abstract={An enhanced fuzzy min-max (EFMM) network is proposed for pattern classification in this paper. The aim is to overcome a number of limitations of the original fuzzy min-max (FMM) network and improve its classification performance. The key contributions are three heuristic rules to enhance the learning algorithm of FMM. First, a new hyperbox expansion rule to eliminate the overlapping problem during the hyperbox expansion process is suggested. Second, the existing hyperbox overlap test rule is extended to discover other possible overlapping cases. Third, a new hyperbox contraction rule to resolve possible overlapping cases is provided. Efficacy of EFMM is evaluated using benchmark data sets and a real medical diagnosis task. The results are better than those from various FMM-based models, support vector machine-based, Bayesian-based, decision tree-based, fuzzy-based, and neural-based classifiers. The empirical findings show that the newly introduced rules are able to realize EFMM as a useful model for undertaking pattern classification problems.},
keywords={fuzzy neural nets;learning (artificial intelligence);minimax techniques;pattern classification;enhanced fuzzy min-max neural network;EFMM network;classification performance;heuristic rules;learning algorithm;hyperbox expansion rule;overlapping problem;hyperbox expansion process;hyperbox overlap test rule;hyperbox contraction rule;overlapping cases;pattern classification problems;Training;Learning systems;Artificial neural networks;Adaptation models;Subspace constraints;Biological system modeling;Fuzzy min-max (FMM) model;hyperbox structure;neural network learning;pattern classification.;Fuzzy min–max (FMM) model;hyperbox structure;neural network learning;pattern classification;Fuzzy Logic;Humans;Machine Learning;Neural Networks (Computer);Pattern Recognition, Automated},
doi={10.1109/TNNLS.2014.2315214},
ISSN={2162-237X},
month={March},}
@INPROCEEDINGS{8511831,
author={T. W. {Kim} and B. R. {Routledge}},
booktitle={2018 IEEE Symposium on Privacy-Aware Computing (PAC)},
title={Informational Privacy, A Right to Explanation, and Interpretable AI},
year={2018},
volume={},
number={},
pages={64-74},
abstract={Businesses increasingly utilize secret algorithms and infringe users' informational privacy. We argue that to best protect users' online privacy, the use of an algorithm that assists with decisions or autonomously makes decisions that impact people requires a right to explanation.},
keywords={artificial intelligence;business data processing;data protection;informational privacy;interpretable AI;online privacy protection;Companies;Mathematical model;Privacy;Decision making;Loans and mortgages;Artificial intelligence;Law;GDPR;A right to explanation;Trust;Privacy;Explainable AI},
doi={10.1109/PAC.2018.00013},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{118383,
author={ and },
booktitle={International 1989 Joint Conference on Neural Networks},
title={A general explanation and interrogation system for neural networks},
year={1989},
volume={},
number={},
pages={594 vol.2-},
abstract={Summary form only given, as follows. The information in a trained neural network is stored as numerical weights in the neural elements and the connectivity pattern of the network. For many applications, it is desirable to have this neural network information converted into symbolic knowledge form for communication with human or machine experts. Techniques are presented for converting the information in a trained network into symbolic form as a set of rules and for obtaining explanations from the network for specific inputs. These two techniques provide the neurocomputer with one advantage of expert systems while retaining the learning and generalization capability of the neural network.<<ETX>>},
keywords={expert systems;explanation;neural nets;explanation;interrogation system;neural networks;numerical weights;connectivity pattern;symbolic knowledge form;neurocomputer;expert systems;learning;generalization capability;Expert systems;Explanation;Neural networks},
doi={10.1109/IJCNN.1989.118383},
ISSN={},
month={},}
@INPROCEEDINGS{170480,
author={B. -. {Zhang} and G. {Veenker}},
booktitle={[Proceedings] 1991 IEEE International Joint Conference on Neural Networks},
title={Neural networks that teach themselves through genetic discovery of novel examples},
year={1991},
volume={},
number={},
pages={690-695 vol.1},
abstract={The authors introduce an active learning paradigm for neural networks. In contrast to the passive paradigm, the learning in the active paradigm is initiated by the machine learner instead of its environment or teacher. The authors present a learning algorithm that uses a genetic algorithm for creating novel examples to teach multilayer feedforward networks. The creative learning networks, based on their own knowledge, discover new examples, criticize and select useful ones, train themselves, and thereby extend their existing knowledge. Experiments on function extrapolation show that the self-teaching neural networks not only reduce the teaching efforts of the human, but the genetically created examples also contribute robustly to the improvement of generalization performance and the interpretation of the connectionist knowledge.<<ETX>>},
keywords={extrapolation;learning systems;neural nets;genetic discovery;active learning paradigm;neural networks;machine learner;multilayer feedforward networks;function extrapolation;teaching efforts;generalization performance;connectionist knowledge;Neural networks;Artificial neural networks;Learning systems;Genetic algorithms;Unsupervised learning;Artificial intelligence;Supervised learning;Computer science;Multi-layer neural network;Extrapolation},
doi={10.1109/IJCNN.1991.170480},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4682483,
author={A. {Vega-Corona} and M. {Zárate-Banda} and J. M. {Barrón-Adame} and R. A. {Martínez-Celorio} and D. {Andina}},
booktitle={2008 Seventh Mexican International Conference on Artificial Intelligence},
title={Design of the Approximation Function of a Pedometer Based on Artificial Neural Network for the Healthy Life Style Promotion in Diabetic Patients},
year={2008},
volume={},
number={},
pages={325-329},
abstract={The present study describes the design of an Artificial Neural Network to synthesize the Approximation Function of a Pedometer for the Healthy Life Style Promotion. Experimentally, the approximation function is synthesized using three basic digital pedometers of low cost, these pedometers were calibrated with an advanced pedometer that calculates calories consumed and computes distance travelled with personal stride input. The synthesized approximation function by means of the designed neural network will allow to reply the calibration experiment for multiple patients with Diabetes Mellitus in Healthy Life Style promotion programs. Artificial Neural Networks have been developed for a wide variety of computational problems in cognition, pattern recognition, and decision making. The Healthy Life Style refer to adequate nutrient ingest, physical activity, time to rest, stress control, and a high self-esteem. The pedometer is a technological device that helps to control the physical activity in the diabetic patient. A brief description of the Artificial Neural Network designed to synthesize the Approximation Function, the obtained Artificial Neural Network structure and results in the Approximation Function synthesis for three patients are presented. The advantages and disadvantages of the method are discussed and our conclusions are presented.},
keywords={approximation theory;diseases;health care;neural nets;approximation function;artificial neural network;healthy life style promotion;diabetic patients;digital pedometers;diabetes mellitus;pattern recognition;decision making;Artificial neural networks;Diabetes;Network synthesis;Cost function;Calibration;Computer networks;Cognition;Pattern recognition;Decision making;Stress control;Artificial Neural Networks;Pedometer;Approximation Function;Diabetes Mellitus;Healthy Life Style},
doi={10.1109/MICAI.2008.24},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7034563,
author={I. C. {Stylios} and V. {Vlachos} and I. {Androulidakis}},
booktitle={2014 22nd Telecommunications Forum Telfor (TELFOR)},
title={Performance comparison of Machine Learning Algorithms for diagnosis of Cardiotocograms with class inequality},
year={2014},
volume={},
number={},
pages={951-954},
abstract={The objective of the present paper is to demonstrate the potential of Computational Intelligence in applications pertaining to the automatic identification - categorisation of Cardiotocograms using Machine Learning Algorithms and Artificial Neural Networks whose purpose is to distinguish between healthy or pathological cases leading to mortality during birth or fetal cerebral palsy. Interest is also placed on the performance of the Machine learning algorithms and the comparison of the classifiers' results.},
keywords={electrocardiography;learning (artificial intelligence);medical diagnostic computing;medical disorders;neural nets;obstetrics;performance comparison;machine learning algorithms;cardiotocogram diagnosis;class inequality;computational intelligence;automatic identification;categorisation;Artificial Neural Networks;healthy cases;pathological cases;mortality;birth;fetal cerebral palsy;Accuracy;Machine learning algorithms;Classification algorithms;Fetal heart rate;Training;Embryo;Educational institutions;Artificial Neural Networks;Cardiotocograms;Machine Learning Algorithms;WEKA},
doi={10.1109/TELFOR.2014.7034563},
ISSN={},
month={Nov},}
@ARTICLE{1528528,
author={O. {Basir} and F. {Karray} and },
journal={IEEE Transactions on Neural Networks},
title={Connectionist-based Dempster-Shafer evidential reasoning for data fusion},
year={2005},
volume={16},
number={6},
pages={1513-1530},
abstract={Dempster-Shafer evidence theory (DSET) is a popular paradigm for dealing with uncertainty and imprecision. Its corresponding evidential reasoning framework is theoretically attractive. However, there are outstanding issues that hinder its use in real-life applications. Two prominent issues in this regard are 1) the issue of basic probability assignments (masses) and 2) the issue of dependence among information sources. This paper attempts to deal with these issues by utilizing neural networks in the context of pattern classification application. First, a multilayer perceptron neural network with the mean squared error as a cost function is implemented to calculate, for each information source, posteriori probabilities for all classes. Second, an evidence structure construction scheme is developed for transferring the estimated posteriori probabilities to a set of masses along with the corresponding focal elements, from a Bayesian decision point of view. Third, a network realization of the Dempster-Shafer evidential reasoning is designed and analyzed, and it is further extended to a DSET-based neural network, referred to as DSETNN, to manipulate the evidence structures. In order to tackle the issue of dependence between sources, DSETNN is tuned for optimal performance through a supervised learning process. To demonstrate the effectiveness of the proposed approach, we apply it to three benchmark pattern classification problems. Experiments reveal that the DSETNN outperforms DSET and provide encouraging results in terms of classification accuracy and the speed of learning convergence.},
keywords={pattern classification;case-based reasoning;uncertainty handling;sensor fusion;neural nets;multilayer perceptrons;mean square error methods;Bayes methods;probability;learning (artificial intelligence);benchmark testing;connectionist-based Dempster-Shafer evidential reasoning;data fusion;Dempster-Shafer evidence theory;evidential reasoning framework;basic probability assignments;information sources;neural networks;multilayer perceptron neural network;mean squared error;cost function;posteriori probability;evidence structure construction scheme;focal elements;Bayesian decision;DSET-based neural network;DSETNN;supervised learning process;benchmark pattern classification;classification accuracy;learning convergence speed;Neural networks;Pattern classification;Uncertainty;Multilayer perceptrons;Multi-layer neural network;Cost function;Probability;Bayesian methods;Supervised learning;Convergence;Data fusion;Dempster–Shafer evidence theory (DSET);DSET-based neural network (DSETNN);neural network;Algorithms;Artificial Intelligence;Computer Simulation;Database Management Systems;Databases, Factual;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated},
doi={10.1109/TNN.2005.853337},
ISSN={1045-9227},
month={Nov},}
@ARTICLE{4806127,
author={J. A. {Starzyk} and H. {He}},
journal={IEEE Transactions on Neural Networks},
title={Spatio–Temporal Memories for Machine Learning: A Long-Term Memory Organization},
year={2009},
volume={20},
number={5},
pages={768-780},
abstract={Design of artificial neural structures capable of reliable and flexible long-term spatio-temporal memory is of paramount importance in machine intelligence. To this end, we propose a novel, biologically inspired, long-term memory (LTM) architecture. We intend to use it as a building block of a neuron-level architecture that is able to mimic natural intelligence through learning, anticipation, and goal-driven behavior. A mutual input enhancement and blocking structure is proposed, and its operation is discussed in detail. The paper focuses on a hierarchical memory organization, storage, recognition, and recall mechanisms. Simulation results of the proposed memory show its effectiveness, adaptability, and robustness. Accuracy of the proposed method is compared to other methods including Levenshtein distance method and a Markov chain.},
keywords={learning (artificial intelligence);Markov processes;neural nets;spatio-temporal memories;machine learning;long-term memory organization;artificial neural structures;machine intelligence;mutual input enhancement;blocking structure;Levenshtein distance method;Markov chain;Machine learning;Intelligent structures;Robustness;Humans;Machine intelligence;Memory architecture;Learning systems;Biological system modeling;Natural language processing;Embodied intelligence;hierarchical structure;long-term memory (LTM);memory robustness;spatio–temporal memory;Algorithms;Artificial Intelligence;Brain;Cognition;Computer Simulation;Feedback;Goals;Humans;Learning;Memory;Models, Neurological;Motivation;Neural Networks (Computer);Neurons;Space Perception;Time Perception},
doi={10.1109/TNN.2009.2012854},
ISSN={1045-9227},
month={May},}
@INPROCEEDINGS{939114,
author={Q. {Zhao}},
booktitle={IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)},
title={Training and retraining of neural network trees},
year={2001},
volume={1},
number={},
pages={726-731 vol.1},
abstract={In machine learning, symbolic approaches usually yield comprehensible results without free parameters for further (incremental) retraining. On the other hand, nonsymbolic (connectionist or neural network based) approaches usually yield black-boxes which are difficult to understand and reuse. The goal of this study is to propose a machine learner that is both incrementally retrainable and comprehensible through integration of decision trees and neural networks. In this paper, we introduce a kind of neural network trees (NNTrees), propose algorithms for their training and retraining, and verify the efficiency of the algorithms through experiments with a digit recognition problem.},
keywords={neural nets;learning (artificial intelligence);decision trees;neural network tree training;neural network tree retraining;machine learning;black-boxes;incremental retraining;decision trees;NNTrees;efficiency;digit recognition problem;Neural networks;Machine learning algorithms;Machine learning;Decision trees;Multi-layer neural network;Feature extraction;Data mining;Image recognition;Evolutionary computation},
doi={10.1109/IJCNN.2001.939114},
ISSN={1098-7576},
month={July},}
@INPROCEEDINGS{5334220,
author={P. S. {Kostka} and E. J. {Tkacz}},
booktitle={2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
title={Rules extraction in SVM and neural network classifiers of atrial fibrillation patients with matched wavelets as a feature generator},
year={2009},
volume={},
number={},
pages={4691-4694},
abstract={Presented paper describes a system of biomedical signal classifiers with preliminary feature extraction stage based on matched wavelets analysis, where two structures of classifier using Neural Networks (NN) and Support Vector Machine (SVM) are applied. As a pilot study the rules extraction algorithm applied for two of mentioned machine learning approaches (NN &amp; SVM) was used. This was made to extract and transform the representation of knowledge gathered in Black Box parameters during classifier learning phase to be better and natural understandable for human user/expert. Proposed system was tested on the set of ECG signals of 20 atrial fibrillation (AF) and 20 control group (CG) patients, divided into learning and verifying subsets, taken from MIT-BiH database. Obtained results showed, that the ability of generalization of created system, expressed as a measure of sensitivity and specificity increased, due to extracting and selectively choosing only the most representative features for analyzed AF detection problem. Classification results achieved by means of constructed matched wavelet, created for given AF detection features were better than indicators obtained for standard wavelet basic functions used in ECG time-frequency decomposition.},
keywords={electrocardiography;feature extraction;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;signal classification;support vector machines;time-frequency analysis;wavelet transforms;SVM;neural network classifiers;atrial fibrillation patients;matched wavelets analysis;feature generator;biomedical signal classifiers;feature extraction;neural networks;support vector machine;rules extraction algorithm;machine learning approaches;Black Box parameters;atrial fibrillation;ECG signals;MIT-BiH database;AF detection problem;standard wavelet basic functions;ECG time-frequency decomposition;Support vector machines;Support vector machine classification;Neural networks;Atrial fibrillation;Electrocardiography;Feature extraction;Wavelet analysis;Signal analysis;Machine learning algorithms;Machine learning;Algorithms;Atrial Fibrillation;Electrocardiography;Humans;Neural Networks (Computer);Pilot Projects;Sensitivity and Specificity;Signal Processing, Computer-Assisted},
doi={10.1109/IEMBS.2009.5334220},
ISSN={1094-687X},
month={Sep.},}
@ARTICLE{8012295,
author={G. {Hurlburt}},
journal={IT Professional},
title={How Much to Trust Artificial Intelligence?},
year={2017},
volume={19},
number={4},
pages={7-11},
abstract={Considerable buzz surrounds artificial intelligence, and, indeed, AI is all around us. As with any software-based technology, it is also prone to vulnerabilities. Here, the author examines how we determine whether AI is sufficiently reliable to do its job and how much we should trust its outcomes.},
keywords={artificial intelligence;trusted computing;trust;artificial intelligence;AI;software-based technology;Artificial intelligence;Cognitive systems;Distributed computing;Software engineering;Machine learning algorithms;artificial intelligence;machine learning;AI;distributed computing},
doi={10.1109/MITP.2017.3051326},
ISSN={1520-9202},
month={},}
@INPROCEEDINGS{7429360,
author={X. {Sun} and X. {Luo} and J. {Liu} and X. {Jiang} and J. {Zhang}},
booktitle={2015 11th International Conference on Semantics, Knowledge and Grids (SKG)},
title={Semantics in Deep Neural-Network Computing},
year={2015},
volume={},
number={},
pages={81-88},
abstract={Artificial Intelligence development is stepping into a new era due to the recent exciting achievements from neural network and statistical machine learning research communities. Statistic neural-computing based machine learning has been deemed as one of promising roads towards realizing the ideal of Artificial Intelligence promoted since last century. Learning is the key in making progress. Statistic machine learning is to obtain a probability distribution or a function from a set of training samples according to a certain optimization target over the training cost based on a predefined model. While there are many significant improvements in image, sound and text recognition and analyzing using neural network based learning strategies, a new open question emerges, that is, what is the next? To ask this question is to mean that the neural network solution is not an ultimate solution, and there will be more challenges to meet in coming future. We discussed these aspects of deep neural network research work in this paper and focused on semantics in deep neural-network computing models. We try to browse how semantics or knowledge are to be involved in deep neural network models and how the semantics and knowledge will be a key factor towards making more intelligent machines. We argue that priori semantics and knowledge in modelling a neural network is important, which could be the key for researchers to design intelligent machine models to perform complex tasks.},
keywords={learning (artificial intelligence);neural nets;statistical distributions;semantics;deep neural-network computing;artificial intelligence development;statistical machine learning research communities;statistic neural-computing based machine learning;probability distribution;training cost;neural network based learning strategies;intelligent machine model design;Artificial intelligence;Semantics;Artificial neural networks;Computational modeling;Biological neural networks;Image classification;semantics;knowledge;artificial intelligence;deep neural network},
doi={10.1109/SKG.2015.42},
ISSN={},
month={Aug},}
@INPROCEEDINGS{200004,
author={X. {Guan} and R. J. {Mural} and J. R. {Einstein} and R. C. {Mann} and E. C. {Uberbacher}},
booktitle={Proceedings Eighth Conference on Artificial Intelligence for Applications},
title={GRAIL: an integrated artificial intelligence system for gene recognition and interpretation},
year={1992},
volume={},
number={},
pages={9-13},
abstract={The development of an integrated artificial intelligence system, GRAIL (gene recognition and analysis Internet link) is described. This system uses a combination of a multi-sensor/neural network, expert system, and parallel search tools to recognize and interpret genes in DNA sequences. A simple electronic mail (E-mail) interface makes the system accessible through Internet. The strength of the system in recognizing and interpreting genes in DNA sequences and the simple E-mail interface have already attracted more than 150 users. The success of the system is largely due to the multi-sensor/neural network approach and the integration of several AI tools. The modular development and flexible framework have made it easier to incorporate new knowledge and tools into the existing system.<<ETX>>},
keywords={biology computing;cellular biophysics;DNA;expert systems;neural nets;parallel processing;integrated artificial intelligence system;GRAIL;gene recognition;Internet link;multi-sensor/neural network;expert system;parallel search tools;DNA sequences;electronic mail;E-mail interface;AI tools;modular development;flexible framework;Artificial intelligence;Sequences;DNA;Neural networks;Expert systems;Humans;Genomics;Bioinformatics;Proteins;Data mining},
doi={10.1109/CAIA.1992.200004},
ISSN={},
month={March},}
@INPROCEEDINGS{4809255,
author={G. S. {Uttreshwar} and A. A. {Ghatol}},
booktitle={2009 IEEE International Advance Computing Conference},
title={Hepatitis B Diagnosis Using Logical Inference And Generalized Regression Neural Networks},
year={2009},
volume={},
number={},
pages={1587-1595},
abstract={Medical diagnosis is considered an art regardless of all standardization efforts made, which is greatly due to the fact that medical diagnosis necessitates an expertise in coping with uncertainty simply not found in today's computing machinery. The researchers are encouraged by the advancement in computer technology to develop software to assist doctors in making decision without necessitating the direct consultation with the specialists. Comprehensibility is very significant for any machine learning technique to be used in computer-aided medical diagnosis. Since an artificial neural network ensemble is composed of multiple artificial neural networks, its comprehensibility is worse than that of a single artificial neural network. For medical problems, a reasonably high-quality solution could be given by the neural network algorithms. In this paper, application of artificial intelligence in typical disease Hepatitis B diagnosis has been investigated. In this research, an intelligent system based on logical inference along with a generalized regression neural network is presented for the diagnosis. An expert system based on logical inference is used to decide what type of hepatitis is possible to appear for a patient, whether it is Hepatitis B or not. Then artificial neural networks will be used in order to do the predictions regarding hepatitis B. The Generalized regression neural network is applied to hepatitis data for predictions regarding the Hepatitis B which gives severity level on the patient. Results obtained show that generalized regression neural network can be successfully used for diagnosing hepatitis B. The role of effective diagnosis and the advantages of data training on neural networks-based automatic medical diagnosis system are suggested by the outcomes.},
keywords={diagnostic reasoning;diseases;learning (artificial intelligence);medical diagnostic computing;medical expert systems;neural nets;regression analysis;Hepatitis B disease diagnosis;logical inference;generalized regression neural network;computer-aided medical diagnosis;computer technology;software development;decision making;machine learning technique;artificial neural network ensemble algorithm;intelligent expert system;Liver diseases;Neural networks;Artificial neural networks;Medical diagnosis;Artificial intelligence;Art;Standardization;Uncertainty;Machinery;Machine learning;Medical Diagnosis;Artificial Intelligence, Neural Networks;Hepatitis B;Generalized regression neural network;Hepatitis B virus (HBV);Hepatitis B DNA;Spleen Palpable;Spiders;Hepatitis B surface Antigen (AgHBs)},
doi={10.1109/IADCC.2009.4809255},
ISSN={},
month={March},}
@ARTICLE{4554032,
author={G. G. {Lendaris}},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
title={Higher Level Application of ADP: A Next Phase for the Control Field?},
year={2008},
volume={38},
number={4},
pages={901-912},
abstract={Two distinguishing features of humanlike control vis-a-vis current technological control are the ability to make use of experience while selecting a control policy for distinct situations and the ability to do so faster and faster as more experience is gained (in contrast to current technological implementations that slow down as more knowledge is stored). The notions of context and context discernment are important to understanding this human ability. Whereas methods known as adaptive control and learning control focus on modifying the design of a controller as changes in context occur, experience-based (EB) control entails <i>selecting</i> a previously designed controller that is appropriate to the current situation. Developing the EB approach entails a shift of the technologist's focus ldquoup a levelrdquo away from designing individual (optimal) controllers to that of developing online algorithms that efficiently and effectively select designs from a repository of existing controller solutions. A key component of the notions presented here is that of higher level learning algorithm. This is a new application of reinforcement learning and, in particular, approximate dynamic programming, with its focus shifted to the posited higher level, and is employed, with very promising results. The author's hope for this paper is to inspire and guide future work in this promising area.},
keywords={adaptive control;dynamic programming;learning (artificial intelligence);learning systems;higher level application;humanlike control;context discernment;adaptive control;learning control;individual controllers;reinforcement learning;dynamic programming;Humans;Optimal control;Learning;Dynamic programming;Artificial intelligence;Control systems;Algorithm design and analysis;System identification;Adaptive control;Artificial neural networks;Approximate dynamic programming (ADP);artificial intelligence (AI);context;context discernment;experience-based identification and control (EBIC);neural networks (NNs);optimal control;reinforcement learning (RL);system identification (SID);Approximate dynamic programming (ADP);artificial intelligence (AI);context;context discernment;experience-based identification and control (EBIC);neural networks (NNs);optimal control;reinforcement learning (RL);system identification (SID);Artificial Intelligence;Biomimetics;Feedback;Humans;Learning;Programming, Linear;Systems Theory},
doi={10.1109/TSMCB.2008.918073},
ISSN={1083-4419},
month={Aug},}
@ARTICLE{1504688,
author={J. {Hawkins} and M. {Boden}},
journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
title={The applicability of recurrent neural networks for biological sequence analysis},
year={2005},
volume={2},
number={3},
pages={243-253},
abstract={Selection of machine learning techniques requires a certain sensitivity to the requirements of the problem. In particular, the problem can be made more tractable by deliberately using algorithms that are biased toward solutions of the requisite kind. In this paper, we argue that recurrent neural networks have a natural bias toward a problem domain of which biological sequence analysis tasks are a subset. We use experiments with synthetic data to illustrate this bias. We then demonstrate that this bias can be exploitable using a data set of protein sequences containing several classes of subcellular localization targeting peptides. The results show that, compared with feed forward, recurrent neural networks will generally perform better on sequence analysis tasks. Furthermore, as the patterns within the sequence become more ambiguous, the choice of specific recurrent architecture becomes more critical.},
keywords={proteins;molecular biophysics;cellular biophysics;molecular configurations;recurrent neural nets;learning (artificial intelligence);feedforward neural nets;biology computing;recurrent neural networks;biological sequence analysis;machine learning;protein sequences;subcellular localization targeting peptides;feed forward recurrent neural networks;Recurrent neural networks;Sequences;Neural networks;Machine learning;Machine learning algorithms;Feeds;Feedforward neural networks;Proteins;Hidden Markov models;Peptides;Index Terms- Machine learning;neural network architecture;recurrent neural network;bias;biological sequence analysis;motif;subcellular localization;pattern recognition;classifier design.;Algorithms;Amino Acid Sequence;Gene Expression Profiling;Molecular Sequence Data;Multigene Family;Neural Networks (Computer);Pattern Recognition, Automated;Proteins;Proteins;Sequence Analysis, Protein;Subcellular Fractions},
doi={10.1109/TCBB.2005.44},
ISSN={1545-5963},
month={July},}
@ARTICLE{4267862,
author={D. {Dancey} and Z. A. {Bandar} and D. {McLean}},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
title={Logistic Model Tree Extraction From Artificial Neural Networks},
year={2007},
volume={37},
number={4},
pages={794-802},
abstract={Artificial neural networks (ANNs) are a powerful and widely used pattern recognition technique. However, they remain "black boxes" giving no explanation for the decisions they make. This paper presents a new algorithm for extracting a logistic model tree (LMT) from a neural network, which gives a symbolic representation of the knowledge hidden within the ANN. Landwehr's LMTs are based on standard decision trees, but the terminal nodes are replaced with logistic regression functions. This paper reports the results of an empirical evaluation that compares the new decision tree extraction algorithm with Quinlan's C4.5 and ExTree. The evaluation used 12 standard benchmark datasets from the university of California, Irvine machine-learning repository. The results of this evaluation demonstrate that the new algorithm produces decision trees that have higher accuracy and higher fidelity than decision trees created by both C4.5 and ExTree.},
keywords={decision making;decision trees;learning (artificial intelligence);neural nets;pattern recognition;regression analysis;logistic model tree extraction;artificial neural networks;pattern recognition;black boxes;decision making;decision trees;logistic regression;machine-learning repository;C4.5;ExTree;Logistics;Artificial neural networks;Neural networks;Decision trees;Data mining;Pattern recognition;Regression tree analysis;Multi-layer neural network;Feedforward neural networks;Intelligent networks;Artificial intelligence;feedforward neural networks;multilayer perceptrons (MPLs);neural networks;Algorithms;Computer Simulation;Decision Support Techniques;Logistic Models;Neural Networks (Computer);Pattern Recognition, Automated},
doi={10.1109/TSMCB.2007.895334},
ISSN={1083-4419},
month={Aug},}
@ARTICLE{5659484,
author={P. A. {Gutierrez} and C. {Hervas-Martinez} and F. J. {Martinez-Estudillo}},
journal={IEEE Transactions on Neural Networks},
title={Logistic Regression by Means of Evolutionary Radial Basis Function Neural Networks},
year={2011},
volume={22},
number={2},
pages={246-263},
abstract={This paper proposes a hybrid multilogistic methodology, named logistic regression using initial and radial basis function (RBF) covariates. The process for obtaining the coefficients is carried out in three steps. First, an evolutionary programming (EP) algorithm is applied, in order to produce an RBF neural network (RBFNN) with a reduced number of RBF transformations and the simplest structure possible. Then, the initial attribute space (or, as commonly known as in logistic regression literature, the covariate space) is transformed by adding the nonlinear transformations of the input variables given by the RBFs of the best individual in the final generation. Finally, a maximum likelihood optimization method determines the coefficients associated with a multilogistic regression model built in this augmented covariate space. In this final step, two different multilogistic regression algorithms are applied: one considers all initial and RBF covariates (multilogistic initial-RBF regression) and the other one incrementally constructs the model and applies cross validation, resulting in an automatic covariate selection [simplelogistic initial-RBF regression (SLIRBF)]. Both methods include a regularization parameter, which has been also optimized. The methodology proposed is tested using 18 benchmark classification problems from well-known machine learning problems and two real agronomical problems. The results are compared with the corresponding multilogistic regression methods applied to the initial covariate space, to the RBFNNs obtained by the EP algorithm, and to other probabilistic classifiers, including different RBFNN design methods [e.g., relaxed variable kernel density estimation, support vector machines, a sparse classifier (sparse multinomial logistic regression)] and a procedure similar to SLIRBF but using product unit basis functions. The SLIRBF models are found to be competitive when compared with the corresponding multilogistic regression methods and the RBFEP method. A measure of statistical significance is used, which indicates that SLIRBF reaches the state of the art.},
keywords={covariance analysis;learning (artificial intelligence);maximum likelihood estimation;probability;radial basis function networks;regression analysis;evolutionary radial basis function neural networks;hybrid multilogistic methodology;radial basis function covariates;RBF covariates;evolutionary programming algorithm;EP algorithm;RBF neural network;RBF transformations;attribute space;logistic regression literature;nonlinear transformations;maximum likelihood optimization method;multilogistic regression model;augmented covariate space;multilogistic regression algorithms;initial covariates;cross validation;automatic covariate selection;simplelogistic initial-RBF regression;SLIRBF;regularization parameter;benchmark classification;machine learning problems;agronomical problems;multilogistic regression methods;probabilistic classifiers;RBFNN design methods;relaxed variable kernel density estimation;support vector machines;sparse classifier;sparse multinomial logistic regression;product unit basis functions;statistical significance;Logistics;Kernel;Algorithm design and analysis;Maximum likelihood estimation;Training;Artificial neural networks;Support vector machines;Artificial neural networks;classification;evolutionary algorithms;evolutionary programming;logistic regression;radial basis function neural networks;Algorithms;Artificial Intelligence;Logistic Models;Mathematical Computing;Neural Networks (Computer);Pattern Recognition, Automated;Pattern Recognition, Automated;Regression Analysis;Software Design;Software Validation},
doi={10.1109/TNN.2010.2093537},
ISSN={1045-9227},
month={Feb},}
@ARTICLE{7103337,
author={J. {Tang} and C. {Deng} and G. {Huang}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Extreme Learning Machine for Multilayer Perceptron},
year={2016},
volume={27},
number={4},
pages={809-821},
abstract={Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.},
keywords={feedforward neural nets;learning (artificial intelligence);multilayer perceptrons;pattern classification;extreme learning machine;multilayer perceptron;learning algorithm;generalized single hidden layer feedforward neural networks;feature learning;ELM-based hierarchical learning framework;self-taught feature extraction;supervised feature classification;random initialized hidden weights;unsupervised multilayer encoding;ELM-based sparse autoencoder;feature representations;ELM random feature mapping;hierarchically encoded outputs;decision making;learning speed;greedy layerwise training;deep learning;DL;hierarchical learning methods;computer vision;Feature extraction;Training;Nonhomogeneous media;Optimization;Least squares approximations;Artificial neural networks;Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping.;Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping},
doi={10.1109/TNNLS.2015.2424995},
ISSN={2162-237X},
month={April},}
@ARTICLE{4523947,
author={X. {Geng} and Z. {Zhou} and K. {Smith-Miles}},
journal={IEEE Transactions on Neural Networks},
title={Individual Stable Space: An Approach to Face Recognition Under Uncontrolled Conditions},
year={2008},
volume={19},
number={8},
pages={1354-1368},
abstract={There usually exist many kinds of variations in face images taken under uncontrolled conditions, such as changes of pose, illumination, expression, etc. Most previous works on face recognition (FR) focus on particular variations and usually assume the absence of others. Instead of such a ldquodivide and conquerrdquo strategy, this paper attempts to directly address <i>face</i> <i>recognition</i> <i>under</i> <i>uncontrolled</i> <i>conditions</i>. The key is the individual stable space (ISS), which only expresses personal characteristics. A neural network named ISNN is proposed to map a raw face image into the ISS. After that, three ISS-based algorithms are designed for FR under uncontrolled conditions. There are no restrictions for the images fed into these algorithms. Moreover, unlike many other FR techniques, they do not require any extra training information, such as the view angle. These advantages make them practical to implement under uncontrolled conditions. The proposed algorithms are tested on three large face databases with vast variations and achieve superior performance compared with other 12 existing FR techniques.},
keywords={face recognition;neural nets;face recognition;individual stable space;neural network;ISNN;face databases;Face recognition;Image recognition;Lighting;Neural networks;Image databases;Pattern recognition;Humans;Algorithm design and analysis;Machine learning algorithms;Testing;Face recognition (FR);individual stable space (ISS);machine learning;neural networks;pattern recognition;Algorithms;Artificial Intelligence;Face;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Neural Networks (Computer);Pattern Recognition, Automated;Sensitivity and Specificity},
doi={10.1109/TNN.2008.2000275},
ISSN={1045-9227},
month={Aug},}
@INPROCEEDINGS{6658012,
author={A. K. {Shrivastava} and A. {Kumar} and A. K. {Rai} and N. {Payal} and A. {Tiwari}},
booktitle={2013 5th International Conference and Computational Intelligence and Communication Networks},
title={ISO 27001 Compliance via Artificial Neural Network},
year={2013},
volume={},
number={},
pages={339-342},
abstract={In this modern world of computerization, lots of data is stored in Computer System & hence requirement to save this data increase day by day. There are lots of Standards which organization can follow to make all the information within their organization safe, but to implement an standard is not easy for every organization, especially with organization which are in the stage of evolution. In this Research Paper we will be presenting a solution using Artificial Intelligence Techniques with the help of which small organization can implement these standards at comparatively low price as well as it will also help organization in Information System Risk Management.},
keywords={information systems;neural nets;organisational aspects;risk management;ISO 27001 compliance;artificial neural network;computer system;research paper;artificial intelligence techniques;information system risk management organization;Standards organizations;Organizations;Risk management;Artificial neural networks;Information security;ISO standards;Artificial Intelligence;Artificial Neural Network;Risk Assessment;ISO 27001},
doi={10.1109/CICN.2013.77},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7439360,
author={M. {Abdelrahim} and C. {Merlos} and T. {Wang}},
booktitle={2016 IEEE Tenth International Conference on Semantic Computing (ICSC)},
title={Hybrid Machine Learning Approaches: A Method to Improve Expected Output of Semi-structured Sequential Data},
year={2016},
volume={},
number={},
pages={342-345},
abstract={This paper proposes an intuitive yet simple machine learning (ML) approach that consist of two generic algorithms augmenting one another to solve problems they are not designed to solve. Since most machine learning algorithms are designed for a particular dataset or task, combining multiple ML algorithms can greatly improve the overall result by either helping tune one another, generalize, or adapt to unknown tasks. In this paper, we attempt to augment the architecture of traditional Artificial Neural Network (ANN) with a state machine acting as a form of short term memory in addition to help divide the work amongst multiple modular ANNs through transitioning from state to state. The result is a larger non-stochastic network that is able to self adjust as it is fed input. We train and test the work on data that is outside either an Artificial Neural Network or a state-machine's normal capability with simplified music notation extracted from midi files. The extracted data are used to simulate inherently sequential data to test the principle. Finally, while we find many large improvements in the augmentation of the ANN's architecture, but discuss further approaches to the system to improve generalization for new data.},
keywords={data mining;data structures;finite state machines;generalisation (artificial intelligence);learning (artificial intelligence);neural nets;hybrid machine learning approach;semistructured sequential data;ML approach;generic algorithms;machine learning algorithms;ML algorithms;artificial neural network;nonstochastic network;state-machine normal capability;music notation extraction;midi files;ANN architecture augmentation;data mining;Artificial neural networks;Algorithm design and analysis;Machine learning algorithms;Prediction algorithms;Feature extraction;Data mining;Complexity theory;Adaption;Sequence-based features;Machine Learning;Artificial Neural Network;State-machines},
doi={10.1109/ICSC.2016.72},
ISSN={},
month={Feb},}
@INPROCEEDINGS{5209797,
author={H. {Shi}},
booktitle={2009 Second International Symposium on Electronic Commerce and Security},
title={Application of Unascertained Neural Networks to Financial Early Warning},
year={2009},
volume={2},
number={},
pages={365-368},
abstract={Artificial neural network (ANN) has outstanding characteristics in machine learning, fault, tolerant, parallel reasoning and processing nonlinear problem abilities. Unascertained system that imitates the human brain's thinking logical is a kind of mathematical tools used to deal with imprecise and uncertain knowledge. Integrating unascertained method with neural network technology, the reasoning process of network coding can be tracked, and the output of the network can be given a physical explanation. A unascertained neural network was set up. It can be compared with the fuzzy network, so that their own advantages and shortcomings can be found and further study can be made on the uncertainty network to improve the uncertainty network more complete.},
keywords={financial data processing;learning (artificial intelligence);neural nets;unascertained neural network;financial early warning;artificial neural network;machine learning;parallel reasoning;processing nonlinear problem;unascertained system;uncertain knowledge;unascertained method;neural network technology;network coding;fuzzy network;uncertainty network;Neural networks;Artificial neural networks;Uncertainty;Biological neural networks;Humans;Machine learning;Network coding;Electronic commerce;Civil engineering;Electronic mail},
doi={10.1109/ISECS.2009.133},
ISSN={},
month={May},}
@INPROCEEDINGS{8002127,
author={H. {Mediouni} and S. {El Hani} and M. {Ouadghiri} and I. {Aboudrar} and I. {Ouachtouk}},
booktitle={2017 IEEE International Electric Machines and Drives Conference (IEMDC)},
title={Artificial intelligence techniques for induction motor drives},
year={2017},
volume={},
number={},
pages={1-6},
abstract={The induction machine has proved to be an appropriate solution for most industrial applications; Moreover, it presents the future of sustainable automotive industry due to the strategies of control and command of which it can be equipped. This paper presents a study of advanced methods applied to the control of induction machine in order to obtain a system satisfying the criterion of robustness. This work has been done as a comparison between the conventional controller and the advanced techniques of control such as fuzzy logic and artificial neural network. The results of various simulation tests highlight the robustness properties of the different control strategies based on orientation of the rotor flux.},
keywords={control engineering computing;electric machine analysis computing;fuzzy control;fuzzy logic;induction motor drives;machine control;neurocontrollers;rotors;artificial intelligence techniques;induction motor drives;induction machine control;industrial applications;sustainable automotive industry;robustness criterion;fuzzy logic;artificial neural network;rotor flux orientation;Rotors;Torque;Electromagnetics;Induction machines;Artificial neural networks;Stators;Robustness;Induction machine;advanced methods;conventional controller;criterion of robustness;fuzzy logic;artificial neural network},
doi={10.1109/IEMDC.2017.8002127},
ISSN={},
month={May},}
@INPROCEEDINGS{714022,
author={W. {Ushida} and T. {Takagi} and T. {Yamaguchi}},
booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
title={Linguistic instructions learning based on associative memories and its application to a facial model},
year={1993},
volume={1},
number={},
pages={750-753 vol.1},
abstract={Human learning on linguistic level is superior to other kinds of learning. If a neural network can be trained by natural language instead of numerical data, we can train machines as well as human beings without detailed training data. Conventional neural networks need numerical data to be trained. On the other hand, a linguistic level learning is able to train machines as if they were human beings. In this paper, we propose a linguistic instructions learning method based on a fuzzy associative memory network, which acquires knowledge from natural language, and we refine a facial expressions model by means of this method.},
keywords={natural languages;computational linguistics;fuzzy neural nets;content-addressable storage;learning (artificial intelligence);knowledge acquisition;natural language interfaces;user interfaces;linguistic instructions learning;fuzzy associative memories;man-machine interface;neural networks;knowledge acquisition;natural language;facial expressions model;Humans;Associative memory;Biological neural networks;Natural languages;Fuzzy systems;Learning systems;Equations;Training data;Machine learning;Information processing},
doi={10.1109/IJCNN.1993.714022},
ISSN={},
month={Oct},}
@ARTICLE{7358047,
author={C. {Li} and Y. {Li}},
journal={IEEE Access},
title={A Review on Synergistic Learning},
year={2016},
volume={4},
number={},
pages={119-134},
abstract={In neuroscience, it is widely believed that learning and memory are primarily based on synaptic plasticity which is a neural mechanism that modifies the strength of connections between neurons. As a counterpart in machine learning, the modification of connection strength (weight) endows artificial neural networks with a powerful learning capability to solve various problems. Independent of modification for synaptic strength, recent experimental results have revealed that a single neuron also has the ability to change its intrinsic excitability to fit the synaptic input. This mechanism is referred to as neuronal intrinsic plasticity (IP) in the literature. Computational learning rules for IP have been developed based on the hypothesis of information maximization with a stable response level. With the discovery of this novel plasticity mechanism, a series of studies has focused on how IP plays a role in biological neural systems and how they benefit the learning performance of artificial neural networks. In this review, corresponding research on synergies between IP and synaptic plasticity mechanisms is presented in both the computational modeling of biological neural systems and the applications of artificial neural networks, and this combination in artificial learning systems is defined as synergistic learning.},
keywords={learning (artificial intelligence);neural nets;synergistic learning;synaptic plasticity;neural mechanism;machine learning;neuronal intrinsic plasticity;biological neural systems;artificial neural networks;artificial learning systems;Synergistic learning;Hebbian theory;Neuroscience;IP networks;Computational modeling;Learning systems;Intrinsic plasticity;synaptic plasticity;synergistic learning;homeostasis;sparse coding;artificial neural networks;machine learning;Intrinsic plasticity;synaptic plasticity;synergistic learning;homeostasis;sparse coding;artificial neural networks;machine learning},
doi={10.1109/ACCESS.2015.2509005},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{191443,
author={J. H. {Johnson} and N. J. {Hallam} and P. D. {Picton}},
booktitle={IEE Colloquium on Neural Nets in Human-Computer Interaction},
title={Safety critical neural computing: explanation and verification in knowledge augmented neural networks},
year={1990},
volume={},
number={},
pages={1/1-1/8},
abstract={Conventional neural networks cannot contain a priori knowledge and cannot explain their output. A mathematical theory of black box classifiers is developed which covers most of the best known neural architectures. The limitations of the non-model based computational paradigm are discussed; these include inability to predict the behaviour of systems with multiple-valued, discontinuous, catastrophic and chaotic state spaces. Worse, they include the inability to detect the presence of such systems, when they are working: outside their 'range of competence', and with data of quality outside their range of experience. Neural networks themselves cannot communicate with human decisionmakers in human terms; often the choice is 'take it or leave it'. Knowledge-based computation does not necessarily have these drawbacks, and can therefore augment the powerful neural computing paradigm where it is weakest. The authors consider three fundamental ways of combining the two computational paradigms and show how the explanation facility of knowledge based systems can be used to induce explanation on the output of neural subsystems. They conclude with an architecture which is generic for safety critical neural computation.<<ETX>>},
keywords={explanation;knowledge based systems;neural nets;safety;knowledge augmented neural networks;mathematical theory;black box classifiers;best known neural architectures;non-model based computational paradigm;predict;multiple-valued;catastrophic;chaotic state spaces;human decisionmakers;human terms;neural computing paradigm;computational paradigms;explanation facility;knowledge based systems;neural subsystems;architecture;safety critical neural computation;Explanation;Knowledge based systems;Neural networks;Safety},
doi={},
ISSN={},
month={Dec},}
@ARTICLE{6222007,
author={Y. {Yang} and Y. {Wang} and X. {Yuan}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Bidirectional Extreme Learning Machine for Regression Problem and Its Learning Effectiveness},
year={2012},
volume={23},
number={9},
pages={1498-1505},
abstract={It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.},
keywords={learning (artificial intelligence);neural nets;regression analysis;bidirectional extreme learning machine;regression problem;learning effectiveness;neural networks;B-ELM;Machine learning;Training;Testing;Learning systems;Helium;Equations;Computer architecture;Feedforward neural network;learning effectiveness;number of hidden nodes;universal approximation;Algorithms;Computer Simulation;Data Interpretation, Statistical;Models, Statistical;Neural Networks (Computer);Pattern Recognition, Automated;Regression Analysis},
doi={10.1109/TNNLS.2012.2202289},
ISSN={2162-237X},
month={Sep.},}
@ARTICLE{8490220,
author={Q. {Zhang} and Y. {Liu} and H. {Han} and J. {Shi} and W. {Wang}},
journal={IEEE Access},
title={Artificial Intelligence Based Diagnosis for Cervical Lymph Node Malignancy Using the Point-Wise Gated Boltzmann Machine},
year={2018},
volume={6},
number={},
pages={60605-60612},
abstract={This paper aims to build an artificial intelligence (AI) architecture for automated extraction of learned-from-data image features from contrast-enhanced ultrasound (CEUS) videos and to evaluate the AI architecture for classification between benign and malignant cervical lymph nodes. An AI architecture for CEUS feature extraction and classification was constructed by using the point-wise gated Boltzmann machine (PGBM). The PGBM consisted of task-relevant and task-irrelevant hidden units for both feature learning and feature selection, and the task-relevant units were connected to the support vector machine (SVM) to yield the likelihood for classification. The synthetic minority over-sampling technique was used to improve the classification ability for an unbalanced data set. Experimental evaluation was performed with the five-fold cross validation on a database of 127 lymph nodes (39 benign and 88 malignant) from 88 patients. The SVM likelihood exhibited a significant difference between benign and malignant cervical lymph nodes (0.74±0.21 versus 0.33±0.28, p <; 0.001). On the test set, the accuracy, precision, sensitivity, specificity, and Youden's index of the AI architecture were 82.55%, 89.58%, 84.75%, 77.56%, and 62.32%, respectively. The AI architecture using the PGBM shows promising classification results, and it may be potentially used in clinical diagnosis for cervical lymph node malignancy.},
keywords={biological organs;biomedical ultrasonics;Boltzmann machines;cancer;feature extraction;feature selection;image classification;image enhancement;image sampling;learning (artificial intelligence);medical image processing;support vector machines;video signal processing;artificial intelligence based diagnosis;cervical lymph node malignancy;point-wise gated Boltzmann machine;artificial intelligence architecture;automated extraction;learned-from-data image features;contrast-enhanced ultrasound videos;AI architecture;malignant cervical lymph nodes;CEUS feature extraction;PGBM;feature learning;feature selection;support vector machine;classification ability;unbalanced data set;task-relevant hidden units;benign cervical lymph nodes;task-irrelevant hidden units;synthetic minority over-sampling technique;five-fold cross validation;SVM likelihood;Youden's index;clinical diagnosis;Lymph nodes;Artificial intelligence;Feature extraction;Cancer;Ultrasonic imaging;Support vector machines;Videos;Artificial intelligence;contrast-enhanced ultrasound;cervical lymph nodes;point-wise gated Boltzmann machine},
doi={10.1109/ACCESS.2018.2873043},
ISSN={2169-3536},
month={},}
@ARTICLE{4182387,
author={I. {Jordanov} and A. {Georgieva}},
journal={IEEE Transactions on Neural Networks},
title={Neural Network Learning With Global Heuristic Search},
year={2007},
volume={18},
number={3},
pages={937-942},
abstract={A novel hybrid global optimization (GO) algorithm applied for feedforward neural networks (NNs) supervised learning is investigated. The network weights are determined by minimizing the traditional mean square error function. The optimization technique, called LP<sub>tau </sub>NM, combines a novel global heuristic search based on LP<sub>tau </sub> low-discrepancy sequences of points, and a simplex local search. The proposed method is initially tested on multimodal mathematical functions and subsequently applied for training moderate size NNs for solving popular benchmark problems. Finally, the results are analyzed, discussed, and compared with such as from backpropagation (BP) (Levenberg-Marquardt) and differential evolution methods},
keywords={feedforward neural nets;learning (artificial intelligence);mean square error methods;optimisation;search problems;global heuristic search;hybrid global optimization algorithm;feedforward neural network supervised learning;mean square error;LPTNM optimization technique;LPT low-discrepancy sequences;simplex local search;multimodal mathematical functions;backpropagation method;differential evolution method;Neural networks;Optimization methods;Mean square error methods;Benchmark testing;Backpropagation;Hypercubes;Feedforward neural networks;Supervised learning;Machine learning;Medical diagnosis;Global optimization (GO);heuristic methods;low- discrepancy sequences;neural network (NN) learning;simplex search;Algorithms;Artificial Intelligence;Computer Simulation;Decision Support Techniques;Information Storage and Retrieval;Models, Statistical;Neural Networks (Computer);Pattern Recognition, Automated},
doi={10.1109/TNN.2007.891633},
ISSN={1045-9227},
month={May},}
@INPROCEEDINGS{1380384,
author={ and },
booktitle={Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826)},
title={Explanation based generalized /spl epsi/-SVM and its application in intelligent project management},
year={2004},
volume={6},
number={},
pages={3454-3459 vol.6},
abstract={Support vector machine works well in classifying populations characterized by abrupt decreases in density functions. Its generalization accuracy, however, is not always optimal in dealing with real world problems with neither Gaussian distributions nor sharp boundaries. Incorporating domain theory about problems and excellent intelligent techniques in machine learning into SVM becomes one of promising alternatives. A novel approach, explanation based generalized /spl epsi/-SVM, which synthesizes SVM, prior knowledge, fuzzy logic and neural network, is proposed. Prior knowledge is expressed as a trained fuzzy neural network. An optimal subset of features is obtained by dynamically reducing feature space dimensionality according to the training derivatives extracted from network. By examining a subset of the practical data sampled from Guangdong Natural Science Foundation and testing the remaining set of data, application shows that explanation based generalized /spl epsi/-SVM performs better than that pure SVM and other traditional classifiers.},
keywords={support vector machines;pattern classification;explanation;generalisation (artificial intelligence);project management;learning (artificial intelligence);fuzzy logic;fuzzy neural nets;generalized /spl epsi/-SVM;intelligent project management;support vector machine;domain theory;machine learning;prior knowledge;fuzzy logic;neural network;trained fuzzy neural network;Project management;Support vector machines;Support vector machine classification;Learning systems;Density functional theory;Gaussian distribution;Machine learning;Network synthesis;Fuzzy logic;Neural networks},
doi={10.1109/ICMLC.2004.1380384},
ISSN={},
month={Aug},}
@INPROCEEDINGS{5566006,
author={Y. {Liu} and Y. {Zhang} and Y. {Zhu} and Z. {Zhao}},
booktitle={2010 International Conference on Intelligent Computing and Cognitive Informatics},
title={A Constructive Neural Network Learning Method Based on Quotient Space and Its Application in Coal Mine Gas Prediction},
year={2010},
volume={},
number={},
pages={172-174},
abstract={This paper uses constructive neural network learning approach to predict gas concentrations, under the framework of quotient space granular computing model. Using quotient space granular computing theory, the problem can be macro-level analysis - examining different particle size between the quotient space conversion, movement, interdependent relations, and the original features of the database information to build grain size, using a variety of granularity, from different levels of analysis of complex gas data makes the learning characteristics of the sample is more obvious, in order to better meet the requirements of machine learning. Constructive neural network learning method achieves the data mining of different particle size structure the quotient space from the micro. At last, the method is applied to predict gas concentration, and the satisfying results are achieved. It is expected that Constructive Neural Network Learning Method will have wide applications.},
keywords={coal;data mining;learning (artificial intelligence);mining;neural nets;particle size;constructive neural network learning method;quotient space;coal mine gas prediction;quotient space granular computing model;macro-level analysis;machine learning;data mining;particle size structure;Artificial neural networks;Learning systems;Time series analysis;Predictive models;Data mining;Prediction algorithms;Classification algorithms;quotient space;granular computing;constructive neural network learning method;coal mine gas prediction},
doi={10.1109/ICICCI.2010.68},
ISSN={},
month={June},}
@INPROCEEDINGS{6566377,
author={M. {Amar} and I. {Gondal} and C. {Wilson}},
booktitle={2013 IEEE 8th Conference on Industrial Electronics and Applications (ICIEA)},
title={Multi-size-window spectral augmentation: Neural network bearing fault classifier},
year={2013},
volume={},
number={},
pages={261-266},
abstract={Features extraction has always been crucial in rotary machines for Condition based machine health monitoring. Time-domain-segmentation being among the preliminary steps for further classification process plays a momentous role. Vibration signals from bearing are quasistationary in nature therefore calculation of constituent frequencies amplitudes in the vibration signal is dependent upon time-segmentation-window size. The proposed research confers the effects of time-segmentation window size on spectral features amplitudes calculation and its impacts on classification accuracy of the Artificial Neural Network (ANN). Using multi-size time-segmentation-window, for comprehensive spectral features calculation, ANN pattern classifier has been trained for enhanced classification. ANN learning assigns importance based relative weights to the links using supervised learning. Experimental results have shown that multi-size-window spectral features for ANN fault classifier perform efficiently for quasi-stationary bearing vibrations.},
keywords={condition monitoring;fault diagnosis;feature extraction;learning (artificial intelligence);machine bearings;mechanical engineering computing;neural nets;signal classification;vibrations;multisize-window spectral augmentation;artificial neural network bearing fault classifier;feature extraction;rotary machines;condition based machine health monitoring;time-domain-segmentation;classification process;vibration signals;time-segmentation-window size;spectral features amplitudes calculation;ANN pattern classifier;ANN learning;importance based relative weights;supervised learning;multi-size-window spectral features;quasi-stationary bearing vibrations;Training;Neurons;Vibrations;Artificial neural networks;Time-frequency analysis;Biological neural networks;Time-domain analysis;Machine Health Monitoring;Neural Network;Spectral Contents;Fault Diagnosis;Bearing Faults},
doi={10.1109/ICIEA.2013.6566377},
ISSN={2156-2318},
month={June},}
@INPROCEEDINGS{685683,
author={R. J. P. {de Figueiredo}},
booktitle={1998 IEEE Symposium on Advances in Digital Filtering and Signal Processing. Symposium Proceedings (Cat. No.98EX185)},
title={D-FANNS (dynamical functional artificial neural networks)-a new avenue for intelligent analog signal processing},
year={1998},
volume={},
number={},
pages={6-},
abstract={Summary form only given. Intelligent signal processing may be defined as the process of mapping a signal x into a binary vector or matrix y, so that y enables the detection, classification, or interpretation of an event present in x. (In the case of an interpretation in an appropriate language, y would represent a digitally coded relational structure.) We denote by f the input-output map of such an intelligent signal processing filter. In a number of applications, it is possible to naturally implement the nonlinear filter map f by an artificial neural network (ANN). We consider the case in which x is an analog signal (waveform) belonging to L2(I), where I is an appropriate interval of the real line R1 (i.e., L2(I) is the space of square integrable functions on I), and propose the realization of f by an artificial neural network in which the synaptic weight actions of the first layer are implemented by a filter bank. We call such a network a dynamical functional artificial neural network (D-FANN) to distinguish it from a conventional functional artificial neural network (FANN), where a synaptic weight action is implemented by a scalar product (integration) in L2(I), between the incoming waveform x and a "distributed" functional weight. Compared with conventional FANNs, D-FANNs permit simple and meaningful causal realizations of intelligent analog signal processors. A novel element in the present paper is the introduction of a "D-FANN gain equation", in a way analogous to that in Kalman filtering. Applications of D-FANNs to real and simulated data are now in progress and these results are discussed.},
keywords={neural nets;signal processing;band-pass filters;filtering theory;nonlinear filters;dynamical functional artificial neural networks;D-FANNS;intelligent analog signal processing;binary vector;matrix;detection;classification;interpretation;digitally coded relational structure;input-output map;intelligent signal processing filter;nonlinear filter map;waveform;synaptic weight;functional artificial neural network;FANN;scalar product;integration;distributed functional weight;D-FANN gain equation;Kalman filtering;simulated data;real data;Artificial neural networks;Artificial intelligence;Signal processing;Signal mapping;Event detection;Intelligent structures;Digital filters;Nonlinear filters;Filter bank;Equations},
doi={10.1109/ADFSP.1998.685683},
ISSN={},
month={June},}
@ARTICLE{8440842,
author={B. C. {Kwon} and M. {Choi} and J. T. {Kim} and E. {Choi} and Y. B. {Kim} and S. {Kwon} and J. {Sun} and J. {Choo}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records},
year={2019},
volume={25},
number={1},
pages={299-309},
abstract={We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.},
keywords={artificial intelligence;data analysis;data visualisation;interactive systems;medical information systems;recurrent neural nets;interactive RNN-based model;EMR data;prediction tasks;RetainVis;individual medical codes;risk predictions;temporal information;increase interactivity;interpretable analytics tool;interpretable networks;interactive recurrent neural networks;electronic medical records;black-box nature;interactively leverage users;design study;visual analytics solution;medical experts;artificial intelligence scientists;iterative design process;newly improved RNN-based model;RNN-based model;visual analytic researchers;interactive visual analytic tool;Machine learning;Medical diagnostic imaging;Task analysis;Predictive models;Computational modeling;Visual analytics;Data models;Interactive Artificial Intelligence;XAI (Explainable Artificial Intelligence);Interpretable Deep Learning;Healthcare},
doi={10.1109/TVCG.2018.2865027},
ISSN={1077-2626},
month={Jan},}
@INPROCEEDINGS{499443,
author={M. E. {Jefferies} and W. -. {Yeap}},
booktitle={Proceedings 1995 Second New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems},
title={Neural network approaches to cognitive mapping},
year={1995},
volume={},
number={},
pages={75-78},
abstract={There are many different approaches to cognitive mapping, arising mostly from the different philosophical backgrounds of the researchers involved. Our own research into the problem of how best to build a representation for one's experience of one's spatial environment is motivated by the need to understand how the human mind works. Neural network approaches to cognitive mapping are as varied as their non-neural network counterparts and range from models which use the network to model the physiology of the brain to models which are merely an abstraction of some aspect of cognitive mapping behaviour. We review four neural network approaches to cognitive mapping with the view to determining what insights they can bring to the cognitive mapping process.},
keywords={cognitive systems;neural nets;knowledge representation;neurophysiology;brain models;cognitive mapping;spatial environment;human mind;neural network approaches;brain physiology modelling;knowledge representation;Neural networks;Biological neural networks;Artificial intelligence;Brain modeling;Animals;Solid modeling;Cognitive robotics;Humans;Psychology;Navigation},
doi={10.1109/ANNES.1995.499443},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1174403,
author={ and and },
booktitle={Proceedings. International Conference on Machine Learning and Cybernetics},
title={Knowledge-increasable learning behaviors research of neural field},
year={2002},
volume={2},
number={},
pages={586-590 vol.2},
abstract={In a hierarchical set of systems, a lower order system is included in the parameter space of a large one as a subset. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. Based on the theoretical analysis of information geometry and differential manifold, this paper studies knowledge-increasable learning behaviors of the neural field, and presents a layered knowledge-increasable artificial neural network model which has the knowledge-increasable and structure-extendible ability. The method helps to provide an explanation of the transformation mechanism of human's recognition system and understand the theory of global architecture of neural networks.},
keywords={neural nets;learning (artificial intelligence);parallel processing;information theory;probability;neural network;knowledge-increasable learning;parameter space;geometrical structures;information geometry;differential manifold;structure-extendible ability;parallel processing;probability distribution;Artificial neural networks;Information geometry;Neural networks;Solid modeling;Humans;Probability distribution;Information analysis;Large-scale systems;Machine learning;Computer science},
doi={10.1109/ICMLC.2002.1174403},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4147189,
author={K. {Revett}},
booktitle={2006 8th Seminar on Neural Network Applications in Electrical Engineering},
title={Data Mining a Prostate Cancer Dataset Using Neural Networks},
year={2006},
volume={},
number={},
pages={157-160},
abstract={Prostate cancer remains one of the leading causes of cancer death worldwide, with a reported incidence rate of 650,000 cases per annum worldwide. The causal factors of prostate cancer still remain to be determined. In this paper, we investigate a medical dataset containing clinical information on 502 prostate cancer patients using the machine learning technique of rough sets and radial basis function neural network. Our preliminary results yield a classification accuracy of 90%, with high sensitivity and specificity (both at approximately 91%). Our results yield a predictive positive value (PPN) of 81% and a predictive negative value (PNV) of 95%},
keywords={cancer;data mining;decision tables;learning (artificial intelligence);medical computing;radial basis function networks;rough set theory;data mining;prostate cancer dataset;neural network;clinical information;machine learning technique;rough set;radial basis function neural network;predictive positive value;predictive negative value;Data mining;Prostate cancer;Neural networks;Rough sets;Testing;Machine learning;Radial basis function networks;Cancer detection;Seminars;Medical diagnostic imaging;cancer classifier;machine learning;prostate cancer dataset;reducts;Rough sets},
doi={10.1109/NEUREL.2006.341201},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{823236,
author={K. {Yamauchi} and S. {Itoh} and N. {Ishii}},
booktitle={IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)},
title={Combination of fast and slow learning neural networks for quick adaptation and pruning redundant cells},
year={1999},
volume={3},
number={},
pages={390-395 vol.3},
abstract={One advantage of the neural network approach is the learning of many instances with a small number of hidden units. However, the small size of neural networks usually necessitates many repeats of the gradient descent algorithm for the learning. To realize quick adaptation of the small size of neural networks, the paper presents a learning system consisting of several neural networks: a fast-learning network (F-Net), a slow-learning network (S-Net) and a main network (Main-Net). The F-Net learns new instances very quickly like k-nearest neighbors, while the S-Net learns the output of the F-Net with a small number of hidden units. The resultant parameter of the S-Net is moved to the Main-Net, which is only for recognition. During the learning of the S-Net, the system does not learn any new instances like the sleeping biological systems.},
keywords={neural nets;learning (artificial intelligence);slow learning neural networks;fast learning neural networks;quick adaptation;pruning;redundant cells;gradient descent algorithm;k-nearest neighbors;Neural networks;Radio access networks;Learning systems;Function approximation;Artificial intelligence;Computer science;Nearest neighbor searches;Biological systems;Computer applications;Application software},
doi={10.1109/ICSMC.1999.823236},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{8439192,
author={P. {Cacivkins} and L. {Lazov} and E. {Teirumnieks} and M. {Sperga} and I. {Dilevka} and A. {Vitins}},
booktitle={2018 IX National Conference with International Participation (ELECTRONICA)},
title={Artificial Neural Networks: What Can They Learn about Color Laser Marking?},
year={2018},
volume={},
number={},
pages={1-4},
abstract={Color laser marking paves the way for many useful applications in modern industry - traceability, anticounterfeiting, etc. Laser marking of materials is an inherently difficult problem with no clear functional relationship between many technological parameters on the input and the results of processing on the output. Some processes cannot be well defined without the use of examples. In this paper we discuss the novel method of training artificial neural networks using real experimental color laser marking data for prediction of results. We conclude the paper by discussing the other potential applications of proposed solution in the field of laser materials processing.},
keywords={image colour analysis;laser materials processing;learning (artificial intelligence);neural nets;learn about color laser marking;technological parameters;artificial neural networks;laser materials processing;Image color analysis;Training;Power lasers;Laser theory;Regression analysis;artificial neural networks;color laser marking;laser materials processing;machine learning},
doi={10.1109/ELECTRONICA.2018.8439192},
ISSN={},
month={May},}
@INPROCEEDINGS{497807,
author={C. P. {Lim} and R. F. {Harrison}},
booktitle={1995 Fourth International Conference on Artificial Neural Networks},
title={Probabilistic Fuzzy ARTMAP: an autonomous neural network architecture for Bayesian probability estimation},
year={1995},
volume={},
number={},
pages={148-153},
abstract={A hybrid utilisation of the Fuzzy ARTMAP (FAM) neural network and the Probabilistic Neural Network (PNN) is proposed for online learning and prediction tasks. FAM is used as an underlying clustering algorithm to classify the input patterns into different recognition categories during the learning phase. Subsequently, a non parametric probability estimation procedure in accordance with the PNN paradigm is employed during the prediction phase. This hybrid approach realises an incremental learning network with implementation of the Bayes strategy for online applications. The effectiveness of this network is assessed with statistical classification problems in both stationary and non stationary environments. Simulation studies illustrate that the network is capable of asymptotically approaching the Bayes optimal classification rates.},
keywords={fuzzy neural nets;neural net architecture;probability;pattern classification;Bayes methods;learning (artificial intelligence);Probabilistic Fuzzy ARTMAP;autonomous neural network architecture;Bayesian probability estimation;hybrid utilisation;FAM;Probabilistic Neural Network;online learning;prediction tasks;underlying clustering algorithm;input patterns;recognition categories;learning phase;non parametric probability estimation procedure;prediction phase;incremental learning network;Bayes strategy;online applications;statistical classification problems;non stationary environments;Fuzzy neural networks;Neural network architecture;Probability;Pattern classification;Bayes procedures;Learning systems},
doi={10.1049/cp:19950545},
ISSN={0537-9989},
month={June},}
@ARTICLE{1353291,
author={K. C. {Tan} and H. J. {Tang}},
journal={IEEE Transactions on Neural Networks},
title={New dynamical optimal learning for linear multilayer FNN},
year={2004},
volume={15},
number={6},
pages={1562-1570},
abstract={This letter presents a new dynamical optimal learning (DOL) algorithm for three-layer linear neural networks and investigates its generalization ability. The optimal learning rates can be fully determined during the training process. The mean squared error (mse) is guaranteed to be stably decreased and the learning is less sensitive to initial parameter settings. The simulation results illustrate that the proposed DOL algorithm gives better generalization performance and faster convergence as compared to standard error back propagation algorithm.},
keywords={learning (artificial intelligence);feedforward neural nets;mean square error methods;stability;dynamical optimal learning algorithm;linear multilayer FNN;three-layer linear neural network;mean squared error;Nonhomogeneous media;Neural networks;Multi-layer neural network;Feedforward neural networks;Stability;Pattern recognition;Chaos;Transfer functions;Convergence;Function approximation;Back propagation;dynamical optimal learning (DOL);feedforward neural networks (FNN);stability;Algorithms;Artificial Intelligence;Computer Simulation;Decision Support Techniques;Feedback;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Linear Models;Neural Networks (Computer);Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated},
doi={10.1109/TNN.2004.830801},
ISSN={1045-9227},
month={Nov},}
@INPROCEEDINGS{713958,
author={G. {Deco} and L. {Parra}},
booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
title={Self-organization in stochastic neural networks},
year={1993},
volume={1},
number={},
pages={479-482 vol.1},
abstract={The maximization of the mutual information between the stochastic outputs neurons and the clamped inputs is used as an unsupervised criterion for training a Boltzmann machine. The resulting learning rule contains two terms corresponding to the Hebbian and anti-Hebbian learning. The two terms are weighted by the amount of transmitted information in the learning synapse, giving an information-theoretic interpretation to the proportionality constant given in the biological rule of Hebb. The anti-Hebbian term causes the convergence of weights. Simulation for the encoder problem demonstrates optimal performance of this method.},
keywords={Boltzmann machines;neural nets;Hebbian learning;unsupervised learning;information theory;self-organization;stochastic neural networks;unsupervised learning;Boltzmann machine;anti-Hebbian learning;Hebbian learning;information-theory;proportionality constant;Intelligent networks;Stochastic processes;Neural networks;Neurons;Mutual information;Information theory;Unsupervised learning;Equations;Research and development;Tin},
doi={10.1109/IJCNN.1993.713958},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1279324,
author={T. {Takeda} and },
booktitle={International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003},
title={A two step algorithm for designing small neural network trees},
year={2003},
volume={1},
number={},
pages={513-517 Vol.1},
abstract={There are mainly two approaches for machine learning. One is symbolic approach and another is sub-symbolic approach. Decision tree (DT) is a typical model for symbolic learning, and neural network (NN) is a popular model for sub-symbolic learning. Neural network tree (NNTree) is a DT with each non-terminal node being an expert NN. NNTree is a learning model that may combine the advantages of both DT and NN. Through experiments we found that the size of an NNTree is usually proportional to the number of training data. Thus, we can produce small trees by using partial training data. In most cases, however, this will decrease the performance of the tree. In this paper, we propose a two-step algorithm to produce small NNTrecs. The first step is to get a small NNTree using partial data, and the second step is to increase the performance through retraining. The effectiveness of this algorithm is verified through experiments with public databases.},
keywords={neural nets;decision trees;learning (artificial intelligence);neural network trees;two step algorithm;machine learning;decision tree;partial training data;Algorithm design and analysis;Neural networks;Training data;Machine learning algorithms;Machine learning;Decision trees;Concrete;Partitioning algorithms;Spatial databases;Process design},
doi={10.1109/ICNNSP.2003.1279324},
ISSN={},
month={Dec},}
@INPROCEEDINGS{271602,
author={R. J. P. {de Figueiredo}},
booktitle={[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics},
title={The role of nonlinear operators in the structure of natural and artificial intelligence},
year={1992},
volume={},
number={},
pages={1326-1331 vol.2},
abstract={Nonlinear operators play a central role in the implementation of functions that characterize natural and artificial intelligence. They are also responsible for providing learning and evolutionary capabilities to intelligent systems, i.e., to systems that implement those functions. In the present work, these issues are discussed in the context of neural networks recently proposed by the author, namely, the optimal interpolative net and the optimal multilayer neural interpolating net.<<ETX>>},
keywords={artificial intelligence;feedforward neural nets;natural intelligence;learning capabilities;OI net;OMNI net;nonlinear operators;artificial intelligence;evolutionary capabilities;optimal interpolative net;multilayer neural interpolating net;Artificial intelligence;Biological neural networks;Humans;Intelligent systems;Multi-layer neural network;Biology computing;Machine intelligence;Artificial neural networks;Intelligent networks;Mathematics},
doi={10.1109/ICSMC.1992.271602},
ISSN={},
month={Oct},}
@INPROCEEDINGS{227262,
author={S. E. {Lee} and B. R. {Holt}},
booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
title={Regression analysis of spectroscopic process data using a combined architecture of linear and nonlinear artificial neural networks},
year={1992},
volume={4},
number={},
pages={549-554 vol.4},
abstract={The authors demonstrate that a combined architecture of linear and nonlinear artificial neural networks offers many advantages over the conventional multilayer feedforward networks and the conventional biased regression methods for the modeling of spectroscopic process data. This direct linear feedthrough (DLF) network is an especially useful tool for modeling process data when the true linear or nonlinear functionality of the system is not known. By just looking at the linear and nonlinear contributions of this DLF network, it is possible to tell if the data are purely linear or nonlinear. For the learning algorithm, sequential quadratic programming improves the training time of the neural network by 2-3 orders of magnitude compared to that of the back-propagation method. The authors also suggest that when the number of data samples is small compared to the size of the network, the risk of under-representing or overfitting the available data can be avoided by using the leave-one-out cross validation technique.<<ETX>>},
keywords={learning (artificial intelligence);neural nets;spectroscopy computing;statistical analysis;spectroscopic process data;artificial neural networks;regression methods;direct linear feedthrough;learning algorithm;sequential quadratic programming;Regression analysis;Spectroscopy;Artificial neural networks;Multi-layer neural network;Quadratic programming;Computer networks;Chemical engineering;Neural networks;Quality control;Current measurement},
doi={10.1109/IJCNN.1992.227262},
ISSN={},
month={June},}
