"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"XZMIENEV","conferencePaper","2017","Shirataki, S.; Yamaguchi, S.","A study on interpretability of decision of machine learning","2017 IEEE International Conference on Big Data (Big Data)","","","10.1109/BigData.2017.8258557","","Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.","2017-12","2019-05-16 16:46:55","2019-05-16 17:48:53","","4830-4831","","","","","","","","","","","","","","","","","","","","Citation Key: 8258557  bibtex*[number=;comments=Presents a specific method for enhancing explainability for models,]  ISSN:","","/home/tim/Zotero/storage/ZBXG7NCC/8258557.html; /home/tim/Zotero/storage/NY97YJXG/Shirataki and Yamaguchi - 2017 - A study on interpretability of decision of machine.pdf","","Analytical models; Big Data; big data analysis; black boxes; data analysis; decision reasons; DVD; highly weighted words; interpretability; learning (artificial intelligence); machine learning; opinions classification; pattern classification; Predictive models; Support Vector Machine; support vector machines; Support vector machines; SVM; text analysis; Tools; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBWFJ6U2","conferencePaper","2018","Marino, D. L.; Wickramasinghe, C. S.; Manic, M.","An Adversarial Approach for Explainable AI in Intrusion Detection Systems","IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society","","","10.1109/IECON.2018.8591457","","Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.","2018-10","2019-05-16 16:46:57","2019-05-16 17:51:07","","3237-3243","","","","","","","","","","","","","","","","","","","","Citation Key: 8591457  bibtex*[number=;comments=Presents a specific method for enhancing explainability for models,]  ISSN: 2577-1647","","/home/tim/Zotero/storage/Z5E9G9JF/8591457.html; /home/tim/Zotero/storage/PS6K757R/Marino et al. - 2018 - An Adversarial Approach for Explainable AI in Intr.pdf","","adversarial approach; adversarial machine learning; Adversarial Machine Learning; Adversarial samples; cyber-security; cyber-security applications; data-driven intrusion detection systems; deep neural networks; Estimation; explainable AI; Explainable AI; IDSs; Intrusion detection; learning (artificial intelligence); Machine learning; machine learning techniques; Mathematical model; multilayer perceptron classifiers; multilayer perceptrons; neural nets; pattern classification; security of data; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRA8CP32","journalArticle","2012","Balachandran, Vipin; Deepak P; Khemani, Deepak","Interpretable and Reconfigurable Clustering of Document Datasets by Deriving Word-Based Rules","Knowledge and Information Systems","","0219-3116","10.1007/s10115-011-0446-9","","Clusters of text documents output by clustering algorithms are often hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clusters and outline the problem of generating clusterings with interpretable and reconfigurable cluster models. We develop two clustering algorithms toward the outlined goal of building interpretable and reconfigurable cluster models. They generate clusters with associated rules that are composed of conditions on word occurrences or nonoccurrences. The proposed approaches vary in the complexity of the format of the rules; RGC employs disjunctions and conjunctions in rule generation whereas RGC-D rules are simple disjunctions of conditions signifying presence of various words. In both the cases, each cluster is comprised of precisely the set of documents that satisfy the corresponding rule. Rules of the latter kind are easy to interpret, whereas the former leads to more accurate clustering. We show that our approaches outperform the unsupervised decision tree approach for rule-generating clustering and also an approach we provide for generating interpretable models for general clusterings, both by significant margins. We empirically show that the purity and f-measure losses to achieve interpretability can be as little as 3 and 5%, respectively using the algorithms presented herein.","2012-09","2019-05-16 16:46:59","2019-05-16 16:46:59","","475-503","","3","32","","","","","","","","","","English","","","","","","","Citation Key: balachandranInterpretableReconfigurableClustering2012  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/CDXYWI2F/Balachandran et al. - 2012 - Interpretable and reconfigurable clustering of doc.pdf","","Data clustering; Interpretability; Text clustering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAX22ARZ","conferencePaper","2018","Bride, Hadrien; Dong, Jie; Dong, Jin Song; Hóu, Zhé","Towards Dependable and Explainable Machine Learning Using Automated Reasoning","Formal Methods and Software Engineering","978-3-030-02450-5","","","","The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.","2018","2019-05-16 16:47:00","2019-05-16 16:47:00","","412-416","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: brideDependableExplainableMachine2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/9ELNBJIK/Bride et al. - 2018 - Towards Dependable and Explainable Machine Learnin.pdf","","","","Sun, Jing; Sun, Meng","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S92DM6HA","conferencePaper","2018","Brown, Andy; Tuor, Aaron; Hutchinson, Brian; Nichols, Nicole","Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection","Proceedings of the First Workshop on Machine Learning for Computing Systems","978-1-4503-5865-1","","10.1145/3217871.3217872","http://doi.acm.org/10.1145/3217871.3217872","","2018","2019-05-16 16:47:00","2019-05-16 16:47:00","","1:1-1:8","","","","","","","MLCS'18","","","","ACM","","","","","","","","","Citation Key: Brown:2018:RNN:3217871.3217872  bibtex*[acmid=3217872;articleno=1;comments=Presents a specific method for enhancing explainability for models,;numpages=8]  event-place: Tempe, AZ, USA","","/home/tim/Zotero/storage/35ZQSISZ/Brown et al. - 2018 - Recurrent Neural Network Attention Mechanisms for .pdf","","Anomaly detection; Attention; Interpretable Machine Learning; Online Training; Recurrent Neural Networks; System Log Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFFR4B4Z","bookSection","2018","Browne, Kieran; Swift, Ben; Gardner, Henry","Critical Challenges for the Visual Representation of Deep Neural Networks","Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent","978-3-319-90403-0","","","","Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.","2018","2019-05-16 16:47:01","2019-05-16 16:47:01","","119-136","","","","","","","Human–Computer Interaction Series","","","","Springer International Publishing","Cham","English","","","","","","","Citation Key: browneCriticalChallengesVisual2018  bibtex*[comments=Reviews the current state of explainability research,]  DOI: 10.1007/978-3-319-90403-0_7","","/home/tim/Zotero/storage/26LU4JZZ/Browne et al. - 2018 - Critical Challenges for the Visual Representation .pdf","","","","Zhou, Jianlong; Chen, Fang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQB6ZMS7","conferencePaper","2018","Costa, Felipe; Ouyang, Sixun; Dolog, Peter; Lawlor, Aonghus","Automatic Generation of Natural Language Explanations","Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion","978-1-4503-5571-1","","10.1145/3180308.3180366","http://doi.acm.org/10.1145/3180308.3180366","","2018","2019-05-16 16:47:02","2019-05-16 17:55:26","","57:1-57:2","","","","","","","IUI '18 Companion","","","","ACM","","","","","","","","","Citation Key: Costa:2018:AGN:3180308.3180366  bibtex*[acmid=3180366;articleno=57;comments=Presents a specific method for enhancing explainability for models,;numpages=2]  event-place: Tokyo, Japan","","/home/tim/Zotero/storage/BPJH36CC/Costa et al. - 2018 - Automatic Generation of Natural Language Explanati.pdf","","Explainability; Explanations; Natural Language Generation; Neural Network; Recommender systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LQS54N2D","conferencePaper","2017","Fernandez, Delia; Woodward, Alejandro; Campos, Victor; Giro-i-Nieto, Xavier; Jou, Brendan; Chang, Shih-Fu","More Cat Than Cute?: Interpretable Prediction of Adjective-Noun Pairs","Proceedings of the Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes","978-1-4503-5509-4","","10.1145/3132515.3132520","http://doi.acm.org/10.1145/3132515.3132520","","2017","2019-05-16 16:47:02","2019-05-16 16:47:02","","61-69","","","","","","","MUSA2 '17","","","","ACM","","","","","","","","","Citation Key: Fernandez:2017:MCC:3132515.3132520  bibtex*[acmid=3132520;comments=Presents a specific method for enhancing explainability for models,;numpages=9]  event-place: Mountain View, California, USA","","/home/tim/Zotero/storage/KZFRVKX8/Fernandez et al. - 2017 - More Cat Than Cute Interpretable Prediction of A.pdf","","adjective noun pairs; affective computing; compound concepts; convolutional neural networks; interpretable models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERDST5A7","conferencePaper","2009","Green, Stephen J.; Lamere, Paul; Alexander, Jeffrey; Maillet, François; Kirk, Susanna; Holt, Jessica; Bourque, Jackie; Mak, Xiao-Wen","Generating Transparent, Steerable Recommendations from Textual Descriptions of Items","Proceedings of the Third ACM Conference on Recommender Systems","978-1-60558-435-5","","10.1145/1639714.1639768","http://doi.acm.org/10.1145/1639714.1639768","","2009","2019-05-16 16:47:02","2019-05-16 16:47:02","","281-284","","","","","","","RecSys '09","","","","ACM","","","","","","","","","Citation Key: Green:2009:GTS:1639714.1639768  bibtex*[acmid=1639768;comments=Presents a specific method for enhancing explainability for models,;numpages=4]  event-place: New York, New York, USA","","/home/tim/Zotero/storage/3ENQXWRB/Green et al. - 2009 - Generating Transparent, Steerable Recommendations .pdf","","explainable recommender; steerable recommender","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRDMAZXH","journalArticle","2018","Guidotti, Riccardo; Monreale, Anna; Ruggieri, Salvatore; Turini, Franco; Giannotti, Fosca; Pedreschi, Dino","A Survey of Methods for Explaining Black Box Models","ACM Comput. Surv.","","0360-0300","10.1145/3236009","http://doi.acm.org/10.1145/3236009","","2018-08","2019-05-16 16:47:02","2019-05-16 17:50:57","","93:1-93:42","","5","51","","","","","","","","","","","","","","","","","Citation Key: Guidotti:2018:SME:3271482.3236009  bibtex*[location=New York, NY, USA;publisher=ACM;acmid=3236009;articleno=93;comments=Reviews the current state of explainability research,;issue_date=January 2019;numpages=42]","","/home/tim/Zotero/storage/7XIYGV69/Guidotti et al. - 2018 - A Survey of Methods for Explaining Black Box Model.pdf","","explanations; interpretability; Open the black box; transparent models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQ9ATYEL","journalArticle","2018","Singh, Chandan; Murdoch, W. James; Yu, Bin","Hierarchical interpretations for neural network predictions","arxiv","","","","http://arxiv.org/pdf/1806.05337v2","Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","2018","2019-05-16 16:47:08","2019-05-16 18:13:40","","","","","","","","","","","","","","","","","","","","","","Citation Key: http://arxiv.org/abs/1806.05337v2  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","Comment: Published in ICLR 2019","/home/tim/Zotero/storage/VASQX8ME/Singh et al. - 2018 - Hierarchical interpretations for neural network pr.pdf","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZYFGYEK","journalArticle","2018","Waa, Jasper van der; Robeer, Marcel; Diggelen, Jurriaan van; Brinkhuis, Matthieu; Neerincx, Mark","Contrastive Explanations with Local Foil Trees","arxiv","","","","http://arxiv.org/pdf/1806.07470v1","Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like ""Why this output (the fact) instead of that output (the foil)?"" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.","2018","2019-05-16 16:47:08","2019-05-16 18:05:47","","","","","","","","","","","","","","","","","","","","","","Citation Key: http://arxiv.org/abs/1806.07470v1  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","Comment: presented at 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden","/home/tim/Zotero/storage/Y8GWYSK7/van der Waa et al. - 2018 - Contrastive Explanations with Local Foil Trees.pdf","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4QMFRSF8","journalArticle","2018","Tsang, Michael; Sun, Youbang; Ren, Dongxu; Liu, Yan","Can I trust you more? Model-Agnostic Hierarchical Explanations","arxiv","","","","http://arxiv.org/pdf/1812.04801v1","Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mahé, a novel approach to provide Model-agnostic hierarchical éxplanations of how powerful machine learning models, such as deep neural networks, capture these interactions as either dependent on or free of the context of data instances. Specifically, Mahé provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. Experimental results show that Mahé obtains improved local interaction interpretations over state-of-the-art methods and successfully explains interactions that are context-free.","2018-12","2019-05-16 16:47:11","2019-05-16 17:55:33","","","","","","","","","","","","","","","","","","","","","","Citation Key: http://arxiv.org/abs/1812.04801v1  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/MILRMNWN/Tsang et al. - 2018 - Can I trust you more Model-Agnostic Hierarchical .pdf","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PXN6QICK","journalArticle","2019","Alishahi, Afra; Chrupała, Grzegorz; Linzen, Tal","Analyzing and Interpreting Neural Networks for NLP: A Report on the First BlackboxNLP Workshop","arxiv","","","","http://arxiv.org/pdf/1904.04063v1","The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques specifically developed for analyzing and understanding the inner-workings and representations acquired by neural models of language. Approaches included: systematic manipulation of input to neural networks and investigating the impact on their performance, testing whether interpretable knowledge can be decoded from intermediate representations acquired by neural networks, proposing modifications to neural network architectures to make their knowledge state or generated output more explainable, and examining the performance of networks on simplified or formal languages. Here we review a number of representative studies in each category.","2019","2019-05-16 16:47:13","2019-05-16 17:52:19","","","","","","","","","","","","","","","","","","","","","","Citation Key: http://arxiv.org/abs/1904.04063v1  bibtex*[comments=Reviews the current state of explainability research,]","","/home/tim/Zotero/storage/8JTKYHX4/Alishahi et al. - 2019 - Analyzing and Interpreting Neural Networks for NLP.pdf","","Computer Science - Computation and Language; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4HYVC9H","conferencePaper","2018","Hu, Ronghang; Andreas, Jacob; Darrell, Trevor; Saenko, Kate","Explainable Neural Computation via Stack Neural Module Networks","Computer Vision – ECCV 2018","978-3-030-01234-2","","","","In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.","2018","2019-05-16 16:47:13","2019-05-16 16:47:13","","55-71","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: huExplainableNeuralComputation2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/SCB4HV2E/Hu et al. - 2018 - Explainable Neural Computation via Stack Neural Mo.pdf","","Interpretable reasoning; Neural module networks; Visual question answering","","Ferrari, Vittorio; Hebert, Martial; Sminchisescu, Cristian; Weiss, Yair","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KQBWMMEG","journalArticle","2018","Ito, Tomoki; Sakaji, Hiroki; Izumi, Kiyoshi; Tsubouchi, Kota; Yamashita, Tatsuo","GINN: Gradient Interpretable Neural Networks for Visualizing Financial Texts","International Journal of Data Science and Analytics","","2364-4168","10.1007/s41060-018-0160-8","","This study aims to visualize financial documents in such a way that even nonexperts can understand the sentiments contained therein. To achieve this, we propose a novel text visualization method using an interpretable neural network (NN) architecture, called a gradient interpretable NN (GINN). A GINN can visualize a market sentiment score from an entire financial document and the sentiment gradient scores in both word and concept units. Moreover, the GINN can visualize important concepts given in various sentence contexts. Such visualization helps nonexperts easily understand financial documents. We theoretically analyze the validity of the GINN and experimentally demonstrate the validity of text visualization produced by the GINN using real financial texts.","2018-12","2019-05-16 16:47:14","2019-05-16 16:47:14","","","","","","","","GINN","","","","","","","English","","","","","","","Citation Key: itoGINNGradientInterpretable2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/32ZV8K69/Ito et al. - 2018 - GINN Gradient Interpretable Neural Networks for V.pdf","","Interpretable neural network; Support system; Text mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C39ISMX4","bookSection","2006","Jin, Yaochu; Sendhoff, Bernhard; Körner, Edgar","Simultaneous Generation of Accurate and Interpretable Neural Network Classifiers","Multi-Objective Machine Learning","978-3-540-33019-6","","","","Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.","2006","2019-05-16 16:47:14","2019-05-16 16:47:14","","291-312","","","","","","","Studies in Computational Intelligence","","","","Springer Berlin Heidelberg","Berlin, Heidelberg","English","","","","","","","Citation Key: jinSimultaneousGenerationAccurate2006  bibtex*[comments=Presents a specific method for enhancing explainability for models,]  DOI: 10.1007/3-540-33019-4_13","","/home/tim/Zotero/storage/5ZFJU7WV/Jin et al. - 2006 - Simultaneous Generation of Accurate and Interpreta.pdf","","Hide Neuron; Mean Square Error; Multiobjective Optimization; Neural Network; Pareto Front","","Jin, Yaochu","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNQHT9WS","conferencePaper","2018","Liu, Ninghao; Huang, Xiao; Li, Jundong; Hu, Xia","On Interpretation of Network Embedding via Taxonomy Induction","Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining","978-1-4503-5552-0","","10.1145/3219819.3220001","http://doi.acm.org/10.1145/3219819.3220001","","2018","2019-05-16 16:47:15","2019-05-16 16:47:15","","1812-1820","","","","","","","KDD '18","","","","ACM","","","","","","","","","Citation Key: Liu:2018:INE:3219819.3220001  bibtex*[acmid=3220001;comments=Presents a specific method for enhancing explainability for models,;numpages=9]  event-place: London, United Kingdom","","/home/tim/Zotero/storage/HQJYEG34/Liu et al. - 2018 - On Interpretation of Network Embedding via Taxonom.pdf","","machine learning interpretation; network embedding; taxonomy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AFRJG66I","conferencePaper","2019","Liu, Ninghao; Du, Mengnan; Hu, Xia","Representation Interpretation with Spatial Encoding and Multimodal Analytics","Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining","978-1-4503-5940-5","","10.1145/3289600.3290960","http://doi.acm.org/10.1145/3289600.3290960","","2019","2019-05-16 16:47:16","2019-05-16 16:47:16","","60-68","","","","","","","WSDM '19","","","","ACM","","","","","","","","","Citation Key: Liu:2019:RIS:3289600.3290960  bibtex*[acmid=3290960;comments=Presents a specific method for enhancing explainability for models,;numpages=9]  event-place: Melbourne VIC, Australia","","/home/tim/Zotero/storage/LPYVP9G8/Liu et al. - 2019 - Representation Interpretation with Spatial Encodin.pdf","","interpretation; recommender systems; representation learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8HBDC4TT","conferencePaper","2013","Otte, Clemens","Safe and Interpretable Machine Learning: A Methodological Review","Computational Intelligence in Intelligent Data Analysis","978-3-642-32378-2","","","","When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.","2013","2019-05-16 16:47:16","2019-05-16 16:47:16","","111-122","","","","","","Safe and Interpretable Machine Learning","Studies in Computational Intelligence","","","","Springer Berlin Heidelberg","","English","","","","","","","Citation Key: otteSafeInterpretableMachine2013  bibtex*[comments=Reviews the current state of explainability research,]","","/home/tim/Zotero/storage/9GAI3BM4/Otte - 2013 - Safe and Interpretable Machine Learning A Methodo.pdf","","Input Space; Methodological Review; Multivariate Adaptive Regression Spline; Symbolic Model; Symbolic Regression","","Moewes, Christian; Nürnberger, Andreas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYXNFPL5","conferencePaper","2019","Pastor, Eliana; Baralis, Elena","Explaining Black Box Models by Means of Local Rules","Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing","978-1-4503-5933-7","","10.1145/3297280.3297328","http://doi.acm.org/10.1145/3297280.3297328","","2019","2019-05-16 16:47:16","2019-05-16 16:47:16","","510-517","","","","","","","SAC '19","","","","ACM","","","","","","","","","Citation Key: Pastor:2019:EBB:3297280.3297328  bibtex*[acmid=3297328;comments=Presents a specific method for enhancing explainability for models,;numpages=8]  event-place: Limassol, Cyprus","","/home/tim/Zotero/storage/CQYTCN24/Pastor and Baralis - 2019 - Explaining Black Box Models by Means of Local Rule.pdf","","interpretability; local model; prediction explanation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBB5SP3S","conferencePaper","2018","Pomarlan, Mihai; Bateman, John","Meaningful Clusterings of Recurrent Neural Network Activations for NLP","Mining Intelligence and Knowledge Exploration","978-3-030-05918-7","","","","Recurrent neural networks have found applications in NLP, but their operation is difficult to interpret. A state automaton that approximates the network would be more interpretable, but for this one needs a method to group network activation states by their behavior. In this paper we propose such a method, and compare it to an existing dimensionality reduction and clustering approach. Our method is better able to group together neural states of similar behavior.","2018","2019-05-16 16:47:16","2019-05-16 16:47:16","","11-20","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: pomarlanMeaningfulClusteringsRecurrent2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/7XZPXGAP/Pomarlan and Bateman - 2018 - Meaningful Clusterings of Recurrent Neural Network.pdf","","Interpretability; Natural language processing; Recurrent neural networks","","Groza, Adrian; Prasath, Rajendra","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDTDERZC","conferencePaper","2018","Pomarlan, Mihai; Bateman, John","Understanding NLP Neural Networks by the Texts They Generate","KI 2018: Advances in Artificial Intelligence","978-3-030-00111-7","","","","Recurrent neural networks have proven useful in natural language processing. For example, they can be trained to predict, and even generate plausible text with few or no spelling and syntax errors. However, it is not clear what grammar a network has learned, or how it keeps track of the syntactic structure of its input. In this paper, we present a new method to extract a finite state machine from a recurrent neural network. A FSM is in principle a more interpretable representation of a grammar than a neural net would be, however the extracted FSMs for realistic neural networks will also be large. Therefore, we also look at ways to group the states and paths through the extracted FSM so as to get a smaller, easier to understand model of the neural network. To illustrate our methods, we use them to investigate how a neural network learns noun-verb agreement from a simple grammar where relative clauses may appear between noun and verb.","2018","2019-05-16 16:47:17","2019-05-16 16:47:17","","284-296","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: pomarlanUnderstandingNLPNeural2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/AKD9XE7P/Pomarlan and Bateman - 2018 - Understanding NLP Neural Networks by the Texts The.pdf","","Interpretability; Natural language processing; Recurrent neural networks","","Trollmann, Frank; Turhan, Anni-Yasmin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKWK5INX","conferencePaper","2016","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","978-1-4503-4232-2","","10.1145/2939672.2939778","http://doi.acm.org/10.1145/2939672.2939778","","2016","2019-05-16 16:47:17","2019-05-16 16:47:17","","1135-1144","","","","","","","KDD '16","","","","ACM","","","","","","","","","Citation Key: Ribeiro:2016:WIT:2939672.2939778  bibtex*[acmid=2939778;comments=Presents a specific method for enhancing explainability for models,;numpages=10]  event-place: San Francisco, California, USA","","/home/tim/Zotero/storage/SG9LMP5C/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf","","black box classifier; explaining machine learning; interpretability; interpretable machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZY3QVWZ9","conferencePaper","2017","Tamagnini, Paolo; Krause, Josua; Dasgupta, Aritra; Bertini, Enrico","Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations","Proceedings of the 2Nd Workshop on Human-In-the-Loop Data Analytics","978-1-4503-5029-7","","10.1145/3077257.3077260","http://doi.acm.org/10.1145/3077257.3077260","","2017","2019-05-16 16:47:17","2019-05-16 16:47:17","","6:1-6:6","","","","","","","HILDA'17","","","","ACM","","","","","","","","","Citation Key: Tamagnini:2017:IBC:3077257.3077260  bibtex*[acmid=3077260;articleno=6;comments=Presents a specific method for enhancing explainability for models,;numpages=6]  event-place: Chicago, IL, USA","","/home/tim/Zotero/storage/HMBS5P88/Tamagnini et al. - 2017 - Interpreting Black-Box Classifiers Using Instance-.pdf","","classification; explanation; machine learning; visual analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RX59ID4M","conferencePaper","2018","Weber, Rosina O.; Johs, Adam J.; Li, Jianfei; Huang, Kent","Investigating Textual Case-Based XAI","Case-Based Reasoning Research and Development","978-3-030-01081-2","","","","This paper demonstrates how case-based reasoning (CBR) can be used for an explainable artificial intelligence (XAI) approach to justify solutions produced by an opaque learning method (i.e., target method), particularly in the context of unstructured textual data. Our general hypothesis is twofold: (1) There exists patterns in the relationship between problems and solutions and there should be data or a body of knowledge that describes how problems and solutions relate; and (2) the identification, manipulation, and learning of such patterns through case features can help create and reuse explanations for solutions produced by the target method. When the target method relies on neural network architectures (e.g., deep learning), the resulting latent space (i.e., word embeddings) becomes useful for finding patterns and semantic relatedness in textual data. In the proposed approach, case problems are input-output pairs from the target method, and case solutions are explanations. We exemplify our approach by explaining recommended citations from Citeomatic - a multi-layer neural-network architecture from the Allen Institute for Artificial Intelligence. Citation analysis is the body of knowledge that describes how query documents (i.e., inputs) relate to recommended citations (i.e., outputs). We build cases and similarity assessment to learn features that represent patterns between problems and solutions that can lead to the reuse of corresponding explanations. The illustrative implementation we present becomes an explanation-augmented citation recommender that targets human-computer trust.","2018","2019-05-16 16:47:18","2019-05-16 16:47:18","","431-447","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: weberInvestigatingTextualCaseBased2018  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/5FNIHJK6/Weber et al. - 2018 - Investigating Textual Case-Based XAI.pdf","","Case-Based reasoning; Citation recommendation; Explainable artificial intelligence; Human-Computer trust; Semantic relatedness; Textual Case-Based reasoning; Word embeddings","","Cox, Michael T.; Funk, Peter; Begum, Shahina","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"564XPRB3","conferencePaper","2019","Zhang, Yin; Liu, Ninghao; Ji, Shuiwang; Caverlee, James; Hu, Xia","An Interpretable Neural Model with Interactive Stepwise Influence","Advances in Knowledge Discovery and Data Mining","978-3-030-16142-2","","","","Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.","2019","2019-05-16 16:47:18","2019-05-16 16:47:18","","528-540","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","","English","","","","","","","Citation Key: zhangInterpretableNeuralModel2019  bibtex*[comments=Presents a specific method for enhancing explainability for models,]","","/home/tim/Zotero/storage/7L837ASW/Zhang et al. - 2019 - An Interpretable Neural Model with Interactive Ste.pdf","","Interpretation; Neural network; Stepwise Influence","","Yang, Qiang; Zhou, Zhi-Hua; Gong, Zhiguo; Zhang, Min-Ling; Huang, Sheng-Jun","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6DJQKDN","journalArticle","2018","Zhang, Quan-shi; Zhu, Song-chun","Visual Interpretability for Deep Learning: A Survey","Frontiers of Information Technology & Electronic Engineering","","2095-9230","10.1631/FITEE.1700808","","This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.","2018-01","2019-05-16 16:47:18","2019-05-16 16:47:18","","27-39","","1","19","","","Visual Interpretability for Deep Learning","","","","","","","English","","","","","","","Citation Key: zhangVisualInterpretabilityDeep2018  bibtex*[comments=Reviews the current state of explainability research,]","","/home/tim/Zotero/storage/VUEBIDWB/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf","","Artificial intelligence; Deep learning; Interpretable model; TP391","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJNSWYCF","journalArticle","2018","Koupaee, Mahnaz; Wang, William Yang","Analyzing and Interpreting Convolutional Neural Networks in NLP","arXiv:1810.09312 [cs, stat]","","","","http://arxiv.org/abs/1810.09312","Convolutional neural networks have been successfully applied to various NLP tasks. However, it is not obvious whether they model different linguistic patterns such as negation, intensiﬁcation, and clause compositionality to help the decision-making process. In this paper, we apply visualization techniques to observe how the model can capture different linguistic features and how these features can affect the performance of the model. Later on, we try to identify the model errors and their sources. We believe that interpreting CNNs is the ﬁrst step to understand the underlying semantic features which can raise awareness to further improve the performance and explainability of CNN models.","2018-10-18","2019-05-16 17:50:46","2019-05-16 17:50:46","2019-05-16 17:50:46","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1810.09312","","/home/tim/Zotero/storage/GWJ4VVWQ/Koupaee and Wang - 2018 - Analyzing and Interpreting Convolutional Neural Ne.pdf","","","Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9TDGKW9C","conferencePaper","2018","Yanagimto, H.; Hashimoto, K.; Okada, M.","Attention Visualization of Gated Convolutional Neural Networks with Self Attention in Sentiment Analysis","2018 International Conference on Machine Learning and Data Engineering (iCMLDE)","","","10.1109/iCMLDE.2018.00024","","Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.","2018-12","2019-05-16 16:46:57","2019-05-16 17:57:44","","77-82","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/FTXAXGBJ/8614007.html; /home/tim/Zotero/storage/YINTJYUI/8614007.html; /home/tim/Zotero/storage/NXJKZ7B3/Yanagimto et al. - 2018 - Attention Visualization of Gated Convolutional Neu.pdf; /home/tim/Zotero/storage/M6YPGGMU/Yanagimto et al. - 2018 - Attention Visualization of Gated Convolutional Neu.pdf","","Amazon reviews; attention visualization; convolutional neural nets; Convolutional neural networks; deep learning; Deep learning; Gated CNN; gated convolutional neural networks; GCNN; Kernel; learning (artificial intelligence); Logic gates; natural language processing; Natural language processing; neural net architecture; neural network architecture; self-attention mechanism; sentient analysis; sentiment analysis; Sentiment analysis; Task analysis; the self-attention mechanism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 International Conference on Machine Learning and Data Engineering (iCMLDE)","","","","","","","","","","","","","","",""
"H28ZX8PP","journalArticle","2019","Cui, Xinrui; Wang, Dan; Wang, Z. Jane","CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks","arXiv:1902.02497 [cs]","","","","http://arxiv.org/abs/1902.02497","With the widespread applications of deep convolutional neural networks (DCNNs), it becomes increasingly important for DCNNs not only to make accurate predictions but also to explain how they make their decisions. In this work, we propose a CHannel-wise disentangled InterPretation (CHIP) model to give the visual interpretation to the predictions of DCNNs. The proposed model distills the class-discriminative importance of channels in networks by utilizing the sparse regularization. Here, we ﬁrst introduce the network perturbation technique to learn the model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present the class-discriminative visual interpretation for speciﬁc predictions of networks. It is noteworthy that the proposed model is able to interpret different layers of networks without re-training. By combining the distilled interpretation knowledge in different layers, we further propose the Reﬁned CHIP visual interpretation that is both high-resolution and class-discriminative. Experimental results on the standard dataset demonstrate that the proposed model provides promising visual interpretation for the predictions of networks in image classiﬁcation task compared with existing visual interpretation methods. Besides, the proposed method outperforms related approaches in the application of ILSVRC 2015 weakly-supervised localization task.","2019-02-07","2019-05-16 16:47:12","2019-05-16 18:05:35","2019-05-16 17:58:42","","","","","","","CHIP","","","","","","","en","","","","","arXiv.org","","arXiv: 1902.02497","Comment: 15 pages, 10 figures","/home/tim/Zotero/storage/L4HNJGU8/Cui et al. - 2019 - CHIP Channel-wise Disentangled Interpretation of .pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q724CCVC","journalArticle","2017","Li, Oscar; Liu, Hao; Chen, Chaofan; Rudin, Cynthia","Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions","arXiv:1710.04806 [cs, stat]","","","","http://arxiv.org/abs/1710.04806","Deep neural networks are widely used for classiﬁcation. These deep models often suffer from a lack of interpretability – they are particularly difﬁcult to understand because of their non-linear nature. As a result, neural networks are often treated as “black box” models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classiﬁcation process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.","2017-10-13","2019-05-16 16:47:05","2019-05-16 18:06:01","2019-05-16 17:59:26","","","","","","","Deep Learning for Case-Based Reasoning through Prototypes","","","","","","","en","","","","","arXiv.org","","arXiv: 1710.04806","Comment: The first two authors contributed equally, 8 pages, accepted in AAAI 2018","/home/tim/Zotero/storage/EJT3PAX6/Li et al. - 2017 - Deep Learning for Case-Based Reasoning through Pro.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"75QHVXJJ","conferencePaper","2019","Card, Dallas; Zhang, Michael; Smith, Noah A.","Deep Weighted Averaging Classifiers","Proceedings of the Conference on Fairness, Accountability, and Transparency","978-1-4503-6125-5","","10.1145/3287560.3287595","http://doi.acm.org/10.1145/3287560.3287595","Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.","2019","2019-05-16 16:47:01","2019-05-16 18:06:08","2019-05-16 17:59:34","369–378","","","","","","","FAT* '19","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","event-place: Atlanta, GA, USA","","/home/tim/Zotero/storage/MD8A9KIL/Card et al. - 2019 - Deep Weighted Averaging Classifiers.pdf","","conformal methods; interpretability credibility","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EZ2GZT8","journalArticle","2019","Yang, Zebin; Zhang, Aijun; Sudjianto, Agus","Enhancing Explainability of Neural Networks through Architecture Constraints","arXiv:1901.03838 [cs, stat]","","","","http://arxiv.org/abs/1901.03838","Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. The neural networks are known to possess good prediction performance, but lack of suﬃcient model explainability. In this paper, we propose to enhance the explainability of neural networks through the following architecture constraints: a) sparse additive subnetworks; b) orthogonal projection pursuit; and c) smooth function approximation. It leads to a sparse, orthogonal and smooth explainable neural network (SOSxNN). The multiple parameters in the SOSxNN model are simultaneously estimated by a modiﬁed mini-batch gradient descent algorithm based on the backpropagation technique for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. The hyperparameters controlling the sparse and smooth constraints are optimized by the grid search. Through simulation studies, we compare the SOSxNN method to several benchmark methods including least absolute shrinkage and selection operator, support vector machine, random forest, and multi-layer perceptron. It is shown that proposed model keeps the ﬂexibility of pursuing prediction accuracy while attaining the improved interpretability, which can be therefore used as a promising surrogate model for complex model approximation. Finally, the real data example from the Lending Club is employed as a showcase of the SOSxNN application.","2019-01-12","2019-05-16 16:47:11","2019-05-16 18:06:16","2019-05-16 17:59:50","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1901.03838","","/home/tim/Zotero/storage/H55K667C/Yang et al. - 2019 - Enhancing Explainability of Neural Networks throug.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5BAHCHU","journalArticle","2017","Samek, Wojciech; Wiegand, Thomas; Müller, Klaus-Robert","Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models","arXiv:1708.08296 [cs, stat]","","","","http://arxiv.org/abs/1708.08296","With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classiﬁcation, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artiﬁcial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this ﬁeld and makes a plea for more interpretability in artiﬁcial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classiﬁcation tasks.","2017-08-28","2019-05-16 16:47:05","2019-05-16 18:06:24","2019-05-16 18:00:32","","","","","","","Explainable Artificial Intelligence","","","","","","","en","","","","","arXiv.org","","arXiv: 1708.08296","Comment: 8 pages, 2 figures","/home/tim/Zotero/storage/EXIUQHAI/Samek et al. - 2017 - Explainable Artificial Intelligence Understanding.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVYRDK3P","conferencePaper","2018","Chhatwal, R.; Gronvall, P.; Huber-Fliflet, N.; Keeling, R.; Zhang, J.; Zhao, H.","Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding","2018 IEEE International Conference on Big Data (Big Data)","","","10.1109/BigData.2018.8622073","","In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.","2018-12","2019-05-16 16:46:58","2019-05-16 18:06:33","","1905-1911","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/HKTLIGNR/8622073.html; /home/tim/Zotero/storage/FCRXQUL3/Chhatwal et al. - 2018 - Explainable Text Classification in Legal Document .pdf","","data-focused engagements; document classification; electronically-stored documents; explainable AI; explainable AI system; explainable predictive coding; explainable predictive coding methods; Law; law administration; legal document review; machine learning; Machine learning; machine learning researchers; pattern classification; predictive coding; Predictive coding; Predictive models; responsive documents; responsive snippets; supervised learning; supervised learning task; technology-based techniques; text analysis; text categorization; Text categorization; text classification; typical legal document review scenarios","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE International Conference on Big Data (Big Data)","","","","","","","","","","","","","","",""
"TZEX5DM7","journalArticle","2018","Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana","Explaining Explanations: An Overview of Interpretability of Machine Learning","arXiv:1806.00069 [cs, stat]","","","","http://arxiv.org/abs/1806.00069","There has recently been a surge of work in explanatory artiﬁcial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufﬁcient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artiﬁcial intelligence.","2018-05-31","2019-05-16 16:47:08","2019-05-16 18:06:40","2019-05-16 18:02:30","","","","","","","Explaining Explanations","","","","","","","en","","","","","arXiv.org","","arXiv: 1806.00069","Comment: The 5th IEEE International Conference on Data Science and Advanced Analytics (DSAA 2018). [Research Track]","/home/tim/Zotero/storage/UNQYT2UZ/Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXEKPW6T","journalArticle","2018","Chen, Runjin; Chen, Hao; Huang, Ge; Ren, Jie; Zhang, Quanshi","Explaining Neural Networks Semantically and Quantitatively","arXiv:1812.07169 [cs]","","","","http://arxiv.org/abs/1812.07169","This paper1 presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically. The analysis of the speciﬁc rationale of each prediction made by the CNN presents a key issue of understanding neural networks, but it is also of signiﬁcant practical values in certain applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, so that we can use the explainable model to provide a quantitative explanation for the CNN prediction. We analyze the typical bias-interpreting problem of the explainable model and develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.","2018-12-17","2019-05-16 16:47:11","2019-05-16 18:06:59","2019-05-16 18:02:59","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1812.07169","","/home/tim/Zotero/storage/7IXI8C39/Chen et al. - 2018 - Explaining Neural Networks Semantically and Quanti.pdf; /home/tim/Zotero/storage/N7G9KR3P/Chen et al. - 2018 - Explaining Neural Networks Semantically and Quanti.pdf","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WYLUNFF9","journalArticle","2017","Arras, Leila; Montavon, Grégoire; Müller, Klaus-Robert; Samek, Wojciech","Explaining Recurrent Neural Network Predictions in Sentiment Analysis","arXiv:1706.07206 [cs, stat]","","","","http://arxiv.org/abs/1706.07206","Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classiﬁcation decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a speciﬁc propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a ﬁve-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.","2017-06-22","2019-05-16 16:47:04","2019-05-16 18:07:09","2019-05-16 18:03:51","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1706.07206","Comment: 9 pages, 4 figures, accepted for EMNLP'17 Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA)","/home/tim/Zotero/storage/YW876XJ4/Arras et al. - 2017 - Explaining Recurrent Neural Network Predictions in.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AP4RMGLH","journalArticle","2016","Vidovic, Marina M.-C.; Görnitz, Nico; Müller, Klaus-Robert; Kloft, Marius","Feature Importance Measure for Non-linear Learning Algorithms","arXiv:1611.07567 [cs, stat]","","","","http://arxiv.org/abs/1611.07567","Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point).","2016-11-22","2019-05-16 16:47:03","2019-05-16 18:07:18","2019-05-16 18:03:55","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1611.07567","Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems","/home/tim/Zotero/storage/Z2BAH4DN/Vidovic et al. - 2016 - Feature Importance Measure for Non-linear Learning.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZU4AZ5H","journalArticle","2018","Hendricks, Lisa Anne; Hu, Ronghang; Darrell, Trevor; Akata, Zeynep","Generating Counterfactual Explanations with Natural Language","arXiv:1806.09809 [cs]","","","","http://arxiv.org/abs/1806.09809","Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classiﬁcation decision if present in an image (e.g., “This is not a Scarlet Tanager because it does not have black wings.”) We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classiﬁcation decision if present in the image. To demonstrate our method we consider a ﬁne-grained image classiﬁcation task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.","2018-06-26","2019-05-16 16:47:09","2019-05-16 18:07:25","2019-05-16 18:03:59","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1806.09809","Comment: presented at 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden","/home/tim/Zotero/storage/DPDIGMVZ/Hendricks et al. - 2018 - Generating Counterfactual Explanations with Natura.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BBCQ3H8M","journalArticle","2019","Ibrahim, Mark; Louie, Melissa; Modarres, Ceena; Paisley, John","Global Explanations of Neural Networks: Mapping the Landscape of Predictions","arXiv:1902.02384 [cs, stat]","","","","http://arxiv.org/abs/1902.02384","A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and speciﬁes which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM’s global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.","2019-02-06","2019-05-16 16:47:12","2019-05-16 18:07:42","2019-05-16 18:05:09","","","","","","","Global Explanations of Neural Networks","","","","","","","en","","","","","arXiv.org","","arXiv: 1902.02384","Comment: published at ACM/AAAI AIES","/home/tim/Zotero/storage/YR6CASJM/Ibrahim et al. - 2019 - Global Explanations of Neural Networks Mapping th.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7VA86EI","journalArticle","","Geng, Yuxia; Chen, Jiaoyan; Jimenez-Ruiz, Ernesto; Chen, Huajun","Human-centric Transfer Learning Explanation via Knowledge Graph","","","","","","Transfer learning which aims at utilizing knowledge learned from one problem (source domain) to solve another different but related problem (target domain) has attracted wide research attentions. However, the current transfer learning methods are mostly uninterpretable, especially to people without ML expertise. In this extended abstract, we brief introduce two knowledge graph (KG) based frameworks towards human understandable transfer learning explanation. The ﬁrst one explains the transferability of features learned by Convolutional Neural Network (CNN) from one domain to another through pre-training and ﬁne-tuning, while the second justiﬁes the model of a target domain predicted by models from multiple source domains in zero-shot learning (ZSL). Both methods utilize KG and its reasoning capability to provide rich and human understandable explanations to the transfer procedure.","","2019-05-16 16:47:12","2019-05-16 18:13:52","","4","","","","","","","","","","","","","en","","","","","Zotero","","","","/home/tim/Zotero/storage/YXWLC66S/Geng et al. - Human-centric Transfer Learning Explanation via Kn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HW2IBR8C","journalArticle","2017","Barratt, Shane","InterpNET: Neural Introspection for Interpretable Deep Learning","arXiv:1710.09511 [cs, stat]","","","","http://arxiv.org/abs/1710.09511","Humans are able to explain their reasoning. On the contrary, deep neural networks are not. This paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classiﬁcation, inspired by physiological evidence of the human visual system’s inner-workings. This paper proposes a neural network design paradigm, termed InterpNET, which can be combined with any existing classiﬁcation architecture to generate natural language explanations of the classiﬁcations. The success of the module relies on the assumption that the network’s computation and reasoning is represented in its internal layer activations. While in principle InterpNET could be applied to any existing classiﬁcation architecture, it is evaluated via an image classiﬁcation and explanation task. Experiments on a CUB bird classiﬁcation and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations. While the current state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a much higher METEOR score of 37.9. Source code is available online2.","2017-10-25","2019-05-16 16:47:05","2019-05-16 18:13:59","2019-05-16 18:09:45","","","","","","","InterpNET","","","","","","","en","","","","","arXiv.org","","arXiv: 1710.09511","Comment: Presented at NIPS 2017 Symposium on Interpretable Machine Learning","/home/tim/Zotero/storage/4R4LK2UG/Barratt - 2017 - InterpNET Neural Introspection for Interpretable .pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52SFB7CG","journalArticle","2018","Liu, Xuan; Wang, Xiaoguang; Matwin, Stan","Interpretable Deep Convolutional Neural Networks via Meta-learning","arXiv:1802.00560 [cs, stat]","","","","http://arxiv.org/abs/1802.00560","Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model’s outputs. The recent movement for “algorithmic fairness” also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a speciﬁc hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to ﬁnd the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a speciﬁc test instance is classiﬁed. Our method achieves global interpretation for all the test instances on the hidden layers without sacriﬁcing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.","2018-02-02","2019-05-16 16:46:57","2019-05-16 18:14:49","2019-05-16 18:10:41","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1802.00560","Comment: 9 pages, 9 figures, 2018 International Joint Conference on Neural Networks, in press","/home/tim/Zotero/storage/DDUDMDC9/Liu et al. - 2018 - Interpretable Deep Convolutional Neural Networks v.pdf","","base learning algorithms; big data; clustering algorithm; CNN-INTE; Computational modeling; convolution; Convolutional Neural Network; deep CNN model; deep learning; feedforward neural nets; interpretability; interpretable deep convolutional neural networks; learning (artificial intelligence); Machine learning; Machine learning algorithms; machine learning approaches; meta-learning; Meta-learning; meta-level test data; meta-level training data; MNIST image dataset; model interpretability; pattern classification; pattern clustering; Prediction algorithms; Predictive models; random forest; random processes; TensorFlow; Training data; Visualization","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H3LJA2KB","journalArticle","2016","Hechtlinger, Yotam","Interpretation of Prediction Models Using the Input Gradient","arXiv:1611.07634 [cs, stat]","","","","http://arxiv.org/abs/1611.07634","State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a “black box”.","2016-11-22","2019-05-16 16:47:04","2019-05-16 18:15:05","2019-05-16 18:11:00","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1611.07634","Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems","/home/tim/Zotero/storage/WUMCQ9HT/Hechtlinger - 2016 - Interpretation of Prediction Models Using the Inpu.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IN9PGMPC","journalArticle","2017","Fong, Ruth; Vedaldi, Andrea","Interpretable Explanations of Black Boxes by Meaningful Perturbation","2017 IEEE International Conference on Computer Vision (ICCV)","","","10.1109/ICCV.2017.371","http://arxiv.org/abs/1704.03296","As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints.","2017-10","2019-05-16 16:47:04","2019-05-16 18:14:57","2019-05-16 18:11:09","3449-3457","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1704.03296","Comment: Final camera-ready paper published at ICCV 2017 (Supplementary materials: http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Fong_Interpretable_Explanations_of_ICCV_2017_supplemental.pdf)","/home/tim/Zotero/storage/96H22FMY/Fong and Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meani.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D59WVMCP","journalArticle","2019","Ish-Horowicz, Jonathan; Udwin, Dana; Flaxman, Seth; Filippi, Sarah; Crawford, Lorin","Interpreting Deep Neural Networks Through Variable Importance","arXiv:1901.09839 [cs, stat]","","","","http://arxiv.org/abs/1901.09839","While the success of deep neural networks (DNNs) is well-established across a variety of domains, our ability to explain and interpret these methods is limited. Unlike previously proposed local methods which try to explain particular classiﬁcation decisions, we focus on global interpretability and ask a universally applicable question: given a trained model, which features are the most important? In the context of neural networks, a feature is rarely important on its own, so our strategy is speciﬁcally designed to leverage partial covariance structures and incorporate variable dependence into feature ranking. Our methodological contributions in this paper are two-fold. First, we propose an effect size analogue for DNNs that is appropriate for applications with highly collinear predictors (ubiquitous in computer vision). Second, we extend the recently proposed “RelATive cEntrality” (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting. RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature signiﬁcance. We apply our framework to three broad application areas: computer vision, natural language processing, and social science.","2019-01-28","2019-05-16 16:47:12","2019-05-16 18:15:14","2019-05-16 18:12:17","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1901.09839","","/home/tim/Zotero/storage/VGQQKC3V/Ish-Horowicz et al. - 2019 - Interpreting Deep Neural Networks Through Variable.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"976IGRZC","journalArticle","2018","Zhang, Xin; Solar-Lezama, Armando; Singh, Rishabh","Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections","arXiv:1802.07384 [cs, stat]","","","","http://arxiv.org/abs/1802.07384","We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network’s output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a ﬁrst-order theorem can be proved efﬁciently by a solver using certain heuristics, and the ﬁnal one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.","2018-02-20","2019-05-16 16:47:07","2019-05-16 18:15:24","2019-05-16 18:12:22","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1802.07384","Comment: 24 pages","/home/tim/Zotero/storage/CN4L7QQJ/Zhang et al. - 2018 - Interpreting Neural Network Judgments via Minimal,.pdf","","","68T01; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAES7TDJ","journalArticle","2016","Kindermans, Pieter-Jan; Schütt, Kristof; Müller, Klaus-Robert; Dähne, Sven","Investigating the influence of noise and distractors on the interpretation of neural networks","arXiv:1611.07270 [cs, stat]","","","","http://arxiv.org/abs/1611.07270","Understanding neural networks is becoming increasingly important. Over the last few years different types of visualisation and explanation methods have been proposed. However, none of them explicitly considered the behaviour in the presence of noise and distracting elements. In this work, we will show how noise and distracting dimensions can inﬂuence the result of an explanation model. This gives a new theoretical insights to aid selection of the most appropriate explanation model within the deep-Taylor decomposition framework.","2016-11-22","2019-05-16 16:47:03","2019-05-16 18:22:08","2019-05-16 18:16:05","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1611.07270","Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems","/home/tim/Zotero/storage/46WW5VDC/Kindermans et al. - 2016 - Investigating the influence of noise and distracto.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGI3DQ33","journalArticle","2018","Harbecke, David; Schwarzenberg, Robert; Alt, Christoph","Learning Explanations from Language Data","arXiv:1808.04127 [cs]","","","","http://arxiv.org/abs/1808.04127","PatternAttribution is a recent method, introduced in the vision domain, that explains classiﬁcations of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.","2018-08-13","2019-05-16 16:47:09","2019-05-16 18:22:16","2019-05-16 18:16:09","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1808.04127","Comment: Appears in 2018 EMNLP Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP)","/home/tim/Zotero/storage/7CBSD2SX/Harbecke et al. - 2018 - Learning Explanations from Language Data.pdf","","","Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7MDX99UI","journalArticle","2018","Gupta, Pankaj; Schütze, Hinrich","LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation","arXiv:1808.01591 [cs]","","","","http://arxiv.org/abs/1808.01591","Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: “How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response” (2) Example2pattern: “How the saliency patterns look like for each category in the data according to the network in decision making”. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classiﬁcation datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.","2018-08-05","2019-05-16 16:47:09","2019-05-16 18:22:23","2019-05-16 18:16:40","","","","","","","LISA","","","","","","","en","","","","","arXiv.org","","arXiv: 1808.01591","Comment: 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP2018) workshop on Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP)","/home/tim/Zotero/storage/YK4AX2GN/Gupta and Schütze - 2018 - LISA Explaining Recurrent Neural Network Judgment.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LL65TH8","journalArticle","2018","Peltola, Tomi","Local Interpretable Model-agnostic Explanations of Bayesian Predictive Models via Kullback-Leibler Projections","arXiv:1810.02678 [cs, stat]","","","","http://arxiv.org/abs/1810.02678","We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation ﬁdelity and complexity. We demonstrate the method in explaining MNIST digit classiﬁcations made by a Bayesian deep convolutional neural network.","2018-10-05","2019-05-16 16:47:10","2019-05-16 18:22:31","2019-05-16 18:16:44","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1810.02678","Comment: Extended abstract/short paper, Proceedings of the 2nd Workshop on Explainable Artificial Intelligence (XAI 2018) at IJCAI/ECAI 2018","/home/tim/Zotero/storage/DEU42LBK/Peltola - 2018 - Local Interpretable Model-agnostic Explanations of.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIE5JZ2F","journalArticle","2019","Zhang, J.; Wang, Y.; Molino, P.; Li, L.; Ebert, D. S.","Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models","IEEE Transactions on Visualization and Computer Graphics","","1077-2626","10.1109/TVCG.2018.2864499","","Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.","2019-01","2019-05-16 16:46:56","2019-05-16 18:22:49","","364-373","","1","25","","","Manifold","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/WZYDPQ3Q/8440091.html; /home/tim/Zotero/storage/WIDG3F8B/Zhang et al. - 2019 - Manifold A Model-Agnostic Framework for Interpret.pdf","","Analytical models; classification tasks; Computational modeling; customizable tabular view; data analysis; Data models; data visualisation; debugging; deep neural networks; diagnosis process; explanation; feature discrimination; generic framework; inspection; Interactive machine learning; internal logic; interpretation; iterative methods; learning (artificial intelligence); Machine learning; machine learning models; manifold; Manifolds; model comparison; model debugging; model development; model-agnostic framework; models outcome; neural nets; performance analysis; potential machine learning; refinement; regression analysis; regression tasks; scatterplot-based visual summary; specific model type; Task analysis; visual analysis techniques; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UW4YRBQP","journalArticle","2018","Montavon, Grégoire; Samek, Wojciech; Müller, Klaus-Robert","Methods for Interpreting and Understanding Deep Neural Networks","Digital Signal Processing","","10512004","10.1016/j.dsp.2017.10.011","http://arxiv.org/abs/1706.07979","This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most eﬃcient use of these techniques on real data. It also discusses a number of practical applications.","2018-02","2019-05-16 16:47:04","2019-05-16 18:22:58","2019-05-16 18:18:22","1-15","","","73","","Digital Signal Processing","","","","","","","","en","","","","","arXiv.org","","arXiv: 1706.07979","Comment: 14 pages, 10 figures","/home/tim/Zotero/storage/4UMS7Y49/Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3MPA7I2H","journalArticle","2016","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","Model-Agnostic Interpretability of Machine Learning","arXiv:1606.05386 [cs, stat]","","","","http://arxiv.org/abs/1606.05386","Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","2016-06-16","2019-05-16 16:47:03","2019-05-16 18:33:11","2019-05-16 18:25:48","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1606.05386","Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY","/home/tim/Zotero/storage/8KHZBY8N/Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9CIJYT5Z","journalArticle","2019","Cui, X.; Wang, D.; Wang, Z. J.","Multi-scale Interpretation Model for Convolutional Neural Networks: Building Trust based on Hierarchical Interpretation","IEEE Transactions on Multimedia","","1520-9210","10.1109/TMM.2019.2902099","","With the rapid development of deep learning models, their performances in various tasks are improved, while meanwhile their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In the paper, we propose a novel Multi-scale INTerpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.","2019","2019-05-16 16:46:58","2019-05-16 18:33:19","","1-1","","","","","","Multi-scale Interpretation Model for Convolutional Neural Networks","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/WEA5PBP7/8653995.html; /home/tim/Zotero/storage/832XTMGJ/Cui et al. - 2019 - Multi-scale Interpretation Model for Convolutional.pdf","","Analytical models; Computational modeling; convolutional neural networks; Feature extraction; Heating systems; Image segmentation; Model interpretability; model-agnostic; multi-scale interpretation; Perturbation methods; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZHU9ME5K","journalArticle","2018","Adadi, A.; Berrada, M.","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","IEEE Access","","2169-3536","10.1109/ACCESS.2018.2870052","","At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","2018","2019-05-16 16:46:56","2019-05-16 18:33:26","","52138-52160","","","6","","","Peeking Inside the Black-Box","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/F6FBEB66/8466590.html; /home/tim/Zotero/storage/XDKEPYBE/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf","","AI-based systems; artificial intelligence; Biological system modeling; black-box models; black-box nature; Conferences; explainable AI; explainable artificial intelligence; Explainable artificial intelligence; fourth industrial revolution; interpretable machine learning; Machine learning; Machine learning algorithms; Market research; Prediction algorithms; XAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88EG68IE","journalArticle","2018","Sushil, Madhumita; Šuster, Simon; Daelemans, Walter","Rule induction for global explanation of trained models","arXiv:1808.09744 [cs, stat]","","","","http://arxiv.org/abs/1808.09744","Understanding the behavior of a trained network and ﬁnding explanations for its outputs is important for improving the network’s performance and generalization ability, and for ensuring trust in automated systems. Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network. However, the relations between different features and classes are lost in most cases. We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network. We ﬁrst calculate the importance of the features in the trained network. We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and ﬁnally ﬁt a rule induction model to explain the model predictions. We ﬁnd that the output rule-sets can explain the predictions of a neural network trained for 4-class text classiﬁcation from the 20 newsgroups dataset to a macro-averaged F-score of 0.80. We make the code available at https://github.com/ clips/interpret_with_rules.","2018-08-29","2019-05-16 16:47:09","2019-05-16 18:41:12","2019-05-16 18:34:07","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1808.09744","Comment: Accepted at the Workshop on 'Analyzing and interpreting neural networks for NLP' (BlackboxNLP), EMNLP 2018","/home/tim/Zotero/storage/FEJFSMAK/Sushil et al. - 2018 - Rule induction for global explanation of trained m.pdf","","","Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YIZTFDPA","journalArticle","2019","Ming, Y.; Qu, H.; Bertini, E.","RuleMatrix: Visualizing and Understanding Classifiers with Rules","IEEE Transactions on Visualization and Computer Graphics","","1077-2626","10.1109/TVCG.2018.2864812","","With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.","2019-01","2019-05-16 16:46:56","2019-05-16 18:41:19","","342-352","","1","25","","","RuleMatrix","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/Y2R73IAR/8440085.html; /home/tim/Zotero/storage/DCGC5QDY/Ming et al. - 2019 - RuleMatrix Visualizing and Understanding Classifi.pdf","","black-box model; Data models; data visualisation; Data visualization; Decision trees; explainable machine learning; interactive systems; interactive visualization technique; knowledge representation; learning (artificial intelligence); Machine learning; machine learning systems; matrix algebra; matrix-based visualization; Neural networks; pattern classification; predictive models; rule matrix; rule visualization; standardized rule-based knowledge representation; Support vector machines; visual analytics; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3UR85G4","journalArticle","2018","Horel, Enguerrand; Mison, Virgile; Xiong, Tao; Giesecke, Kay; Mangu, Lidia","Sensitivity based Neural Networks Explanations","arXiv:1812.01029 [cs, stat]","","","","http://arxiv.org/abs/1812.01029","Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque ""black boxes"". The difﬁculty of interpreting the predictions of a neural network often prevents its use in ﬁelds where explainability is important, such as the ﬁnancial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.","2018-12-03","2019-05-16 16:47:11","2019-05-16 18:41:27","2019-05-16 18:34:39","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1812.01029","","/home/tim/Zotero/storage/A5E9JKV7/Horel et al. - 2018 - Sensitivity based Neural Networks Explanations.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E74F5A7F","conferencePaper","2018","Wu, Huijun; Wang, Chen; Yin, Jie; Lu, Kai; Zhu, Liming","Sharing Deep Neural Network Models with Interpretation","Proceedings of the 2018 World Wide Web Conference","978-1-4503-5639-8","","10.1145/3178876.3185995","https://doi.org/10.1145/3178876.3185995","Despite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs in the tree give users significantly better understanding of the model decision boundaries and paves the way for trustworthy model sharing.","2018","2019-05-16 16:47:18","2019-05-16 18:41:34","2019-05-16 18:34:57","177–186","","","","","","","WWW '18","","","","International World Wide Web Conferences Steering Committee","Republic and Canton of Geneva, Switzerland","","","","","","ACM Digital Library","","event-place: Lyon, France","","/home/tim/Zotero/storage/GT5RXPZA/Wu et al. - 2018 - Sharing Deep Neural Network Models with Interpreta.pdf","","decision boundary; deep neural networks; interpretability; model sharing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PM9PWQ6W","journalArticle","2019","Hohman, Fred; Park, Haekyu; Robinson, Caleb; Chau, Duen Horng","Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations","arXiv:1904.02323 [cs]","","","","http://arxiv.org/abs/1904.02323","Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce ﬁnal predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed based off of millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present SUMMIT, the ﬁrst interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. SUMMIT introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-inﬂuence aggregation identiﬁes relationships among such neurons. SUMMIT combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. SUMMIT scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where SUMMIT helps us discover multiple surprising insights into a state-of-the-art image classiﬁer’s learned representations and informs future neural network architecture design. The SUMMIT visualization runs in modern web browsers and is open-sourced.","2019-04-03","2019-05-16 16:47:12","2019-05-16 18:41:41","2019-05-16 18:35:20","","","","","","","Summit","","","","","","","en","","","","","arXiv.org","","arXiv: 1904.02323","Comment: [v1]: Manuscript currently under review; [v2]: fix author name","/home/tim/Zotero/storage/CC8R9NSE/Hohman et al. - 2019 - Summit Scaling Deep Learning Interpretability by .pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E75FICUD","journalArticle","1999","Taha, I. A.; Ghosh, J.","Symbolic interpretation of artificial neural networks","IEEE Transactions on Knowledge and Data Engineering","","1041-4347","10.1109/69.774103","","Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.","1999-05","2019-05-16 16:46:54","2019-05-16 18:41:48","","448-463","","3","11","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/TF4W3JYB/774103.html; /home/tim/Zotero/storage/Z9642ZGL/Taha and Ghosh - 1999 - Symbolic interpretation of artificial neural netwo.pdf","","adjustable detail level; artificial neural networks; Artificial neural networks; binary rules; breast cancer data set; Computer networks; connectionist architecture; Data mining; domain knowledge mapping; domain knowledge representation; embedded knowledge; explanation; explanation power; feedforward networks; feedforward neural nets; Fuzzy neural networks; Fuzzy sets; hidden layer; hybrid intelligent systems; Intelligent systems; iris data set; knowledge based systems; Knowledge based systems; knowledge refinement; knowledge representation; Knowledge representation; knowledge-based systems; learning (artificial intelligence); Military computing; network training; neural net architecture; Neural networks; output decision validation; partial rules; performance measures; rule evaluation technique; rule extraction; rule ordering; sigmoidal units; symbol manipulation; symbolic interpretation; truth maintenance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RTEGZJR","journalArticle","2018","Weld, Daniel S.; Bansal, Gagan","The Challenge of Crafting Intelligible Intelligence","arXiv:1803.04263 [cs]","","","","http://arxiv.org/abs/1803.04263","Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.","2018-03-09","2019-05-16 16:47:07","2019-05-16 18:41:56","2019-05-16 18:35:59","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1803.04263","Comment: arXiv admin note: text overlap with arXiv:1603.08507 by other authors","/home/tim/Zotero/storage/5C3GUYCN/Weld and Bansal - 2018 - The Challenge of Crafting Intelligible Intelligenc.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AESBUSM","journalArticle","2016","Lapuschkin, Sebastian; Binder, Alexander; Montavon, Grégoire; Müller, Klaus-Robert; Samek, Wojciech","The LRP Toolbox for Artificial Neural Networks","J. Mach. Learn. Res.","","1532-4435","","http://dl.acm.org/citation.cfm?id=2946645.3007067","The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pretrained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.","2016-01","2019-05-16 16:47:15","2019-05-16 18:42:20","2019-05-16 18:36:22","3938–3942","","1","17","","","","","","","","","","","","","","","ACM Digital Library","","","","/home/tim/Zotero/storage/DXDBQW2A/Lapuschkin et al. - 2016 - The LRP Toolbox for Artificial Neural Networks.pdf","","artificial neural networks; computer vision; deep learning; explaining classifiers; layer-wise relevance propagation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39QZARHT","journalArticle","2018","Al-Shedivat, Maruan; Dubey, Avinava; Xing, Eric P.","The Intriguing Properties of Model Explanations","arXiv:1801.09808 [cs]","","","","http://arxiv.org/abs/1801.09808","Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions. In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs). We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect performance of the model. Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous.","2018-01-29","2019-05-16 16:47:06","2019-05-16 18:42:13","2019-05-16 18:36:25","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1801.09808","Comment: Interpretable ML Symposium, NIPS 2017","/home/tim/Zotero/storage/88FRYTA8/Al-Shedivat et al. - 2018 - The Intriguing Properties of Model Explanations.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EA2ZZ8PA","conferencePaper","2019","Cai, Carrie J.; Jongejan, Jonas; Holbrook, Jess","The Effects of Example-based Explanations in a Machine Learning Interface","Proceedings of the 24th International Conference on Intelligent User Interfaces","978-1-4503-6272-6","","10.1145/3301275.3302289","http://doi.acm.org/10.1145/3301275.3302289","The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal.","2019","2019-05-16 16:47:01","2019-05-16 18:42:05","2019-05-16 18:36:26","258–262","","","","","","","IUI '19","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","event-place: Marina del Ray, California","","/home/tim/Zotero/storage/GEFI6G3M/Cai et al. - 2019 - The Effects of Example-based Explanations in a Mac.pdf","","example-based explanations; explainable AI; human-AI interaction; machine learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MPS2568Q","conferencePaper","2019","Kim, D.; Lim, W.; Hong, M.; Kim, H.","The Structure of Deep Neural Network for Interpretable Transfer Learning","2019 IEEE International Conference on Big Data and Smart Computing (BigComp)","","","10.1109/BIGCOMP.2019.8679150","","Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.","2019-02","2019-05-16 16:46:58","2019-05-16 18:42:27","","1-4","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/CAIRYHTZ/8679150.html; /home/tim/Zotero/storage/Y7UVIMKN/Kim et al. - 2019 - The Structure of Deep Neural Network for Interpret.pdf","","complex model; Computational modeling; Convolution; Data models; deep neural network; feature extraction; Feature extraction; feature extraction layer; high-quality data; image classification; interpretability; Interpretability; interpretable features; interpretable transfer learning; Korean characters classification; learning (artificial intelligence); Machine Learning; natural language processing; neural nets; Neural networks; pretrained model; Task analysis; Training; Transfer Learning; transfer learning task","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE International Conference on Big Data and Smart Computing (BigComp)","","","","","","","","","","","","","","",""
"UR3STN4I","journalArticle","2018","Alvarez-Melis, David; Jaakkola, Tommi S.","Towards Robust Interpretability with Self-Explaining Neural Networks","arXiv:1806.07538 [cs, stat]","","","","http://arxiv.org/abs/1806.07538","Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around speciﬁc predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classiﬁers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization speciﬁcally tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","2018-06-19","2019-05-16 16:47:08","2019-05-16 18:42:36","2019-05-16 18:37:12","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1806.07538","Comment: NeurIPS 2018","/home/tim/Zotero/storage/U5U95UMM/Alvarez-Melis and Jaakkola - 2018 - Towards Robust Interpretability with Self-Explaini.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KUR2JBY2","journalArticle","2016","Goyal, Yash; Mohapatra, Akrit; Parikh, Devi; Batra, Dhruv","Towards Transparent AI Systems: Interpreting Visual Question Answering Models","arXiv:1608.08974 [cs]","","","","http://arxiv.org/abs/1608.08974","Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research ﬁelds in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Speciﬁcally, we are interested in ﬁnding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques – guided backpropagation and occlusion – to ﬁnd important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.","2016-08-31","2019-05-16 16:47:03","2019-05-16 18:42:44","2019-05-16 18:37:35","","","","","","","Towards Transparent AI Systems","","","","","","","en","","","","","arXiv.org","","arXiv: 1608.08974","","/home/tim/Zotero/storage/WRLAUIWY/Goyal et al. - 2016 - Towards Transparent AI Systems Interpreting Visua.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NU737C2C","journalArticle","2017","Lengerich, Benjamin J.; Konam, Sandeep; Xing, Eric P.; Rosenthal, Stephanie; Veloso, Manuela","Towards Visual Explanations for Convolutional Neural Networks via Input Resampling","arXiv:1707.09641 [cs, stat]","","","","http://arxiv.org/abs/1707.09641","The predictive power of neural networks often costs model interpretability. Several techniques have been developed for explaining model outputs in terms of input features; however, it is diﬃcult to translate such interpretations into actionable insight. Here, we propose a framework to analyze predictions in terms of the model’s internal features by inspecting information ﬂow through the network. Given a trained network and a test image, we select neurons by two metrics, both measured over a set of images created by perturbations to the input image: (1) magnitude of the correlation between the neuron activation and the network output and (2) precision of the neuron activation. We show that the former metric selects neurons that exert large inﬂuence over the network output while the latter metric selects neurons that activate on generalizable features. By comparing the sets of neurons selected by these two metrics, our framework suggests a way to investigate the internal attention mechanisms of convolutional neural networks.","2017-07-30","2019-05-16 16:47:05","2019-05-16 18:42:53","2019-05-16 18:37:44","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1707.09641","Comment: Presented at ICML 2017 Workshop on Visualization for Deep Learning","/home/tim/Zotero/storage/KKM5ZWHC/Lengerich et al. - 2017 - Towards Visual Explanations for Convolutional Neur.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VISTAL2","journalArticle","2018","Ross, Andrew Slavin","Training Machine Learning Models by Regularizing their Explanations","arXiv:1810.00869 [cs, stat]","","","","http://arxiv.org/abs/1810.00869","Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).","2018-09-29","2019-05-16 16:47:10","2019-05-16 18:43:02","2019-05-16 18:38:11","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1810.00869","Comment: Harvard CSE master's thesis; includes portions of arxiv:1703.03717 and arxiv:1711.09404","/home/tim/Zotero/storage/NZ4BPU4G/Ross - 2018 - Training Machine Learning Models by Regularizing t.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUZKRNTM","journalArticle","2017","Ming, Yao; Cao, Shaozu; Zhang, Ruixiang; Li, Zhen; Chen, Yuanzhe; Song, Yangqiu; Qu, Huamin","Understanding Hidden Memories of Recurrent Neural Networks","arXiv:1710.10777 [cs]","","","","http://arxiv.org/abs/1710.10777","Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.","2017-10-30","2019-05-16 16:47:05","2019-05-16 18:43:11","2019-05-16 18:38:22","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1710.10777","Comment: Published at IEEE Conference on Visual Analytics Science and Technology (IEEE VAST 2017)","/home/tim/Zotero/storage/IZT597UG/Ming et al. - 2017 - Understanding Hidden Memories of Recurrent Neural .pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RGLHEU6P","journalArticle","2019","Nguyen, Anh; Yosinski, Jason; Clune, Jeff","Understanding Neural Networks via Feature Visualization: A survey","arXiv:1904.08939 [cs, stat]","","","","http://arxiv.org/abs/1904.08939","A neuroscience method to understanding the brain is to ﬁnd and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artiﬁcial or biological brain to ﬁre strongly. Those methods are known as Activation Maximization (AM) [ ] or Feature Visualization via Optimization. In this chapter, we ( ) review existing AM techniques in the literature; ( ) discuss a probabilistic interpretation for AM; and ( ) review the applications of AM in debugging and explaining networks.","2019-04-18","2019-05-16 16:47:13","2019-05-16 18:43:17","2019-05-16 18:38:29","","","","","","","Understanding Neural Networks via Feature Visualization","","","","","","","en","","","","","arXiv.org","","arXiv: 1904.08939","Comment: A book chapter in an Interpretable ML book (http://www.interpretable-ml.org/book/)","/home/tim/Zotero/storage/5PRIAJME/Nguyen et al. - 2019 - Understanding Neural Networks via Feature Visualiz.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFS2Y7XP","journalArticle","2018","Zhang, Quanshi; Yang, Yu; Liu, Yuchen; Wu, Ying Nian; Zhu, Song-Chun","Unsupervised Learning of Neural Networks to Explain Neural Networks","arXiv:1805.07468 [cs]","","","","http://arxiv.org/abs/1805.07468","This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e. explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which ﬁrst disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More speciﬁcally, the explainer contains interpretable conv-layers, where each ﬁlter disentangles the representation of a speciﬁc object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have signiﬁcantly boosted the interpretability of CNN features.","2018-05-18","2019-05-16 16:47:07","2019-05-16 18:43:27","2019-05-16 18:39:18","","","","","","","","","","","","","","en","","","","","arXiv.org","","arXiv: 1805.07468","","/home/tim/Zotero/storage/IJNY2M4W/Zhang et al. - 2018 - Unsupervised Learning of Neural Networks to Explai.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F8BFJ22T","conferencePaper","2018","Nie, S.; Healey, C.; Padia, K.; Leeman-Munk, S.; Benson, J.; Caira, D.; Sethi, S.; Devarajan, R.","Visualizing Deep Neural Networks for Text Analytics","2018 IEEE Pacific Visualization Symposium (PacificVis)","","","10.1109/PacificVis.2018.00031","","Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.","2018-04","2019-05-16 16:46:56","2019-05-16 18:43:47","","180-189","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","/home/tim/Zotero/storage/8Q65M566/8365991.html; /home/tim/Zotero/storage/SBWXJ29A/Nie et al. - 2018 - Visualizing Deep Neural Networks for Text Analytic.pdf","","Biological neural networks; clustering techniques; Computational modeling; convolutional connected layers; convolutional layers; Convolutional neural networks; data visualisation; deep learning; deep neural networks; DNN models; DNNs; feedforward neural nets; fully connected layers; human centered computing; information visualization; interaction techniques; internal activation patterns; learning (artificial intelligence); machine learning; natural language processing; network activation; neural network researchers; Neurons; speech models; summarization techniques; Task analysis; text analysis; text analytics; TNNVis; training patterns; visual encodings; Visualization; visualization design; visualization system","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 IEEE Pacific Visualization Symposium (PacificVis)","","","","","","","","","","","","","","",""
"ATRLZJ3D","journalArticle","2017","Oramas, Jose; Wang, Kaili; Tuytelaars, Tinne","Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks","arXiv:1712.06302 [cs, stat]","","","","http://arxiv.org/abs/1712.06302","Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identiﬁed features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNetbased visualizations. Moreover, we introduce an8Flower, a dataset speciﬁcally designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST, ILSVRC12, Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.","2017-12-18","2019-05-16 16:47:06","2019-05-16 18:43:37","2019-05-16 18:39:36","","","","","","","Visual Explanation by Interpretation","","","","","","","en","","","","","arXiv.org","","arXiv: 1712.06302","Comment: Accepted at International Conference on Learning Representations (ICLR) 2019. Project website: http://homes.esat.kuleuven.be/~joramas/projects/visualExplanationByInterpretation","/home/tim/Zotero/storage/TV5PT2ZB/Oramas et al. - 2017 - Visual Explanation by Interpretation Improving Vi.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"642HR5SK","journalArticle","2017","Arras, Leila; Horn, Franziska; Montavon, Grégoire; Müller, Klaus-Robert; Samek, Wojciech","""What is Relevant in a Text Document?"": An Interpretable Machine Learning Approach","PLOS ONE","","1932-6203","10.1371/journal.pone.0181142","http://arxiv.org/abs/1612.07843","Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text’s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classiﬁcation decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classiﬁers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classiﬁer, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classiﬁcation decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classiﬁcation accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.","2017-08-11","2019-05-16 16:47:04","2019-05-16 18:43:54","2019-05-16 18:40:10","e0181142","","8","12","","PLoS ONE","""What is Relevant in a Text Document?","","","","","","","en","","","","","arXiv.org","","arXiv: 1612.07843","Comment: 19 pages, 7 figures","/home/tim/Zotero/storage/MQ5QUNKK/Arras et al. - 2017 - What is Relevant in a Text Document An Interpr.pdf","","","Computer Science - Computation and Language; Computer Science - Information Retrieval; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AKWASNL4","journalArticle","2019","Paçacı, Görkem; Johnson, David; McKeever, Steve; Hamfelt, Andreas","""Why did you do that?"": Explaining black box models with Inductive Synthesis","arXiv:1904.09273 [cs]","","","","http://arxiv.org/abs/1904.09273","By their nature, the composition of black box models is opaque. This makes the ability to generate explanations for the response to stimuli challenging. The importance of explaining black box models has become increasingly important given the prevalence of AI and ML systems and the need to build legal and regulatory frameworks around them. Such explanations can also increase trust in these uncertain systems. In our paper we present RICE, a method for generating explanations of the behaviour of black box models by (1) probing a model to extract model output examples using sensitivity analysis; (2) applying CNPInduce, a method for inductive logic program synthesis, to generate logic programs based on critical input-output pairs; and (3) interpreting the target program as a human-readable explanation. We demonstrate the application of our method by generating explanations of an artiﬁcial neural network trained to follow simple traﬃc rules in a hypothetical self-driving car simulation. We conclude with a discussion on the scalability and usability of our approach and its potential applications to explanation-critical scenarios.","2019-04-17","2019-05-16 16:47:13","2019-05-16 18:44:02","2019-05-16 18:40:13","","","","","","","""Why did you do that?","","","","","","","en","","","","","arXiv.org","","arXiv: 1904.09273","Comment: 12 pages, 1 figure, accepted for publication at the Solving Problems with Uncertainties workshop as part of ICCS 2019, Faro, Portugal, June 12-14","/home/tim/Zotero/storage/L8UHB7LR/Paçacı et al. - 2019 - Why did you do that Explaining black box model.pdf","","","97R40 (Primary) 03B48 (Secondary); Computer Science - Artificial Intelligence; Computer Science - Machine Learning; D.2.1; I.2.2; I.2.3","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V49Y2BJ8","journalArticle","2018","Zharov, Yaroslav; Korzhenkov, Denis; Shvechikov, Pavel; Tuzhilin, Alexander","YASENN: Explaining Neural Networks via Partitioning Activation Sequences","arXiv:1811.02783 [cs, stat]","","","","http://arxiv.org/abs/1811.02783","We introduce a novel approach to feed-forward neural network interpretation based on partitioning the space of sequences of neuron activations. In line with this approach, we propose a model-speciﬁc interpretation method, called YASENN. Our method inherits many advantages of modelagnostic distillation, such as an ability to focus on the particular input region and to express an explanation in terms of features different from those observed by a neural network. Moreover, examination of distillation error makes the method applicable to the problems with low tolerance to interpretation mistakes. Technically, YASENN distills the network with an ensemble of layer-wise gradient boosting decision trees and encodes the sequences of neuron activations with leaf indices. The ﬁnite number of unique codes induces a partitioning of the input space. Each partition may be described in a variety of ways, including examination of an interpretable model (e.g. a logistic regression or a decision tree) trained to discriminate between objects of those partitions. Our experiments provide an intuition behind the method and demonstrate revealed artifacts in neural network decision making.","2018-11-07","2019-05-16 16:47:10","2019-05-16 18:44:16","2019-05-16 18:40:38","","","","","","","YASENN","","","","","","","en","","","","","arXiv.org","","arXiv: 1811.02783","","/home/tim/Zotero/storage/7Q8CRB2A/Zharov et al. - 2018 - YASENN Explaining Neural Networks via Partitionin.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8FWDEMK8","conferencePaper","","Laugel, Thibault; Lesot, Marie-Jeanne; Marsala, Christophe; Renard, Xavier; Detyniecki, Marcin","Comparison-based Inverse Classification for Interpretability in Machine Learning","","","","","","In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classiﬁer, considering the case where no information is available, neither on the classiﬁer itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classiﬁcation approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classiﬁcation must be explained, the proposed method consists in identifying a close neighbor classiﬁed diﬀerently, where the closeness deﬁnition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classiﬁer.","","2019-05-16 16:47:15","2019-05-16 19:09:01","","13","","","","","","","","","","","","","en","","","","","Zotero","","","","/home/tim/Zotero/storage/DKTETURB/Laugel et al. - Comparison-based Inverse Classification for Interp.pdf","","Comparison-based; Inverse classification; Local explanation; Post-hoc interpretability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9ZSYEFVG","conferencePaper","2016","Kasneci, Gjergji; Gottron, Thomas","LICON: A Linear Weighting Scheme for the Contribution ofInput Variables in Deep Artificial Neural Networks","Proceedings of the 25th ACM International on Conference on Information and Knowledge Management","978-1-4503-4073-1","","10.1145/2983323.2983746","http://doi.acm.org/10.1145/2983323.2983746","In recent years artificial neural networks have become the method of choice for many pattern recognition tasks. Despite their overwhelming success, a rigorous and easy to interpret mathematical explanation of the influence of input variables on a output produced by a neural network is still missing. We propose a generic framework as well as a concrete method for quantifying the influence of individual input signals on the output computed by a deep neural network. Inspired by the variable weighting scheme in the log-linear combination of variables in logistic regression, the proposed method provides linear models for specific observations of the input variables. This linear model locally approximates the behaviour of the neural network and can be used to quantify the influence of input variables in a principled way. We demonstrate the effectiveness of the proposed method in experiments on various synthetic and real-world datasets.","2016","2019-05-16 16:47:15","2019-05-16 19:17:07","2019-05-16 19:15:59","45–54","","","","","","LICON","CIKM '16","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","event-place: Indianapolis, Indiana, USA","","/home/tim/Zotero/storage/PVNHJJB7/Kasneci and Gottron - 2016 - LICON A Linear Weighting Scheme for the Contribut.pdf","","artificial neural networks; contribution; explanation; input variables; linear weighting scheme","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6SKBP3RW","conferencePaper","2018","Ito, Tomoki; Sakaji, Hiroki; Tsubouchi, Kota; Izumi, Kiyoshi; Yamashita, Tatsuo","Text-Visualizing Neural Network Model: Understanding Online Financial Textual Data","Advances in Knowledge Discovery and Data Mining","978-3-319-93040-4","","","","This study aims to visualize financial documents to swiftly obtain market sentiment information from these documents and determine the reason for which sentiment decisions are made. This type of visualization is considered helpful for nonexperts to easily understand technical documents such as financial reports. To achieve this, we propose a novel interpretable neural network (NN) architecture called gradient interpretable NN (GINN). GINN can visualize both the market sentiment score from a whole financial document and the sentiment gradient scores in concept units. We experimentally demonstrate the validity of text visualization produced by GINN using a real textual dataset.","2018","2019-05-16 16:47:14","2019-05-16 19:16:55","","247-259","","","","","","Text-Visualizing Neural Network Model","Lecture Notes in Computer Science","","","","Springer International Publishing","","en","","","","","Springer Link","","","","/home/tim/Zotero/storage/622IT537/Ito et al. - 2018 - Text-Visualizing Neural Network Model Understandi.pdf","","Interpretable neural network; Support system; Text mining","","Phung, Dinh; Tseng, Vincent S.; Webb, Geoffrey I.; Ho, Bao; Ganji, Mohadeseh; Rashidi, Lida","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""