
@article{shehDefiningExplainableAI2018,
  title = {Defining {{Explainable AI}} for {{Requirements Analysis}}},
  volume = {32},
  issn = {1610-1987},
  abstract = {Explainable artificial intelligence (XAI) has become popular in the last few years. The artificial intelligence (AI) community in general, and the machine learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements? In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.},
  language = {en},
  number = {4},
  journal = {KI - K\"unstliche Intelligenz},
  doi = {10.1007/s13218-018-0559-3},
  author = {Sheh, Raymond and Monteath, Isaac},
  month = nov,
  year = {2018},
  keywords = {Decision trees,Explainable AI,Machine learning,Neural networks,Requirements analysis},
  pages = {261-266},
  file = {/home/tim/Zotero/storage/K9GUDZBX/Sheh and Monteath - 2018 - Defining Explainable AI for Requirements Analysis.pdf}
}

@inproceedings{alonsoBibliometricAnalysisExplainable2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {A {{Bibliometric Analysis}} of the {{Explainable Artificial Intelligence Research Field}}},
  isbn = {978-3-319-91473-2},
  abstract = {This paper presents the results of a bibliometric study of the recent research on eXplainable Artificial Intelligence (XAI) systems. We took a global look at the contributions of scholars in XAI as well as in the subfields of AI that are mostly involved in the development of XAI systems. It is worthy to remark that we found out that about one third of contributions in XAI come from the fuzzy logic community. Accordingly, we went in depth with the actual connections of fuzzy logic contributions with AI to promote and improve XAI systems in the broad sense. Finally, we outlined new research directions aimed at strengthening the integration of different fields of AI, including fuzzy logic, toward the common objective of making AI accessible to people.},
  language = {en},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
  publisher = {{Springer International Publishing}},
  author = {Alonso, Jose M. and Castiello, Ciro and Mencar, Corrado},
  editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2018},
  keywords = {Comprehensibility,Explainable AI,Interpretability,Interpretable Fuzzy Systems,Understandability},
  pages = {3-15}
}

@incollection{mohantyBlackBoxUnderstanding2018,
  address = {Berkeley, CA},
  title = {Inside the {{Black Box}}: {{Understanding AI Decision Making}}},
  isbn = {978-1-4842-3808-0},
  shorttitle = {Inside the {{Black Box}}},
  abstract = {AI's ability to keep improving its predictive capabilities just by learning from the data and without significant involvement from humans to explain exactly how to accomplish the tasks is a big deal. Why?},
  language = {en},
  booktitle = {How to {{Compete}} in the {{Age}} of {{Artificial Intelligence}}: {{Implementing}} a {{Collaborative Human}}-{{Machine Strategy}} for {{Your Business}}},
  publisher = {{Apress}},
  author = {Mohanty, Soumendra and Vyas, Sachin},
  editor = {Mohanty, Soumendra and Vyas, Sachin},
  year = {2018},
  pages = {91-124},
  doi = {10.1007/978-1-4842-3808-0_4}
}

@incollection{rasExplanationMethodsDeep2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Explanation {{Methods}} in {{Deep Learning}}: {{Users}}, {{Values}}, {{Concerns}} and {{Challenges}}},
  isbn = {978-3-319-98131-4},
  shorttitle = {Explanation {{Methods}} in {{Deep Learning}}},
  abstract = {Issues regarding explainable AI involve four components: users, laws and regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods/interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Ras, Gabri\"elle and {van Gerven}, Marcel and Haselager, Pim},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Artificial intelligence,Deep neural networks,Explainable AI,Explanation methods,Interpretability},
  pages = {19-36},
  file = {/home/tim/Zotero/storage/GZ7R6V8C/Ras et al. - 2018 - Explanation Methods in Deep Learning Users, Value.pdf},
  doi = {10.1007/978-3-319-98131-4_2}
}

@article{zhangVisualInterpretabilityDeep2018,
  title = {Visual Interpretability for Deep Learning: A Survey},
  volume = {19},
  issn = {2095-9230},
  shorttitle = {Visual Interpretability for Deep Learning},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human\textendash{}computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  language = {en},
  number = {1},
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  doi = {10.1631/FITEE.1700808},
  author = {Zhang, Quan-shi and Zhu, Song-chun},
  month = jan,
  year = {2018},
  keywords = {Artificial intelligence,Deep learning,Interpretable model,TP391},
  pages = {27-39},
  file = {/home/tim/Zotero/storage/9YW5PKAF/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf}
}

@article{carabantesBlackboxArtificialIntelligence2019,
  title = {Black-Box Artificial Intelligence: An Epistemological and Critical Analysis},
  issn = {1435-5655},
  shorttitle = {Black-Box Artificial Intelligence},
  abstract = {The artificial intelligence models with machine learning that exhibit the best predictive accuracy, and therefore, the most powerful ones, are, paradoxically, those with the most opaque black-box architectures. At the same time, the unstoppable computerization of advanced industrial societies demands the use of these machines in a growing number of domains. The conjunction of both phenomena gives rise to a control problem on AI that in this paper we analyze by dividing the issue into two. First, we carry out an epistemological examination of the AI's opacity in light of the latest techniques to remedy it. And second, we evaluate the rationality of delegating tasks in opaque agents.},
  language = {en},
  journal = {AI \& SOCIETY},
  doi = {10.1007/s00146-019-00888-w},
  author = {Carabantes, Manuel},
  month = apr,
  year = {2019},
  keywords = {Artificial intelligence,Deep neural networks,GDPR,Instrumental reason,Machine learning,Philosophy of technology,XAI},
  file = {/home/tim/Zotero/storage/24Q7MRVK/Carabantes - 2019 - Black-box artificial intelligence an epistemologi.pdf}
}

@incollection{abdollahiTransparencyFairMachine2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Transparency in {{Fair Machine Learning}}: The {{Case}} of {{Explainable Recommender Systems}}},
  isbn = {978-3-319-90403-0},
  shorttitle = {Transparency in {{Fair Machine Learning}}},
  abstract = {Machine Learning (ML) models are increasingly being used in many sectors, ranging from health and education to justice and criminal investigation. Therefore, building a fair and transparent model which conveys the reasoning behind its predictions is of great importance. This chapter discusses the role of explanation mechanisms in building fair machine learning models and explainable ML technique. We focus on the special case of recommender systems because they are a prominent example of a ML model that interacts directly with humans. This is in contrast to many other traditional decision making systems that interact with experts (e.g. in the health-care domain). In addition, we discuss the main sources of bias that can lead to biased and unfair models. We then review the taxonomy of explanation styles for recommender systems and review models that can provide explanations for their recommendations. We conclude by reviewing evaluation metrics for assessing the power of explainability in recommender systems.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Abdollahi, Behnoush and Nasraoui, Olfa},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {21-35},
  doi = {10.1007/978-3-319-90403-0_2}
}

@inproceedings{marateaDeepNeuralNetworks2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Deep {{Neural Networks}} and {{Explainable Machine Learning}}},
  isbn = {978-3-030-12544-8},
  abstract = {From a general perspective, the most impressive results [1, 6] in Machine Learning have been recently obtained via black-box models, being Deep Neural Networks (DNNs) the major player in the game.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Maratea, Antonio and Ferone, Alessio},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  keywords = {Deep Neural Networks,Granular Computing,XAI},
  pages = {253-256},
  file = {/home/tim/Zotero/storage/ND6ZGFSS/Maratea and Ferone - 2019 - Deep Neural Networks and Explainable Machine Learn.pdf}
}

@article{ECR2018BOOK2018,
  title = {{{ECR}} 2018 - {{BOOK OF ABSTRACTS}}},
  volume = {9},
  issn = {1869-4101},
  language = {en},
  number = {1},
  journal = {Insights into Imaging},
  doi = {10.1007/s13244-018-0603-8},
  month = apr,
  year = {2018},
  pages = {1-642},
  file = {/home/tim/Zotero/storage/ZMRNQ2FK/2018 - ECR 2018 - BOOK OF ABSTRACTS.pdf}
}

@inproceedings{chimatapuExplainableAIFuzzy2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{AI}} and {{Fuzzy Logic Systems}}},
  isbn = {978-3-030-04070-3},
  abstract = {The recent advances in computing power coupled with the rapid increases in the quantity of available data has led to a resurgence in the theory and applications of Artificial Intelligence (AI). However, the use of complex AI algorithms like Deep Learning, Random Forests, etc., could result in a lack of transparency to users which is termed as black/opaque box models. Thus, for AI to be trusted and widely used by governments and industries, there is a need for greater transparency through the creation of explainable AI (XAI) systems. In this paper, we introduce the concepts of XAI and give an overview of hybrid systems which employ fuzzy logic systems which can hold great promise for creating trusted and explainable AI systems.},
  language = {en},
  booktitle = {Theory and {{Practice}} of {{Natural Computing}}},
  publisher = {{Springer International Publishing}},
  author = {Chimatapu, Ravikiran and Hagras, Hani and Starkey, Andrew and Owusu, Gilbert},
  editor = {Fagan, David and {Mart\'in-Vide}, Carlos and O'Neill, Michael and {Vega-Rodr\'iguez}, Miguel A.},
  year = {2018},
  keywords = {Deep fuzzy systems,Explainable AI,Fuzzy logic systems,XAI},
  pages = {3-20},
  file = {/home/tim/Zotero/storage/QDHC8B6N/Chimatapu et al. - 2018 - Explainable AI and Fuzzy Logic Systems.pdf}
}

@incollection{turnerControllingCreations2019,
  address = {Cham},
  title = {Controlling the {{Creations}}},
  isbn = {978-3-319-96235-1},
  abstract = {Turner explains how in order to implement constraints into AI directly, we will need to address both moral and technical questions: Which norms should be chosen? How can these be implemented? Potential basic laws for robots include: a law of identification, requiring that AI makes its status clear; a law of explanation, requiring that at least some parts of AI's reasoning be divulged; a laws on avoiding bias; and a law setting out any limits to areas where AI can operate. Finally, a kill switch law might make it mandatory that AI systems include a mechanism for safely interrupting their processes or operations, either temporarily or permanently.},
  language = {en},
  booktitle = {Robot {{Rules}} : {{Regulating Artificial Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Turner, Jacob},
  editor = {Turner, Jacob},
  year = {2019},
  pages = {319-369},
  doi = {10.1007/978-3-319-96235-1_8}
}

@article{rehseExplainableProcessPredictions2019,
  title = {Towards {{Explainable Process Predictions}} for {{Industry}} 4.0 in the {{DFKI}}-{{Smart}}-{{Lego}}-{{Factory}}},
  issn = {1610-1987},
  abstract = {With the advent of digitization on the shopfloor and the developments of Industry 4.0, companies are faced with opportunities and challenges alike. This can be illustrated by the example of AI-based process predictions, which can be valuable for real-time process management in a smart factory. However, to constructively collaborate with such a prediction, users need to establish confidence in its decisions. Explainable artificial intelligence (XAI) has emerged as a new research area to enable humans to understand, trust, and manage the AI they work with. In this contribution, we illustrate the opportunities and challenges of process predictions and XAI for Industry 4.0 with the DFKI-Smart-Lego-Factory. This fully automated factory prototype built out of LEGO\textbackslash{}(\^\textbackslash{}circledR\textbackslash{}) bricks demonstrates the potentials of Industry 4.0 in an innovative, yet easily accessible way. It includes a showcase that predicts likely process outcomes and uses state-of-the-art XAI techniques to explain them to its workers and visitors.},
  language = {en},
  journal = {KI - K\"unstliche Intelligenz},
  doi = {10.1007/s13218-019-00586-1},
  author = {Rehse, Jana-Rebecca and Mehdiyev, Nijat and Fettke, Peter},
  month = apr,
  year = {2019},
  keywords = {Explainable artificial Intelligence,Industry 4.0,Process prediction,Smart factories},
  file = {/home/tim/Zotero/storage/LBKGX9G9/Rehse et al. - 2019 - Towards Explainable Process Predictions for Indust.pdf}
}

@incollection{galitskyExplainableMachineLearning2019,
  address = {Cham},
  title = {Explainable {{Machine Learning}} for {{Chatbots}}},
  isbn = {978-3-030-04299-8},
  abstract = {Machine learning (ML) has been successfully applied to a wide variety of fields ranging from information retrieval, data mining, and speech recognition, to computer graphics, visualization, and human-computer interaction. However, most users often treat a machine learning model as a black box because of its incomprehensible functions and unclear working mechanism (Liu et al. 2017). Without a clear understanding of how and why a model works, the development of high performance models for chatbots typically relies on a time-consuming trial-and-error process. As a result, academic and industrial ML chatbot developers are facing challenges that demand more transparent and explainable systems for better understanding and analyzing ML models, especially their inner working mechanisms.In this Chapter we focus on explainability. We first discuss what is explainable ML and how its features are desired by users. We then draw an example chatbot-related classification problem and show how it is solved by a transparent rule-based or ML method. After that we present a decision support-enabled chatbot that shares its explanations to back up its decisions and tackles that of a human peer. We conclude this chapter with a learning framework representing a deterministic inductive approach with complete explainability.},
  language = {en},
  booktitle = {Developing {{Enterprise Chatbots}}: {{Learning Linguistic Structures}}},
  publisher = {{Springer International Publishing}},
  author = {Galitsky, Boris and Goldberg, Saveli},
  editor = {Galitsky, Boris},
  year = {2019},
  pages = {53-83},
  doi = {10.1007/978-3-030-04299-8_3}
}

@incollection{riegerStructuringNeuralNetworks2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Structuring {{Neural Networks}} for {{More Explainable Predictions}}},
  isbn = {978-3-319-98131-4},
  abstract = {Machine learning algorithms such as neural networks are more useful, when their predictions can be explained, e.g. in terms of input variables. Often simpler models are more interpretable than more complex models with higher performance. In practice, one can choose a readily interpretable (possibly less predictive) model. Another solution is to directly explain the original, highly predictive model. In this chapter, we present a middle-ground approach where the original neural network architecture is modified parsimoniously in order to reduce common biases observed in the explanations. Our approach leads to explanations that better separate classes in feed-forward networks, and that also better identify relevant time steps in recurrent neural networks.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Rieger, Laura and Chormai, Pattarawat and Montavon, Gr\'egoire and Hansen, Lars Kai and M\"uller, Klaus-Robert},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Convolutional neural networks,Interpretable machine learning,Recurrent neural networks},
  pages = {115-131},
  doi = {10.1007/978-3-319-98131-4_5}
}

@inproceedings{huExplainableNeuralComputation2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
  isbn = {978-3-030-01234-2},
  abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  publisher = {{Springer International Publishing}},
  author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  keywords = {Interpretable reasoning,Neural module networks,Visual question answering},
  pages = {55-71},
  file = {/home/tim/Zotero/storage/D3U374F3/Hu et al. - 2018 - Explainable Neural Computation via Stack Neural Mo.pdf}
}

@article{vellidoImportanceInterpretabilityVisualization2019,
  title = {The Importance of Interpretability and Visualization in Machine Learning for Applications in Medicine and Health Care},
  issn = {1433-3058},
  abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
  language = {en},
  journal = {Neural Computing and Applications},
  doi = {10.1007/s00521-019-04051-w},
  author = {Vellido, Alfredo},
  month = feb,
  year = {2019},
  keywords = {Explainability,Health care,Interpretability,Machine learning,Medicine,Visualization},
  file = {/home/tim/Zotero/storage/SI2SPSZ7/Vellido - 2019 - The importance of interpretability and visualizati.pdf}
}

@inproceedings{mencarPavingWayExplainable2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Paving the {{Way}} to {{Explainable Artificial Intelligence}} with {{Fuzzy Modeling}}},
  isbn = {978-3-030-12544-8},
  abstract = {Explainable Artificial Intelligence (XAI) is a relatively new approach to AI with special emphasis to the ability of machines to give sound motivations about their decisions and behavior. Since XAI is human-centered, it has tight connections with Granular Computing (GrC) in general, and Fuzzy Modeling (FM) in particular. However, although FM has been originally conceived to provide easily understandable models to users, this property cannot be taken for grant but it requires careful design choices. Furthermore, full integration of FM into XAI requires further processing, such as Natural Language Generation (NLG), which is a matter of current research.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Mencar, Corrado and Alonso, Jos\'e M.},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  pages = {215-227},
  file = {/home/tim/Zotero/storage/B8TUU224/Mencar and Alonso - 2019 - Paving the Way to Explainable Artificial Intellige.pdf}
}

@incollection{aakurInherentExplainabilityPattern2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {On the {{Inherent Explainability}} of {{Pattern Theory}}-{{Based Video Event Interpretations}}},
  isbn = {978-3-319-98131-4},
  abstract = {The ability of artificial intelligence systems to offer explanations for its decisions is central to building user confidence and structuring smart human-machine interactions. Expressing the rationale behind such a system's output is an important aspect of human-machine interaction as AI continues to be prominent in general, everyday use-cases. In this paper, we introduce a novel framework integrating Grenander's pattern theory structures to produce inherently explainable, symbolic representations for activity interpretations. These representations provide semantically rich and coherent interpretations of video activity using connected structures of detected (grounded) concepts, such as objects and actions, that are bound by semantics through background concepts not directly observed, i.e. contextualization cues. We use contextualization cues to establish semantic relationships among concepts to infer a deeper interpretation of events than what can be directly sensed. We propose the use of six questions that can be used to gain insight into the models ability to justify its decision and enhance its ability to interact with humans. The six questions are designed to (1) build an understanding of how the model is able to infer interpretations, (2) enable us to walk through its decision-making process, and (3) understand its drawbacks and possibly address them. We demonstrate the viability of this idea on video data using a dialog model that uses interpretations to generate explanations grounded in both video data and semantics.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Aakur, Sathyanarayanan N. and {de Souza}, Fillipe D. M. and Sarkar, Sudeep},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Activity interpretation,ConceptNet,Explainability,Semantics},
  pages = {277-299},
  doi = {10.1007/978-3-319-98131-4_11}
}

@incollection{liemPsychologyMeetsMachine2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Psychology {{Meets Machine Learning}}: {{Interdisciplinary Perspectives}} on {{Algorithmic Job Candidate Screening}}},
  isbn = {978-3-319-98131-4},
  shorttitle = {Psychology {{Meets Machine Learning}}},
  abstract = {In a rapidly digitizing world, machine learning algorithms are increasingly employed in scenarios that directly impact humans. This also is seen in job candidate screening. Data-driven candidate assessment is gaining interest, due to high scalability and more systematic assessment mechanisms. However, it will only be truly accepted and trusted if explainability and transparency can be guaranteed. The current chapter emerged from ongoing discussions between psychologists and computer scientists with machine learning interests, and discusses the job candidate screening problem from an interdisciplinary viewpoint. After introducing the general problem, we present a tutorial on common important methodological focus points in psychological and machine learning research. Following this, we both contrast and combine psychological and machine learning approaches, and present a use case example of a data-driven job candidate assessment system, intended to be explainable towards non-technical hiring specialists. In connection to this, we also give an overview of more traditional job candidate assessment approaches, and discuss considerations for optimizing the acceptability of technology-supported hiring solutions by relevant stakeholders. Finally, we present several recommendations on how interdisciplinary collaboration on the topic may be fostered.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Liem, Cynthia C. S. and Langer, Markus and Demetriou, Andrew and Hiemstra, Annemarie M. F. and Sukma Wicaksana, Achmadnoer and Born, Marise Ph. and K\"onig, Cornelius J.},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainability,Interdisciplinarity,Job candidate screening,Machine learning,Methodology,Multimodal analysis,Psychology},
  pages = {197-253},
  doi = {10.1007/978-3-319-98131-4_9}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
  isbn = {978-3-319-98131-4},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Accountability,Interpretability,Machine learning,Transparency},
  pages = {3-17},
  doi = {10.1007/978-3-319-98131-4_1}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
  series = {International {{Centre}} for {{Mechanical Sciences}}},
  title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
  isbn = {978-3-7091-2668-4},
  shorttitle = {Machine {{Learning}}},
  abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
  language = {en},
  booktitle = {Learning, {{Networks}} and {{Statistics}}},
  publisher = {{Springer Vienna}},
  author = {Bratko, I.},
  editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
  year = {1997},
  keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
  pages = {163-177}
}

@inproceedings{otteSafeInterpretableMachine2013,
  series = {Studies in {{Computational Intelligence}}},
  title = {Safe and {{Interpretable Machine Learning}}: {{A~Methodological Review}}},
  isbn = {978-3-642-32378-2},
  shorttitle = {Safe and {{Interpretable Machine Learning}}},
  abstract = {When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.},
  language = {en},
  booktitle = {Computational {{Intelligence}} in {{Intelligent Data Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Otte, Clemens},
  editor = {Moewes, Christian and N\"urnberger, Andreas},
  year = {2013},
  keywords = {Input Space,Methodological Review,Multivariate Adaptive Regression Spline,Symbolic Model,Symbolic Regression},
  pages = {111-122}
}

@incollection{jinInterpretabilityImprovementRBFbased2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Interpretability Improvement of {{RBF}}-Based Neurofuzzy Systems Using Regularized Learning},
  isbn = {978-3-540-37057-4},
  abstract = {Radial-basis-function (RBF) networks are mathematically equivalent to a class of fuzzy systems under mild conditions. Therefore, RBF networks have widely been used in learning of neurofuzzy systems to improve the performance. However, in most cases, the interpretability of fuzzy system will get lost after neural network learning. This chapter proposes a learning method using interpretability based regularization for neurofuzzy systems. This method can either be used in extracting interpretable fuzzy rules from RBF networks or in improving the interpretability of RBF-based neurofuzzy systems. Two simulation examples are presented to show the effectiveness of the proposed method.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jin, Yaochu},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Fuzzy Inference System,Fuzzy Rule,Fuzzy System,Membership Function,Radial Basis Function Network},
  pages = {605-620},
  doi = {10.1007/978-3-540-37057-4_26}
}

@inproceedings{oitaReverseEngineeringCreativity2020,
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  title = {Reverse {{Engineering Creativity}} into {{Interpretable Neural Networks}}},
  isbn = {978-3-030-12385-7},
  abstract = {In the field of AI the ultimate goal is to achieve generic intelligence, also called ``true AI'', but which depends on the successful enablement of imagination and creativity in artificial agents. To address this problem, this paper presents a novel deep learning framework for creativity, called INNGenuity. Pursuing an interdisciplinary implementation of creativity conditions, INNGenuity aims at the resolution of the various flaws of current AI learning architectures, which stem from the opacity of their models. Inspired by the neuroanatomy of the brain during creative cognition, the proposed framework's hybrid architecture blends both symbolic and connectionist AI, inline with Minsky's ``society of mind''. At its core, semantic gates are designed to facilitate an input/output flow of semantic structures and enable the usage of aligning mechanisms between neural activation clusters and semantic graphs. Having as goal alignment maximization, such a system would enable interpretability through the creation of labeled patterns of computation, and propose unaligned but relevant computation patterns as novel and useful, therefore creative.},
  language = {en},
  booktitle = {Advances in {{Information}} and {{Communication}}},
  publisher = {{Springer International Publishing}},
  author = {Oita, Marilena},
  editor = {Arai, Kohei and Bhatia, Rahul},
  year = {2020},
  keywords = {Creativity,Imagination,Interpretability,Knowledge,Neural architecture,Neural networks,Semantic networks},
  pages = {235-247}
}

@incollection{russellHumanInformationInteraction2017,
  address = {Cham},
  title = {Human {{Information Interaction}}, {{Artificial Intelligence}}, and {{Errors}}},
  isbn = {978-3-319-59719-5},
  abstract = {In a time of pervasive and increasingly transparent computing, humans will interact more with information objects, and less with the computing devices that define them. Artificial Intelligence (AI) will be the proxy for humans' interaction with information. Because interaction creates opportunities for error, the trend towards AI-augmented human information interaction (HII) will mandate an increased emphasis on cognition-oriented information science research, and new ways of thinking about errors and error handling. In this chapter, a review of HII and its relationship to AI is presented, with a focus on errors in this context.},
  language = {en},
  booktitle = {Autonomy and {{Artificial Intelligence}}: {{A Threat}} or {{Savior}}?},
  publisher = {{Springer International Publishing}},
  author = {Russell, Stephen and Moskowitz, Ira S. and Raglin, Adrienne},
  editor = {Lawless, W.F. and Mittu, Ranjeev and Sofge, Donald and Russell, Stephen},
  year = {2017},
  pages = {71-101},
  doi = {10.1007/978-3-319-59719-5_4}
}

@article{zerilliTransparencyAlgorithmicHuman2018,
  title = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}: {{Is There}} a {{Double Standard}}?},
  issn = {2210-5441},
  shorttitle = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}},
  abstract = {We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems, show in what respects AI fares little worse or better and argue that at least some regulatory proposals for explainable AI could end up setting the bar higher than is necessary or indeed helpful. The demands of practical reason require the justification of action to be pitched at the level of practical reason. Decision tools that support or supplant practical reasoning should not be expected to aim higher than this. We cast this desideratum in terms of Daniel Dennett's theory of the ``intentional stance'' and argue that since the justification of action for human purposes takes the form of intentional stance explanation, the justification of algorithmic decisions should take the same form. In practice, this means that the sorts of explanations for algorithmic decisions that are analogous to intentional stance explanations should be preferred over ones that aim at the architectural innards of a decision tool.},
  language = {en},
  journal = {Philosophy \& Technology},
  doi = {10.1007/s13347-018-0330-6},
  author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
  month = sep,
  year = {2018},
  keywords = {Algorithmic decision-making,Explainable AI,Intentional stance,Transparency},
  file = {/home/tim/Zotero/storage/DJLZ77QS/Zerilli et al. - 2018 - Transparency in Algorithmic and Human Decision-Mak.pdf}
}

@incollection{kayaMultimodalPersonalityTrait2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Multimodal {{Personality Trait Analysis}} for {{Explainable Modeling}} of {{Job Interview Decisions}}},
  isbn = {978-3-319-98131-4},
  abstract = {Automatic analysis of job interview screening decisions is useful for establishing the nature of biases that may play a role in such decisions. In particular, assessment of apparent personality gives insights into the first impressions evoked by a candidate. Such analysis tools can be used for training purposes, if they can be configured to provide appropriate and clear feedback. In this chapter, we describe a multimodal system that analyzes a short video of a job candidate, producing apparent personality scores and a prediction about whether the candidate will be invited for a further job interview or not. This system provides a visual and textual explanation about its decision, and was ranked first in the ChaLearn 2017 Job Candidate Screening Competition. We discuss the application scenario and the considerations from a broad perspective.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kaya, Heysem and Salah, Albert Ali},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable machine learning,Job candidate screening,Multimodal affective computing,Personality trait analysis},
  pages = {255-275},
  doi = {10.1007/978-3-319-98131-4_10}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
  isbn = {978-3-319-99740-7},
  shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
  abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial intelligence,Explainable AI,Knowledge extraction,Machine learning,Privacy},
  pages = {1-8},
  file = {/home/tim/Zotero/storage/UPAN5NTZ/Holzinger et al. - 2018 - Current Advances, Trends and Challenges of Machine.pdf}
}

@inproceedings{goebelExplainableAINew2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{AI}}: {{The New}} 42?},
  isbn = {978-3-319-99740-7},
  shorttitle = {Explainable {{AI}}},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning},
  pages = {295-303},
  file = {/home/tim/Zotero/storage/2PBCRQ7H/Goebel et al. - 2018 - Explainable AI The New 42.pdf}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
  isbn = {978-3-319-03200-9},
  abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Lisboa, P. J. G.},
  editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
  year = {2013},
  keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
  pages = {15-21},
  file = {/home/tim/Zotero/storage/LRZ3MW87/Lisboa - 2013 - Interpretability in Machine Learning  Principles .pdf}
}

@incollection{jinSimultaneousGenerationAccurate2006,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Simultaneous {{Generation}} of {{Accurate}} and {{Interpretable Neural Network Classifiers}}},
  isbn = {978-3-540-33019-6},
  abstract = {Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.},
  language = {en},
  booktitle = {Multi-{{Objective Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jin, Yaochu and Sendhoff, Bernhard and K\"orner, Edgar},
  editor = {Jin, Yaochu},
  year = {2006},
  keywords = {Hide Neuron,Mean Square Error,Multiobjective Optimization,Neural Network,Pareto Front},
  pages = {291-312},
  file = {/home/tim/Zotero/storage/TJXC8RBG/Jin et al. - 2006 - Simultaneous Generation of Accurate and Interpreta.pdf},
  doi = {10.1007/3-540-33019-4_13}
}

@incollection{kochGroupCognitionCollaborative2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Group {{Cognition}} and {{Collaborative AI}}},
  isbn = {978-3-319-90403-0},
  abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Koch, Janin and Oulasvirta, Antti},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {293-312},
  doi = {10.1007/978-3-319-90403-0_15}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
  isbn = {978-3-030-04503-6},
  abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
  language = {en},
  booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
  editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
  year = {2018},
  keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
  pages = {295-308},
  file = {/home/tim/Zotero/storage/4NFATM85/Zhou et al. - 2018 - Measuring Interpretability for Different Types of .pdf}
}

@inproceedings{hsuMultivariateTimeSeries2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Multivariate {{Time Series Early Classification}} with {{Interpretability Using Deep Learning}} and {{Attention Mechanism}}},
  isbn = {978-3-030-16142-2},
  abstract = {Multivariate time-series early classification is an emerging topic in data mining fields with wide applications like biomedicine, finance, manufacturing, etc. Despite of some recent studies on this topic that delivered promising developments, few relevant works can provide good interpretability. In this work, we consider simultaneously the important issues of model performance, earliness, and interpretability to propose a deep-learning framework based on the attention mechanism for multivariate time-series early classification. In the proposed model, we used a deep-learning method to extract the features among multiple variables and capture the temporal relation that exists in multivariate time-series data. Additionally, the proposed method uses the attention mechanism to identify the critical segments related to model performance, providing a base to facilitate the better understanding of the model for further decision making. We conducted experiments on three real datasets and compared with several alternatives. While the proposed method can achieve comparable performance results and earliness compared to other alternatives, more importantly, it can provide interpretability by highlighting the important parts of the original data, rendering it easier for users to understand how the prediction is induced from the data.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Hsu, En-Yu and Liu, Chien-Liang and Tseng, Vincent S.},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  keywords = {Attention,Deep neural network,Early classification on time-series},
  pages = {541-553},
  file = {/home/tim/Zotero/storage/F5VFRTAP/Hsu et al. - 2019 - Multivariate Time Series Early Classification with.pdf}
}

@article{bharadhwajExplanationsTemporalRecommendations2018,
  title = {Explanations for {{Temporal Recommendations}}},
  volume = {32},
  issn = {1610-1987},
  abstract = {Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
  language = {en},
  number = {4},
  journal = {KI - K\"unstliche Intelligenz},
  doi = {10.1007/s13218-018-0560-x},
  author = {Bharadhwaj, Homanga and Joshi, Shruti},
  month = nov,
  year = {2018},
  keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
  pages = {267-272},
  file = {/home/tim/Zotero/storage/TLKPAEUT/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf}
}

@inproceedings{zhangInterpretableNeuralModel2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {An {{Interpretable Neural Model}} with {{Interactive Stepwise Influence}}},
  isbn = {978-3-030-16142-2},
  abstract = {Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhang, Yin and Liu, Ninghao and Ji, Shuiwang and Caverlee, James and Hu, Xia},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  keywords = {Interpretation,Neural network,Stepwise Influence},
  pages = {528-540},
  file = {/home/tim/Zotero/storage/L3NSU5YW/Zhang et al. - 2019 - An Interpretable Neural Model with Interactive Ste.pdf}
}

@incollection{kimExplainableDeepDriving2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Explainable {{Deep Driving}} by {{Visualizing Causal Attention}}},
  isbn = {978-3-319-98131-4},
  abstract = {Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable\textemdash{}they should provide easy-to-interpret rationales for their behavior\textemdash{}so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here, we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolutional network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 h of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network highlights interpretable features that are used by humans while driving, and causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kim, Jinkyu and Canny, John},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable AI,Self-driving vehicles,Visual attention},
  pages = {173-193},
  doi = {10.1007/978-3-319-98131-4_8}
}

@incollection{ivancevicIntroductionHumanComputational2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Introduction: {{Human}} and {{Computational Mind}}},
  isbn = {978-3-540-71561-0},
  shorttitle = {Introduction},
  language = {en},
  booktitle = {Computational {{Mind}}: {{A Complex Dynamics Perspective}}},
  publisher = {{Springer Berlin Heidelberg}},
  editor = {Ivancevic, Vladimir G. and Ivancevic, Tijana T.},
  year = {2007},
  keywords = {Adaptive Resonance Theory,Cellular Automaton,Horn Clause,Human Mind},
  pages = {1-269},
  file = {/home/tim/Zotero/storage/5Y4AXSEY/Ivancevic and Ivancevic - 2007 - Introduction Human and Computational Mind.pdf},
  doi = {10.1007/978-3-540-71561-0_1}
}

@incollection{nissanAccountingSocialSpatial2012,
  address = {Dordrecht},
  series = {Law, {{Governance}} and {{Technology Series}}},
  title = {Accounting for {{Social}}, {{Spatial}}, and {{Textual Interconnections}}},
  isbn = {978-90-481-8990-8},
  abstract = {This is a chapter about what link analysis and data mining can do for criminal investigation. It is a long and complex chapter, in which a variety of techniques and topics are accommodated. It is divided in two parts, one about methods, and the other one about real-case studies. We begin by discussing social networks and their visualisation, as well as what unites them with or distinguishes them from link analysis (which itself historically arose from the disciplinary context of ergonomics). Having considered applications of link analysis to criminal investigation, we turn to crime risk assessment, to geographic information systems for mapping crimes, to detection, and then to multiagent architectures and their application to policing. We then turn to the challenge of handling a disparate mass of data, and introduce the reader to data warehousing, XML, ontologies, legal ontologies, and financial fraud ontology. A section about automated summarisation and its application to law is followed by a discussion of text mining and its application to law, and by a section on support vector machines for information retrieval, text classification, and matching. A section follows, about stylometrics, determining authorship, handwriting identification and its automation, and questioned documents evidence. We next discuss classification, clustering, series analysis, and association in knowledge discovery from legal databases; then, inconsistent data; rule induction (including in law); using neural networks in the legal context; fuzzy logic; and genetic algorithms. Before turning to case studies of link analysis and data mining, we take a broad view of digital resources and uncovering perpetration: email mining, computer forensics, and intrusion detection. We consider the Enron email database; the discovery of social coalitions with the SIGHTS text mining system, and recursive data mining. We discuss digital forensics, digital steganography, and intrusion detection (the use of learning techniques, the detection of masquerading, and honeypots for trapping intruders). Case studies include, for example: investigating Internet auction fraud with NetProbe; graph mining for malware detection with Polonium; link analysis with Coplink; a project of the U.S. Federal Defense Financial Accounting Service; information extraction tools for integration with a link analysis tool; the Poznan ontology model for the link analysis of fuel fraud; and fiscal fraud detection with the Pisa SNIPER project.},
  language = {en},
  booktitle = {Computer {{Applications}} for {{Handling Legal Evidence}}, {{Police Investigation}} and {{Case Argumentation}}},
  publisher = {{Springer Netherlands}},
  author = {Nissan, Ephraim},
  editor = {Nissan, Ephraim},
  year = {2012},
  pages = {483-765},
  doi = {10.1007/978-90-481-8990-8_6}
}

@incollection{thompsonArtificialIntelligenceArtificial1996,
  address = {Dordrecht},
  series = {Boston {{Studies}} in the {{Philosophy}} of {{Science}}},
  title = {Artificial {{Intelligence}}, {{Artificial Life}}, and the {{Symbol}}-{{Matter Problem}}},
  isbn = {978-94-009-0113-1},
  abstract = {What is the relation between matter and form? This question is of course as old as philosophy itself. But it also arises at the foundations of two recent scientific endeavours \textemdash{} the computational approach to the mind-brain in cognitive science and artificial intelligence (AI), and the synthetic approach to living systems in theoretical biology and artificial life (AL). In these fields the question arises primarily in connection with the status of symbols, that is, items that are physically realized, formally identified, and semantically interpretable.},
  language = {en},
  booktitle = {Qu\'ebec {{Studies}} in the {{Philosophy}} of {{Science}}: {{Part II}}: {{Biology}}, {{Psychology}}, {{Cognitive Science}} and {{Economics Essays}} in {{Honor}} of {{Hugues Leblanc}}},
  publisher = {{Springer Netherlands}},
  author = {Thompson, Evan},
  editor = {Marion, Mathieu and Cohen, Robert S.},
  year = {1996},
  keywords = {Artificial Life,Genetic Code,Intentional Stance,Multiple Realizability,Syntactic Feature},
  pages = {63-80},
  doi = {10.1007/978-94-009-0113-1_5}
}

@incollection{jinKnowledgeDiscoveryExtracting2003,
  address = {Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Knowledge {{Discovery}} by {{Extracting Interpretable Fuzzy Rules}}},
  isbn = {978-3-7908-1771-3},
  abstract = {Data, information and knowledge are three closely related but different concepts cepts. The following are the definitions abstracted from Webster's dictionary: Data: 1. factual information (as measurements or statistics) used as a basis for reasoning, discussion, or calculation 2. information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful 3. information in numerical form that can be digitally transmitted or processed. Information: 1. the communication or reception of knowledge or intelligence 2. knowledge obtained from investigation, study, or instruction. Knowledge: 1. the fact or condition of knowing something with familiarity gained through experience or association 2. acquaintance with or understanding of a science, art, or technique 3. the fact or condition of being aware of something.},
  language = {en},
  booktitle = {Advanced {{Fuzzy Systems Design}} and {{Applications}}},
  publisher = {{Physica-Verlag HD}},
  author = {Jin, Yaochu},
  editor = {Jin, Yaochu},
  year = {2003},
  keywords = {Fuzzy Rule,Fuzzy Subset,Fuzzy System,Membership Function,Radial Basis Function Network},
  pages = {173-204},
  doi = {10.1007/978-3-7908-1771-3_7}
}

@incollection{browneCriticalChallengesVisual2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
  isbn = {978-3-319-90403-0},
  abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {119-136},
  doi = {10.1007/978-3-319-90403-0_7}
}

@incollection{alonsoRoleInterpretableFuzzy2019,
  address = {Cham},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  title = {The {{Role}} of {{Interpretable Fuzzy Systems}} in {{Designing Cognitive Cities}}},
  isbn = {978-3-030-00317-3},
  abstract = {In recent years, there has been a huge effort connecting all kind of devices to Internet. From small devices (e.g., e-health monitoring sensors or mobile phones) that we carry daily in what is called the body-area-network, to big devices (such as cars), passing by all devices (e.g., TVs or refrigerators) at home. In modern cities, everything (at work, at home, and even in the streets) is connected to Internet. Accordingly, the amount of data in Internet grows dramatically every day. With this regard, humans face two main challenges: (1) to extract valuable knowledge from the given Big Data and (2) to become part of the equation, i.e., to become active actors in the Internet of Things. To do so, researchers and developers have created a novel generation of intelligent systems which are producing more and more intelligent devices, yielding what is called smart cities. Fuzzy systems are used in many applications in the context of Smart Cities. Now, it is time to address the effective interaction between intelligent systems and citizens with the aim of passing from smart to Cognitive Cities. Moreover, the use of interpretable fuzzy systems can facilitate such interaction and pave the way towards Cognitive Cities.},
  language = {en},
  booktitle = {Designing {{Cognitive Cities}}},
  publisher = {{Springer International Publishing}},
  author = {Alonso, Jos\'e M. and Castiello, Ciro and Mencar, Corrado},
  editor = {Portmann, Edy and Tabacchi, Marco E. and Seising, Rudolf and Habenstein, Astrid},
  year = {2019},
  keywords = {Cognitive City,Collaborative intelligence,Computational Theory of Perceptions,Fuzzy Logic,Human-machine communication,Interpretability,Linguistic descriptions of complex phenomena},
  pages = {131-152},
  doi = {10.1007/978-3-030-00317-3_6}
}

@incollection{liuInterpretabilityComputationalModels2016,
  address = {Cham},
  series = {Studies in {{Computational Intelligence}}},
  title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
  isbn = {978-3-319-30319-2},
  abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
  language = {en},
  booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  year = {2016},
  keywords = {Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Machine learning,Rule based networks,Rule based systems,Sentiment prediction},
  pages = {199-220},
  doi = {10.1007/978-3-319-30319-2_9}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
  isbn = {978-3-319-71746-3},
  shorttitle = {Interpretable {{Probabilistic Embeddings}}},
  abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
  publisher = {{Springer International Publishing}},
  author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
  editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
  year = {2018},
  pages = {167-180}
}

@article{curchoeArtificialIntelligenceMachine2019,
  title = {Artificial Intelligence and Machine Learning for Human Reproduction and Embryology Presented at {{ASRM}} and {{ESHRE}} 2018},
  issn = {1573-7330},
  abstract = {Sixteen artificial intelligence (AI) and machine learning (ML) approaches were reported at the 2018 annual congresses of the American Society for Reproductive Biology (9) and European Society for Human Reproduction and Embryology (7). Nearly every aspect of patient care was investigated, including sperm morphology, sperm identification, identification of empty or oocyte containing follicles, predicting embryo cell stages, predicting blastocyst formation from oocytes, assessing human blastocyst quality, predicting live birth from blastocysts, improving embryo selection, and for developing optimal IVF stimulation protocols. This represents a substantial increase in reports over 2017, where just one abstract each was reported at ASRM (AI) and ESHRE (ML). Our analysis reveals wide variability in how AI and ML methods are described (from not at all or very generic to fully describing the architectural framework) and large variability on accepted dataset sizes (from just 3 patients with 16 follicles in the smallest dataset to 661,060 images of 11,898 human embryos in one of the largest). AI and ML are clearly burgeoning methodologies in human reproduction and embryology and would benefit from early application of reporting standards.},
  language = {en},
  journal = {Journal of Assisted Reproduction and Genetics},
  doi = {10.1007/s10815-019-01408-x},
  author = {Curchoe, Carol Lynn and Bormann, Charles L.},
  month = jan,
  year = {2019},
  keywords = {Artificial intelligence,ASHRE,ASRM,Embryology,Human reproduction,Machine learning},
  file = {/home/tim/Zotero/storage/3QTHNKUM/Curchoe and Bormann - 2019 - Artificial intelligence and machine learning for h.pdf}
}

@incollection{dengJointIntroductionNatural2018,
  address = {Singapore},
  title = {A {{Joint Introduction}} to {{Natural Language Processing}} and to {{Deep Learning}}},
  isbn = {978-981-10-5209-5},
  abstract = {In this chapter, we set up the fundamental framework for the book. We first provide an introduction to the basics of natural language processing (NLP) as an integral part of artificial intelligence. We then survey the historical development of NLP, spanning over five decades, in terms of three waves. The first two waves arose as rationalism and empiricism, paving ways to the current deep learning wave. The key pillars underlying the deep learning revolution for NLP consist of (1) distributed representations of linguistic entities via embedding, (2) semantic generalization due to the embedding, (3) long-span deep sequence modeling of natural language, (4) hierarchical networks effective for representing linguistic levels from low to high, and (5) end-to-end deep learning methods to jointly solve many NLP tasks. After the survey, several key limitations of current deep learning technology for NLP are analyzed. This analysis leads to five research directions for future advances in NLP.},
  language = {en},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {1-22},
  doi = {10.1007/978-981-10-5209-5_1}
}

@incollection{pace-siggeWhereCorpusLinguistics2018,
  address = {Cham},
  title = {Where {{Corpus Linguistics}} and {{Artificial Intelligence}} ({{AI}}) {{Meet}}},
  isbn = {978-3-319-90719-2},
  abstract = {This chapter will provide a platform to showcase the more recent developments that have grown out of the early laid groundwork. The latest theories in the field of linguistics will be presented, based on empirical data taken from naturally occurring language. In particular, the lexical priming theory will be introduced as a way to explain structures of language that corpus linguists have uncovered. Furthermore, the chapter will discuss the development of increasingly sophisticated algorithms that also deal with the use of language. Here, the focus will be on key achievements in the 1980s by IBM which created a solid foundation for applications that are now widely used in mobile and desktop devices\textemdash{}namely ``assistants'' like Amazon's Echo, Apple's SIRI or Google's (and Android's) Google Go.},
  language = {en},
  booktitle = {Spreading {{Activation}}, {{Lexical Priming}} and the {{Semantic Web}}: {{Early Psycholinguistic Theories}}, {{Corpus Linguistics}} and {{AI Applications}}},
  publisher = {{Springer International Publishing}},
  author = {{Pace-Sigge}, Michael},
  editor = {{Pace-Sigge}, Michael},
  year = {2018},
  keywords = {Digital translators,Hoey,Lexical priming,LSTM,N-gram model,Norvig,Quillian},
  pages = {29-82},
  doi = {10.1007/978-3-319-90719-2_3}
}

@incollection{mainzerComplexSystemsEvolution2007,
  address = {Berlin, Heidelberg},
  title = {Complex {{Systems}} and the {{Evolution}} of {{Artificial Life}} and {{Intelligence}}},
  isbn = {978-3-540-72228-1},
  abstract = {All kinds of complex dynamical systems can be modeled by computational systems (Sect. 6.1). Their concepts are inspired by the successful technical applications of nonlinear dynamics to solid-state physics, spin-glass physics, chemical parallel computers, optical parallel computers, laser systems, and the human brain (Sect. 6.2). The cellular neural network (CNN) model has recently become an influential paradigm in complexity research (Sect. 6.3). Like the universal Turing machine model for digital computers, there is a universal CNN machine for modeling analog neural computers. CNNs are used not only for pattern recognition, but to simulate various types of pattern formation (Sect. 6.4). Exciting applications of artificial neural networks already exist in the fields of organic computing, neurobionics, medicine, and robotics (Sect. 6.5). Natural life and intelligence depends decisively on the evolution of organisms and brains. Therefore, embodied life and mind lead to embodied artificial intelligence and embodied artificial life of embodied robotics (Sect. 6.6).},
  language = {en},
  booktitle = {Thinking in {{Complexity}}: {{The Computional Dynamics}} of {{Matter}}, {{Mind}} and {{Mankind}}},
  publisher = {{Springer Berlin Heidelberg}},
  editor = {Mainzer, Klaus},
  year = {2007},
  keywords = {Boolean Function,Cellular Automaton,Cellular Neural Network,Organic Computing,Truth Table},
  pages = {227-310},
  doi = {10.1007/978-3-540-72228-1_6}
}

@incollection{mainzerComplexSystemsEvolution1997,
  address = {Berlin, Heidelberg},
  title = {Complex {{Systems}} and the {{Evolution}} of {{Artificial Intelligence}}},
  isbn = {978-3-662-13214-2},
  abstract = {Can machines think? This famous question from Turing has new topicality in the framework of complex systems. The chapter starts with a short history of computer science since Leibniz and his program for mechanizing thinking (mathesis universalis) (Sect. 5.1). The modern theory of computability enables us to distinguish complexity classes of problems, meaning the order of corresponding functions describing the computational time of their algorithms or computational programs. Modern computer science is interested not only in the complexity of universal problem solving but also in the complexity of knowledge-based programs. Famous examples are expert systems simulating the problem solving behavior of human experts in their specialized fields. Further on, we ask if a higher efficiency of problem solving may be expected from quantum computers and quantum complexity theory (Sect. 5.2).},
  language = {en},
  booktitle = {Thinking in {{Complexity}}: {{The Complex Dynamics}} of {{Matter}}, {{Mind}}, and {{Mankind}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mainzer, Klaus},
  editor = {Mainzer, Klaus},
  year = {1997},
  keywords = {Cellular Automaton,Certainty Factor,Computer Virus,Expert System,Turing Machine},
  pages = {171-252},
  doi = {10.1007/978-3-662-13214-2_5}
}

@inproceedings{muzzioliFutureFuzzySets2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {The {{Future}} of {{Fuzzy Sets}} in {{Finance}}: {{New Challenges}} in {{Machine Learning}} and {{Explainable AI}}},
  isbn = {978-3-030-12544-8},
  shorttitle = {The {{Future}} of {{Fuzzy Sets}} in {{Finance}}},
  abstract = {Traditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as possible before modeling, and, as a result, part of the original information is lost. On the other hand, machine learning does not impose rigid pre-assumptions about the problem and data distributions since the underlying ratio is to ``learn from data'', without the need for data reduction or a priori knowledge before the learning.},
  language = {en},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Muzzioli, Silvia},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  keywords = {Big data,Explainable AI,Finance,Fuzzy sets,Machine learning},
  pages = {265-268},
  file = {/home/tim/Zotero/storage/Y9XPTH5V/Muzzioli - 2019 - The Future of Fuzzy Sets in Finance New Challenge.pdf}
}

@inproceedings{brideDependableExplainableMachine2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
  isbn = {978-3-030-02450-5},
  abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
  language = {en},
  booktitle = {Formal {{Methods}} and {{Software Engineering}}},
  publisher = {{Springer International Publishing}},
  author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
  editor = {Sun, Jing and Sun, Meng},
  year = {2018},
  pages = {412-416},
  file = {/home/tim/Zotero/storage/8Y3ZXFCV/Bride et al. - 2018 - Towards Dependable and Explainable Machine Learnin.pdf}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
  isbn = {978-3-030-02628-8},
  abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
  language = {en},
  booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
  editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
  year = {2018},
  keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
  pages = {133-140},
  file = {/home/tim/Zotero/storage/GR58FCWK/Silva et al. - 2018 - Towards Complementary Explanations Using Deep Neur.pdf}
}

@incollection{liaoMiningHumanInterpretable2006,
  address = {Boston, MA},
  series = {Massive {{Computing}}},
  title = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}: {{An Overview}}},
  isbn = {978-0-387-34296-2},
  shorttitle = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}},
  abstract = {This chapter focuses on one particular class of data mining methodologies that expresses the mined knowledge in the form of fuzzy If-Then rules or fuzzy decision trees that can be easily understood by a human. Past studies on generating fuzzy If-Then rules (mostly from exemplar crisp data and a few from exemplar fuzzy data) are grouped into six major categories: grid partitioning, fuzzy clustering, genetic algorithms, neural networks, hybrid methods, and others. The representative method in each category is detailed. The latest improvements and advancements in each category are also reviewed. Similarly, past studies on generating fuzzy decision trees (from exemplar nominal and/or numeric data as well as from exemplar fuzzy data) are surveyed. The essence of each method is presented. Moreover, we discuss selected studies that address most of the necessary conditions for a fuzzy model to be interpretable and highlight areas for future studies. To give an idea of where fuzzy modeling methods have been applied, major application areas are also summarized.},
  language = {en},
  booktitle = {Data {{Mining}} and {{Knowledge Discovery Approaches Based}} on {{Rule Induction Techniques}}},
  publisher = {{Springer US}},
  author = {Liao, T. Warren},
  editor = {Triantaphyllou, Evangelos and Felici, Giovanni},
  year = {2006},
  keywords = {Data mining,Fuzzy clustering,Fuzzy decision trees,Fuzzy If-Then rules,Fuzzy modeling,Fuzzy-neural networks,Genetic algorithms,Neural networks},
  pages = {495-550},
  doi = {10.1007/0-387-34296-6_15}
}

@article{schubbachJudgingMachinesPhilosophical2019,
  title = {Judging Machines: Philosophical Aspects of Deep Learning},
  issn = {1573-0964},
  shorttitle = {Judging Machines},
  abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
  language = {en},
  journal = {Synthese},
  doi = {10.1007/s11229-019-02167-z},
  author = {Schubbach, Arno},
  month = mar,
  year = {2019},
  keywords = {Algorithm,Artificial intelligence,Computation,Deep learning,Explanation,Judgment,Justification,Kant,Machine learning},
  file = {/home/tim/Zotero/storage/DAHFWILU/Schubbach - 2019 - Judging machines philosophical aspects of deep le.pdf}
}

@article{zhongArtificialIntelligenceDrug2018,
  title = {Artificial Intelligence in Drug Design},
  volume = {61},
  issn = {1869-1889},
  abstract = {Thanks to the fast improvement of the computing power and the rapid development of the computational chemistry and biology, the computer-aided drug design techniques have been successfully applied in almost every stage of the drug discovery and development pipeline to speed up the process of research and reduce the cost and risk related to preclinical and clinical trials. Owing to the development of machine learning theory and the accumulation of pharmacological data, the artificial intelligence (AI) technology, as a powerful data mining tool, has cut a figure in various fields of the drug design, such as virtual screening, activity scoring, quantitative structure-activity relationship (QSAR) analysis, de novo drug design, and in silico evaluation of absorption, distribution, metabolism, excretion and toxicity (ADME/T) properties. Although it is still challenging to provide a physical explanation of the AI-based models, it indeed has been acting as a great power to help manipulating the drug discovery through the versatile frameworks. Recently, due to the strong generalization ability and powerful feature extraction capability, deep learning methods have been employed in predicting the molecular properties as well as generating the desired molecules, which will further promote the application of AI technologies in the field of drug design.},
  language = {en},
  number = {10},
  journal = {Science China Life Sciences},
  doi = {10.1007/s11427-018-9342-2},
  author = {Zhong, Feisheng and Xing, Jing and Li, Xutong and Liu, Xiaohong and Fu, Zunyun and Xiong, Zhaoping and Lu, Dong and Wu, Xiaolong and Zhao, Jihui and Tan, Xiaoqin and Li, Fei and Luo, Xiaomin and Li, Zhaojun and Chen, Kaixian and Zheng, Mingyue and Jiang, Hualiang},
  month = oct,
  year = {2018},
  keywords = {ADME/T,artificial intelligence,deep learning,drug design,QSAR},
  pages = {1191-1204},
  file = {/home/tim/Zotero/storage/TM3SYSZ9/Zhong et al. - 2018 - Artificial intelligence in drug design.pdf}
}

@incollection{panesarEthicsIntelligence2019,
  address = {Berkeley, CA},
  title = {Ethics of {{Intelligence}}},
  isbn = {978-1-4842-3799-1},
  abstract = {From supermarket checkouts to airport check-ins and digital healthcare to Internet banking, the use of data and AI for decision-making is ubiquitous. There has been an astronomic growth in data availability over the last two decades, fueled by, first, connectivity, and now the Internet of Things. Traditional data science teams focus on the use of data for the creation, implementation, validation, and evaluation of machine learning models that can be for predictive analytics.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{AI}} for {{Healthcare}}	: {{Big Data}} for {{Improved Health Outcomes}}},
  publisher = {{Apress}},
  author = {Panesar, Arjun},
  editor = {Panesar, Arjun},
  year = {2019},
  pages = {207-254},
  doi = {10.1007/978-1-4842-3799-1_6}
}

@article{jinExtractingInterpretableFuzzy2003,
  title = {Extracting {{Interpretable Fuzzy Rules}} from {{RBF Networks}}},
  volume = {17},
  issn = {1573-773X},
  abstract = {Radial basis function networks and fuzzy rule systems are functionally equivalent under some mild conditions. Therefore, the learning algorithms developed in the field of artificial neural networks can be used to adapt the parameters of fuzzy systems. Unfortunately, after the neural network learning, the structure of the original fuzzy system is changed and interpretability, which is considered to be one of the most important features of fuzzy systems, is usually impaired. This Letter discusses the differences between RBF networks and interpretable fuzzy systems. Based on these discussions, a method for extracting interpretable fuzzy rules from RBF networks is suggested. Simulation examples are given to embody the idea of this paper.},
  language = {en},
  number = {2},
  journal = {Neural Processing Letters},
  doi = {10.1023/A:1023642126478},
  author = {Jin, Yaochu and Sendhoff, Bernhard},
  month = apr,
  year = {2003},
  keywords = {Artificial Intelligence,Artificial Neural Network,Basis Function,Complex System,Neural Network},
  pages = {149-164},
  file = {/home/tim/Zotero/storage/8JJ8MLNZ/Jin and Sendhoff - 2003 - Extracting Interpretable Fuzzy Rules from RBF Netw.pdf}
}

@incollection{dengEpilogueFrontiersNLP2018,
  address = {Singapore},
  title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
  isbn = {978-981-10-5209-5},
  shorttitle = {Epilogue},
  abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
  language = {en},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {309-326},
  doi = {10.1007/978-981-10-5209-5_11}
}

@article{waltlIncreasingTransparencyAlgorithmic2018,
  title = {Increasing {{Transparency}} in {{Algorithmic}}- {{Decision}}-{{Making}} with {{Explainable AI}}},
  volume = {42},
  issn = {1862-2607},
  abstract = {Systems that can autonomously make decisions based on artificial intelligence are becoming ubiquitous. These decisions can affect sensitive areas in our daily life. For example, the price of goods in e-commerce transactions (e.g., via dynamic pricing), or the credit rating of customers (e.g., via automatic credit scoring). The need for more transparency of algorithmic-decision-making (ADM) is not only moving into the focus of lawmakers, but it is also desirable from an engineering perspective. This brief paper investigates different levels of transparency in ADM, and discusses how and to which degree auditing and testing can increase transparency of ADM.},
  language = {en},
  number = {10},
  journal = {Datenschutz und Datensicherheit - DuD},
  doi = {10.1007/s11623-018-1011-4},
  author = {Waltl, Bernhard and Vogl, Roland},
  month = oct,
  year = {2018},
  pages = {613-617},
  file = {/home/tim/Zotero/storage/HF98RF7S/Waltl and Vogl - 2018 - Increasing Transparency in Algorithmic- Decision-M.pdf}
}

@incollection{bikdashInterpretabilityComplexityModular2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {Interpretability, {{Complexity}}, and {{Modular Structure}} of {{Fuzzy Systems}}},
  isbn = {978-3-540-37057-4},
  abstract = {Zadeh's original motivation for fuzzy logic and the Fuzzy Rule-Based System (FRBS) was linguistic and hence possessed highly interpretable components. But as the complexity of a typical FRBS increases, it often becomes more like an uninterpretable neural network, and the Principle of Incompatibility predicts a degradation in interpretability for the same accuracy. This is particularly true of the so-called Takagi-Sugeno-Kang (TSK), or simply the Sugeno, approximator. We argue that imposing additional structure on the TSK system can significantly improve the tradeoff inherent in the Principle of Incompatibility. A promising structure was proposed recently in which the membership functions are local and sufficiently differentiable, and the consequent polynomials are rule-centered. This structure leads to the general interpretation that the consequent polynomials are Taylor series expansions. On this interpretation, a foundation for an algebra and a calculus of FRBSs can be built. We will illustrate these aspects of the proposed structure and discuss issues of modularity, functionality, and scalability of FRBSs.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bikdash, Marwan},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Fuzzy Inference System,Fuzzy System,Local Learning,Membership Function,Taylor Series Expansion},
  pages = {355-378},
  doi = {10.1007/978-3-540-37057-4_15}
}

@incollection{zhouMultipleObjectiveLearning2006,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Multiple {{Objective Learning}} for {{Constructing Interpretable Takagi}}-{{Sugeno Fuzzy Model}}},
  isbn = {978-3-540-33019-6},
  abstract = {This chapter discusses the interpretability of Takagi-Sugeno (TS) fuzzy systems. A new TS fuzzy model, whose membership functions are characterized by linguistic modifiers, is presented. The tradeoff between global approximation and local model interpretation has been achieved by minimizing a multiple objective performance measure. In the proposed model, the local models match the global model well and the erratic behaviors of local models are remedied effectively. Furthermore, the transparency of partitioning of input space has been improved during parameter adaptation.},
  language = {en},
  booktitle = {Multi-{{Objective Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zhou, Shang-Ming and Gan, John Q.},
  editor = {Jin, Yaochu},
  year = {2006},
  keywords = {Consequent Parameter,Fuzziness Measure,Fuzzy Model,Fuzzy System,Local Model},
  pages = {385-403},
  file = {/home/tim/Zotero/storage/PV76WSLV/Zhou and Gan - 2006 - Multiple Objective Learning for Constructing Inter.pdf},
  doi = {10.1007/3-540-33019-4_17}
}

@article{ziemkeConstructionRealityRobot2001,
  title = {The {{Construction}} of `{{Reality}}' in the {{Robot}}: {{Constructivist Perspectives}} on {{Situated Artificial Intelligence}} and {{Adaptive Robotics}}},
  volume = {6},
  issn = {1572-8471},
  shorttitle = {The {{Construction}} of `{{Reality}}' in the {{Robot}}},
  abstract = {This paper discusses different approaches incognitive science and artificial intelligenceresearch from the perspective of radicalconstructivism, addressing especially theirrelation to the biologically based theories ofvon Uexk\"ull, Piaget as well as Maturana andVarela. In particular recent work in `New AI' and adaptive robotics on situated and embodiedintelligence is examined, and we discuss indetail the role of constructive processes asthe basis of situatedness in both robots andliving organisms.},
  language = {en},
  number = {1},
  journal = {Foundations of Science},
  doi = {10.1023/A:1011394317088},
  author = {Ziemke, Tom},
  month = mar,
  year = {2001},
  keywords = {adaptive robotics,artificial intelligence,embodied cognition,radical constructivism,situatedness},
  pages = {163-233},
  file = {/home/tim/Zotero/storage/5NCIIVIG/Ziemke - 2001 - The Construction of Reality in the Robot Constr.pdf}
}

@article{dimidukPerspectivesImpactMachine2018,
  title = {Perspectives on the {{Impact}} of {{Machine Learning}}, {{Deep Learning}}, and {{Artificial Intelligence}} on {{Materials}}, {{Processes}}, and {{Structures Engineering}}},
  volume = {7},
  issn = {2193-9772},
  abstract = {The fields of machining learning and artificial intelligence are rapidly expanding, impacting nearly every technological aspect of society. Many thousands of published manuscripts report advances over the last 5 years or less. Yet materials and structures engineering practitioners are slow to engage with these advancements. Perhaps the recent advances that are driving other technical fields are not sufficiently distinguished from long-known informatics methods for materials, thereby masking their likely impact to the materials, processes, and structures engineering (MPSE). Alternatively, the diverse nature and limited availability of relevant materials data pose obstacles to machine-learning implementation. The glimpse captured in this overview is intended to draw focus to selected distinguishing advances, and to show that there are opportunities for these new technologies to have transformational impacts on MPSE. Further, there are opportunities for the MPSE fields to contribute understanding to the emerging machine-learning tools from a physics basis. We suggest that there is an immediate need to expand the use of these new tools throughout MPSE, and to begin the transformation of engineering education that is necessary for ongoing adoption of the methods.},
  language = {en},
  number = {3},
  journal = {Integrating Materials and Manufacturing Innovation},
  doi = {10.1007/s40192-018-0117-8},
  author = {Dimiduk, Dennis M. and Holm, Elizabeth A. and Niezgoda, Stephen R.},
  month = sep,
  year = {2018},
  keywords = {Artificial intelligence,Deep learning,Digital engineering,ICME,Machine learning,MGI,Multiscale modeling},
  pages = {157-172},
  file = {/home/tim/Zotero/storage/I992PVW9/Dimiduk et al. - 2018 - Perspectives on the Impact of Machine Learning, De.pdf}
}

@inproceedings{maesPolicySearchSpace2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Policy {{Search}} in a {{Space}} of {{Simple Closed}}-Form {{Formulas}}: {{Towards Interpretability}} of {{Reinforcement Learning}}},
  isbn = {978-3-642-33492-4},
  shorttitle = {Policy {{Search}} in a {{Space}} of {{Simple Closed}}-Form {{Formulas}}},
  abstract = {In this paper, we address the problem of computing interpretable solutions to reinforcement learning (RL) problems. To this end, we propose a search algorithm over a space of simple closed-form formulas that are used to rank actions. We formalize the search for a high-performance policy as a multi-armed bandit problem where each arm corresponds to a candidate policy canonically represented by its shortest formula-based representation. Experiments, conducted on standard benchmarks, show that this approach manages to determine both efficient and interpretable solutions.},
  language = {en},
  booktitle = {Discovery {{Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Maes, Francis and Fonteneau, Raphael and Wehenkel, Louis and Ernst, Damien},
  editor = {Ganascia, Jean-Gabriel and Lenca, Philippe and Petit, Jean-Marc},
  year = {2012},
  keywords = {Formula Discovery,Interpretability,Reinforcement Learning},
  pages = {37-51},
  file = {/home/tim/Zotero/storage/3WX72E8P/Maes et al. - 2012 - Policy Search in a Space of Simple Closed-form For.pdf}
}

@article{kuwajimaImprovingTransparencyDeep2019,
  title = {Improving Transparency of Deep Neural Inference Process},
  issn = {2192-6360},
  abstract = {Deep learning techniques are rapidly advanced recently and becoming a necessity component for widespread systems. However, the inference process of deep learning is black box and is not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of (1) structural feature analysis: lists of the features contributing to inference process, (2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and (3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75\% workers answer that input data and result of our feature analysis are consistent, and 70\% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provides suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.},
  language = {en},
  journal = {Progress in Artificial Intelligence},
  doi = {10.1007/s13748-019-00179-x},
  author = {Kuwajima, Hiroshi and Tanaka, Masayuki and Okutomi, Masatoshi},
  month = apr,
  year = {2019},
  keywords = {Black box,Deep neural network,Explainable AI,Transparency,Visual attribute,Visualization},
  file = {/home/tim/Zotero/storage/PUQL2N9V/Kuwajima et al. - 2019 - Improving transparency of deep neural inference pr.pdf}
}

@incollection{lughoferModelExplanationInterpretation2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
  isbn = {978-3-319-90403-0},
  abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
  language = {en},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Lughofer, Edwin},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {177-221},
  doi = {10.1007/978-3-319-90403-0_10}
}

@inproceedings{mayrRegularInferenceArtificial2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Regular {{Inference}} on {{Artificial Neural~Networks}}},
  isbn = {978-3-319-99740-7},
  abstract = {This paper explores the general problem of explaining the behavior of artificial neural networks (ANN). The goal is to construct a representation which enhances human understanding of an ANN as a sequence classifier, with the purpose of providing insight on the rationale behind the classification of a sequence as positive or negative, but also to enable performing further analyses, such as automata-theoretic formal verification. In particular, a probabilistic algorithm for constructing a deterministic finite automaton which is approximately correct with respect to an artificial neural network is proposed.},
  language = {en},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Mayr, Franz and Yovine, Sergio},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Artificial neural networks,Deterministic finite automata,Probably Approximately Correct learning,Sequence classification},
  pages = {350-369},
  file = {/home/tim/Zotero/storage/X2Z44SXK/Mayr and Yovine - 2018 - Regular Inference on Artificial NeuralNetworks.pdf}
}

@incollection{vandegevelNexusArtificialIntelligence2013,
  address = {Berlin, Heidelberg},
  series = {{{SpringerBriefs}} in {{Economics}}},
  title = {The {{Nexus Between Artificial Intelligence}} and {{Economics}}},
  isbn = {978-3-642-33648-5},
  abstract = {We review recent developments in artificial intelligence and relate them to economics. Artificial intelligence represents the technology most likely to lead to a singularity, an infinite rate of innovation and productivity growth. This could occur via dramatic increases in life expectancy, the development of whole brain emulation, and innovations in robotics. We argue that there is no reason to believe that artificial intelligence would increase human happiness. We describe some recent development in agent-based modeling in economics, which can be interpreted as the introduction of artificially intelligent agents into economics. We argue that classical economic theory, which assumes that all agents are rational and have infinite computational ability, is very relevant in describing the behavior of future artificially intelligent entities. Economic implications of accelerating innovation, greater longevity, and the introduction of robot labor are considered.},
  language = {en},
  booktitle = {The {{Nexus}} between {{Artificial Intelligence}} and {{Economics}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{van de Gevel}, Ad J. W. and Noussair, Charles N.},
  editor = {{van de Gevel}, Ad. J. W. and Noussair, Charles N.},
  year = {2013},
  keywords = {Artificial consciousness,Artificial economics,Artificial happiness,Artificial intelligence,Artificial life,Logistic growth,Methuselarity,Robotics,Singularity,Whole brain emulation},
  pages = {1-110},
  file = {/home/tim/Zotero/storage/XKKUQ4TZ/van de Gevel and Noussair - 2013 - The Nexus Between Artificial Intelligence and Econ.pdf},
  doi = {10.1007/978-3-642-33648-5_1}
}

@inproceedings{atzmuellerDeclarativeAspectsExplicative2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Declarative {{Aspects}} in {{Explicative Data Mining}} for {{Computational Sensemaking}}},
  isbn = {978-3-030-00801-7},
  abstract = {Computational sensemaking aims to develop methods and systems to ``make sense'' of complex data and information. The ultimate goal is then to provide insights and enhance understanding for supporting subsequent intelligent actions. Understandability and interpretability are key elements of that process as well as models and patterns captured therein. Here, declarativity helps to include guiding knowledge structures into the process, while explication provides interpretability, transparency, and explainability. This paper provides an overview of the key points and important developments in these areas, and outlines future potential and challenges.},
  language = {en},
  booktitle = {Declarative {{Programming}} and {{Knowledge Management}}},
  publisher = {{Springer International Publishing}},
  author = {Atzmueller, Martin},
  editor = {Seipel, Dietmar and Hanus, Michael and Abreu, Salvador},
  year = {2018},
  keywords = {Computational sensemaking,Data mining,Declarative modeling,Domain knowledge,Explicative data analysis,Knowledge graph,Statistical relational learning},
  pages = {97-114},
  file = {/home/tim/Zotero/storage/744VFK8M/Atzmueller - 2018 - Declarative Aspects in Explicative Data Mining for.pdf}
}

@incollection{menciaLearningInterpretableRules2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Learning {{Interpretable Rules}} for {{Multi}}-{{Label Classification}}},
  isbn = {978-3-319-98131-4},
  abstract = {Multi-label classification (MLC) is a supervised learning problem in which, contrary to standard multiclass classification, an instance can be associated with several class labels simultaneously. In this chapter, we advocate a rule-based approach to multi-label classification. Rule learning algorithms are often employed when one is not only interested in accurate predictions, but also requires an interpretable theory that can be understood, analyzed, and qualitatively evaluated by domain experts. Ideally, by revealing patterns and regularities contained in the data, a rule-based theory yields new insights in the application domain. Recently, several authors have started to investigate how rule-based models can be used for modeling multi-label data. Discussing this task in detail, we highlight some of the problems that make rule learning considerably more challenging for MLC than for conventional classification. While mainly focusing on our own previous work, we also provide a short overview of related work in this area.},
  language = {en},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Menc\'ia, Eneldo Loza and F\"urnkranz, Johannes and H\"ullermeier, Eyke and Rapp, Michael},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Label-dependencies,Multi-label classification,Rule learning,Separate-and-conquer},
  pages = {81-113},
  doi = {10.1007/978-3-319-98131-4_4}
}

@incollection{bundyArtificialIntelligenceTechniques1997,
  address = {Berlin, Heidelberg},
  title = {Artificial {{Intelligence Techniques}}},
  isbn = {978-3-642-60359-4},
  abstract = {A viewer-centred representation making explicit the depths, local orientations and discontinuities of visible surfaces, created and maintained from a number of cues, e.g., Stereopsis261 and Optical Flow190. It was thought by Marr to be at the limit of pure perception, i.e., subsequent processes are no longer completely data-driven, and for him it provides a representation of objective physical reality that precedes the decomposition of the scene into objects.},
  language = {en},
  booktitle = {Artificial {{Intelligence Techniques}}: {{A Comprehensive Catalogue}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bundy, Alan},
  editor = {Bundy, Alan},
  year = {1997},
  keywords = {Certainty Factor,Horn Clause,Inductive Logic Program,Natural Deduction,Phrase Structure Grammar},
  pages = {1-129},
  doi = {10.1007/978-3-642-60359-4_1}
}

@inproceedings{ponceVersatilityArtificialHydrocarbon2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Versatility of {{Artificial Hydrocarbon Networks}} for {{Supervised Learning}}},
  isbn = {978-3-030-02837-4},
  abstract = {Surveys on supervised machine show that each technique has strengths and weaknesses that make each of them more suitable for a particular domain or learning task. No technique is capable to tackle every supervised learning task, and it is difficult to comply with all possible desirable features of each particular domain. However, it is important that a new technique comply with the most requirements and desirable features of as many domains and learning tasks as possible. In this paper, we presented artificial hydrocarbon networks (AHN) as versatile and efficient supervised learning method. We determined the ability of AHN to solve different problem domains, with different data-sources and to learn different tasks. The analysis considered six applications in which AHN was successfully applied.},
  language = {en},
  booktitle = {Advances in {{Soft Computing}}},
  publisher = {{Springer International Publishing}},
  author = {Ponce, Hiram and {Mart\'inez-Villase\~nor}, Ma Lourdes},
  editor = {Castro, F\'elix and {Miranda-Jim\'enez}, Sabino and {Gonz\'alez-Mendoza}, Miguel},
  year = {2018},
  keywords = {Artificial organic networks,Interpretability,Machine learning,Versatility},
  pages = {3-16},
  file = {/home/tim/Zotero/storage/Z6RFFT9J/Ponce and Martnez-Villaseor - 2018 - Versatility of Artificial Hydrocarbon Networks for.pdf}
}

@article{27thAnnualComputational2018,
  title = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018): {{Part One}}},
  volume = {19},
  issn = {1471-2202},
  shorttitle = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018)},
  language = {en},
  number = {2},
  journal = {BMC Neuroscience},
  doi = {10.1186/s12868-018-0452-x},
  month = oct,
  year = {2018},
  pages = {64},
  file = {/home/tim/Zotero/storage/W6XXQSHX/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf}
}

@incollection{sutherlandDictionary1991,
  address = {London},
  series = {Dictionary {{Series}}},
  title = {The {{Dictionary}}},
  isbn = {978-1-349-12428-2},
  abstract = {A. An abbreviation for 1. AMPLITUDE; 2. AMPERE; 3. RESPONSE AMPLITUDE.},
  language = {en},
  booktitle = {Macmillan {{Dictionary}} of {{Psychology}}},
  publisher = {{Palgrave Macmillan UK}},
  author = {Sutherland, Stuart},
  editor = {Sutherland, Stuart},
  year = {1991},
  keywords = {Conditioned Stimulus,Hair Cell,Intelligence Quotient,Noun Phrase,Vocal Tract},
  pages = {1-486},
  doi = {10.1007/978-1-349-12428-2_1}
}

@incollection{harveyDeepLearningBreast2019,
  address = {Cham},
  title = {Deep {{Learning}} in {{Breast Cancer Screening}}},
  isbn = {978-3-319-94878-2},
  abstract = {Traditional computer aided detection (CAD) systems for breast cancer screening relied on machine learning with human-coded feature-engineering. They have largely failed to fulfill the promise of improving screening accuracy and workflow efficiency, and are often associated with increased recall rates and avoidable screening costs due to high instances of false positive markings. Advances in machine learning (such as deep learning) are on the cusp of providing more effective, more efficient, and even more patient-centric breast cancer screening support than ever before. By leveraging the consistent high sensitivity and specificity performance of autonomous systems, in combination with expert human oversight, the potential for efficient single-reader software-supported screening programs with low recall rates is on the horizon.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Medical Imaging}}: {{Opportunities}}, {{Applications}} and {{Risks}}},
  publisher = {{Springer International Publishing}},
  author = {Harvey, Hugh and Heindl, Andreas and Khara, Galvin and Korkinof, Dimitrios and O'Neill, Michael and Yearsley, Joseph and Karpati, Edith and Rijken, Tobias and Kecskemethy, Peter and Forrai, Gabor},
  editor = {Ranschaert, Erik R. and Morozov, Sergey and Algra, Paul R.},
  year = {2019},
  keywords = {Breast cancer,CAD,Deep learning,Mammography,Screening},
  pages = {187-215},
  doi = {10.1007/978-3-319-94878-2_14}
}

@article{harnadComputationJustInterpretable1994,
  title = {Computation Is Just Interpretable Symbol Manipulation; Cognition Isn't},
  volume = {4},
  issn = {1572-8641},
  abstract = {Computation is interpretable symbol manipulation. Symbols are objects that are manipulated on the basis of rules operating only on theirshapes, which are arbitrary in relation to what they can be interpreted as meaning. Even if one accepts the Church/Turing Thesis that computation is unique, universal and very near omnipotent, not everything is a computer, because not everything can be given a systematic interpretation; and certainly everything can't be givenevery systematic interpretation. But even after computers and computation have been successfully distinguished from other kinds of things, mental states will not just be the implementations of the right symbol systems, because of the symbol grounding problem: The interpretation of a symbol system is not intrinsic to the system; it is projected onto it by the interpreter. This is not true of our thoughts. We must accordingly be more than just computers. My guess is that the meanings of our symbols are grounded in the substrate of our robotic capacity to interact with that real world of objects, events and states of affairs that our symbols are systematically interpretable as being about.},
  language = {en},
  number = {4},
  journal = {Minds and Machines},
  doi = {10.1007/BF00974165},
  author = {Harnad, Stevan},
  month = nov,
  year = {1994},
  keywords = {Causality,cognition,computation,consciousness,continuity,implementation,robotics,semantics,sensorimotor transduction,symbol systems,syntax,Turing Machine,Turing Test},
  pages = {379-390},
  file = {/home/tim/Zotero/storage/KW5ZV7MH/Harnad - 1994 - Computation is just interpretable symbol manipulat.pdf}
}

@article{memmiConnectionismArtificialIntelligence1990,
  title = {Connectionism and Artificial Intelligence as Cognitive Models},
  volume = {4},
  issn = {1435-5655},
  abstract = {The current renewal of connectionist techniques using networks of neuron-like units has started to have an influence on cognitive modelling. However, compared with classical artificial intelligence methods, the position of connectionism is still not clear. In this article artificial intelligence and connectionism are systematically compared as cognitive models so as to bring out the advantages and shortcomings of each. The problem of structured representations appears to be particularly important, suggesting likely research directions.},
  language = {en},
  number = {2},
  journal = {AI \& SOCIETY},
  doi = {10.1007/BF01889639},
  author = {Memmi, Daniel},
  month = apr,
  year = {1990},
  keywords = {Artificial intelligence,Cognitive modelling,Epistemology,Neural networks,Representations},
  pages = {115-136},
  file = {/home/tim/Zotero/storage/43PQQWMN/Memmi - 1990 - Connectionism and artificial intelligence as cogni.pdf}
}

@inproceedings{laugelComparisonBasedInverseClassification2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}}},
  isbn = {978-3-319-91473-2},
  abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
  language = {en},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
  publisher = {{Springer International Publishing}},
  author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2018},
  keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
  pages = {100-111}
}

@article{kunapuliDecisionSupportToolRenal2018,
  title = {A {{Decision}}-{{Support Tool}} for {{Renal Mass Classification}}},
  volume = {31},
  issn = {1618-727X},
  abstract = {We investigate the viability of statistical relational machine learning algorithms for the task of identifying malignancy of renal masses using radiomics-based imaging features. Features characterizing the texture, signal intensity, and other relevant metrics of the renal mass were extracted from multiphase contrast-enhanced computed tomography images. The recently developed formalism of relational functional gradient boosting (RFGB) was used to learn human-interpretable models for classification. Experimental results demonstrate that RFGB outperforms many standard machine learning approaches as well as the current diagnostic gold standard of visual qualification by radiologists.},
  language = {en},
  number = {6},
  journal = {Journal of Digital Imaging},
  doi = {10.1007/s10278-018-0100-0},
  author = {Kunapuli, Gautam and Varghese, Bino A. and Ganapathy, Priya and Desai, Bhushan and Cen, Steven and Aron, Manju and Gill, Inderbir and Duddalwar, Vinay},
  month = dec,
  year = {2018},
  keywords = {Clinical decision support,Multiphase CT,Radiomics,Renal mass,Statistical relational learning},
  pages = {929-939},
  file = {/home/tim/Zotero/storage/Z6CV6ADF/Kunapuli et al. - 2018 - A Decision-Support Tool for Renal Mass Classificat.pdf}
}

@incollection{fiordalisoTradeoffAccuracyInterpretability2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {About the Trade-off between Accuracy and Interpretability of {{Takagi}}-{{Sugeno}} Models in the Context of Nonlinear Time Series Forecasting},
  isbn = {978-3-540-37057-4},
  abstract = {The focus of this chapter is related to the question of how to find appropriate Takagi-Sugeno (TS) rules in the framework of (chaotic) time series forecasting. We propose a generalization of the conventional TS system (GTS) allowing to evaluate the importance of any rule in the inference process. The added value of this feature has been put in light on two well-known chaotic time series.Local (clustering-based) and global (gradient-based) learning strategies for GTS systems are compared in terms of interpretability and accuracy. It appears that there is an unavoidable compromise between these two objectives. Global learning seems to be superior in term of accuracy but cannot achieve the same level of interpretability as the local approach.We also present an application of the GTS model in the field of forecasts combination with the target of generating interpretable final rules. This has led us to put additional constraints in the model. More precisely, we force the local linear models (rules conclusions) to achieve convex combination of the forecasts (input patterns). The proposed system may be useful for a wide range of applications where a consensus is required, including forecasts synthesis, controllers and patterns classifiers aggregation.},
  language = {en},
  booktitle = {Interpretability {{Issues}} in {{Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Fiordaliso, Antonio},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Chaotic Time Series,Fuzzy Model,Fuzzy System,Radial Basis Function,Time Series Forecast},
  pages = {406-430},
  doi = {10.1007/978-3-540-37057-4_17}
}

@incollection{ishibuchiInterpretabilityIssuesFuzzy2003,
  address = {Berlin, Heidelberg},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretability {{Issues}} in {{Fuzzy Genetics}}-{{Based Machine Learning}} for {{Linguistic Modelling}}},
  isbn = {978-3-540-39906-3},
  abstract = {This chapter discusses several issues related to the design of linguistic models with high interpretability using fuzzy genetics-based machine learning (GBML) algorithms. We assume that a set of linguistic terms has been given for each variable. Thus our modelling task is to find a small number of fuzzy rules from possible combinations of the given linguistic terms. First we formulate a three-objective optimization problem, which simultaneously minimizes the total squared error, the number of fuzzy rules, and the total rule length. Next we show how fuzzy GBML algorithms can be applied to our problem in the framework of multi-objective optimization as well as single-objective optimization. Then we point out a possibility that misleading fuzzy rules can be generated when general and specific fuzzy rules are simultaneously used in a single linguistic model. Finally we show that non-standard inclusion-based fuzzy reasoning removes such an undesirable possibility.},
  language = {en},
  booktitle = {Modelling with {{Words}}: {{Learning}}, {{Fusion}}, and {{Reasoning}} within a {{Formal Linguistic Represntation Framework}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ishibuchi, Hisao and Yamamoto, Takashi},
  editor = {Lawry, Jonathan and Shanahan, Jimi and L. Ralescu, Anca},
  year = {2003},
  pages = {209-228},
  file = {/home/tim/Zotero/storage/TKJTL9YH/Ishibuchi and Yamamoto - 2003 - Interpretability Issues in Fuzzy Genetics-Based Ma.pdf},
  doi = {10.1007/978-3-540-39906-3_11}
}

@inproceedings{keneseiCombinationofToolsMethodLearning2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Combination}}-of-{{Tools Method}} for {{Learning Interpretable Fuzzy Rule}}-{{Based Classifiers}} from {{Support Vector Machines}}},
  isbn = {978-3-540-77226-2},
  abstract = {A new approach is proposed for the data-based identification of transparent fuzzy rule-based classifiers. It is observed that fuzzy rule-based classifiers work in a similar manner as kernel function-based support vector machines (SVMs) since both model the input space by nonlinearly maps into a feature space where the decision can be easily made. Accordingly, trained SVM can be used for the construction of fuzzy rule-based classifiers. However, the transformed SVM does not automatically result in an interpretable fuzzy model because the SVM results in a complex rule-base, where the number of rules is approximately 40-60\% of the number of the training data. Hence, reduction of the SVM-initialized classifier is an essential task. For this purpose, a three-step reduction algorithm is developed based on the combination of previously published model reduction techniques. In the first step, the identification of the SVM is followed by the application of the Reduced Set method to decrease the number of kernel functions. The reduced SVM is then transformed into a fuzzy rule-based classifier. The interpretability of a fuzzy model highly depends on the distribution of the membership functions. Hence, the second reduction step is achieved by merging similar fuzzy sets based on a similarity measure. Finally, in the third step, an orthogonal least-squares method is used to reduce the number of rules and re-estimate the consequent parameters of the fuzzy rule-based classifier. The proposed approach is applied for the Wisconsin Breast Cancer, Iris and Wine classification problems to compare its performance to other methods.},
  language = {en},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} - {{IDEAL}} 2007},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kenesei, Tamas and Roubos, Johannes A. and Abonyi, Janos},
  editor = {Yin, Hujun and Tino, Peter and Corchado, Emilio and Byrne, Will and Yao, Xin},
  year = {2007},
  keywords = {Classification,Fuzzy classifier,Model Reduction,Support Vector Machine},
  pages = {477-486},
  file = {/home/tim/Zotero/storage/TITG8KZZ/Kenesei et al. - 2007 - A Combination-of-Tools Method for Learning Interpr.pdf}
}

@incollection{mencarInterpretabilityFuzzyInformation2009,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Interpretability of {{Fuzzy Information Granules}}},
  isbn = {978-3-540-92916-1},
  abstract = {Human-Centric Information Processing requires tight communication processes between users and computers. These two actors, however, traditionally use different paradigms for representing and manipulating information. Users are more inclined in managing perceptual information, usually expressed in natural language, whilst computers are formidable number-crunching systems, capable of manipulating information expressed in precise form. Fuzzy information granules could be used as a common interface for communicating information and knowledge, because of their ability of representing perceptual information in a computer manageable form. Nonetheless, this connection could be established only if information granules are interpretable, i.e. they are semantically co-intensive with human knowledge. Interpretable information granulation opens several methodological issues, regarding the representation and manipulation of information granules, the interpretability constraints and the granulation processes. By taking into account all such issues, effective Information Processing systems could be designed with a strong Human-Centric imprint.},
  language = {en},
  booktitle = {Human-{{Centric Information Processing Through Granular Modelling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mencar, Corrado},
  editor = {Bargiela, Andrzej and Pedrycz, Witold},
  year = {2009},
  keywords = {Fuzzy Information,Linguistic Term,Membership Function,Natural Language,Perceptual Information},
  pages = {95-118},
  doi = {10.1007/978-3-540-92916-1_5}
}

@inproceedings{benrimohAifredHealthDeep2018,
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Aifred {{Health}}, a {{Deep Learning Powered Clinical Decision Support System}} for {{Mental Health}}},
  isbn = {978-3-319-94042-7},
  abstract = {Aifred Health, one of the top two teams in the first round of the IBM Watson AI XPRIZE competition, is using deep learning to solve the problem of treatment selection and prognosis prediction in mental health, starting with depression. Globally, depression affects over 300 million people and is the leading cause of disability. While a range of effective treatments do exist, patients' responses to treatments vary to a large degree. Some patients spend years going through a frustrating `trial-and-error' process in order to find an effective treatment. The Aifred Health solution is a deep learning-powered Clinical Decision Support System (CDSS) aimed at helping clinicians select the most effective treatment plans for depression in collaboration with their patients. In this chapter, we discuss problem of treatment selection in depression and explore the technical, clinical, and ethical dimensions of building a CDSS for mental health based on deep learning technology.},
  language = {en},
  booktitle = {The {{NIPS}} '17 {{Competition}}: {{Building Intelligent Systems}}},
  publisher = {{Springer International Publishing}},
  author = {Benrimoh, David and Fratila, Robert and Israel, Sonia and Perlman, Kelly and Mirchi, Nykan and Desai, Sneha and Rosenfeld, Ariel and Knappe, Sabrina and Behrmann, Jason and Rollins, Colleen and You, Raymond Penh and Aifred Health Team, The},
  editor = {Escalera, Sergio and Weimer, Markus},
  year = {2018},
  pages = {251-287}
}

@article{wangLearningPerformancePrediction2019,
  title = {Learning Performance Prediction via Convolutional {{GRU}} and Explainable Neural Networks in E-Learning Environments},
  volume = {101},
  issn = {1436-5057},
  abstract = {Students learning performance prediction is a challenging task due to the dynamic, virtual environments and the personalized needs for different individuals. To ensure that learners' potential problems can be identified as early as possible, this paper aim to develop a predictive model for effective learning feature extracting, learning performance predicting and result reasoning. We first proposed a general learning feature quantification method to convert the raw data from e-learning systems into sets of independent learning features. Then, a weighted avg-pooling is chosen instead of typical max-pooling in a novel convolutional GRU network for learning performance prediction. Finally, an improved parallel xNN is provided to explain the prediction results. The relevance of positive/negative between features and result could help students find out which part should be improved. Experiments have been carried out over two real online courses data. Results show that our proposed approach performs favorably compared with several other state-of-the-art methods.},
  language = {en},
  number = {6},
  journal = {Computing},
  doi = {10.1007/s00607-018-00699-9},
  author = {Wang, Xizhe and Wu, Pengze and Liu, Guang and Huang, Qionghao and Hu, Xiaoling and Xu, Haijiao},
  month = jun,
  year = {2019},
  keywords = {68T01,68U35,Deep neural network,E-learning environments,Learning feature quantification,Learning performance prediction},
  pages = {587-604},
  file = {/home/tim/Zotero/storage/7IXL7NYP/Wang et al. - 2019 - Learning performance prediction via convolutional .pdf}
}

@article{yankovskayaTradeoffSearchMethods2017,
  title = {Tradeoff Search Methods between Interpretability and Accuracy of the Identification Fuzzy Systems Based on Rules},
  volume = {27},
  issn = {1555-6212},
  abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
  language = {en},
  number = {2},
  journal = {Pattern Recognition and Image Analysis},
  doi = {10.1134/S1054661817020134},
  author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
  month = apr,
  year = {2017},
  keywords = {accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,machine learning,metaheuristic,pattern recognition,synergy},
  pages = {243-265},
  file = {/home/tim/Zotero/storage/X2YDZAVK/Yankovskaya et al. - 2017 - Tradeoff search methods between interpretability a.pdf}
}

@inproceedings{munkhdalaiAdvancedNeuralNetwork2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Advanced {{Neural Network Approach}}, {{Its Explanation}} with {{LIME}} for {{Credit Scoring Application}}},
  isbn = {978-3-030-14802-7},
  abstract = {Neural network models have achieved a human-level performance in many application domains, including image classification, speech recognition and machine translation. However, in credit scoring application, neural network approach has been useless because of its black box nature that the relationship between contextual input and output cannot be completely understood. In this study, we investigate the advanced neural network approach and its' explanation for credit scoring. We use the LIME technique to interpret the black box of such neural network and verify its' trustworthiness by comparing a high interpretable logistic model. The results show that neural network models give higher accuracy and equivalent explanation with the logistic model.},
  language = {en},
  booktitle = {Intelligent {{Information}} and {{Database Systems}}},
  publisher = {{Springer International Publishing}},
  author = {Munkhdalai, Lkhagvadorj and Wang, Ling and Park, Hyun Woo and Ryu, Keun Ho},
  editor = {Nguyen, Ngoc Thanh and Gaol, Ford Lumban and Hong, Tzung-Pei and Trawi\'nski, Bogdan},
  year = {2019},
  keywords = {Credit scoring,LIME,Neural network},
  pages = {407-419},
  file = {/home/tim/Zotero/storage/7UDYI2UH/Munkhdalai et al. - 2019 - Advanced Neural Network Approach, Its Explanation .pdf}
}

@incollection{schetininAdvancedFeatureRecognition2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Advanced {{Feature Recognition}} and {{Classification Using Artificial Intelligence Paradigms}}},
  isbn = {978-3-540-47518-7},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Recognition}} and {{Classification}} of {{Astrophysical}} and {{Medical Images}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Schetinin, V. and Zharkova, Valentina and Brazhnikov, A. and Zharkov, S. I. and Salerno, Emanuele and Bedini, Luigi and Kuruoglu, Ercan E. and Tonazzini, Anna and Zazula, Damjan and Cigale, Boris and Yoshida, Hiroyuki},
  editor = {Zharkova, Valentina and Jain, Lakhmi C.},
  year = {2007},
  keywords = {Cellular Neural Network,Compute Tomography Colonography,Independent Component Analysis,Source Separation},
  pages = {151-338},
  file = {/home/tim/Zotero/storage/32C6JQ4M/Schetinin et al. - 2007 - Advanced Feature Recognition and Classification Us.pdf},
  doi = {10.1007/978-3-540-47518-7_4}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
  address = {Cham},
  title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
  isbn = {978-3-319-91596-8},
  abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
  language = {en},
  booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
  publisher = {{Springer International Publishing}},
  author = {Wodecki, Andrzej},
  editor = {Wodecki, Andrzej},
  year = {2019},
  pages = {133-246},
  file = {/home/tim/Zotero/storage/59B7SQNM/Wodecki - 2019 - Influence of Artificial Intelligence on Activities.pdf},
  doi = {10.1007/978-3-319-91596-8_3}
}

@incollection{aguirreDescriptionSeveralCharacteristics2003,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  title = {A Description of Several Characteristics for Improving the Accuracy and Interpretability of Inductive Linguistic Rule Learning Algorithms},
  isbn = {978-3-540-37058-1},
  abstract = {The learning algorithms can be an useful tool for helping to the humans to understand the behavior of phenomena from a set of samples. In particular, those algorithms that represent the knowledge obtained by linguistic fuzzy rules are appropriate for this task. However, it is not sufficient that the knowledge representation is close to the humans comprehension. Furthermore, it is necessary that the knowledge is expressed as simple as possible.In this chapter, we classify some techniques, models and tools for improving the knowledge obtained by inductive linguistic rule learning algorithms from three different points of view: those that increase the knowledge interpretability, those that increase the knowledge accuracy keeping its interpretability and those that simultaneously increase the accuracy and interpretability of the knowledge.In this study, we have considered fuzzy rules expressed by the Disjunctive Normal Form (DNF).},
  language = {en},
  booktitle = {Accuracy {{Improvements}} in {{Linguistic Fuzzy Modeling}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Aguirre, Eugenio and Gonzalez, Antonio and P\'erez, Ra\'ul},
  editor = {Casillas, Jorge and Cord\'on, Oscar and Herrera, Francisco and Magdalena, Luis},
  year = {2003},
  keywords = {Feature Selection,Feature Subset,Fuzzy Logic Controller,Fuzzy Rule,Learning Algorithm},
  pages = {249-276},
  doi = {10.1007/978-3-540-37058-1_11}
}

@incollection{aggarwalModelBasedCollaborativeFiltering2016,
  address = {Cham},
  title = {Model-{{Based Collaborative Filtering}}},
  isbn = {978-3-319-29659-3},
  abstract = {The neighborhood-based methods of the previous chapter can be viewed as generalizations of k-nearest neighbor classifiers, which are commonly used in machine learning.},
  language = {en},
  booktitle = {Recommender {{Systems}}: {{The Textbook}}},
  publisher = {{Springer International Publishing}},
  author = {Aggarwal, Charu C.},
  editor = {Aggarwal, Charu C.},
  year = {2016},
  pages = {71-138},
  doi = {10.1007/978-3-319-29659-3_3}
}

@inproceedings{maymiHumanMachineTeamingCyberspace2018a,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Human-{{Machine Teaming}} and {{Cyberspace}}},
  isbn = {978-3-319-91470-1},
  abstract = {Artificial Intelligence is becoming the key enabler of solutions to a variety of problems including those associated with cyberspace operations. Based on our analysis of cyber threats and opportunities in the coming years, we assess it as very likely that teams consisting of humans and synthetic agents will routinely work together in many if not most organizations. To fully leverage the potential of these teams, we must continue to develop new paradigms in human-machine teaming. Specifically, we must address three areas that are currently in their infancy. Firstly, we need interfaces that allow all teammates to communicate effectively with each other and seamlessly transfer tasks among them. This must be true regardless of whether the endpoints are human or not. Secondly, we will need cybersecurity operators with broad knowledge and skills. They must know how their synthetic teammates ``think,'' when to task them and when to question their reports. Thirdly, our AI systems must be able to explain their decision-making processes to their human teammates. This paper provides an overview of cyberspace threats and opportunities in the next ten years and how these will impact human-machine teaming. We then apply the key lessons we have learned while working a multitude of advanced research projects at the intersection of human and AI agents to cyberspace operations. Finally, we propose areas of research that will allow humans and machines to better collaborate in the future.},
  language = {en},
  booktitle = {Augmented {{Cognition}}: {{Intelligent Technologies}}},
  publisher = {{Springer International Publishing}},
  author = {Maym\'i, Fernando J. and Thomson, Robert},
  editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
  year = {2018},
  keywords = {Artificial intelligence,Cyberspace,Human-machine teaming},
  pages = {299-315},
  file = {/home/tim/Zotero/storage/4T95BJQU/Maym and Thomson - 2018 - Human-Machine Teaming and Cyberspace.pdf}
}

@incollection{rovatsosAnticipatoryArtificialIntelligence2017a,
  address = {Cham},
  title = {Anticipatory {{Artificial Intelligence}}},
  isbn = {978-3-319-31737-3},
  abstract = {Anticipation occupies a special role in Artificial Intelligence (AI), not only because replicating human anticipatory processes is naturally a part of the aim to replicate human intelligence, but also because the design, implementation, and evaluation of AI systems involves a mix of several, interdependent anticipatory processes, some of which are carried out by human designers and users, and others by the AI system. This chapter provides an introduction to key AI technologies, investigates to what extent they involve anticipatory processes, and explores the consequences of such anticipation on the predictability of AI systems for humans. Our analysis suggests that the relationship between human and AI anticipation is complementary - the more pronounced the anticipatory capabilities of the AI system, the harder it may be for humans to anticipate their behaviour. If this turns out to be correct, building safe, responsible, and ethically sound AI will require developing more understandable and explainable AI methods in the future.},
  language = {en},
  booktitle = {Handbook of {{Anticipation}}: {{Theoretical}} and {{Applied Aspects}} of the {{Use}} of {{Future}} in {{Decision Making}}},
  publisher = {{Springer International Publishing}},
  author = {Rovatsos, Michael},
  editor = {Poli, Roberto},
  year = {2017},
  keywords = {Anticipation,Anticipatory process,Artificial Intelligence,Computational modelling},
  pages = {1-33},
  file = {/home/tim/Zotero/storage/EWZIPQNT/Rovatsos - 2017 - Anticipatory Artificial Intelligence.pdf},
  doi = {10.1007/978-3-319-31737-3_45-1}
}

@inproceedings{fabra-boludaModellingMachineLearning2018,
  series = {Studies in {{Applied Philosophy}}, {{Epistemology}} and {{Rational Ethics}}},
  title = {Modelling {{Machine Learning Models}}},
  isbn = {978-3-319-96448-5},
  abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
  language = {en},
  booktitle = {Philosophy and {{Theory}} of {{Artificial Intelligence}} 2017},
  publisher = {{Springer International Publishing}},
  author = {{Fabra-Boluda}, Ra\"ul and Ferri, C\`esar and {Hern\'andez-Orallo}, Jos\'e and {Mart\'inez-Plumed}, Fernando and {Ram\'irez-Quintana}, M. Jos\'e},
  editor = {M\"uller, Vincent C.},
  year = {2018},
  pages = {175-186}
}

@incollection{quekCognitiveInterpretationThermographic2010,
  address = {Berlin, Heidelberg},
  series = {Intelligent {{Systems Reference Library}}},
  title = {A {{Cognitive Interpretation}} of {{Thermographic Images Using Novel Fuzzy Learning Semantic Memories}}},
  isbn = {978-3-642-13639-9},
  abstract = {Fuzzy neural network (FNN), a hybrid of fuzzy logic and neural network, is computationally powerful, robust and able to model a complex nonlinear problem domain via the extraction and self-tuning of fuzzy IF-THEN rules. This yields a powerful semantic learning memory system that is useful to build intelligent decision support tools. Thermal imaging has been effectively used in the detection of infrared spectrum for the screening of potential SARS patients. This chapter proposes a cognitive approach in the study of the correlation and semantic interpretation of superficial thermal images against the true internal body temperature. Comparison between global and local semantic memories as well as Mamdani and TSK model of FNNs are presented. Existing infrared systems are commonly used at various boarder checkpoints and these have high false-negative rate. The use of FNN as a back-end of the system can significantly improves and hence serves the role of an intelligent medical decision support tool with high degree of accuracy. Extensive experimentations are conducted on real-life data taken from the Emergency Department (A\&E), Tan Tock Seng Hospital (the designated SARS center in Singapore). The performance of FNN as the thermal analysis decision support system, providing plausible semantic interpretation and understanding, is highly encouraging.},
  language = {en},
  booktitle = {Handbook on {{Decision Making}}: {{Vol}} 1: {{Techniques}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Quek, C. and Irawan, W. and Ng, E.},
  editor = {Jain, Lakhmi C. and Lim, Chee Peng},
  year = {2010},
  keywords = {brain-inspired,cognitive approach,fuzzy rules,intelligent decision support system,learning memories,local and global semantic networks,Mamdani and TSK models,neural fuzzy networks,Semantic interpretation,temperature correlation,Thermal imaging,thermographs},
  pages = {427-452},
  doi = {10.1007/978-3-642-13639-9_17}
}

@inproceedings{weijtersInterpretableNeuralNetworks1998a,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretable Neural Networks with {{BP}}-{{SOM}}},
  isbn = {978-3-540-69350-5},
  abstract = {Artificial Neural Networks (ANNS) are used successfully in industry and commerce. This is not surprising since neural networks are especially competitive for complex tasks for which insufficient domain-specific knowledge is available. However, interpretation of models induced by ANNS is often extremely difficult. BP-SOM is an relatively novel neural network architecture and learning algorithm which offers possibilities to overcome this limitation. BP-SOM is a combination of a multi-layered feed-forward network (MFN) trained with the back-propagation learning rule (BP), and Kohonen's self-organizing maps (sorts). In earlier reports, it has been shown that BP-SOM improved the generalization performance as compared to that of BP, while at the same time it decreased the number of necessary hidden units without loss of generalization performance. In this paper we demonstrate that BP-SOM trained networks results in uniform and clustered hidden layer representations appropriate for interpretation of the networks functionality.},
  language = {en},
  booktitle = {Tasks and {{Methods}} in {{Applied Artificial Intelligence}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Weijters, Ton and {van den Bosch}, Antal},
  editor = {{Pasqual del Pobil}, Angel and Mira, Jos\'e and Ali, Moonis},
  year = {1998},
  keywords = {Generalization Performance,Hide Unit,Neural Network Architecture,Test Instance,Training Instance},
  pages = {564-573},
  file = {/home/tim/Zotero/storage/2LXNZQEA/Weijters and van den Bosch - 1998 - Interpretable neural networks with BP-SOM.pdf}
}

@article{nadinMachineIntelligenceChimera2018,
  title = {Machine Intelligence: A Chimera},
  issn = {1435-5655},
  shorttitle = {Machine Intelligence},
  abstract = {The notion of computation has changed the world more than any previous expressions of knowledge. However, as know-how in its particular algorithmic embodiment, computation is closed to meaning. Therefore, computer-based data processing can only mimic life's creative aspects, without being creative itself. AI's current record of accomplishments shows that it automates tasks associated with intelligence, without being intelligent itself. Mistaking the abstract (computation) for the concrete (computer) has led to the religion of ``everything is an output of computation''\textemdash{}even the humankind that conceived the computer. The hypostatized role of computers explains the increased dependence on them. The convergence machine called deep learning is only the most recent form through which the deterministic theology of the machine claims more than what it actually is: extremely effective data processing. A proper understanding of complexity, as well as the need to distinguish between the reactive nature of the artificial and the anticipatory nature of the living are suggested as practical responses to the challenges posed by machine theology.},
  language = {en},
  journal = {AI \& SOCIETY},
  doi = {10.1007/s00146-018-0842-8},
  author = {Nadin, Mihai},
  month = apr,
  year = {2018},
  keywords = {Anticipatory,Convergence,G-complexity,Hypostatize,Meaning},
  file = {/home/tim/Zotero/storage/3D6LEH4E/Nadin - 2018 - Machine intelligence a chimera.pdf}
}

@article{muggletonUltraStrongMachineLearning2018,
  title = {Ultra-{{Strong Machine Learning}}: Comprehensibility of Programs Learned with {{ILP}}},
  volume = {107},
  issn = {1573-0565},
  shorttitle = {Ultra-{{Strong Machine Learning}}},
  abstract = {During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell's, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper we provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. We present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment we directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie's sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. We believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.},
  language = {en},
  number = {7},
  journal = {Machine Learning},
  doi = {10.1007/s10994-018-5707-3},
  author = {Muggleton, Stephen H. and Schmid, Ute and Zeller, Christina and {Tamaddoni-Nezhad}, Alireza and Besold, Tarek},
  month = jul,
  year = {2018},
  keywords = {Comprehensibility,Inductive logic programming,Ultra-strong machine learning},
  pages = {1119-1140},
  file = {/home/tim/Zotero/storage/8TKKSVDG/Muggleton et al. - 2018 - Ultra-Strong Machine Learning comprehensibility o.pdf}
}

@incollection{ben-menahemDemiseDogmaticUniverse2009,
  address = {Berlin, Heidelberg},
  title = {Demise of the {{Dogmatic Universe}}},
  isbn = {978-3-540-68832-7},
  language = {en},
  booktitle = {Historical {{Encyclopedia}} of {{Natural}} and {{Mathematical Sciences}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{Ben-Menahem}, Ari},
  editor = {{Ben-Menahem}, Ari},
  year = {2009},
  pages = {2801-5079},
  doi = {10.1007/978-3-540-68832-7_8}
}


