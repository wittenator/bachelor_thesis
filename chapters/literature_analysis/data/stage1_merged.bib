
@incollection{aggarwalMachineLearningShallow2018,
  address = {Cham},
  title = {Machine {{Learning}} with {{Shallow Neural Networks}}},
  isbn = {978-3-319-94463-0},
  abstract = {Conventional machine learning often uses optimization and gradient-descent methods for learning parameterized models. Examples of such models include linear regression, support vector machines, logistic regression, dimensionality reduction, and matrix factorization. Neural networks are also parameterized models that are learned with continuous optimization methods.},
  language = {English},
  booktitle = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
  publisher = {{Springer International Publishing}},
  author = {Aggarwal, Charu C.},
  editor = {Aggarwal, Charu C.},
  year = {2018},
  pages = {53-104},
  doi = {10.1007/978-3-319-94463-0_2}
}

@incollection{shoombuatongRevivalInterpretableQSAR2017,
  address = {Cham},
  series = {Challenges and {{Advances}} in {{Computational Chemistry}} and {{Physics}}},
  title = {Towards the {{Revival}} of {{Interpretable QSAR Models}}},
  isbn = {978-3-319-56850-8},
  abstract = {Quantitative structure-activity relationship (QSAR) has been instrumental in aiding medicinal chemists and physical scientists in understanding how modification of substituents at different positions on a molecular structure exert its influence on the observed biological activity and physicochemical property, respectively. QSAR has received great attention owing to its predictive capability and as such efforts had been directed toward obtaining models with high prediction performance. However, to be useful QSAR models need to be informative and interpretable in which the underlying molecular features that contribute to the increase or decrease of the biological activity are revealed by the model. Thus, the aim of this chapter is to briefly review the general concepts of QSAR modeling, its development and discussions on key issues influencing and contributing to the interpretability of QSAR models.},
  language = {English},
  booktitle = {Advances in {{QSAR Modeling}}: {{Applications}} in {{Pharmaceutical}}, {{Chemical}}, {{Food}}, {{Agricultural}} and {{Environmental Sciences}}},
  publisher = {{Springer International Publishing}},
  author = {Shoombuatong, Watshara and Prathipati, Philip and Owasirikul, Wiwat and Worachartcheewan, Apilak and Simeon, Saw and Anuwongcharoen, Nuttapat and Wikberg, Jarl E. S. and Nantasenamat, Chanin},
  editor = {Roy, Kunal},
  year = {2017},
  keywords = {Data mining,Machine learning,QSAR,Cheminformatics,Chemogenomics,Drug design,Drug discovery,Interpretable,Proteochemometrics,QSPR,Quantitative structure-activity relationship,Quantitative structure-property relationship},
  pages = {3-55},
  doi = {10.1007/978-3-319-56850-8_1}
}

@article{NUTSBOLTSBEHAVIORAL2017,
  title = {{{THE}} ``{{NUTS AND BOLTS}}'' {{OF BEHAVIORAL INTERVENTION DEVELOPMENT}}: {{STUDY DESIGNS}}, {{METHODS AND FUNDING OPPORTUNITIES}}},
  volume = {51},
  issn = {1532-4796},
  shorttitle = {{{THE}} ``{{NUTS AND BOLTS}}'' {{OF BEHAVIORAL INTERVENTION DEVELOPMENT}}},
  language = {English},
  number = {1},
  journal = {Annals of Behavioral Medicine},
  doi = {10.1007/s12160-017-9903-3},
  month = mar,
  year = {2017},
  pages = {1-2867}
}

@article{Abstracts2017Society2017,
  title = {Abstracts from the 2017 {{Society}} of {{General Internal Medicine Annual Meeting}}},
  volume = {32},
  issn = {1525-1497},
  language = {English},
  number = {2},
  journal = {Journal of General Internal Medicine},
  doi = {10.1007/s11606-017-4028-8},
  month = apr,
  year = {2017},
  pages = {83-808},
  file = {/home/tim/Zotero/storage/2D5PK28I/2017 - Abstracts from the 2017 Society of General Interna.pdf;/home/tim/Zotero/storage/2X39J8SF/2017 - Abstracts from the 2017 Society of General Interna.pdf;/home/tim/Zotero/storage/7QLA2JRM/2017 - Abstracts from the 2017 Society of General Interna.pdf;/home/tim/Zotero/storage/V3RQA9TM/2017 - Abstracts from the 2017 Society of General Interna.pdf}
}

@inproceedings{goebelExplainableAINew2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{AI}}: {{The New}} 42?},
  isbn = {978-3-319-99740-7},
  shorttitle = {Explainable {{AI}}},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
  language = {English},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Machine learning,Artificial intelligence,Explainable AI,Explainability},
  pages = {295-303}
}

@inproceedings{pomarlanUnderstandingNLPNeural2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Understanding {{NLP Neural Networks}} by the {{Texts They Generate}}},
  isbn = {978-3-030-00111-7},
  abstract = {Recurrent neural networks have proven useful in natural language processing. For example, they can be trained to predict, and even generate plausible text with few or no spelling and syntax errors. However, it is not clear what grammar a network has learned, or how it keeps track of the syntactic structure of its input. In this paper, we present a new method to extract a finite state machine from a recurrent neural network. A FSM is in principle a more interpretable representation of a grammar than a neural net would be, however the extracted FSMs for realistic neural networks will also be large. Therefore, we also look at ways to group the states and paths through the extracted FSM so as to get a smaller, easier to understand model of the neural network. To illustrate our methods, we use them to investigate how a neural network learns noun-verb agreement from a simple grammar where relative clauses may appear between noun and verb.},
  language = {English},
  booktitle = {{{KI}} 2018: {{Advances}} in {{Artificial Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Pomarlan, Mihai and Bateman, John},
  editor = {Trollmann, Frank and Turhan, Anni-Yasmin},
  year = {2018},
  keywords = {Recurrent neural networks,Interpretability,Natural language processing},
  pages = {284-296}
}

@incollection{kashyapPracticalConceptsMachine2017,
  address = {Berkeley, CA},
  title = {The {{Practical Concepts}} of {{Machine Learning}}},
  isbn = {978-1-4842-2988-0},
  abstract = {This is an important chapter because it discusses the basic and practical concepts of machine learning (ML). I did not take the academic book style to explain these concepts. I have directed my thoughts and energy to provide you with the concepts that are useful during practical decision making. Hence, while explaining the concepts, terminologies, and technical details, I use examples and case studies that are be helpful in extracting relevant insight from the chapter.},
  language = {English},
  booktitle = {Machine {{Learning}} for {{Decision Makers}}: {{Cognitive Computing Fundamentals}} for {{Better Decision Making}}},
  publisher = {{Apress}},
  author = {Kashyap, Patanjali},
  editor = {Kashyap, Patanjali},
  year = {2017},
  pages = {35-90},
  doi = {10.1007/978-1-4842-2988-0_2}
}

@inproceedings{otteSafeInterpretableMachine2013,
  series = {Studies in {{Computational Intelligence}}},
  title = {Safe and {{Interpretable Machine Learning}}: {{A~Methodological Review}}},
  isbn = {978-3-642-32378-2},
  shorttitle = {Safe and {{Interpretable Machine Learning}}},
  abstract = {When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.},
  language = {English},
  booktitle = {Computational {{Intelligence}} in {{Intelligent Data Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Otte, Clemens},
  editor = {Moewes, Christian and N\"urnberger, Andreas},
  year = {2013},
  keywords = {Input Space,Methodological Review,Multivariate Adaptive Regression Spline,Symbolic Model,Symbolic Regression},
  pages = {111-122}
}

@inproceedings{pomarlanMeaningfulClusteringsRecurrent2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Meaningful {{Clusterings}} of {{Recurrent Neural Network Activations}} for {{NLP}}},
  isbn = {978-3-030-05918-7},
  abstract = {Recurrent neural networks have found applications in NLP, but their operation is difficult to interpret. A state automaton that approximates the network would be more interpretable, but for this one needs a method to group network activation states by their behavior. In this paper we propose such a method, and compare it to an existing dimensionality reduction and clustering approach. Our method is better able to group together neural states of similar behavior.},
  language = {English},
  booktitle = {Mining {{Intelligence}} and {{Knowledge Exploration}}},
  publisher = {{Springer International Publishing}},
  author = {Pomarlan, Mihai and Bateman, John},
  editor = {Groza, Adrian and Prasath, Rajendra},
  year = {2018},
  keywords = {Recurrent neural networks,Interpretability,Natural language processing},
  pages = {11-20}
}

@inproceedings{weberInvestigatingTextualCaseBased2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Investigating {{Textual Case}}-{{Based XAI}}},
  isbn = {978-3-030-01081-2},
  abstract = {This paper demonstrates how case-based reasoning (CBR) can be used for an explainable artificial intelligence (XAI) approach to justify solutions produced by an opaque learning method (i.e., target method), particularly in the context of unstructured textual data. Our general hypothesis is twofold: (1) There exists patterns in the relationship between problems and solutions and there should be data or a body of knowledge that describes how problems and solutions relate; and (2) the identification, manipulation, and learning of such patterns through case features can help create and reuse explanations for solutions produced by the target method. When the target method relies on neural network architectures (e.g., deep learning), the resulting latent space (i.e., word embeddings) becomes useful for finding patterns and semantic relatedness in textual data. In the proposed approach, case problems are input-output pairs from the target method, and case solutions are explanations. We exemplify our approach by explaining recommended citations from Citeomatic - a multi-layer neural-network architecture from the Allen Institute for Artificial Intelligence. Citation analysis is the body of knowledge that describes how query documents (i.e., inputs) relate to recommended citations (i.e., outputs). We build cases and similarity assessment to learn features that represent patterns between problems and solutions that can lead to the reuse of corresponding explanations. The illustrative implementation we present becomes an explanation-augmented citation recommender that targets human-computer trust.},
  language = {English},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  publisher = {{Springer International Publishing}},
  author = {Weber, Rosina O. and Johs, Adam J. and Li, Jianfei and Huang, Kent},
  editor = {Cox, Michael T. and Funk, Peter and Begum, Shahina},
  year = {2018},
  keywords = {Explainable artificial intelligence,Case-Based reasoning,Citation recommendation,Human-Computer trust,Semantic relatedness,Textual Case-Based reasoning,Word embeddings},
  pages = {431-447}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
  isbn = {978-3-319-98131-4},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
  language = {English},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Accountability,Machine learning,Transparency,Interpretability},
  pages = {3-17},
  doi = {10.1007/978-3-319-98131-4_1}
}

@article{kuwajimaImprovingTransparencyDeep2019,
  title = {Improving {{Transparency}} of {{Deep Neural Inference Process}}},
  issn = {2192-6360},
  abstract = {Deep learning techniques are rapidly advanced recently and becoming a necessity component for widespread systems. However, the inference process of deep learning is black box and is not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of (1) structural feature analysis: lists of the features contributing to inference process, (2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and (3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75\% workers answer that input data and result of our feature analysis are consistent, and 70\% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provides suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.},
  language = {English},
  journal = {Progress in Artificial Intelligence},
  doi = {10.1007/s13748-019-00179-x},
  author = {Kuwajima, Hiroshi and Tanaka, Masayuki and Okutomi, Masatoshi},
  month = apr,
  year = {2019},
  keywords = {Transparency,Visualization,Explainable AI,Deep neural network,Black box,Visual attribute}
}

@article{27thAnnualComputational2018,
  title = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018): {{Part One}}},
  volume = {19},
  issn = {1471-2202},
  shorttitle = {27th {{Annual Computational Neuroscience Meeting}} ({{CNS}}*2018)},
  language = {English},
  number = {2},
  journal = {BMC Neuroscience},
  doi = {10.1186/s12868-018-0452-x},
  month = oct,
  year = {2018},
  pages = {64},
  file = {/home/tim/Zotero/storage/6XTX58YR/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf;/home/tim/Zotero/storage/GTY893D9/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf;/home/tim/Zotero/storage/H66GY9TM/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf;/home/tim/Zotero/storage/RGERCVWS/2018 - 27th Annual Computational Neuroscience Meeting (CN.pdf}
}

@article{bench-caponHistoryAILaw2012,
  title = {A {{History}} of {{AI}} and {{Law}} in 50 {{Papers}}: 25~{{Years}} of the {{International Conference}} on {{AI}} and {{Law}}},
  volume = {20},
  issn = {1572-8382},
  shorttitle = {A {{History}} of {{AI}} and {{Law}} in 50 {{Papers}}},
  abstract = {We provide a retrospective of 25 years of the International Conference on AI and Law, which was first held in 1987. Fifty papers have been selected from the thirteen conferences and each of them is described in a short subsection individually written by one of the 24 authors. These subsections attempt to place the paper discussed in the context of the development of AI and Law, while often offering some personal reactions and reflections. As a whole, the subsections build into a history of the last quarter century of the field, and provide some insights into where it has come from, where it is now, and where it might go.},
  language = {English},
  number = {3},
  journal = {Artificial Intelligence and Law},
  doi = {10.1007/s10506-012-9131-x},
  author = {{Bench-Capon}, Trevor and Araszkiewicz, Micha\l{} and Ashley, Kevin and Atkinson, Katie and Bex, Floris and Borges, Filipe and Bourcier, Daniele and Bourgine, Paul and Conrad, Jack G. and Francesconi, Enrico and Gordon, Thomas F. and Governatori, Guido and Leidner, Jochen L. and Lewis, David D. and Loui, Ronald P. and McCarty, L. Thorne and Prakken, Henry and Schilder, Frank and Schweighofer, Erich and Thompson, Paul and Tyrrell, Alex and Verheij, Bart and Walton, Douglas N. and Wyner, Adam Z.},
  month = sep,
  year = {2012},
  keywords = {Artificial intelligence and law,Legal informatics,Models of legal reasoning},
  pages = {215-319},
  file = {/home/tim/Zotero/storage/2EZDL3ZL/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf;/home/tim/Zotero/storage/8UXTGM2T/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf;/home/tim/Zotero/storage/DPQ2A5JH/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf;/home/tim/Zotero/storage/MXT2RSC8/Bench-Capon et al. - 2012 - A history of AI and Law in 50 papers 25 years of .pdf}
}

@incollection{sarkarAnalyzingMovieReviews2018,
  address = {Berkeley, CA},
  title = {Analyzing {{Movie Reviews Sentiment}}},
  isbn = {978-1-4842-3207-1},
  abstract = {In this chapter, we continue with our focus on case-study oriented chapters, where we will focus on specific real-world problems and scenarios and how we can use Machine Learning to solve them. We will cover aspects pertaining to natural language processing (NLP), text analytics, and Machine Learning in this chapter. The problem at hand is sentiment analysis or opinion mining, where we want to analyze some textual documents and predict their sentiment or opinion based on the content of these documents. Sentiment analysis is perhaps one of the most popular applications of natural language processing and text analytics with a vast number of websites, books and tutorials on this subject. Typically sentiment analysis seems to work best on subjective text, where people express opinions, feelings, and their mood. From a real-world industry standpoint, sentiment analysis is widely used to analyze corporate surveys, feedback surveys, social media data, and reviews for movies, places, commodities, and many more. The idea is to analyze and understand the reactions of people toward a specific entity and take insightful actions based on their sentiment.},
  language = {English},
  booktitle = {Practical {{Machine Learning}} with {{Python}}: {{A Problem}}-{{Solver}}'s {{Guide}} to {{Building Real}}-{{World Intelligent Systems}}},
  publisher = {{Apress}},
  author = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
  editor = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
  year = {2018},
  pages = {331-372},
  doi = {10.1007/978-1-4842-3207-1_7}
}

@inproceedings{huExplainableNeuralComputation2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
  isbn = {978-3-030-01234-2},
  abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
  language = {English},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  publisher = {{Springer International Publishing}},
  author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  keywords = {Interpretable reasoning,Neural module networks,Visual question answering},
  pages = {55-71}
}

@incollection{ngomoIntroductionLinkedData2014,
  address = {Cham},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Introduction to {{Linked Data}} and {{Its Lifecycle}} on the {{Web}}},
  isbn = {978-3-319-10587-1},
  abstract = {With Linked Data, a very pragmatic approach towards achieving the vision of the Semantic Web has gained some traction in the last years. The term Linked Data refers to a set of best practices for publishing and interlinking structured data on the Web. While many standards, methods and technologies developed within by the Semantic Web community are applicable for Linked Data, there are also a number of specific characteristics of Linked Data, which have to be considered. In this article we introduce the main concepts of Linked Data. We present an overview of the Linked Data life-cycle and discuss individual approaches as well as the state-of-the-art with regard to extraction, authoring, linking, enrichment as well as quality of Linked Data. We conclude the chapter with a discussion of issues, limitations and further research and development challenges of Linked Data. This article is an updated version of a similar lecture given at Reasoning Web Summer School 2013.},
  language = {English},
  booktitle = {Reasoning {{Web}}. {{Reasoning}} on the {{Web}} in the {{Big Data Era}}: 10th {{International Summer School}} 2014, {{Athens}}, {{Greece}}, {{September}} 8-13, 2014. {{Proceedings}}},
  publisher = {{Springer International Publishing}},
  author = {Ngomo, Axel-Cyrille Ngonga and Auer, S\"oren and Lehmann, Jens and Zaveri, Amrapali},
  editor = {Koubarakis, Manolis and Stamou, Giorgos and Stoilos, Giorgos and Horrocks, Ian and Kolaitis, Phokion and Lausen, Georg and Weikum, Gerhard},
  year = {2014},
  keywords = {Inductive Logic Programming,Link Data,Link Open Data,Resource Description Framework,SPARQL Query},
  pages = {1-99},
  file = {/home/tim/Zotero/storage/4CGZCE9V/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf;/home/tim/Zotero/storage/PSZELU36/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf;/home/tim/Zotero/storage/QBWWKMYZ/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf;/home/tim/Zotero/storage/ZDSGP8Z5/Ngomo et al. - 2014 - Introduction to Linked Data and Its Lifecycle on t.pdf},
  doi = {10.1007/978-3-319-10587-1_1}
}

@incollection{browneCriticalChallengesVisual2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
  isbn = {978-3-319-90403-0},
  abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
  language = {English},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {119-136},
  doi = {10.1007/978-3-319-90403-0_7}
}

@incollection{kochGroupCognitionCollaborative2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Group {{Cognition}} and {{Collaborative AI}}},
  isbn = {978-3-319-90403-0},
  abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
  language = {English},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Koch, Janin and Oulasvirta, Antti},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {293-312},
  doi = {10.1007/978-3-319-90403-0_15}
}

@incollection{galitskyDevelopingConversationalNatural2019,
  address = {Cham},
  title = {Developing {{Conversational Natural Language Interface}} to a {{Database}}},
  isbn = {978-3-030-04299-8},
  abstract = {In this Chapter we focus on a problem of a natural language access to a database, well-known and highly desired to be solved. We start with the modern approaches based on deep learning and analyze lessons learned from unusable database access systems. This chapter can serve as a brief introduction to neural networks for learning logic representations. Then a number of hybrid approaches are presented and their strong points are analyzed. Finally, we describe our approach that relies on parsing, thesaurus and disambiguation via chatbot communication mode. The conclusion is that a reliable and flexible database access via NL needs to employ a broad spectrum of linguistic, knowledge representation and learning techniques. We conclude this chapter by surveying the general technology trends related to NL2SQL, observing how AI and ML are seeping into virtually everything and represent a major battleground for technology providers.},
  language = {English},
  booktitle = {Developing {{Enterprise Chatbots}}: {{Learning Linguistic Structures}}},
  publisher = {{Springer International Publishing}},
  author = {Galitsky, Boris},
  editor = {Galitsky, Boris},
  year = {2019},
  pages = {85-120},
  doi = {10.1007/978-3-030-04299-8_4}
}

@article{kotsiantisMachineLearningReview2006,
  title = {Machine {{Learning}}: {{A Review}} of {{Classification}} and {{Combining Techniques}}},
  volume = {26},
  issn = {1573-7462},
  shorttitle = {Machine {{Learning}}},
  abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy\textemdash{}ensembles of classifiers.},
  language = {English},
  number = {3},
  journal = {Artificial Intelligence Review},
  doi = {10.1007/s10462-007-9052-3},
  author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.},
  month = nov,
  year = {2006},
  keywords = {Classifiers,Data mining techniques,Intelligent data analysis,Learning algorithms},
  pages = {159-190}
}

@article{Abstracts2016Society2016,
  title = {Abstracts from the 2016 {{Society}} of {{General Internal Medicine Annual Meeting}}},
  volume = {31},
  issn = {1525-1497},
  language = {English},
  number = {2},
  journal = {Journal of General Internal Medicine},
  doi = {10.1007/s11606-016-3657-7},
  month = may,
  year = {2016},
  pages = {85-922},
  file = {/home/tim/Zotero/storage/4RZEFRHJ/2016 - Abstracts from the 2016 Society of General Interna.pdf;/home/tim/Zotero/storage/9FSQBZUF/2016 - Abstracts from the 2016 Society of General Interna.pdf;/home/tim/Zotero/storage/CRN5ZLR6/2016 - Abstracts from the 2016 Society of General Interna.pdf;/home/tim/Zotero/storage/SXRNDBJW/2016 - Abstracts from the 2016 Society of General Interna.pdf}
}

@article{ScientificProgrammeAbstracts2003,
  title = {Scientific {{Programme}} \textemdash{} {{Abstracts}}},
  volume = {13},
  issn = {1432-1084},
  language = {English},
  number = {1},
  journal = {European Radiology},
  doi = {10.1007/BF03323651},
  month = feb,
  year = {2003},
  pages = {93-589}
}

@article{itoGINNGradientInterpretable2018,
  title = {{{GINN}}: {{Gradient Interpretable Neural Networks}} for {{Visualizing Financial Texts}}},
  issn = {2364-4168},
  shorttitle = {{{GINN}}},
  abstract = {This study aims to visualize financial documents in such a way that even nonexperts can understand the sentiments contained therein. To achieve this, we propose a novel text visualization method using an interpretable neural network (NN) architecture, called a gradient interpretable NN (GINN). A GINN can visualize a market sentiment score from an entire financial document and the sentiment gradient scores in both word and concept units. Moreover, the GINN can visualize important concepts given in various sentence contexts. Such visualization helps nonexperts easily understand financial documents. We theoretically analyze the validity of the GINN and experimentally demonstrate the validity of text visualization produced by the GINN using real financial texts.},
  language = {English},
  journal = {International Journal of Data Science and Analytics},
  doi = {10.1007/s41060-018-0160-8},
  author = {Ito, Tomoki and Sakaji, Hiroki and Izumi, Kiyoshi and Tsubouchi, Kota and Yamashita, Tatsuo},
  month = dec,
  year = {2018},
  keywords = {Interpretable neural network,Support system,Text mining}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
  address = {Cham},
  title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
  isbn = {978-3-319-91596-8},
  abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
  language = {English},
  booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
  publisher = {{Springer International Publishing}},
  author = {Wodecki, Andrzej},
  editor = {Wodecki, Andrzej},
  year = {2019},
  pages = {133-246},
  doi = {10.1007/978-3-319-91596-8_3}
}

@article{zhangVisualInterpretabilityDeep2018,
  title = {Visual {{Interpretability}} for {{Deep Learning}}: {{A Survey}}},
  volume = {19},
  issn = {2095-9230},
  shorttitle = {Visual {{Interpretability}} for {{Deep Learning}}},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human\textendash{}computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  language = {English},
  number = {1},
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  doi = {10.1631/FITEE.1700808},
  author = {Zhang, Quan-shi and Zhu, Song-chun},
  month = jan,
  year = {2018},
  keywords = {Deep learning,Artificial intelligence,Interpretable model,TP391},
  pages = {27-39},
  file = {/home/tim/Zotero/storage/583VPWTS/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf;/home/tim/Zotero/storage/IT37HP8S/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf}
}

@incollection{dengEpilogueFrontiersNLP2018,
  address = {Singapore},
  title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
  isbn = {978-981-10-5209-5},
  shorttitle = {Epilogue},
  abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
  language = {English},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {309-326},
  doi = {10.1007/978-981-10-5209-5_11}
}

@inproceedings{maragoudakisMiningNaturalLanguage2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Mining {{Natural Language Programming Directives}} with {{Class}}-{{Oriented Bayesian Networks}}},
  isbn = {978-3-540-88192-6},
  abstract = {Learning a programming language is a painstaking process, as it requires knowledge of its syntax, apart from knowing the basic process of representing logical sequences to programming stages. This fact deteriorates the coding process and expels most users from programming. Particularly for novice users or persons with vision problems, learning of how to program and tracing the syntax errors could be improved dramatically by using the most natural of all interfaces, i.e. natural language. Towards this orientation, we suggest a wider framework for allowing programming using natural language. The framework can be easily extended to support different object-oriented programming languages such as C, C++, Visual Basic or Java. Our suggested model is named ``Language Oriented Basic'' and it concerns an intelligent interface that supports code creation, modification and control in Visual Basic. Users can use simple-structured Greek sentences in natural language and the system can output the corresponding syntactic tree. When users declare end of input, the system transforms the syntactic trees to source code. Throughout the whole interaction process, users can check the under-development code in order to verify its correspondence to their expectations. Due to the fact that using natural language can cause a great degree of ambiguity, Bayesian networks and learning from examples have been utilized as an attempt to reason on the most probable programming representation, given a natural language input sentence. In order to enhance the classifier, we propose a novel variation of Bayesian networks that favor the classification process. Experimental results have depicted precision and recall measures in a range of 73\% and 70\% respectively.},
  language = {English},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Maragoudakis, Manolis and Cosmas, Nikolaos and Garbis, Aristogiannis},
  editor = {Tang, Changjie and Ling, Charles X. and Zhou, Xiaofang and Cercone, Nick J. and Li, Xue},
  year = {2008},
  keywords = {Bayesian Classifier,Bayesian Network,Conditional Independence Assumption,Conditional Probability Table,Natural Language},
  pages = {15-26}
}

@article{AnnualCongressEuropean2018,
  title = {Annual {{Congress}} of the {{European Association}} of {{Nuclear Medicine October}} 13 \textendash{} 17, 2018 {{D\"usseldorf}}, {{Germany}}},
  volume = {45},
  issn = {1619-7089},
  language = {English},
  number = {1},
  journal = {European Journal of Nuclear Medicine and Molecular Imaging},
  doi = {10.1007/s00259-018-4148-3},
  month = oct,
  year = {2018},
  pages = {1-844}
}

@article{SPR20192019,
  title = {{{SPR}} 2019},
  volume = {49},
  issn = {1432-1998},
  language = {English},
  number = {1},
  journal = {Pediatric Radiology},
  doi = {10.1007/s00247-019-04376-7},
  month = apr,
  year = {2019},
  pages = {1-245},
  file = {/home/tim/Zotero/storage/22F643RD/2019 - SPR 2019.pdf;/home/tim/Zotero/storage/XPGD3MNE/2019 - SPR 2019.pdf}
}

@incollection{sarkarMachineLearningBasics2018,
  address = {Berkeley, CA},
  title = {Machine {{Learning Basics}}},
  isbn = {978-1-4842-3207-1},
  abstract = {The idea of making intelligent, sentient, and self-aware machines is not something that suddenly came into existence in the last few years. In fact a lot of lore from Greek mythology talks about intelligent machines and inventions having self-awareness and intelligence of their own. The origins and the evolution of the computer have been really revolutionary over a period of several centuries, starting from the basic Abacus and its descendant the slide rule in the 17th Century to the first general purpose computer designed by Charles Babbage in the 1800s. In fact, once computers started evolving with the invention of the Analytical Engine by Babbage and the first computer program, which was written by Ada Lovelace in 1842, people started wondering and contemplating that could there be a time when computers or machines truly become intelligent and start thinking for themselves. In fact, the renowned computer scientist, Alan Turing, was highly influential in the development of theoretical computer science, algorithms, and formal language and addressed concepts like artificial intelligence and Machine Learning as early as the 1950s. This brief insight into the evolution of making machines learn is just to give you an idea of something that has been out there since centuries but has recently started gaining a lot of attention and focus.},
  language = {English},
  booktitle = {Practical {{Machine Learning}} with {{Python}}: {{A Problem}}-{{Solver}}'s {{Guide}} to {{Building Real}}-{{World Intelligent Systems}}},
  publisher = {{Apress}},
  author = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
  editor = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
  year = {2018},
  pages = {3-65},
  doi = {10.1007/978-1-4842-3207-1_1}
}

@inproceedings{1224095,
  title = {A New Method for Explaining Neural Network Reasoning},
  volume = {4},
  abstract = {This paper presents a new method for explaining the reasoning results of a trained neural network. The method considers the most significant attribute first under the guidance of a relative strength of effect analysis and eliminates irrelevant points. Following the adaptive search in the dynamic state space, a set of relevant points are extracted and form the basis of the explanation of the neural network reasoning. Combining a relative strength of effect analysis with the relevant points, a case based explanation approach is put forward. As an illustration, an experiment with a small data set on the relationship between weather conditions and play decisions is presented to demonstrate the utility of the proposed approach.},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}}, 2003.},
  doi = {10.1109/IJCNN.2003.1224095},
  author = {Hinde, C. and Gillingwater, D.},
  month = jul,
  year = {2003},
  keywords = {Data mining,learning (artificial intelligence),Artificial neural networks,inference mechanisms,Computer science,neural nets,Neural networks,weather conditions,Computational intelligence,explanation,Information analysis,adaptive search,case based explanation approach,dynamic state space,Intelligent structures,Knowledge representation,neural network reasoning,play decisions,relative strength of effect analysis,relevant point extraction,State-space methods,trained neural network,Training data},
  pages = {3256-3260 vol.4},
  issn = {1098-7576}
}

@inproceedings{1259707,
  title = {Information Geometry on Extendable Hierarchical Large Scale Neural Network Model},
  volume = {3},
  abstract = {In this paper, an extendable hierarchical large scale neural network model is developed based on the theoretical analysis of information geometry. In a hierarchical set of systems, a lower order system is included in the parameter space of a larger one as a subset. Such a parameter space has rich geometrical structures that are responsible for the dynamic behaviors of learning. Extendable hierarchical large scale neural network divides a task into small tasks, and each task is fulfilled by a small network under the principle of divide and conquer to improve the performance of a single network. By studying the dual manifold architecture for a family of neural networks and analyzing the hierarchical expansion of this model based on information geometry, the paper proposes a new method to construct the extendable hierarchical large scale neural network model that has knowledge-increasable and structure-extendible ability. The method helps to provide explanation of the transformation mechanism of human recognition system and understand the theory of global architecture of neural network.},
  booktitle = {Proceedings of the 2003 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}} ({{IEEE Cat}}. {{No}}.{{03EX693}})},
  doi = {10.1109/ICMLC.2003.1259707},
  month = nov,
  year = {2003},
  keywords = {learning (artificial intelligence),Electronic mail,Computer science,neural nets,Neural networks,Information analysis,Humans,Solid modeling,Large-scale systems,cognition,dual flat manifold architecture,extendable hierarchical large scale neural network model,geometry,hierarchical systems,human recognition system,information geometry,Information geometry,Information theory,large-scale systems,learning behaviors,lower order system,parameter space,Probability distribution,statistical distributions},
  pages = {1380-1384 Vol.3},
  note = {ISSN:}
}

@article{1262324,
  title = {Machine Printed Text and Handwriting Identification in Noisy Document Images},
  volume = {26},
  issn = {0162-8828},
  abstract = {In this paper, we address the problem of the identification of text in noisy document images. We are especially focused on segmenting and identifying between handwriting and machine printed text because: 1) Handwriting in a document often indicates corrections, additions, or other supplemental information that should be treated differently from the main content and 2) the segmentation and recognition techniques requested for machine printed and handwritten text are significantly different. A novel aspect of our approach is that we treat noise as a separate class and model noise based on selected features. Trained Fisher classifiers are used to identify machine printed text and handwriting from noise and we further exploit context to refine the classification. A Markov Random Field-based (MRF) approach is used to model the geometrical structure of the printed text, handwriting, and noise to rectify misclassifications. Experimental results show that our approach is robust and can significantly improve page segmentation in noisy document collections.},
  number = {3},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2004.1262324},
  author = {Doermann, D.},
  month = mar,
  year = {2004},
  keywords = {Models,Artificial Intelligence,feature extraction,Markov processes,Image analysis,text analysis,document image processing,Text analysis,Text recognition,Automated,Automatic Data Processing,Computer-Assisted,Documentation,Image Enhancement,Image Interpretation,Image segmentation,Information Storage and Retrieval,Pattern Recognition,Reading,Writing,Algorithms,Computer Graphics,handwriting recognition,Handwriting recognition,image segmentation,Numerical Analysis,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,User-Computer Interface,Noise robustness,Markov random field,Markov random fields,Statistical,Context modeling,fisher classifiers,handwriting identification,image enhancement,Image enhancement,machine printed text,noisy document images,page segmentation,recognition techniques,Solid modeling,Stochastic Processes,Subtraction Technique,text identification},
  pages = {337-353}
}

@article{1359749,
  title = {Artificial Neural Networks for Document Analysis and Recognition},
  volume = {27},
  issn = {0162-8828},
  abstract = {Artificial neural networks have been extensively applied to document analysis and recognition. Most efforts have been devoted to the recognition of isolated handwritten and printed characters with widely recognized successful results. However, many other document processing tasks, like preprocessing, layout analysis, character segmentation, word recognition, and signature verification, have been effectively faced with very promising results. This paper surveys the most significant problems in the area of offline document image processing, where connectionist-based approaches have been applied. Similarities and differences between approaches belonging to different categories are discussed. A particular emphasis is given on the crucial role of prior knowledge for the conception of both appropriate architectures and learning algorithms. Finally, the paper provides a critical analysts on the reviewed approaches and depicts the most promising research guidelines in the field. In particular, a second generation of connectionist-based models are foreseen which are based on appropriate graphical representations of the learning environment.},
  number = {1},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2005.4},
  author = {Marinai, S. and Gori, M. and Soda, G.},
  month = jan,
  year = {2005},
  keywords = {learning (artificial intelligence),Artificial neural networks,Artificial Intelligence,Image analysis,neural networks,Neural networks,artificial neural networks,document image processing,Text analysis,Character recognition,Image recognition,Optical character recognition software,recurrent neural nets,Automated,Automatic Data Processing,Computer-Assisted,Documentation,Image Enhancement,Image Interpretation,Image segmentation,Information Storage and Retrieval,Pattern Recognition,Reading,Algorithms,character recognition,character segmentation,Computer Graphics,connectionist based approach,document image analysis,document image analysis and recognition,document image recognition,document preprocessing,Face recognition,graphical representations,Handwriting,handwriting recognition,Handwriting recognition,handwritten character recognition,handwritten recognition,image segmentation,Index Terms- Character segmentation,layout analysis,learning algorithms,Neural Networks (Computer),Numerical Analysis,offline document image processing,preprocessing,recursive neural networks,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,signature verification,User-Computer Interface,word recognition,word recognition.},
  pages = {23-35}
}

@inproceedings{1562962,
  title = {Customized Explanation in Expert System for Earthquake Prediction},
  abstract = {A main line of research for introducing explanation capabilities in knowledge-based system supposes that explanations are used to transmit knowledge from the machine to a user and to improve learning ability of the latter. A reasonable explanation should be adapted to different level of users because different users have different knowledge. To describe the difference and then generate a personalized explanation is the main problem. In this paper, a novel method called FUM-CE (fuzzy user model based customized explanation) is proposed, in which a fuzzy user model called FUM is defined, and then several new algorithms are proposed to initialize FUM, update FUM, and extract correlative knowledge for explanation based on the FUM. FUM-CE can provide different and suitable explanation for the different users with different domain knowledge, by which the understandability and acceptability of the expert system for earthquake prediction are improved},
  booktitle = {17th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}}'05)},
  doi = {10.1109/ICTAI.2005.54},
  month = nov,
  year = {2005},
  keywords = {Machine learning,Knowledge engineering,Artificial intelligence,fuzzy set theory,explanation,Humans,Knowledge based systems,expert systems,Expert systems,Fuzzy systems,Earthquake engineering,earthquake prediction,earthquakes,expert system,fuzzy user model based customized explanation,geophysics computing,Hybrid intelligent systems,knowledge-based system,Problem-solving},
  pages = {5 pp.-371},
  issn = {1082-3409}
}

@inproceedings{1575741,
  title = {A Model for Detecting and Merging Vertically Spanned Table Cells in Plain Text Documents},
  abstract = {A spanned cell in a table is a single, complete unit that physically occupies multiple columns and/or multiple rows. Spanned cells are common in tables, and they are a significant cause of error in the extraction of tables from free text documents. In this paper, we present a model for the detection and merging of vertically spanned cells for tables presented in plain text documents. Our model and algorithm are based purely on the layout features of the tables, and they require no semantic understanding of the documents. When tested on the 98 tables appearing in 40 randomly selected documents from a corpus of company announcements from the Australian Stock Exchange (ASX), our algorithm achieves an accuracy of 86.79\% in detecting and merging vertically spanned cells.},
  booktitle = {Eighth {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}}'05)},
  doi = {10.1109/ICDAR.2005.21},
  author = {Long, V. and Dale, R. and Cassidy, S.},
  month = aug,
  year = {2005},
  keywords = {Data mining,Testing,Robustness,Australia,text analysis,Text analysis,document semantic understanding,free text documents,IEEE news,Merging,plain text documents,Stock markets,Terminology,vertically spanned table cell detection,vertically spanned table cell merging},
  pages = {1242-1246 Vol. 2},
  issn = {1520-5363}
}

@article{18thISoPAnnual2018,
  title = {18th {{ISoP Annual Meeting}} ``{{Pharmacovigilance}} without {{Borders}}'' {{Geneva}}, {{Switzerland}}, 11\textendash{}14 {{November}}, 2018},
  volume = {41},
  issn = {1179-1942},
  language = {English},
  number = {11},
  journal = {Drug Safety},
  doi = {10.1007/s40264-018-0719-2},
  month = nov,
  year = {2018},
  pages = {1103-1273}
}

@inproceedings{236591,
  title = {Building a Banking System Specification Using Machine Learning},
  abstract = {Transforming user requirements into software specification is a complex and demanding task. Artificial intelligence methods such as machine learning (ML) can assist in the software specification process by providing support to system designers. This paper presents an approach based on explanation-based learning (EBL), a ML technique in which a concept is learned by building an explanation. The approach is presented in the context of the system LISE (Learning in Software Engineering). LISE converts a user requirement for a software module into an operational module definition using EBL with an incomplete theory. An example where LISE is used to build the specification of a banking system is illustrated.{$<$}{$>$}},
  booktitle = {Proceedings {{First International Conference}} on {{Artificial Intelligence Applications}} on {{Wall Street}}},
  doi = {10.1109/AIAWS.1991.236591},
  author = {Genest, J.},
  month = oct,
  year = {1991},
  keywords = {Programming,Software engineering,Machine learning,Buildings,learning (artificial intelligence),Artificial intelligence,machine learning,Mathematics,Banking,explanation,Software libraries,explanation-based learning,bank data processing,banking system specification,case-based reasoning,formal specification,Learning in Software Engineering,LISE,Multilevel systems,Software design,user requirements},
  pages = {263-268},
  note = {ISSN:}
}

@article{25thAnnualConference2018,
  title = {25th {{Annual Conference}} of the {{International Society}} for {{Quality}} of {{Life Research}}},
  volume = {27},
  issn = {1573-2649},
  language = {English},
  number = {1},
  journal = {Quality of Life Research},
  doi = {10.1007/s11136-018-1946-9},
  month = oct,
  year = {2018},
  pages = {1-190}
}

@inproceedings{346487,
  title = {Text-Based Systems and Information Management: Artificial Intelligence Confronts Matters of Scale},
  abstract = {Many of the more ambitious goals of artificial intelligence have proved unattainable because of the failure of the many small, successful systems to scale up. The general use of technologies such as natural language interfaces and expert systems has done little to alleviate the basic difficulties and overwhelming cost of knowledge engineering. At the same time, emerging text processing techniques, including data extraction from text and new text retrieval methods, offer a means of accessing stores of information many times larger than any organized knowledge base or database. Although knowledge acquisition from text is at the heart of the information management problem, interpreting text, paradoxically, requires large amounts of knowledge, mainly about the way words are used in context. In other words, before intelligent text processing systems can be trained to mine for useful knowledge, they must already have enough knowledge to interpret what they read. The point at which there is "enough", is still a matter of debate, as no real program seems close to having enough knowledge to achieve general human-like understanding. Current research in large-scale natural language processing has come, rightly, to focus on lexical acquisition as the key to future progress. Unfortunately, the current state of the art is quite far from the recipe for acquiring knowledge about words, because it leans too heavily on resources that are available, without consideration for what is needed.{$<$}{$>$}},
  booktitle = {Proceedings {{Sixth International Conference}} on {{Tools}} with {{Artificial Intelligence}}. {{TAI}} 94},
  doi = {10.1109/TAI.1994.346487},
  author = {Jacobs, P. S.},
  month = nov,
  year = {1994},
  keywords = {Databases,Data mining,Knowledge engineering,Artificial intelligence,artificial intelligence,Information management,information retrieval,Natural languages,Text processing,Information retrieval,knowledge acquisition,Costs,data extraction,expert systems,Expert systems,human-like understanding,information management,intelligent text processing systems,knowledge engineering,large-scale natural language processing,lexical acquisition,natural language interfaces,text processing techniques,text retrieval methods,text-based systems,word processing},
  pages = {235-236},
  note = {ISSN:}
}

@inproceedings{395650,
  title = {Intelligent Document Understanding: {{Understanding}} Photographs with Captions},
  abstract = {The interaction of textual and photographic information in document understanding is explored. Specifically, a computational model whereby textual captions are used as collateral information in the interpretation of the corresponding photographs is presented. The final understanding of the picture and caption reflects a consolidation of the information obtained from each of the two sources and can thus be used in intelligent information retrieval tasks. The problem of performing general-purpose vision without apriori knowledge is very difficult at best. The concept of using collateral information in scene understanding has been explored in systems that use general scene context in the task of object identification. The work described extends this notion by incorporating picture specific information. Finally, as a test of the model, a multi-stage system PICTION which uses captions to identify humans in an accompanying photograph is described. This provides a computationally less expensive alternative to traditional methods of face recognition since it does not require a pre-stored database of face models for all people to be identified.{$<$}{$>$}},
  booktitle = {Proceedings of 2nd {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}} '93)},
  doi = {10.1109/ICDAR.1993.395650},
  author = {Srihari, R. K.},
  month = oct,
  year = {1993},
  keywords = {Computational modeling,knowledge based systems,database,document image processing,document understanding,Information analysis,Text analysis,Content based retrieval,Image retrieval,captions,computational complexity,computational model,Computer vision,face models,face recognition,image recognition,Information retrieval,intelligent document understanding,intelligent information retrieval,Layout,Machine vision,multi-stage system,object identification,optical character recognition,photographic information,PICTION,picture specific information,scene understanding,System testing,textual captions,textual information},
  pages = {664-667},
  note = {ISSN:}
}

@inproceedings{395697,
  title = {Understanding Structured Text Documents by a Model Based Document Analysis System},
  abstract = {A document analysis system which is capable of extracting the semantics of specific text portions of structured documents is presented. The architecture of the analysis system is based on a knowledge representation scheme, a semantic network, called Resco-Frame Representation of Structured Documents. It allows the definition of knowledge about document components as well as knowledge about analysis algorithms in a uniform, simple, but powerful representation formalism. Hence, this architecture enables the analysis system to exploit the specific power of both the algorithmic knowledge describing the properties of algorithms and the declarative knowledge about properties of text objects in documents. The inference engine and the control algorithm show how these two knowledge sources are combined and utilized. The flexibility of the representation formalism Fresco, the recognition results and the computational complexity of the inference algorithm are presented in two different applications.{$<$}{$>$}},
  booktitle = {Proceedings of 2nd {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}} '93)},
  doi = {10.1109/ICDAR.1993.395697},
  author = {Bayer, T. A.},
  month = oct,
  year = {1993},
  keywords = {Algorithm design and analysis,Data mining,Engines,Document image processing,Text analysis,Image recognition,Optical character recognition software,semantic networks,Inference algorithms,document handling,Computational complexity,control algorithm,declarative knowledge,Electronics packaging,inference engine,knowledge representation scheme,model based document analysis system,model-based reasoning,representation formalism,semantic network,structured text documents,text objects},
  pages = {448-453},
  note = {ISSN:}
}

@inproceedings{413883,
  title = {A Method to Interpret {{3D}} Motion Using Neural Networks},
  volume = {3},
  abstract = {This study proposes a 3D motion interpretation method which uses a neural network system consisting of three kinds of neural networks. This system estimates the solutions of 3D motion of an object by interpreting three optical flow (OF - motion vector field calculated from images) patterns of the same object obtained from three different view points. Though the interpretation system is trained using only basic 3D motions consisting of a single motion component, the system can interpret unknown multiple 3D motions consisting of several motion components. The generalization capacity of the proposed system is confirmed using diverse test patterns. Also the robustness of the system to noise is proved experimentally. The experimental results show that this method has suitable features for applying to real images.{$<$}{$>$}},
  booktitle = {Proceedings of 1st {{International Conference}} on {{Image Processing}}},
  doi = {10.1109/ICIP.1994.413883},
  author = {Miyauchi, A. and Watanabe, A. and Miyauchi, M.},
  month = nov,
  year = {1994},
  keywords = {learning (artificial intelligence),neural nets,Cameras,Neural networks,Computer vision,System testing,Noise robustness,2D motion interpretation network,3D motion interpretation method,3D motion interpretation network,experimental results,Helium,Image motion analysis,image sequences,motion components,motion estimation,Motion estimation,motion vector field,neural network system,noise robustness,Optical computing,optical flow normalisation network,Parameter estimation,real images,test patterns},
  pages = {83-87 vol.3},
  note = {ISSN:}
}

@article{4359385,
  title = {Script-{{Independent Text Line Segmentation}} in {{Freestyle Handwritten Documents}}},
  volume = {30},
  issn = {0162-8828},
  abstract = {Text line segmentation in freestyle handwritten documents remains an open document analysis problem. Curvilinear text lines and small gaps between neighboring text lines present a challenge to algorithms developed for machine printed or hand-printed documents. In this paper, we propose a novel approach based on density estimation and a state-of-the-art image segmentation technique, the level set method. From an input document image, we estimate a probability map, where each element represents the probability that the underlying pixel belongs to a text line. The level set method is then exploited to determine the boundary of neighboring text lines by evolving an initial estimate. Unlike connected component based methods ( [1], [2] for example), the proposed algorithm does not use any script-specific knowledge. Extensive quantitative experiments on freestyle handwritten documents with diverse scripts, such as Arabic, Chinese, Korean, and Hindi, demonstrate that our algorithm consistently outperforms previous methods. Further experiments show the proposed algorithm is robust to scale change, rotation, and noise.},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2007.70792},
  author = {Li, Y. and Zheng, Y. and Doermann, D. and Jaeger, S.},
  month = aug,
  year = {2008},
  keywords = {Algorithm design and analysis,probability,Artificial Intelligence,State estimation,estimation theory,Image analysis,text analysis,document image processing,Text analysis,Character recognition,Automated,Automatic Data Processing,Computer-Assisted,Document and Text Processing,Documentation,Image Enhancement,Image Interpretation,Image segmentation,Information Storage and Retrieval,Pattern Recognition,Algorithms,Handwriting,handwritten character recognition,image segmentation,Reproducibility of Results,Sensitivity and Specificity,connected component based method,Document analysis,document analysis problem,freestyle handwritten document image segmentation,hand-printed document,Handwriting analysis,Level set,level set method,machine printed document,Noise robustness,Piecewise linear approximation,Pixel,probability map estimation,script-independent curvilinear text line segmentation,set theory},
  pages = {1313-1329}
}

@article{4492785,
  title = {Document {{Image Retrieval}} through {{Word Shape Coding}}},
  volume = {30},
  issn = {0162-8828},
  abstract = {This paper presents a document retrieval technique that is capable of searching document images without optical character recognition (OCR). The proposed technique retrieves document images by a new word shape coding scheme, which captures the document content through annotating each word image by a word shape code. In particular, we annotate word images by using a set of topological shape features including character ascenders/descenders, character holes, and character water reservoirs. With the annotated word shape codes, document images can be retrieved by either query keywords or a query document image. Experimental results show that the proposed document image retrieval technique is fast, efficient, and tolerant to various types of document degradation.},
  number = {11},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2008.89},
  author = {Lu, S. and Li, L. and Tan, C. L.},
  month = nov,
  year = {2008},
  keywords = {Databases,Artificial Intelligence,image retrieval,document image processing,Character recognition,Degradation,Optical character recognition software,annotated word shape codes,Automated,Automatic Data Processing,character ascenders,character descenders,character holes,character water reservoirs,Computer-Assisted,Computing Methodologies,Content based retrieval,Database Management Systems,Document analysism,Document and Text Processing,Document Capture,document content,document degradation,document image retrieval,document image searching,Documentation,Factual,image coding,Image coding,Image Enhancement,Image Interpretation,Image retrieval,Image segmentation,Image/video retrieval,Information Storage and Retrieval,Language,Pattern Recognition,query document image,query keywords,Reading,Reservoirs,Shape,Text processing,topological shape features,Vision and Scene Understanding,Water resources,word image annotation,word shape coding},
  pages = {1913-1918}
}

@inproceedings{4761259,
  title = {Incremental Machine Learning Techniques for Document Layout Understanding},
  abstract = {In real-world digital libraries, artificial intelligence techniques are essential for tackling the automatic document processing task with sufficient flexibility. The great variability in document kind, content and shape requires powerful representation formalisms to catch all the domain complexity. The continuous flow of new documents requires adaptable techniques that can progressively adjust the acquired knowledge on documents as long as new evidence becomes available, even extending if needed the set of recognized document types. Both these issues have not yet been thoroughly studied. This paper presents an incremental first-order logic learning framework for automatically dealing with various kinds of evolution in digital repositories content: evolution in the definition of class definitions, evolution in the set of known classes and evolution by addition of new unknown classes. Experiments show that the approach can be applied to real-world.},
  booktitle = {2008 19th {{International Conference}} on {{Pattern Recognition}}},
  doi = {10.1109/ICPR.2008.4761259},
  author = {Ferilli, S. and Biba, M. and Basile, T. M. A. and Esposito, F.},
  month = dec,
  year = {2008},
  keywords = {Data mining,Machine learning,learning (artificial intelligence),Artificial intelligence,Production systems,artificial intelligence,document image processing,Learning systems,Shape,Automatic logic units,digital libraries,Digital systems,document layout understanding,domain complexity,first-order logic learning,incremental machine learning,representation formalisms,Software libraries,Technology management},
  pages = {1-4},
  issn = {1051-4651}
}

@inproceedings{5158992,
  title = {A {{Study}} and {{Application}} on {{Machine Learning}} of {{Artificial Intellligence}}},
  abstract = {This thesis elaborated the concept, significance and main strategy of machine learning as well as the basic structure of machine learning system. By combining several basic ideas of main strategies, great effort are laid on introducing several machine learning methods, such as Rote learning, Explanation-based learning, Learning from instruction, Learning by deduction, Learning by analogy and Inductive learning, etc. Meanwhile, comparison and analysis are made upon their respective advantages and limitations. At the end of the article, it proposes the research objective of machine learning and points out its development trend.Machine learning is a fundamental way that enable the computer to have the intelligence ; Its application which had been used mainly the method of induction and the synthesis, rather than the deduction has already reached many fields of Artificial Intelligence.},
  booktitle = {2009 {{International Joint Conference}} on {{Artificial Intelligence}}},
  doi = {10.1109/JCAI.2009.55},
  author = {Xue, M. and Zhu, C.},
  month = apr,
  year = {2009},
  keywords = {Machine learning,Computational modeling,learning (artificial intelligence),Artificial intelligence,artificial intelligence,machine learning,Machine learning algorithms,Physiology,AI,Application software,Learning systems,Humans,algorithm,explanation-based learning,Inductive learning,Intelligent robots,Intelligent systems,learning by analogy,learning by deduction,learning from instruction,learning strategy,machine learning system,rote learning,system structure},
  pages = {272-274},
  note = {ISSN:}
}

@inproceedings{5212590,
  title = {Constructing Financial Distress Prediction Model Using Group Method of Data Handling Technique},
  volume = {5},
  abstract = {Companies in financial distress make the creditors, shareholders, employees, investors and other participants of the related firms suffer great losses. In order to prevent the companies run into bankruptcy, financial distress prediction has been a useful tool for distinguishing companies in financial distress from those healthy. Statistical methods and artificial intelligence techniques have been widely used to deal with this issue. Many studies indicated that artificial neural networks outperform many statistical methods. However, artificial neural networks have the drawback of failing to interpret the classification results. This paper uses an artificial intelligence technique-group method of data handling technique to overcome this drawback. The sample data are collected from Taiwan listed companies in the Taiwan Stock Exchange Corporation. The result illustrates that the accuracy rates of classification of group method of data handling models are larger than 90\% and the models of the group method of data handling obtain better accuracy than the models of discriminant analysis and logistic regression.},
  booktitle = {2009 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  doi = {10.1109/ICMLC.2009.5212590},
  author = {Liao, M.},
  month = jul,
  year = {2009},
  keywords = {financial data processing,Companies,Logistics,Machine learning,data handling,pattern classification,Artificial neural networks,Predictive models,investment,Artificial intelligence,statistical analysis,neural nets,Neural networks,Investments,artificial intelligence technique,artificial neural network,Artificial neural network,creditor,data classification,Data handling,data handling technique,Financial distress prediction,financial distress prediction model,group method,Group method of data handling,investor,Statistical analysis,statistical method,stock markets,Taiwan stock exchange corporation},
  pages = {2897-2902},
  issn = {2160-133X}
}

@inproceedings{5267552,
  title = {Application of Artificial Neural Networks in Lithofacies Interpretation Used for {{3D}} Geological Modelling},
  volume = {4},
  abstract = {This paper represents a study using Artificial Neural Networks (ANN) to perform automatic interpretation of lithofacies in a reservoir scale. This technique having been used successfully to interpret lithofacies automatically in the Sha20 Block, Shanian oilfield. Description and interpretation from a cored section in the key well was used to train the Supervised neural network. Having trained the network, it was then used to recognise and interpret the units vertically and laterally in the studied reservoir. The unsupervised neural network was run to classify the cored interval into 2 and 6 classes respectively and the results were then compared with the supervised network output. The results were observed to be over 87\% accurate. Then a 3D geological model was built using the sequential indicator simulation method, the excellent results obtained from the developed model shows that the method is quite effective and gets satisfying prediction precision for the lithofacies in reservoir modeling.},
  booktitle = {2009 {{ISECS International Colloquium}} on {{Computing}}, {{Communication}}, {{Control}}, and {{Management}}},
  doi = {10.1109/CCCM.2009.5267552},
  author = {Ma, X. and Zhang, J. and Zhao, H.},
  month = aug,
  year = {2009},
  keywords = {Computer networks,Petroleum,Artificial neural networks,Predictive models,neural nets,Neural networks,artificial neural networks,Reservoirs,Costs,Cellular neural networks,training,Artificial Neural Networks,3D geological modelling,geology,Geology,hydrocarbon reservoirs,Lithofacies,lithofacies interpretation,modelling,Permeability,sequential indicator simulation method,Sha20 Block,Shanian oilfield,supervised neural network,unsupervised neural network},
  pages = {451-454},
  issn = {2154-9613}
}

@article{5290725,
  title = {Exemplar-Based {{Visualization}} of {{Large Document Corpus}} ({{InfoVis2009}}-1115)},
  volume = {15},
  issn = {1077-2626},
  abstract = {With the rapid growth of the World Wide Web and electronic information services, text corpus is becoming available online at an incredible rate. By displaying text data in a logical layout (e.g., color graphs), text visualization presents a direct way to observe the documents as well as understand the relationship between them. In this paper, we propose a novel technique, Exemplar-based visualization (EV), to visualize an extremely large text corpus. Capitalizing on recent advances in matrix approximation and decomposition, EV presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function. The probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding. By selecting the representative exemplars, we obtain a compact approximation of the data. This makes the visualization highly efficient and flexible. In addition, the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of large text corpus. Empirically, we demonstrate the superior performance of EV through extensive experiments performed on the publicly available text data sets.},
  number = {6},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2009.140},
  author = {Chen, Y. and Wang, L. and Dong, M. and Hua, J.},
  month = nov,
  year = {2009},
  keywords = {Principal component analysis,optimisation,Indexing,biology computing,Computer science,Text mining,data visualisation,iterative methods,Data visualization,Web sites,Drugs,Exemplar,exemplar-based visualization,iterative optimization,large document corpus,large-scale document visualization,Large-scale systems,matrix approximation,Matrix decomposition,multidimensional projection.,Multidimensional systems,parameter embedding,text corpus,text visualization},
  pages = {1161-1168}
}

@inproceedings{5333385,
  title = {Study on Interpretable Fuzzy Classification System Based on Neural Networks},
  abstract = {This paper describes a comprehensive method to construct fuzzy classification system considering both precision and interpretability. Fuzzy classification system, initialized by modified Gath-Geva fuzzy clustering algorithm, is transformed into neural network. After training the neural network, fuzzy sets similarity measure is adopt to merge redundant fuzzy sets to improve interpretability, and a constraint genetic algorithm is applied to improve precision. The simulation result on Iris data problem demonstrates the effectiveness of the proposed method.},
  booktitle = {2009 {{ICCAS}}-{{SICE}}},
  month = aug,
  year = {2009},
  keywords = {Electronic mail,Transportation,neural network,neural nets,genetic algorithms,fuzzy set theory,neural networks,Neural networks,Genetic algorithms,interpretability,Fuzzy neural networks,Fuzzy sets,Clustering algorithms,constraint genetic algorithm,Fuzzy classification system,Fuzzy systems,interpretable fuzzy classification system,Iris,Iris data problem,modified Gath-Geva fuzzy clustering algorithm,Telecommunication traffic},
  pages = {5318-5321},
  note = {ISSN:}
}

@inproceedings{5383110,
  title = {A Discriminative Learning Approach for Orientation Detection of {{Urdu}} Document Images},
  abstract = {Orientation detection is an important preprocessing step for accurate recognition of text from document images. Many existing orientation detection techniques are based on the fact that in Roman script text ascenders occur more likely than descenders, but this approach is not applicable to document of other scripts like Urdu, Arabic, etc. In this paper, we propose a discriminative learning approach for orientation detection of Urdu documents with varying layouts and fonts. The main advantage of our approach is that it can be applied to documents of other scripts easily and accurately. Our approach is based on classification of individual connected component orientation in the document image, and then the orientation of the page image is determined via majority count. A convolutional neural network is trained as discriminative learning model for the labeled Urdu books dataset with four target orientations: 0, 90, 180 and 270 degrees. We demonstrate the effectiveness of our method on dataset of Urdu documents categorized into the layouts of book, novel and poetry. We achieved 100\% orientation detection accuracy on a test set of 328 document images.},
  booktitle = {2009 {{IEEE}} 13th {{International Multitopic Conference}}},
  doi = {10.1109/INMIC.2009.5383110},
  author = {Rashid, S. F. and Bukhari, S. S. and Shafait, F. and Breuel, T. M.},
  month = dec,
  year = {2009},
  keywords = {learning (artificial intelligence),Artificial intelligence,neural nets,Neural networks,convolutional neural network,classification,text analysis,document image processing,Image recognition,Optical character recognition software,text recognition,Text recognition,natural language processing,Shape,Books,Cellular neural networks,discriminative learning approach,Learning,orientation detection,Pattern recognition,Roman script,text ascenders,Urdu books dataset,Urdu document images},
  pages = {1-5},
  note = {ISSN:}
}

@inproceedings{5483913,
  title = {Extended Character Defect Model for Recognition of Text from Maps},
  abstract = {Topographic maps contain a small amount of text compared to other forms of printed documents. Furthermore, the text and graphical components typically intersect with one another thus making the extraction of text a very difficult task. Creating training sets with a suitable size from the actual characters in maps would therefore require the laborious processing of many maps with similar features and the manual extraction of character samples. This paper extends the types of defects represented by Baird's document image degradation model in order to create pseudo randomly generated training sets that closely mimic the various artifacts and defects encountered in characters extracted from maps. Two Hidden Markov Models are then trained and used to recognize the text. Tests performed on extracted street labels show an improvement in performance from 88.4\% when only the original Baird's model is used to a character recognition rate of 93.2\% when the extended defect model is used for training.},
  booktitle = {2010 {{IEEE Southwest Symposium}} on {{Image Analysis Interpretation}} ({{SSIAI}})},
  doi = {10.1109/SSIAI.2010.5483913},
  author = {Pezeshk, A. and Tutwiler, R. L.},
  month = may,
  year = {2010},
  keywords = {Data mining,Feature extraction,learning (artificial intelligence),Artificial neural networks,feature extraction,text analysis,document image processing,cartography,Character recognition,Degradation,document image degradation model,extended character defect model,Graphics,hidden Markov models,Hidden Markov models,Hidden Markov Models,Image recognition,Optical character recognition software,pseudo randomly generated training sets,text recognition,Text recognition,topographic maps},
  pages = {85-88},
  note = {ISSN:}
}

@inproceedings{5579790,
  title = {Intelligent {{Expertise Classification}} Approach: {{An}} Innovative Artificial Intelligence Approach to Accelerate Network Data Visualization},
  volume = {6},
  abstract = {In order to visualize huge network traffic, data visualization applications are being developed and used to complement network data visualization. With today's network data visualization tools, it is only possible to view small portions of data and consuming lots of time to process the data. The network data process time can be reduced with the innovative artificial intelligence approach, which can effectively accelerate the network data visualization and consequently classify network data into different level of details according diverse computer users' expertise level on network. In the last few years, many visualization tools have been developed; either suffers from time to process the network data or the low understanding from the network data visualization efficiency. In this paper, we proposed an innovative intelligence approach, namely Intelligent Expertise Classification Algorithm (IECA) based on diverse computer users' expertise level in order to improve the network data process time as well as the understanding level among computer users. The approach architecture details and its requirements such as expertise level from diverse computer users and processed data from data mining classification will be discussed. Numbers of experiments have been carried out on 100 computer users from different fields and different level of computer expertise to evaluate the approach effectiveness. It features fast intelligent expertise classification and support network data understanding performance.},
  booktitle = {2010 3rd {{International Conference}} on {{Advanced Computer Theory}} and {{Engineering}}({{ICACTE}})},
  doi = {10.1109/ICACTE.2010.5579790},
  author = {Manickam, S.},
  month = aug,
  year = {2010},
  keywords = {computer network security,Data mining,pattern classification,data mining,telecommunication traffic,Visualization,artificial intelligence,data visualisation,Data visualization,artificial intelligence approach,data mining classification,innovative artificial intelligence approach,intelligent expertise classification algorithm,intelligent expertise classification approach,Lead,network data classification,network data process,network data visualization,network traffic visualization},
  pages = {V6-437-V6-440},
  issn = {2154-7505}
}

@inproceedings{5580484,
  title = {Video Text Detection and Localization Based on Localized Generalization Error Model},
  volume = {4},
  abstract = {Texts in videos provide plenteous information for video analysis such as video indexing, understanding and retrieval. We propose a neural network based method detecting text in the video frames in this work. The proposed method consists of three major steps: feature extraction, text region detection and candidate region refinement. Firstly, we extract texture features from four edge maps yielded from the target video frame. Secondly, a Radial Basis Function Neural Network (RBFNN) optimized by the Localized Generalization Error Model (L-GEM) is applied to detect text candidates. Finally, a false detection of text is applied to fine tune the result. Experimental results demonstrate that the proposed method is efficient for different font-colors, font-sizes and language in complex background.},
  booktitle = {2010 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  doi = {10.1109/ICMLC.2010.5580484},
  author = {Ma, X. and Ng, W. W. Y. and Chan, P. P. K. and Yeung, D. S.},
  month = jul,
  year = {2010},
  keywords = {Computer architecture,Machine learning,Feature extraction,radial basis function networks,feature extraction,Training,text analysis,Classification algorithms,Neurons,edge detection,Image edge detection,edge maps,localized generalization error model,Localized generalization error model (LGEM),radial basis function neural network,Radial basis function neural network (RBFNN),Text detection,texture features extraction,video indexing,video retrieval,video signal processing,video text detection,video understanding},
  pages = {2161-2166},
  issn = {2160-133X}
}

@inproceedings{5590681,
  title = {The {{Effects}} of {{Documents Lineage}} on {{Use}} of {{Explanation}} in {{Document}}-{{Driven DSS}}},
  volume = {1},
  abstract = {In document-driven DSS, the decisions are both based on the inheritance among the documents and the acceptance of advices for users. Research in the field of DSS has shown that providing explanations may improve acceptance of decision makers. The most important part of explanations in document-driven DSS lies in tracing the contents and the classification of interrelated documents. But current document-driven DSS is lack of a mechanism to record the citation and cluster the related documents. This paper tries to find out the trace routes among documents to improve the explanations. First, a document lineage model is established to present the citation and delivery mechanism in documents. Second, the document DNA is used to build the routes of documents transferences. Third, the whole lineage in documents is integrated by routes. Finally, a system frame for explanations mechanism in document-driven DSS was described.},
  booktitle = {2010 {{Second International Conference}} on {{Intelligent Human}}-{{Machine Systems}} and {{Cybernetics}}},
  doi = {10.1109/IHMSC.2010.67},
  author = {Chen, Z. and Dong, H.},
  month = aug,
  year = {2010},
  keywords = {data mining,decision making,Decision making,pattern clustering,Computers,Explanation,information retrieval,text analysis,Decision support systems,Blood,decision support system,decision support systems,DNA,document driven DSS,Document lineage,document lineage model,document transference,Document-driven DSS,Face},
  pages = {243-248},
  note = {ISSN:}
}

@inproceedings{598997,
  title = {Model Matching in Intelligent Document Understanding},
  volume = {1},
  abstract = {Intelligent Document Understanding (IDU) is the process of converting scanned document pages into an electronic, processable form. We have previously presented a IDU system architecture suitable for this task which uses a hybrid bottom-up/top-down control strategy. In this paper we focus on a specific subproblem that arises within the chosen framework, concerned with selecting an appropriate page layout structure. A detailed analysis of the problem using an error propagation model, allows computationally simple search strategies to be developed. A multistage layout formation algorithm is proposed and its performance is critically assessed when implemented using two different Layout Object selection criterion. The first selection criterion is based on a maximal column area coverage; the second is based on a probabilistic Layout Object selection. Both techniques have been incorporated into the hybrid IDU system and the results presented indicate its superiority over previously reported systems.},
  booktitle = {Proceedings of 3rd {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  doi = {10.1109/ICDAR.1995.598997},
  author = {Farrow, G. S. D. and Xydeas, C. S. and Oakley, J. P.},
  month = aug,
  year = {1995},
  keywords = {Computer architecture,Process control,Computational modeling,Control systems,document image processing,Degradation,Intelligent structures,Computer vision,intelligent document understanding,optical character recognition,Image databases,search problems,appropriate page layout structure,computationally simple search strategies,error propagation model,hybrid bottom-up/top-down control strategy,maximal column area coverage,model matching,Object detection,Performance analysis,probabilistic layout object selection},
  pages = {293-296 vol.1},
  note = {ISSN:}
}

@article{5993545,
  title = {A {{New Formulation}} for {{Feedforward Neural Networks}}},
  volume = {22},
  issn = {1045-9227},
  abstract = {Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
  number = {10},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2011.2163169},
  author = {Razavi, S. and Tolson, B. A.},
  month = oct,
  year = {2011},
  keywords = {optimisation,Optimization,learning (artificial intelligence),Models,Artificial Intelligence,Training,Biological neural networks,feedforward neural nets,Neurons,Algorithms,Neural Networks (Computer),black box model,derivative free optimization algorithm,error response surface,Feedback,feedforward neural network,Feedforward neural networks,function approximation,Function approximation,function approximation techniques,generalisation (artificial intelligence),generalization,generalization ability,geometrical interpretation,internal behavior,learning ability,measure of regularization,neural network functional geometry,Neurological,Nickel,reformulated neural network,ReNN approach,Software Design,training,training method},
  pages = {1588-1598}
}

@inproceedings{6102474,
  title = {Automated Measures for Interpretable Dimensionality Reduction for Visual Classification: {{A}} User Study},
  abstract = {This paper studies the interpretability of transformations of labeled higher dimensional data into a 2D representation (scatterplots) for visual classification.\textsuperscript{1}In this context, the term interpretability has two components: the interpretability of the visualization (the image itself) and the interpretability of the visualization axes (the data transformation functions). We define a data transformation function as any linear or non-linear function of the original variables mapping the data into 1D. Even for a small dataset, the space of possible data transformations is beyond the limit of manual exploration, therefore it is important to develop automated techniques that capture both aspects of interpretability so that they can be used to guide the search process without human intervention. The goal of the search process is to find a smaller number of interpretable data transformations for the users to explore. We briefly discuss how we used such automated measures in an evolutionary computing based data dimensionality reduction application for visual analytics. In this paper, we present a two-part user study in which we separately investigated how humans rated the visualizations of labeled data and comprehensibility of mathematical expressions that could be used as data transformation functions. In the first part, we compared human perception with a number of automated measures from the machine learning and visual analytics literature. In the second part, we studied how various structural properties of an expression related to its interpretability.},
  booktitle = {2011 {{IEEE Conference}} on {{Visual Analytics Science}} and {{Technology}} ({{VAST}})},
  doi = {10.1109/VAST.2011.6102474},
  author = {Icke, I. and Rosenberg, A.},
  month = oct,
  year = {2011},
  keywords = {learning (artificial intelligence),pattern classification,Indexes,Visualization,Support vector machines,machine learning,data analysis,visualization,visual analytics,data visualisation,Data visualization,Humans,2D representation,Atmospheric measurements,data transformation,data transformation functions,evolutionary computing,expression structural properties,human perception,interpretable dimensionality reduction,labeled data visualization,labeled higher dimensional data,mathematical expression comprehensibility,Particle measurements,transformation interpretability,user study,visual classification,visualization axis interpretability},
  pages = {281-282},
  note = {ISSN:}
}

@inproceedings{6154802,
  title = {Automatic {{Text Classification}} of Sports Blog Data},
  abstract = {Automatic Text Classification is a semi-supervised machine learning task that automatically assigns a given text document to a set of pre-defined categories based on the features extracted from its textual content. This paper attempts to automatically classify the textual entries made by bloggers on various sports blogs, to the appropriate category of sport by following steps like pre-processing, feature extraction and na\"ive Bayesian classification. Empirical evaluation of this technique has resulted in a classification accuracy of approximately 87\% over the test set. In addition to classifying the textual entries of sports blogs, it is proposed that the extracted features themselves be further classified under more meaningful heads which results in generation of a semantic resource that lends greater understanding to the classification task. This semantic resource can be used for data mining requirements that arise in the future.},
  booktitle = {2012 {{Computing}}, {{Communications}} and {{Applications Conference}}},
  doi = {10.1109/ComComAp.2012.6154802},
  author = {Dalal, M. K. and Zaveri, M. A.},
  month = jan,
  year = {2012},
  keywords = {Feature extraction,learning (artificial intelligence),pattern classification,data mining,feature extraction,Semantics,Training,machine learning,text analysis,Text categorization,Bayes methods,Accuracy,automatic text classification,Bayesian methods,Blogs,data mining requirements,heuristics,intelligent data mining,naïve Bayes classification,naïve Bayesian classification,semantic resource,semantic Web,semi-supervised machine learning task,sport,sports blog data,text document,Web sites},
  pages = {219-222},
  note = {ISSN:}
}

@inproceedings{618903,
  title = {A Neural Network That Explains as Well as Predicts Financial Market Behavior},
  abstract = {When a neural network makes a financial prediction, the user may benefit from knowing which previous time periods are illustrative of the current time period. The authors describe a high-performance neural network that in addition to predicting stock market direction, allows the user to visualize the relationship between current conditions and previous conditions that led to similar predictions. Visualization is accomplished by forming a gated multi-expert network using funnel-shaped multilayer dimensionality reduction networks. The neck of the funnel is a two-neuron layer that displays the training data and the decision boundaries in a two-dimensional space. This architecture facilitates a) interactive design of the decision functions and b) explanation of the relevance of past decisions from the training set to the current decision. They describe a stock market prediction system whose design incorporates a visual neural network for prediction, wavelet transforms and tapped delay lines for feature extraction, and a genetic algorithm for feature selection. This system shows that the visual neural network provides the low error rates (i.e., accurate predictions) of multi-expert networks along with the visual explanatory power of nonlinear dimensionality reduction.},
  booktitle = {Proceedings of the {{IEEE}}/{{IAFE}} 1997 {{Computational Intelligence}} for {{Financial Engineering}} ({{CIFEr}})},
  doi = {10.1109/CIFER.1997.618903},
  author = {Ornes, C. and Sklansky, J.},
  month = mar,
  year = {1997},
  keywords = {Algorithm design and analysis,financial data processing,multilayer perceptrons,architecture,feature extraction,neural net architecture,genetic algorithms,Two dimensional displays,feedforward neural nets,Neural networks,visualization,explanation,Training data,data visualisation,Data visualization,decision support systems,stock markets,Stock markets,Multi-layer neural network,2D space,current conditions,decision boundaries,Delay lines,financial market behavior explanation,financial market behavior prediction,funnel-shaped multilayer dimensionality reduction networks,gated multi-expert network,genetic algorithm,high-performance neural network,interactive decision function design,Neck,past decision relevance,prediction theory,previous conditions,tapped delay lines,time periods,training data,two-neuron layer,visual neural network,wavelet transforms,Wavelet transforms},
  pages = {43-49},
  note = {ISSN:}
}

@inproceedings{619849,
  title = {Automatic Knowledge Acquisition for Spatial Document Interpretation},
  volume = {1},
  abstract = {In this paper, a qualitative representation for the layout of structured documents as well as classes of documents is presented, which is established by means of supervised learning from a labeled training set of documents. For this formal representation, an inference algorithm has been developed, adopted from error-tolerant subgraph isomorphism, which assigns logic labels to the layout objects of a test document.},
  booktitle = {Proceedings of the {{Fourth International Conference}} on {{Document Analysis}} and {{Recognition}}},
  doi = {10.1109/ICDAR.1997.619849},
  author = {Walischewski, H.},
  month = aug,
  year = {1997},
  keywords = {Data mining,fault tolerant computing,learning (artificial intelligence),inference mechanisms,supervised learning,document image processing,Information analysis,Training data,automatic knowledge acquisition,Computer science education,document classes,error-tolerant subgraph isomorphism,formal representation,graph theory,Humans,inference algorithm,Inference algorithms,Inference mechanisms,knowledge acquisition,Knowledge acquisition,knowledge representation,labeled training set,layout objects,logic labels,Logic testing,qualitative representation,spatial document interpretation,structured document layout,Writing},
  pages = {243-247 vol.1},
  note = {ISSN:}
}

@inproceedings{6460845,
  title = {A Learning Framework for Degraded Document Image Binarization Using {{Markov Random Field}}},
  abstract = {Document image binarization is an important preprocessing technique for document image analysis that segments the text from the document image backgrounds. Many techniques have been proposed and successfully applied in different applications, such as document image retrieval. However, these techniques may perform poorly on degraded document images. In this paper, we propose a learning framework that makes use of the Markov Random Field to improve the performance of the existing document image binarization methods for those degraded document images. Extensive experiments on the recent Document Image Bina-rization Contest datasets demonstrate that significant improvements of the existing binarization methods when applying our proposed framework.},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Pattern Recognition}} ({{ICPR2012}})},
  author = {Su, B. and Lu, S. and Tan, C. L.},
  month = nov,
  year = {2012},
  keywords = {learning (artificial intelligence),Mathematical model,Markov processes,image retrieval,text analysis,document image processing,Text analysis,document image retrieval,random processes,document image analysis,image segmentation,degraded document image binarization methods,document image backgrounds,document image binarization contest datasets,Equations,Image edge detection,learning framework,Markov random field,Markov random fields,text segmentation,Vectors},
  pages = {3200-3203},
  issn = {1051-4651}
}

@inproceedings{6486324,
  title = {Application of {{Neuro}}-{{Fuzzy}} Model for Text and Speech Understanding Systems},
  abstract = {The problem of speech and text understanding and its application to the spoken dialogue systems is investigated in the paper. The Neuro-Fuzzy model has been applied for solution this problem and received satisfactory results. Mathematical model and software developed for building human-computer dialogue system.},
  booktitle = {2012 {{IV International Conference}} "{{Problems}} of {{Cybernetics}} and {{Informatics}}" ({{PCI}})},
  doi = {10.1109/ICPCI.2012.6486324},
  author = {Rustamov, S.},
  month = sep,
  year = {2012},
  keywords = {human computer interaction,neural nets,fuzzy set theory,text analysis,interactive systems,dialogue systems,human-computer dialogue system,learning user intention,mathematical model,neuro-fuzzy model,speech synthesis,speech understanding,speech understanding systems,spoken dialogue systems,text understanding systems},
  pages = {1-4},
  note = {ISSN:}
}

@article{6634108,
  title = {A {{Systematic Review}} on the {{Practice}} of {{Evaluating Visualization}}},
  volume = {19},
  issn = {1077-2626},
  abstract = {We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90\% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.},
  number = {12},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2013.126},
  author = {Isenberg, T. and Isenberg, P. and Chen, J. and Sedlmair, M. and M\"oller, T.},
  month = dec,
  year = {2013},
  keywords = {History,Mathematical model,Systematics,visualization,Automated,Computer-Assisted,Image Enhancement,Image Interpretation,Information Storage and Retrieval,Pattern Recognition,data visualisation,Data visualization,Humans,Algorithms,Computer Graphics,Reproducibility of Results,Sensitivity and Specificity,User-Computer Interface,information visualization,coding scheme,domain-specific work practices,encoding,Encoding,Evaluation,IEEE information visualization,IEEE visualization conference,Imaging,meta-level,requirements analyses,scientific visualization,systematic review,Three-Dimensional,validation,visual tools,visualization evaluation},
  pages = {2818-2827}
}

@article{6634123,
  title = {Characterizing and {{Visualizing Predictive Uncertainty}} in {{Numerical Ensembles Through Bayesian Model Averaging}}},
  volume = {19},
  issn = {1077-2626},
  abstract = {Numerical ensemble forecasting is a powerful tool that drives many risk analysis efforts and decision making tasks. These ensembles are composed of individual simulations that each uniquely model a possible outcome for a common event of interest: e.g., the direction and force of a hurricane, or the path of travel and mortality rate of a pandemic. This paper presents a new visual strategy to help quantify and characterize a numerical ensemble's predictive uncertainty: i.e., the ability for ensemble constituents to accurately and consistently predict an event of interest based on ground truth observations. Our strategy employs a Bayesian framework to first construct a statistical aggregate from the ensemble. We extend the information obtained from the aggregate with a visualization strategy that characterizes predictive uncertainty at two levels: at a global level, which assesses the ensemble as a whole, as well as a local level, which examines each of the ensemble's constituents. Through this approach, modelers are able to better assess the predictive strengths and weaknesses of the ensemble as a whole, as well as individual models. We apply our method to two datasets to demonstrate its broad applicability.},
  number = {12},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2013.138},
  author = {Gosink, L. and Bensema, K. and Pulsipher, T. and Obermaier, H. and Henry, M. and Childs, H. and Joy, K. I.},
  month = dec,
  year = {2013},
  keywords = {learning (artificial intelligence),Mathematical model,Models,Predictive models,statistical analysis,Numerical models,Automated,Pattern Recognition,data visualisation,Data visualization,Algorithms,Computer Graphics,Reproducibility of Results,Sensitivity and Specificity,User-Computer Interface,Bayes methods,Bayes Theorem,Bayesian model averaging framework,Computer Simulation,Data Interpretation,ensemble constituents,event-of-interest prediction,ground truth observations,numerical ensemble forecasting,numerical ensembles,predictive uncertainty characterization,predictive uncertainty visualization,Statistical,statistical aggregate,statistical visualization,uncertainty handling,Uncertainty visualization,visual strategy,visualization strategy},
  pages = {2703-2712}
}

@article{6875959,
  title = {{{VarifocalReader}} \textemdash{} {{In}}-{{Depth Visual Analysis}} of {{Large Text Documents}}},
  volume = {20},
  issn = {1077-2626},
  abstract = {Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview-detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus-context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.},
  number = {12},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2014.2346677},
  author = {Koch, S. and John, M. and W\"orner, M. and M\"uller, A. and Ertl, T.},
  month = dec,
  year = {2014},
  keywords = {Data mining,learning (artificial intelligence),Navigation,machine learning,Natural language processing,Text mining,visual analytics,text analysis,natural language processing,data visualisation,text mining,Data visualization,distant reading,document analysis,Document handling,focus-context techniques,in-depth visual analysis,Interactive systems,intermediate text levels,literary analysis,machine learning techniques,Tag clouds,text documents,varifocalreader,visual abstraction},
  pages = {1723-1732}
}

@inproceedings{6991412,
  title = {Quality Assessment Method for Software Development Process Document Based on Software Document Characteristics Metric},
  abstract = {To deliver the software product which conforms to customer's actual needs has become an important issue of software development companies. The appropriate Software Development Life Cycle (SDLC) which is the process consists of a sequence of activities performed for developing that software product is selected. During those activities, there are various information related to software product development and are used to communicate among parties involved. This information is often specified in SDLC documents using natural language. Unfortunately, the problems of interpretation and difficulty of understanding are arisen and often caused by characteristic of natural language itself, which is ambiguous, and the inappropriateness of document structure. These problems which are some of the interested open questions in software requirements specifications area may influence on software product discrepancy from customer's actual needs. To mitigate these problems, this paper proposes a method for assessing quality of SDLC documents characteristics focusing on document contents and structure. The measurement process model is used as a guideline for proposing the method and the measurement information model is applied to define metrics which are used to assess SDLC documents characteristics directly. A Software Requirements Specifications (SRS) document was used to illustrate our proposed method as a case study. The result of the proposed method can be used to indicate the quality level of SDLC documents and appeared flaws, which leads to the improvement of SDLC document quality. The improved SDLC documents can enhance the quality of communication among stakeholders and support the software product development to meet customer's actual needs. These results can also be stored as a lesson learned and be applied for the future similar situation.},
  booktitle = {Ninth {{International Conference}} on {{Digital Information Management}} ({{ICDIM}} 2014)},
  doi = {10.1109/ICDIM.2014.6991412},
  author = {Thitisathienkul, P. and Prompoon, N.},
  month = sep,
  year = {2014},
  keywords = {Software,Databases,Standards,Natural languages,document handling,formal specification,Characteristics,communication quality enhancement,customer needs,document contents,document structure,Document Structure,measurement information model,measurement process model,natural language characteristic,Quality assessment,quality assessment method,Quality Assessment Method,SDLC Document,SDLC document characteristics assessment,SDLC document quality level,SDLC documents,software development companies,software development life cycle,software development process document,software document characteristics metric,Software measurement,Software Metric,software metrics,software product deliver,software product development,software product discrepancy,software product lines,software quality,software requirement specification document,software requirement specifications,SRS document},
  pages = {182-188},
  note = {ISSN:}
}

@inproceedings{7066257,
  title = {Extraction of Arbitrary Text in Natural Scene Image Based on Stroke Width Transform},
  abstract = {Text extraction plays an important role in numerous applications. Research on its method still need to be improved in order to achieve better performance, to increase the reliability of text extraction system and to deal with complex cases of text extraction. The majority of the text extraction methods are focusing on horizontal and near horizontal text lines; however, text in natural scene might be in arbitrary line in real time. Thus, this paper aims to solve the issue of extracting the arbitrary oriented text by suggesting a method for text detection and localization based on the Stroke Width Transform. The proposed method is tested on an arbitrary text dataset and ICDAR dataset. The result of the experiment shows that the proposed method adapts well to the arbitrary text.},
  booktitle = {2014 14th {{International Conference}} on {{Intelligent Systems Design}} and {{Applications}}},
  doi = {10.1109/ISDA.2014.7066257},
  author = {Jameson, J. and Abdullah, S. N. H. S.},
  month = nov,
  year = {2014},
  keywords = {Licenses,Integrated circuits,Text analysis,Optical character recognition software,Image edge detection,text detection,arbitrary text extraction,ICDAR dataset,natural scene image,Scene understanding,stroke width transform,Stroke Width Transform,Text detection and localization,Text extraction,text localization,transforms,Transforms},
  pages = {124-128},
  issn = {2164-7143}
}

@inproceedings{716778,
  title = {A Novel Pattern Searching Method Using Neural Networks and Correlation},
  volume = {2},
  abstract = {A novel pattern searching method using neural networks and correlation is presented. This method combines the quickness and adaptiveness of neural networks with the accuracy of the mathematical correlation approach. Images are divided into small sub-images which are presented to the trained neural network. Sub-images that may contain the pattern or partial pattern are selected by the neural network. The neural network also provides the approximate location of the pattern, therefore the selected sub-images can be adjusted to contain the complete pattern. Desired patterns can be located by measuring the new sub-images' correlation values against the reference models in a small area. Experiments show that this superior method is able to find the desired patterns. Moreover, this method is much faster and more adaptable than traditional pattern searching methods.},
  booktitle = {Proceedings of 1993 {{International Conference}} on {{Neural Networks}} ({{IJCNN}}-93-{{Nagoya}}, {{Japan}})},
  doi = {10.1109/IJCNN.1993.716778},
  author = {Chiu, C. and Oki, T. and Paolellia, P.},
  month = oct,
  year = {1993},
  keywords = {neural nets,Correlation,neural networks,Neural networks,Character recognition,Image recognition,image recognition,Pixel,Pattern recognition,adaptiveness,Brightness,correlation methods,Inspection,Manufacturing automation,mathematical correlation,pattern searching method,Printed circuits,sub-images},
  pages = {1277-1280 vol.2},
  note = {ISSN:}
}

@inproceedings{7193210,
  title = {Text Recognition from Images},
  abstract = {Text recognition in images is a research area which attempts to develop a computer system with the ability to automatically read the text from images. These days there is a huge demand in storing the information available in paper documents format in to a computer storage disk and then later reusing this information by searching process. One simple way to store information from these paper documents in to computer system is to first scan the documents and then store them as images. But to reuse this information it is very difficult to read the individual contents and searching the contents form these documents line-by-line and word-by-word. The challenges involved in this the font characteristics of the characters in paper documents and quality of images. Due to these challenges, computer is unable to recognize the characters while reading them. Thus there is a need of character recognition mechanisms to perform Document Image Analysis (DIA) which transforms documents in paper format to electronic format. In this paper we have discuss method for text recognition from images. The objective of this paper is to recognition of text from image for better understanding of the reader by using particular sequence of different processing module.},
  booktitle = {2015 {{International Conference}} on {{Innovations}} in {{Information}}, {{Embedded}} and {{Communication Systems}} ({{ICIIECS}})},
  doi = {10.1109/ICIIECS.2015.7193210},
  author = {Manwatkar, P. M. and Yadav, S. H.},
  month = mar,
  year = {2015},
  keywords = {Feature extraction,Computers,Biological neural networks,document image processing,Character recognition,text recognition,Text recognition,Neurons,Image segmentation,character recognition,document image analysis,text detection,computer storage disk,computer system,DIA,Document Image Analysis (DIA),electronic format,font characteristics,image quality,image resolution,image text recognition,image texture},
  pages = {1-6},
  note = {ISSN:}
}

@inproceedings{7311974,
  title = {Analyzing Students Pauses during Reading and Explaining a Story},
  abstract = {In this paper we present a semi-automated analysis of student reading performance from the perspective of her text reading level and text understanding. Silences (pauses) between uttered words or read sentences as well as silences between verbalizations given by students are the key points in the analysis of their learning activities. Pause is an essential element in the analysis of a text, which also gives good control over interactions during the processes of text reading and explanation of understanding. This study presents the results specific to pauses in the reading and verbalization using Praat, a tool to analyze spoken productions. Correlations between students' fluency, story understanding, and mean pause duration of reading and explanation phases show consistent results across texts for reading. Results about pauses during explaining yielded low correlations, showing that other variables may influence the pausing behaviors during explaining.},
  booktitle = {2015 14th {{RoEduNet International Conference}} - {{Networking}} in {{Education}} and {{Research}} ({{RoEduNet NER}})},
  doi = {10.1109/RoEduNet.2015.7311974},
  author = {Denisleam Molomer, S. and {Trausan-Matu}, S. and Dessus, P. and Bianco, M.},
  month = sep,
  year = {2015},
  keywords = {computer aided instruction,Explanation,text analysis,text understanding,Reading,Decision support systems,explaining,explanation phases,learning activities,linguistics,mean pause duration,Pause,pausing behaviors,Praat tool,read sentences,reading phases,semiautomated analysis,silences,speech processing,spoken productions,story understanding,student reading performance,students fluency,students pauses,text reading level,uttered words,verbalizations},
  pages = {90-93},
  issn = {2068-1038}
}

@inproceedings{7333750,
  title = {Text Line Extraction in Document Images},
  abstract = {Text line extraction in document images is an important prerequisite for many content based image understanding applications. In this paper, we propose an accurate and robust method for generic text line extraction, which can be applied on large categories of document images, diverse languages, and text lines with different orientations. Firstly, the candidate connected components are extracted from document image using Maximal Stable Extremal Region (MSER) with the noises filtered by Adaboost and Convolution Neural Network (CNN). Then, the coarse text lines are generated from hierarchical edges reconstruction and cut by local linearity of text lines in the document spanning tree. Finally, for accurate text line extraction, the cut multi-components are re-connected based on text line energy minimization in terms of text line consistency and the fitting error. Experimental results on multilingual test dataset demonstrate the effectiveness and robust of the proposed method, which yields higher performance compared with state-of-the-art methods.},
  booktitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  doi = {10.1109/ICDAR.2015.7333750},
  author = {Wang, L. and Fan, W. and Sun, J. and Naoi, S. and Hiroshi, T.},
  month = aug,
  year = {2015},
  keywords = {learning (artificial intelligence),Robustness,feature extraction,Surveillance,neural nets,trees (mathematics),CNN,text analysis,document image processing,Image recognition,natural language processing,Image segmentation,Adaboost,Benchmark testing,candidate connected components,content based image understanding applications,convolution neural network,document images,document spanning tree,edge detection,fitting error,generic text line extraction,hierarchical edge reconstruction,hierarchical edge reconstruction and cut,image reconstruction,Integrated optics,maximal stable extremal region,MSER,multilingual test dataset,noise filtering,Optical imaging,text line consistency,text line energy minimization,text line extraction},
  pages = {191-195},
  note = {ISSN:}
}

@inproceedings{7333799,
  title = {Deep Learning Based Language and Orientation Recognition in Document Analysis},
  abstract = {In practical applications of document understanding, if the documents have multiple languages and orientations, the conventional OCR systems can not be directly applied. This is because those OCR systems are usually designed for texts of single language and normal orientation. To solve this problem, many non-character based recognition approaches were proposed. However, the performance of those methods were not comparable with the mature OCR systems. Consequently, a better idea is to recognize the language type and orientation before the OCR is applied. Besides, the characters of different languages have very ambiguous shape, so it is very difficult to extract stable feature for the recognition. Recently, the convolutional neural networks (CNN) have achieved great success in pattern recognition tasks. Therefore, for such difficult tasks, the CNN is one of the best choice. In this paper, we first applied CNN to the recognition of the document properties. A novel sliding window voting process is proposed to reduce the network scale and fully use the information of the text line. In the experiments, our method had very high recognition rate. The results proved the advantage of the proposed method and which also can be applied to create a document understanding system with OCR systems.},
  booktitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  doi = {10.1109/ICDAR.2015.7333799},
  author = {Chen, L. and Wang, S. and Fan, W. and Sun, J. and Satoshi, N.},
  month = aug,
  year = {2015},
  keywords = {deep learning,neural nets,CNN,convolutional neural networks,document image processing,Optical character recognition software,document analysis,image recognition,document properties recognition,document understanding system,Kernel,language recognition,orientation recognition,recognition rate,sliding window voting process,text detection,text line information},
  pages = {436-440},
  note = {ISSN:}
}

@article{750572,
  title = {Visualization of Neural-Network Gaps Based on Error Analysis},
  volume = {10},
  issn = {1045-9227},
  abstract = {Presents a methodology for detection of neural-network gaps (NNGs) based on error analysis and the visualization that is applicable to the n-dimensional I/O domain. The generalization problem in artificial neural networks (ANN) training is analyzed and the concept of NNGs is introduced. The NNGs are highly undesirable in ANN generalization and methods for detecting, analyzing, and eliminating them are necessary. Previous methods for NNG detection, based on two-dimensional (2-D) and three dimensional (3-D) visualization, were not applicable for ANNs with more than three inputs. Experiments demonstrate advantages of this new methodology, which allows better understanding of the NNG phenomena using a quantitative approach.},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/72.750572},
  author = {Kantardzic, M. M. and Aly, A. A. and Elmaghraby, A. S.},
  month = mar,
  year = {1999},
  keywords = {Computer networks,Performance evaluation,Testing,learning (artificial intelligence),Artificial neural networks,Visualization,neural nets,Two dimensional displays,Neural networks,visualization,Particle measurements,generalisation (artificial intelligence),error analysis,Error analysis,generalization problem,n-dimensional I/O domain,neural-network gaps,quantitative approach,Supervised learning},
  pages = {419-426}
}

@article{7552539,
  title = {Evaluating the {{Visualization}} of {{What}} a {{Deep Neural Network Has Learned}}},
  volume = {28},
  issn = {2162-237X},
  abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ``importance'' of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
  number = {11},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  doi = {10.1109/TNNLS.2016.2599820},
  author = {Samek, W. and Binder, A. and Montavon, G. and Lapuschkin, S. and M\"uller, K.},
  month = nov,
  year = {2017},
  keywords = {Algorithm design and analysis,learning (artificial intelligence),image classification,neural nets,Biological neural networks,interpretable machine learning,deep neural network,Learning systems,Neurons,Convolutional neural networks,data visualisation,complex machine learning tasks,data visualization,Deconvolution,deconvolution method,DNN,explaining classification,Heating,heatmap,ILSVRC2012,MIT Places data sets,multilayer nonlinear structure,relevance models,relevance propagation algorithm,Sensitivity,sensitivity-based approach,SUN397},
  pages = {2660-2673}
}

@inproceedings{7557899,
  title = {Why {{Visualization}} Is an {{AI}}-Complete {{Problem}} (and {{Why That Matters}})},
  abstract = {Artificial Intelligence (AI) has infiltrated almost every scientific and social endeavour, including everything from medical research to the sociology of crowd control. But the foundation of AI continues to be based on digital representations of knowledge, and computational reasoning therewith. Because so much of modern knowledge infrastructure and social behaviour is connected to AI, understanding the role of AI in each such endeavour not only helps accelerate progress in those fields in which it applies, but also creates the challenges to extend the foundation for modern AI methods. The simple hypothesis herein is that so-called AI-complete problems have a role in helping to articulate the appropriate integration of AI within other disciplines. With the current growth of interest in "big data" and visualization, we argue that relatively simple formal structures provide a basis for the claim that visualization is an AI-complete problem. The value of confirming this claim is largely to encourage stronger formalizations of the visualization process in terms of the AI foundations of representation and reasoning. This connection will help ensure that relevant components of AI are appropriately applied and integrated, to provide value for a basis of a theory of visualization. The sketch of this claim here is based on the simple idea that visualization is an abstraction process, and that abstractions from partial information, however voluminous, directly confronts the non monotonic reasoning challenge, thus the need for caution in engineering visualization systems without carefully considering the consequences of visual abstraction. This is particularly important with interactive visualization, which has recently formed the basis for such fields as visual analytics.},
  booktitle = {2016 20th {{International Conference Information Visualisation}} ({{IV}})},
  doi = {10.1109/IV.2016.53},
  author = {Goebel, R.},
  month = jul,
  year = {2016},
  keywords = {Big Data,data structures,Semantics,Complexity theory,Artificial intelligence,Visualization,artificial intelligence,data analysis,Context,visual analytics,data visualisation,Data visualization,visual abstraction,AI integration,AI methods,AI-complete problem,AI-complete visualization incomplete knowledge,Cognition,computational reasoning,digital knowledge representations,engineering visualization systems,formal structures,interactive visualization,modern knowledge infrastructure,monotonic reasoning challenge,partial information abstraction process,social behaviour,visualization process formalization},
  pages = {27-32},
  issn = {2375-0138}
}

@article{774103,
  title = {Symbolic Interpretation of Artificial Neural Networks},
  volume = {11},
  issn = {1041-4347},
  abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
  number = {3},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  doi = {10.1109/69.774103},
  author = {Taha, I. A. and Ghosh, J.},
  month = may,
  year = {1999},
  keywords = {Computer networks,Data mining,learning (artificial intelligence),Artificial neural networks,knowledge based systems,neural net architecture,feedforward neural nets,Neural networks,artificial neural networks,explanation,Knowledge representation,knowledge representation,Intelligent systems,adjustable detail level,binary rules,breast cancer data set,connectionist architecture,domain knowledge mapping,domain knowledge representation,embedded knowledge,explanation power,feedforward networks,Fuzzy neural networks,Fuzzy sets,hidden layer,hybrid intelligent systems,iris data set,Knowledge based systems,knowledge refinement,knowledge-based systems,Military computing,network training,output decision validation,partial rules,performance measures,rule evaluation technique,rule extraction,rule ordering,sigmoidal units,symbol manipulation,symbolic interpretation,truth maintenance},
  pages = {448-463}
}

@inproceedings{777686,
  title = {Distributed Knowledge-Based Parsing for Document Analysis and Understanding},
  abstract = {Document Analysis and Understanding (DAU) is a complex AI application with high industrial impact. For the increasing demands upon the bandwidth and quality of the analysis it is crucial to enable different analysis modules to collaborate. For making collaboration possible, we first examine the question of whether there exists a common ontological basis which can serve as a platform for communication of different DAU modules. Once communication is enabled, we investigate the second question, how DAU modules originally designed as stand-alone systems must be modified in order to benefit from collaboration with others.},
  booktitle = {Proceedings {{IEEE Forum}} on {{Research}} and {{Technology Advances}} in {{Digital Libraries}}},
  doi = {10.1109/ADL.1999.777686},
  author = {Klein, B. and Abecker, A.},
  month = may,
  year = {1999},
  keywords = {groupware,Data mining,Collaboration,Bandwidth,Ontologies,knowledge based systems,Artificial intelligence,Automata,Information analysis,Text analysis,Text recognition,document handling,analysis modules,collaboration,common ontological basis,complex AI application,DAU,DAU modules,distributed knowledge based parsing,document analysis and understanding,grammars,industrial impact,SGML},
  pages = {6-15},
  issn = {1092-9959}
}

@inproceedings{7801719,
  title = {Multi-Document {{Summarization}} by {{Creating Synthetic Document Vector Based}} on {{Language Model}}},
  abstract = {Multi-document summarization is to create summaries covering the major information that multiple documents tell in common. For this point, the existing methods are based on hand-crafted features for word and sentence. However, it is difficult to figure out the core contents of each document with the hand-crafted features because they have the limited information presented the given documents. Moreover, there exists a limit to figure out the major information because documents with the same meaning used to be paraphrased depending on their writers. Therefore, it is necessary to represent the semantic meanings of documents as well as sentences through understanding natural language. In this paper, we propose a new multi-document summarization system by creating a synthetic document vector covering the whole documents based on Language Model, whose is well-known for learning the semantic features in text. We experimented with DUC 2004 dataset provided by Document Understanding Conference (DUC) and the results show that our method summarizes multiple documents effectively based on their core contents.},
  booktitle = {2016 {{Joint}} 8th {{International Conference}} on {{Soft Computing}} and {{Intelligent Systems}} ({{SCIS}}) and 17th {{International Symposium}} on {{Advanced Intelligent Systems}} ({{ISIS}})},
  doi = {10.1109/SCIS-ISIS.2016.0132},
  author = {Kim, D. and Lee, J.},
  month = aug,
  year = {2016},
  keywords = {Computational modeling,Semantics,Redundancy,Context,Hidden Markov models,natural language processing,Natural languages,Intelligent systems,word processing,Core content,document handling,document understanding conference,DUC 2004 dataset,hand-crafted features,Language model,Major Information,Multi-document summarization,multidocument summarization system,natural language model,semantic document meanings,semantic text feature learning,synthetic document vector,Synthetic document vector},
  pages = {605-609},
  note = {ISSN:}
}

@inproceedings{7821768,
  title = {A Non-Biological {{AI}} Approach towards Natural Language Understanding},
  abstract = {The problem being addressed in this paper is that using brute force in Natural Language Processing and Machine Learning combined with advanced statistics will only approximate meaning and thus will not deliver in terms of real text understanding. Counting words and tracking word order or parsing by syntax will also result in probability and guesswork at best. Their vendors struggle in delivering accurate quality and this results in ill-functioning applications. The newer generation methodologies like Deep Learning and Cognitive Computing are breaking barriers in the (Big Data) fields of Internet of Things, Robotics and Image/Video Recognition but cannot be successfully deployed for text without huge amounts of training and sample data. In the short term, we believe non-biological Artificial Intelligence will produce the best results for text understanding. Miia applied advanced Linguistic and Semantic Technologies combined with ConceptNet modeling and Machine Learning to successfully cater deep intelligent and cross-language quality to several industries.},
  booktitle = {2016 {{Future Technologies Conference}} ({{FTC}})},
  doi = {10.1109/FTC.2016.7821768},
  author = {Stephen, L. and Geert, D. and Andreas, K. and Frank, P.},
  month = dec,
  year = {2016},
  keywords = {Internet of Things,Companies,Law,Big Data,learning (artificial intelligence),mobile computing,Artificial Intelligence,Engines,Semantics,Artificial intelligence,Machine Learning,deep learning,machine learning,Natural Language Processing,Data Mining,text analysis,advanced linguistic technologies,advanced statistics,Business Intelligence,cognitive computing,ConceptNet modeling,ConceptNet Modelling,cross-language quality,Data Analysis,image/video recognition,Linguistics,meaning approximation,natural language processing,natural language understanding,Natural Language Understanding,Natural languages,nonbiological AI approach,nonbiological artificial intelligence,robotics,semantic networks,semantic technologies,Sentiment Analysis,statistics,syntax parsing,text understanding,word counting,word order tracking},
  pages = {1300-1302},
  note = {ISSN:}
}

@inproceedings{7846290,
  title = {Quaternion {{Neural Networks}} for {{Spoken Language Understanding}}},
  abstract = {Machine Learning (ML) techniques have allowed a great performance improvement of different challenging Spoken Language Understanding (SLU) tasks. Among these methods, Neural Networks (NN), or Multilayer Perceptron (MLP), recently received a great interest from researchers due to their representation capability of complex internal structures in a low dimensional subspace. However, MLPs employ document representations based on basic word level or topic-based features. Therefore, these basic representations reveal little in way of document statistical structure by only considering words or topics contained in the document as a ``bag-of-words'', ignoring relations between them. We propose to remedy this weakness by extending the complex features based on Quaternion algebra presented in [1] to neural networks called QMLP. This original QMLP approach is based on hyper-complex algebra to take into consideration features dependencies in documents. New document features, based on the document structure itself, used as input of the QMLP, are also investigated in this paper, in comparison to those initially proposed in [1]. Experiments made on a SLU task from a real framework of human spoken dialogues showed that our QMLP approach associated with the proposed document features outperforms other approaches, with an accuracy gain of 2\% with respect to the MLP based on real numbers and more than 3\% with respect to the first Quaternion-based features proposed in [1]. We finally demonstrated that less iterations are needed by our QMLP architecture to be efficient and to reach promising accuracies.},
  booktitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  doi = {10.1109/SLT.2016.7846290},
  author = {Parcollet, T. and Morchid, M. and Bousquet, P. and Dufour, R. and Linar\`es, G. and De Mori, R.},
  month = dec,
  year = {2016},
  keywords = {Computational modeling,learning (artificial intelligence),multilayer perceptrons,Artificial neural networks,Machine Learning,machine learning,Natural language processing,Neural Network,natural language processing,Neurons,document handling,spoken language understanding,Algebra,Quaternions,algebra,document representations,document statistical structure,MLP,multilayer perceptron,Quaternion,quaternion algebra,quaternion neural networks,SLU,Spoken Language Understanding},
  pages = {362-368},
  note = {ISSN:}
}

@inproceedings{7846312,
  title = {Recurrent Convolutional Neural Networks for Structured Speech Act Tagging},
  abstract = {Spoken language understanding (SLU) is one of the important problem in natural language processing, and especially in dialog system. Fifth Dialog State Tracking Challenge (DSTC5) introduced a SLU challenge task, which is automatic tagging to speech utterances by two speaker roles with speech acts tag and semantic slots tag. In this paper, we focus on speech acts tagging. We propose local coactivate multi-task learning model for capturing structured speech acts, based on sentence features by recurrent convolutional neural networks. An experiment result, shows that our model outperformed all other submitted entries, and were able to capture coactivated local features of category and attribute, which are parts of speech act.},
  booktitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  doi = {10.1109/SLT.2016.7846312},
  author = {Ushio, T. and Shi, H. and Endo, M. and Yamagami, K. and Horii, N.},
  month = dec,
  year = {2016},
  keywords = {learning (artificial intelligence),pattern classification,Training,feedforward neural nets,neural networks,Neural networks,text analysis,Text categorization,text classification,Hidden Markov models,recurrent neural nets,natural language processing,Neurons,speech processing,Tagging,attribute feature,automatic tagging,category feature,dialog system,DSTC5,Fifth-Dialog State Tracking Challenge,local coactivate multitask learning model,multi-task learning,recurrent convolutional neural networks,semantic-slot tag,sentence feature,SLU challenge task,Speech,speech act tagging,speech utterances,speech-act tag,spoken language understanding,structured speech act tagging},
  pages = {518-524},
  note = {ISSN:}
}

@inproceedings{791799,
  title = {A {{Handwriting Understanding Environment}} ({{HUE}}) for Rapid Prototyping in Handwriting and Document Analysis Research},
  abstract = {HUE (the Handwriting Understanding Environment) is a software framework for handwriting and document analysis built around a two-level programming model in which components are implemented in a system programming language (typically C++) and are connected together into prototype systems using the scripting language Tcl Tk. HUE is an extended version of TABS (a previous handwriting analysis framework), and incorporates the authors' experience of using TABS for around 2 years in data-intensive handwriting and document analysis research and evaluation. HUE currently contains 94 C++ components, 7 native data types, 11 custom-built Tcl Tk packages, a novel dynamic user interface, and several demonstration systems implemented as Tcl scripts.},
  booktitle = {Proceedings of the {{Fifth International Conference}} on {{Document Analysis}} and {{Recognition}}. {{ICDAR}} '99 ({{Cat}}. {{No}}.{{PR00318}})},
  doi = {10.1109/ICDAR.1999.791799},
  author = {Cracknell, C. and Downton, A. C.},
  month = sep,
  year = {1999},
  keywords = {Libraries,Prototypes,document image processing,Text analysis,document analysis,Computer vision,Handwriting recognition,handwritten character recognition,C++ components,Computer languages,custom-built Tcl Tk packages,demonstration systems,Design engineering,dynamic user interface,handwriting analysis,Handwriting Understanding Environment,Image processing,native data types,object-oriented programming,Programming profession,rapid prototyping,software framework,software prototyping,system programming language,Systems engineering and theory,TABS,Tcl Tk scripting language,two-level programming model,user interfaces},
  pages = {362-365},
  note = {ISSN:}
}

@inproceedings{7960721,
  title = {Fast Text Classification with {{Naive Bayes}} Method on {{Apache Spark}}},
  abstract = {The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.},
  booktitle = {2017 25th {{Signal Processing}} and {{Communications Applications Conference}} ({{SIU}})},
  doi = {10.1109/SIU.2017.7960721},
  author = {O{\u g}ul, \.{I}. \"U. and \"Ozcan, C. and Hakda{\u g}l\i, \"O.},
  month = may,
  year = {2017},
  keywords = {distributed processing,Internet of Things,IoT,Big data,Machine learning,learning (artificial intelligence),pattern classification,Standards,social networking (online),machine learning,data analysis,public domain software,Text mining,text analysis,Text categorization,Apache Spark,Apache Spark architecture,Art,Bayes methods,Classification,distributed data structure,distributed in-memory data collection,fast data storage,fast text classification,Hadoop architecture,interpretation data,irrelevant data deletion,Java,meaning extraction,Naive Bayes,Naive Bayes method,open source social news site,Reddit,Sparks},
  pages = {1-4},
  note = {ISSN:}
}

@inproceedings{7982151,
  title = {Artificial Intelligence for {{DGA}} Interpretation Methods Using Weighting Factor},
  abstract = {The accuracy of conventional DGA interpretation methods can be different when each of these methods are used in different places or different circumstances. Rogers Ratio Method (RRM), IEC Ratio Method (IRM) (Basic Gas Ratios Method), GB/T 7252 (National Standard of the People's Republic of China) are popular conventional methods for interpreting the possible faults indicator of transformer in Indonesia and China. This research proposes artificial intelligence to interpret DGA by combining conventional method and artificial intelligence method using weighting factor. The artificial intelligence method which are used in this research is fuzzy logic. DGA practical data which used as refer data for data mining in this research were taken from China and Indonesia. This research also uses Thompson tau method to filter the data from outlier and fuzzy c means clustering to cluster the data to make sure the data are used is valid and good enough to be used to build artificial intelligence through data mining. The output of this research is to create an artificial intelligence and the combination between artificial intelligence which have been built with conventional method to interpret DGA whether there is any fault in transformer or not.},
  booktitle = {2017 1st {{International Conference}} on {{Electrical Materials}} and {{Power Equipment}} ({{ICEMPE}})},
  doi = {10.1109/ICEMPE.2017.7982151},
  author = {Subroto, C. and Zhang, G.},
  month = may,
  year = {2017},
  keywords = {Data mining,power engineering computing,data mining,Artificial intelligence,artificial intelligence,fuzzy logic,Fuzzy logic,Fuzzy Logic,fuzzy set theory,basic gas ratios method,DGA,DGA interpretation methods,Discharges (electric),Fuzzy C Means,fuzzy c means clustering,IEC Ratio Method,IRM,Partial discharges,Power transformer insulation,power transformers,Rogers ratio method,RRM,Thompson tau method,transformer faults indicator,weighting factor,Weighting factor},
  pages = {85-88},
  note = {ISSN:}
}

@inproceedings{8047390,
  title = {Category {{Classification}} of {{Text Data}} with {{Machine Learning Technique}} for {{Visualizing Flow}} of {{Conversation}} in {{Counseling}}},
  abstract = {The beginner counselors have more likely to continue counseling in their own interest, they have a high tendency to make great use of the closed-ended question in order to confirm the interpretation with the client. While expert counselors are instructing the counseling skill to beginner counselors, we consider that the reaction of a client for a beginner counselor's question is important to visualize in an appropriate method. To respond the request, we have developed a system for visualizing the flow of conversation in counseling. However, the expert counselor as the system user requires to correct the initial classification result manually, and the work burden is large, because the accuracy of the category classification of conversation data is very low in the current system. To improve this problem, we have implemented on the category classification method of text data with SVM (Support Vector Machine) as machine learning technique to visualize the flow of conversation in counseling. In addition, we have compared and evaluated with results of the initial classification method of the current system. As these results, we have shown that the accuracy rate of the classification method with SVM become higher than the results in the current system.},
  booktitle = {2017 {{Nicograph International}} ({{NicoInt}})},
  doi = {10.1109/NICOInt.2017.35},
  author = {Hayashida, Y. and Uetsuji, T. and Ebara, Y. and Koyamada, K.},
  month = jun,
  year = {2017},
  keywords = {Data models,learning (artificial intelligence),pattern classification,support vector machines,Machine Learning,Visualization,Support vector machines,machine learning,text analysis,data visualisation,category classification,conversation flow visualization,counseling,Counseling,Data visualization,Dictionaries,Employee welfare,Employment,psychology,Psychology,support vector machine,SVM,Text Classification,text data},
  pages = {37-40},
  note = {ISSN:}
}

@inproceedings{8128175,
  title = {{{MOSIS}} \textemdash{} {{Multi}}-Outcrop Sharing Interpretation System},
  abstract = {The use of LiDAR and multiples digital images jointly with 3-D reconstruction techniques for creating 3-D models of natural outcrops and surfaces studies have increased dramatically in the last few years. These techniques have provided an enormous amount of data for interpretation by geoscientists. However, these researchers have no available software capable of offering a user experience comparable to the fieldwork. The majority of solutions have considered desktop systems, which presents inherent limitations due to the 2-D characteristics of displays and loss of immersion into the 3-D model, or up until expensive and complex stereoscopic based approaches to improve the 3-D user experience do not offer well suitable solutions. To address these limitations, this paper presents a low-cost completely disruptive solution for processing, visualizing, sharing and directly handling Digital Outcrop Models with the support of a full interpretation toolset, the MOSIS System. The proposed system provides a fully immersive computational environment, capable of teleporting virtually geoscientists to the fieldwork, giving an awareness of being there physically with an extensible toolset for the DOM's interpretation. Besides, desktop, web and mobile versions of MOSIS have been under development and fulfill the lack of tools for digital outcrop modeling.},
  booktitle = {2017 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  doi = {10.1109/IGARSS.2017.8128175},
  author = {Gonzaga, L. and Veronez, M. R. and Alves, D. N. and Bordin, F. and Kannenberg, G. L. and Marson, F. P. and Tognoli, F. M. W. and Inocencio, L. C.},
  month = jul,
  year = {2017},
  keywords = {Standards,Visualization,Tools,Three-dimensional displays,interpretation,data visualisation,Data visualization,image reconstruction,Geology,Solid modeling,radar imaging,3-D model,3-D reconstruction techniques,3-D visualization,complex stereoscopic based approaches,digital outcop model (DOM),digital outcrop modeling,digital outcrop models,DOM interpretation,fully immersive computational environment,geophysical techniques,GPU Computing,immersive visualization,interpretation toolset,MOSIS,MOSIS System,multioutcrop sharing multiples digital images,natural outcrops,optical radar,stereo image processing,virtual outcrop},
  pages = {5209-5212},
  issn = {2153-7003}
}

@inproceedings{8215791,
  title = {Development of an {{Interpretable Neural Network Model}} for {{Creation}} of {{Polarity Concept Dictionaries}}},
  abstract = {Automatic creation of polarity dictionaries is an important issue, as explanations of prediction models are often required in the financial industry. This paper proposes a novel method of developing an interpretable and predictable neural network model. The neural network model we built can extract polarity scores of concepts from documents. Furthermore, we can detect pairwise interactions between concepts, and create polarity concept dictionaries using our neural network model. The model was built using vector representations of words and polarity scores for about 100 words provided by financial professionals, and we obtained about a hundred times more polarity scores for unknown words through backpropagation. First, we analyze the properties of our method from a theoretical point of view. We then confirm its capabilities by conducting simulations of assigning polarity scores to unknown words and detecting interactions using artificial data. We subsequently estimate sentiment tags using real financial textual datasets. Compared with other conventional methods, the proposed approach can forecast sentiments with higher F1 scores. Finally, we develop a polarity concept dictionary based on Yahoo! Finance board textual data.},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  doi = {10.1109/ICDMW.2017.159},
  author = {Ito, T. and Sakaji, H. and Izumi, K. and Tsubouchi, K. and Yamashita, T.},
  month = nov,
  year = {2017},
  keywords = {financial data processing,Data mining,Industries,data mining,Sentiment analysis,Artificial neural networks,Predictive models,Analytical models,neural nets,text analysis,natural language processing,Dictionaries,automatic creation,backpropagation,detecting interactions,dictionaries,financial industry,interpretable network model,interpretable neural network model,Neural Network Model,polarity concept dictionaries,polarity dictionaries,polarity scores,predictable neural network model,prediction models,Text-mining,unknown words,vector representations,Yahoo! Finance board textual data},
  pages = {1122-1131},
  issn = {2375-9259}
}

@inproceedings{8256457,
  title = {An Approach to Mental Image Based Understanding of Natural Language: {{Focused}} on Static and Dynamic Spatial Relations},
  abstract = {It must be rather difficult for ordinary people to communicate with robots using special technical languages. Therefore, it must be more desirable for them to use natural language (NL) for such a purpose because it is the most conventional among them. This work proposes a methodology for natural language understanding through an AI system named Conversation Management System (CMS) based on Mental Image Directed Semantic Theory proposed by M. Yokota. CMS is intended to enable a robot to understand NL in the same way as people do, and actually can reach the most plausible semantic interpretation of an input text and return desirable outcomes by employing word concepts, postulates, and inference rules. Recently, the authors have applied several spatial terms in English language, for example verbs, prepositions (e.g. between, along, left, right, and so on). We found that the methodology is outstanding from conventional approaches with the attempt to provide robots understand NL based on mental image model. This paper focuses on how CMS understands static spatial (3D) relations expressed in NL.},
  booktitle = {2017 {{IEEE}} 8th {{International Conference}} on {{Awareness Science}} and {{Technology}} ({{iCAST}})},
  doi = {10.1109/ICAwST.2017.8256457},
  author = {Khummongkol, R. and Yokota, M.},
  month = nov,
  year = {2017},
  keywords = {Conferences,Semantics,Artificial intelligence,Robots,artificial intelligence,human-robot interaction,semantic interpretation,natural language processing,natural language understanding,Natural languages,NL,Cognition,AI system,CMS,Conversation Management System,dynamic spatial relations,English language,human — robot interaction,mental image based understanding,Mental Image Directed Semantic Theory,mental image model,Rivers,robot communication,spatial terms,special technical languages,static relations,static spatial 3D relations},
  pages = {254-259},
  issn = {2325-5994}
}

@inproceedings{8258557,
  title = {A Study on Interpretability of Decision of Machine Learning},
  abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2017.8258557},
  author = {Shirataki, S. and Yamaguchi, S.},
  month = dec,
  year = {2017},
  keywords = {Big Data,learning (artificial intelligence),pattern classification,support vector machines,big data analysis,Predictive models,Analytical models,Training,Tools,Support vector machines,machine learning,data analysis,Support Vector Machine,interpretability,text analysis,SVM,black boxes,decision reasons,DVD,highly weighted words,opinions classification},
  pages = {4830-4831},
  note = {ISSN:}
}

@inproceedings{8260658,
  title = {{{HDLTex}}: {{Hierarchical Deep Learning}} for {{Text Classification}}},
  abstract = {Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
  booktitle = {2017 16th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  doi = {10.1109/ICMLA.2017.0-134},
  author = {Kowsari, K. and Brown, D. E. and Heidarysafa, M. and Jafari Meimandi, K. and Gerber, M. S. and Barnes, L. E.},
  month = dec,
  year = {2017},
  keywords = {Computer architecture,Machine learning,learning (artificial intelligence),pattern classification,data mining,Mathematical model,neural nets,Recurrent neural networks,Support vector machines,Deep Learning,Text Mining,supervised learning,text analysis,Kernel,deep learning architectures,Deep Neural Networks,Document Classification,document classification methods,document collections,document hierarchy,HDLTex,hierarchical classification,Hierarchical Deep,Hierarchical Learning,information processing methods,multiclass classification,supervised classifiers,Text classification,text organization},
  pages = {364-371},
  note = {ISSN:}
}

@inproceedings{8268978,
  title = {Deep Quaternion Neural Networks for Spoken Language Understanding},
  abstract = {Deep Neural Networks (DNN) received a great interest from researchers due to their capability to construct robust abstract representations of heterogeneous documents in a latent subspace. Nonetheless, mere real-valued deep neural networks require an appropriate adaptation, such as the convolution process, to capture latent relations between input features. Moreover, real-valued deep neural networks reveal little in way of document internal dependencies, by only considering words or topics contained in the document as an isolate basic element. Quaternion-valued multi-layer perceptrons (QMLP), and autoencoders (QAE) have been introduced to capture such latent dependencies, alongside to represent multidimensional data. Nonetheless, a three-layered neural network does not benefit from the high abstraction capability of DNNs. The paper proposes first to extend the hyper-complex algebra to deep neural networks (QDNN) and, then, introduces pre-trained deep quaternion neural networks (QDNN-AE) with dedicated quaternion encoder-decoders (QAE). The experiments conduced on a theme identification task of spoken dialogues from the DECODA data set show, inter alia, that the QDNN-AE reaches a promising gain of 2.2\% compared to the standard real-valued DNN-AE.},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  doi = {10.1109/ASRU.2017.8268978},
  author = {Parcollet, T. and Morchid, M. and Linar\`es, G.},
  month = dec,
  year = {2017},
  keywords = {learning (artificial intelligence),multilayer perceptrons,neural network,Task analysis,Biological neural networks,machine learning,deep neural networks,natural language processing,Speech,spoken language understanding,Algebra,autoencoders,deep quaternion neural networks,document internal dependencies,QAE,QMLP,quaternion-valued multilayer perceptrons,Quaternions,real-valued deep neural networks,speech recognition,spoken dialogues,Telephone sets},
  pages = {504-511},
  note = {ISSN:}
}

@inproceedings{8275810,
  title = {Discriminative {{Topic Sparse Representation}} for {{Text Categorization}}},
  volume = {1},
  abstract = {In text categorization, feature representation for dimensionality reduction is a key step. Usually, some commonly used methods, e.g., latent semantic analysis (LSA), yield a dense representation or a dense transformation matrix, which is difficult to precisely characterize the document-topic or the topic-word relationship. This paper proposes a novel discriminative topic sparse representation (DTSR) approach for text categorization, in which two stages are included: the topic dictionary construction and sparse representation. Firstly, a discriminative and interpretable dictionary is constructed to characterize the topic-word relationship. The dictionary contains all category center vectors as well as some semantic topic vectors generated by a latent Dirichlet allocation (LDA) model. Furthermore, each document can be represented with a sparse form to obtain a good document-topic relationship. Experimental results on well-known benchmark datasets indicate that the proposed method not only achieves a satisfactory classification performance but also provides a reasonable sparse semantic meaningful.},
  booktitle = {2017 10th {{International Symposium}} on {{Computational Intelligence}} and {{Design}} ({{ISCID}})},
  doi = {10.1109/ISCID.2017.54},
  author = {Zheng, W. and Liu, Y. and Lu, H. and Tang, H.},
  month = dec,
  year = {2017},
  keywords = {Resource management,learning (artificial intelligence),pattern classification,Semantics,Training,text analysis,text categorization,Text categorization,Dictionaries,dense transformation matrix,dimensionality reduction,Discriminative,discriminative dictionary,discriminative topic sparse representation approach,document-topic relationship,feature representation,interpretable dictionary,Large scale integration,latent Dirichlet allocation model,latent semantic analysis,Semantic,semantic topic vectors,Sparse matrices,Sparse Representation,Text Categorization,Topic,topic dictionary construction,topic-word relationship,vectors},
  pages = {454-457},
  issn = {2473-3547}
}

@inproceedings{8276047,
  title = {Onto-Based Sentiment Classification Using Machine Learning Techniques},
  abstract = {Sentiment analysis is a methodology used to analyse the emotion or view of an individual to a situation or topic. In present scenario, Social media is the source for the collection of individual's feedbacks, user's emotions, reviews and personal experiences which lead to a need for efficient mining of the text to derive knowledge. An optimal classification of text based on emotion is an unsolved problem in text mining. To extract knowledge from text many machine learning tools and techniques were proposed. An onto-based process is proposed to analyse the customer's emotion in this paper. The input emotional text that needs to be classified is given as input to the NLP and processed and an emotional ontology is created for better understanding of the semantics and relationships. When adding new instances, Ontology can be automatically classify them based on emotional relationship. The Emowords from ontology can be further classified using any of the standard machine learning techniques which definitively gives a better performance. This paper is a review of all the machine learning techniques that can be applied on the semantic analysis of sentiments.},
  booktitle = {2017 {{International Conference}} on {{Innovations}} in {{Information}}, {{Embedded}} and {{Communication Systems}} ({{ICIIECS}})},
  doi = {10.1109/ICIIECS.2017.8276047},
  author = {Saranya, K. and Jayanthy, S.},
  month = mar,
  year = {2017},
  keywords = {ontology,learning (artificial intelligence),pattern classification,data mining,Sentiment analysis,Ontologies,ontologies (artificial intelligence),knowledge based systems,Semantics,Social network services,social networking (online),Support vector machines,machine learning,Text mining,NLP,text analysis,natural language processing,text mining,customer,emotion recognition,emotional ontology,emotional relationship,Emowords,feedbacks,input emotional text,machine learning tools,Onto-based sentiment classification,optimal classification,semantics,sentiment analysis,sentimental analysis,Social media,standard machine learning techniques},
  pages = {1-5},
  note = {ISSN:}
}

@inproceedings{8308186,
  title = {Understanding of a Convolutional Neural Network},
  abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
  booktitle = {2017 {{International Conference}} on {{Engineering}} and {{Technology}} ({{ICET}})},
  doi = {10.1109/ICEngTechnol.2017.8308186},
  author = {Albawi, S. and Mohammed, T. A. and {Al-Zawi}, S.},
  month = aug,
  year = {2017},
  keywords = {Feature extraction,learning (artificial intelligence),deep learning,image classification,machine learning,feedforward neural nets,convolutional neural network,artificial neural networks,CNN,convolutional neural networks,computer vision,Image recognition,natural language processing,Neurons,Convolutional neural networks,Convolution,Image edge detection,convolutional connected layers,Artificial Neural Networks,artificial neural network,classical methods performance,Deep Neural Network,deeper hidden layers,fully-connected layers,image data,largest image classification data,mathematical linear operation,matrixes called convolution,multilayers,multiple layers,nonlinearity layer,pooling,term Deep Learning},
  pages = {1-6},
  note = {ISSN:}
}

@inproceedings{8310088,
  title = {Deep Learning for Automatic Sale Receipt Understanding},
  abstract = {As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.},
  booktitle = {2017 {{Seventh International Conference}} on {{Image Processing Theory}}, {{Tools}} and {{Applications}} ({{IPTA}})},
  doi = {10.1109/IPTA.2017.8310088},
  author = {{Raoui-Outach}, R. and {Million-Rousseau}, C. and Benoit, A. and Lambert, P.},
  month = nov,
  year = {2017},
  keywords = {Deep learning,Machine learning,learning (artificial intelligence),Semantics,Task analysis,data analysis,feedforward neural nets,text analysis,document image processing,Text analysis,Character recognition,Optical character recognition software,Object detection,accurate consumption statistics,automatic sale receipt understanding,automatic understanding,classical image,data analytics,Deep Convolutional Neural Networks,double check processing,essential consumption statistics,essential information,high confidence level,highest possible confidence,industrial context,Object Detection,paper damages,purchased products,Receipt image understanding,related prices,sales management,scanned document analysis,scanning quality,Semantic Analysis,smart-phone,store brand,text processings,tool chain,tool-chain},
  pages = {1-6},
  issn = {2154-512X}
}

@inproceedings{8320258,
  title = {Extractive {{Text Summarization Using Word Vector Embedding}}},
  abstract = {These days, text summarization is an active research field to identify the relevant information from large documents produced in various domains such as finance, news media, academics, politics, etc. Text summarization is the process of shortening the documents by preserving the important contents of the text. This can be achieved through extractive and abstractive summarization. In this paper, we have proposed an approach to extract a good set of features followed by neural network for supervised extractive summarization. Our experimental results on Document Understanding Conferences 2002 dataset show the effectiveness of the proposed method against various online extractive text summarizers.},
  booktitle = {2017 {{International Conference}} on {{Machine Learning}} and {{Data Science}} ({{MLDS}})},
  doi = {10.1109/MLDS.2017.12},
  author = {Jain, A. and Bhatia, D. and Thakur, M. K.},
  month = dec,
  year = {2017},
  keywords = {Data mining,Computational modeling,Testing,Feature extraction,Mathematical model,Training,Machine Learning,Neural networks,Neural Network,text analysis,abstractive summarization,active research field,Document Understanding Conferences 2002 dataset,Extractive Text Summarization,news media,online extractive text summarizers,supervised extractive summarization,word vector,Word Vector Embedding},
  pages = {51-55},
  note = {ISSN:}
}

@inproceedings{8332874,
  title = {A {{Semantic Text Similarity Model}} for {{Double Short Chinese Sequences}}},
  abstract = {Semantic Text Similarity plays a major role in natural language processing. In recent years, researchers have paid considerable attention to Semantic Text Similarity. Some breakthroughs have been made in English, but there are two disadvantages when these models are applied to Chinese: Single sequence models don't consider semantic ambiguity such as polysemy, synonym; these models don't consider that Chinese stop words are important for Chinese word segmentation, voice analysis, semantic understanding. Firstly, in order to overcome the first problem, we proposed the double short text sequences model that has two identical LSTM (Long Short-Term Memory) processing two text sequences at the same time. Secondly, in order to overcome the second problem, according to the characteristics of Chinese, we used the Chinese semantic similarity data sets designed by experts to train and test the model, and retained the stop words in the model training process. Finally, the proposed model was compared with the Semantic Text Similarity model based on CNN (Convolution Neural Network) and the Baidu Semantic Text Similarity model. The results show that the model is greater than the previous two in terms of accuracy, recall rate and so on, and the generalization ability is improved also.},
  booktitle = {2018 {{International Conference}} on {{Intelligent Transportation}}, {{Big Data Smart City}} ({{ICITBS}})},
  doi = {10.1109/ICITBS.2018.00190},
  author = {Shancheng, T. and Yunyue, B. and Fuyu, M.},
  month = jan,
  year = {2018},
  keywords = {Data models,Computational modeling,learning (artificial intelligence),Semantics,Analytical models,Training,deep learning,Encyclopedias,feedforward neural nets,CNN,text analysis,natural language processing,Training data,convolution neural network,Baidu semantic text similarity model,Chinese semantic similarity data sets,Chinese short text,Chinese stop words,Chinese word segmentation,double sequence,double short Chinese sequences,double short text sequences model,Long Short-Term Memory,semantic ambiguity,Semantic similarity,semantic understanding,single sequence models},
  pages = {736-739},
  note = {ISSN:}
}

@inproceedings{8356909,
  title = {A {{Proposal}} of {{Visualization Method}} for {{Critical Area}} in {{Computer Go}}},
  abstract = {Deep Learning for the game of Go recently had a tremendous success with the victory of AlphaGo against Ke Jie in May 2017. However, there is no clear understanding of why they perform so well. In this paper, we introduce a visualization technique that performs a sensitivity analysis of the classifier output by occluding portions of the input Go board, revealing which parts of the board are important for predicting the next move. Using this tool, we start with the experiment about the accuracy of the critical area revealed. We also suppose that by showing the critical area, it will allow Go beginners to understand the board visually that they may have been confused about.},
  booktitle = {2017 {{Conference}} on {{Technologies}} and {{Applications}} of {{Artificial Intelligence}} ({{TAAI}})},
  doi = {10.1109/TAAI.2017.42},
  author = {Pang, Y. and Ito, T.},
  month = dec,
  year = {2017},
  keywords = {Machine learning,learning (artificial intelligence),pattern classification,Training,deep learning,Visualization,sensitivity analysis,computer games,Deep Learning,data visualisation,Data visualization,AlphaGo,classifier output,computer go,Computer Go,critical area,Go game,input Go board,Ke Jie,visualization method},
  pages = {62-65},
  issn = {2376-6824}
}

@inproceedings{8365991,
  title = {Visualizing {{Deep Neural Networks}} for {{Text Analytics}}},
  abstract = {Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.},
  booktitle = {2018 {{IEEE Pacific Visualization Symposium}} ({{PacificVis}})},
  doi = {10.1109/PacificVis.2018.00031},
  author = {Nie, S. and Healey, C. and Padia, K. and {Leeman-Munk}, S. and Benson, J. and Caira, D. and Sethi, S. and Devarajan, R.},
  month = apr,
  year = {2018},
  keywords = {Computational modeling,learning (artificial intelligence),deep learning,Visualization,Task analysis,Biological neural networks,machine learning,feedforward neural nets,deep neural networks,text analysis,natural language processing,Neurons,Convolutional neural networks,data visualisation,clustering techniques,convolutional connected layers,convolutional layers,DNN models,DNNs,fully connected layers,human centered computing,information visualization,interaction techniques,internal activation patterns,network activation,neural network researchers,speech models,summarization techniques,text analytics,TNNVis,training patterns,visual encodings,visualization design,visualization system},
  pages = {180-189},
  issn = {2165-8773}
}

@inproceedings{8371950,
  title = {Converting {{Diagrams}}, {{Formulas}}, {{Tables}}, {{Graphics}} and {{Pictures}} into {{SPN}} and {{NL}}-Text {{Sentences}} for {{Automatic Deep Understanding}} of {{Technical Documents}}},
  abstract = {Most of the technical documents are composed by several modalities, like diagrams, tables, formulas, graphics, pictures and natural language text. Each of these modalities and their associations significantly contribute to the overall deep understanding of the technical document and the knowledge represented in it. Here for us all these modalities, except NL text, are considered as "images". Thus, each technical document mainly is composed by NL text sentences and "images". Thus, in this paper we present a methodology where all these modalities can be expressed into the same two modalities (natural languages text sentences and SPN graphs) for better associations and deeper understanding of a technical document. This deeper understanding will come from two different contributions. The first unique contribution will be an enrichment of the NL text part with additional NL text sentences extracted from the "images" of the technical document. The second unique contribution will come from the SPM models of these images that enrich the main diagram by generating a simulator for the system that technical document describes.},
  booktitle = {2017 {{IEEE}} 29th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  doi = {10.1109/ICTAI.2017.00047},
  author = {Bourbakis, N.},
  month = nov,
  year = {2017},
  keywords = {Databases,Visualization,stochastic processes,text analysis,document image processing,Graphics,Image recognition,Text recognition,natural language processing,Natural languages,Shape,graph theory,knowledge representation,Diagrams,Formulas,natural languages text sentences,NL text sentences,NL text Sentences,Petri nets,Pictures,SPN,SPN graphs,stochastic Petri-net forms,Tables,technical document,Technical Documents},
  pages = {247-254},
  issn = {2375-0197}
}

@article{8399509,
  title = {Spatial {{Sequential Recurrent Neural Network}} for {{Hyperspectral Image Classification}}},
  volume = {11},
  issn = {1939-1404},
  abstract = {In hyperspectral image processing, classification is one of the most popular research topics. In recent years, research progress made in deep-learning-based hierarchical feature extraction and classification has shown a great power in many applications. In this paper, we propose a novel local spatial sequential (LSS) method, which is used in a recurrent neural network (RNN). Using this model, we can extract local and semantic information for hyperspectral image classification. First, we extract low-level features from hyperspectral images, including texture and differential morphological profiles. Second, we combine the low-level features together and propose a method to construct the LSS features. Afterwards, we build an RNN and use the LSS features as the input to train the network for optimizing the system parameters. Finally, the high-level semantic features generated by the RNN is fed into a softmax layer for the final classification. In addition, a nonlocal spatial sequential method is presented for the recurrent neural network model (NLSS-RNN) to further enhance the classification performance. NLSS-RNN finds nonlocal similar structures to a given pixel and extracts corresponding LSS features, which not only preserve the local spatial information, but also integrate the information of nonlocal similar samples. The experimental results on three publicly accessible datasets show that our proposed method can obtain competitive performance compared with several state-of-the-art classifiers.},
  number = {11},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  doi = {10.1109/JSTARS.2018.2844873},
  author = {Zhang, X. and Sun, Y. and Jiang, K. and Li, C. and Jiao, L. and Zhou, H.},
  month = nov,
  year = {2018},
  keywords = {Computer architecture,Deep learning,Machine learning,Feature extraction,learning (artificial intelligence),feature extraction,image classification,Recurrent neural networks,recurrent neural nets,Feedforward neural networks,deep-learning-based hierarchical feature extraction,high-level semantic feature,high-level semantic features,hyperspectral image (HSI) classification,hyperspectral image classification,hyperspectral image processing,hyperspectral images,Hyperspectral imaging,local information,local spatial information,local spatial sequential method,low-level feature,low-level features,LSS feature extraction,NLSS-RNN,nonlocal spatial sequential method,popular research topics,recurrent neural network (RNN),recurrent neural network model,semantic information,spatial sequential recurrent neural network},
  pages = {4141-4155}
}

@article{8432512,
  title = {Hyperspectral {{Unmixing}} via {{Deep Convolutional Neural Networks}}},
  volume = {15},
  issn = {1545-598X},
  abstract = {Hyperspectral unmixing (HU) is a method used to estimate the fractional abundances corresponding to endmembers in each of the mixed pixels in the hyperspectral remote sensing image. In recent times, deep learning has been recognized as an effective technique for hyperspectral image classification. In this letter, an end-to-end HU method is proposed based on the convolutional neural network (CNN). The proposed method uses a CNN architecture that consists of two stages: the first stage extracts features and the second stage performs the mapping from the extracted features to obtain the abundance percentages. Furthermore, a pixel-based CNN and cube-based CNN, which can improve the accuracy of HU, are presented in this letter. More importantly, we also use dropout to avoid overfitting. The evaluation of the complete performance is carried out on two hyperspectral data sets: Jasper Ridge and Urban. Compared with that of the existing method, our results show significantly higher accuracy.},
  number = {11},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  doi = {10.1109/LGRS.2018.2857804},
  author = {Zhang, X. and Sun, Y. and Zhang, J. and Wu, P. and Jiao, L.},
  month = nov,
  year = {2018},
  keywords = {Feature extraction,learning (artificial intelligence),Artificial neural networks,feature extraction,Indexes,deep learning,image classification,convolutional neural network,recurrent neural nets,Convolution,hyperspectral image classification,Hyperspectral imaging,Kernel,CNN architecture,Convolutional neural networks (CNNs),deep convolutional neural networks,end-to-end HU method,end-to-end model,hyperspectral data sets,hyperspectral imaging,hyperspectral remote sensing image,hyperspectral unmixing,Jasper Ridge dataset,pixel-based CNN,spectral unmixing,spectral–spatial information,Urban dataset},
  pages = {1755-1759}
}

@article{8440085,
  title = {{{RuleMatrix}}: {{Visualizing}} and {{Understanding Classifiers}} with {{Rules}}},
  volume = {25},
  issn = {1077-2626},
  abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2018.2864812},
  author = {Ming, Y. and Qu, H. and Bertini, E.},
  month = jan,
  year = {2019},
  keywords = {Data models,Machine learning,learning (artificial intelligence),pattern classification,Visualization,Support vector machines,Neural networks,explainable machine learning,visual analytics,data visualisation,interactive systems,Data visualization,knowledge representation,Decision trees,black-box model,interactive visualization technique,machine learning systems,matrix algebra,matrix-based visualization,predictive models,rule matrix,rule visualization,standardized rule-based knowledge representation},
  pages = {342-352}
}

@article{8440091,
  title = {Manifold: {{A Model}}-{{Agnostic Framework}} for {{Interpretation}} and {{Diagnosis}} of {{Machine Learning Models}}},
  volume = {25},
  issn = {1077-2626},
  abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2018.2864499},
  author = {Zhang, J. and Wang, Y. and Molino, P. and Li, L. and Ebert, D. S.},
  month = jan,
  year = {2019},
  keywords = {performance analysis,Data models,Machine learning,Computational modeling,learning (artificial intelligence),Analytical models,Visualization,neural nets,Task analysis,data analysis,deep neural networks,interpretation,explanation,classification tasks,customizable tabular view,data visualisation,debugging,diagnosis process,feature discrimination,generic framework,inspection,Interactive machine learning,internal logic,iterative methods,machine learning models,manifold,Manifolds,model comparison,model debugging,model development,model-agnostic framework,models outcome,potential machine learning,refinement,regression analysis,regression tasks,scatterplot-based visual summary,specific model type,visual analysis techniques},
  pages = {364-373}
}

@article{8440842,
  title = {{{RetainVis}}: {{Visual Analytics}} with {{Interpretable}} and {{Interactive Recurrent Neural Networks}} on {{Electronic Medical Records}}},
  volume = {25},
  issn = {1077-2626},
  abstract = {We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2018.2865027},
  author = {Kwon, B. C. and Choi, M. and Kim, J. T. and Choi, E. and Kim, Y. B. and Kwon, S. and Sun, J. and Choo, J.},
  month = jan,
  year = {2019},
  keywords = {Data models,Machine learning,Computational modeling,Healthcare,Predictive models,medical information systems,electronic medical records,artificial intelligence,Task analysis,data analysis,black-box nature,recurrent neural nets,data visualisation,artificial intelligence scientists,design study,EMR data,increase interactivity,individual medical codes,Interactive Artificial Intelligence,interactive recurrent neural networks,interactive RNN-based model,interactive systems,interactive visual analytic tool,interactively leverage users,interpretable analytics tool,Interpretable Deep Learning,interpretable networks,iterative design process,Medical diagnostic imaging,medical experts,newly improved RNN-based model,prediction tasks,RetainVis,risk predictions,RNN-based model,temporal information,visual analytic researchers,Visual analytics,visual analytics solution,XAI (Explainable Artificial Intelligence)},
  pages = {299-309}
}

@article{8466590,
  title = {Peeking {{Inside}} the {{Black}}-{{Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  volume = {6},
  issn = {2169-3536},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  journal = {IEEE Access},
  doi = {10.1109/ACCESS.2018.2870052},
  author = {Adadi, A. and Berrada, M.},
  year = {2018},
  keywords = {Machine learning,Conferences,Market research,Biological system modeling,artificial intelligence,Machine learning algorithms,Explainable artificial intelligence,interpretable machine learning,explainable artificial intelligence,explainable AI,black-box models,XAI,AI-based systems,black-box nature,fourth industrial revolution,Prediction algorithms},
  pages = {52138-52160}
}

@inproceedings{8468758,
  title = {The {{Construction}} of {{Undergraduate Machine Learning Course}} in the {{Artificial Intelligence Era}}},
  abstract = {Machine learning technology has been greatly developed in the last decade, which makes artificial intelligence reach a revolutionary breakthrough and lets us really perceive the potential of artificial intelligence in changing human life. In order to improve the understanding and application ability of artificial intelligence, carrying out the corresponding machine learning course is of significance for the students during the undergraduate period. This paper probes into the teaching content, teaching form and other aspects of the undergraduate machine learning course based on this issue and proposes a teaching method driven by application scenarios to guide the undergraduate students to understand the development, current situation and frontier technology of machine learning. In the experimental design, the students' theoretical knowledge is fully considered, the practical questions are simplified, and the students' ability to think and solve problems is also raised, so as to lay a theoretical and practical basis for further study of machine learning.},
  booktitle = {2018 13th {{International Conference}} on {{Computer Science Education}} ({{ICCSE}})},
  doi = {10.1109/ICCSE.2018.8468758},
  author = {Sun, W. and Gao, X.},
  month = aug,
  year = {2018},
  keywords = {Machine learning,learning (artificial intelligence),artificial intelligence,machine learning,Machine learning algorithms,computer aided instruction,educational courses,Education,further education,Prediction algorithms,Classification algorithms,artificial intelligence era,computer science education,Decision trees,machine learning course,machine learning technology,teaching,teaching content,teaching form,undergraduate,undergraduate machine learning course,undergraduate period,undergraduate students},
  pages = {1-5},
  issn = {2473-9464}
}

@inproceedings{8489172,
  title = {Interpretable {{Deep Convolutional Neural Networks}} via {{Meta}}-Learning},
  abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for ``algorithmic fairness'' also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  doi = {10.1109/IJCNN.2018.8489172},
  author = {Liu, X. and Wang, X. and Matwin, S.},
  month = jul,
  year = {2018},
  keywords = {big data,Machine learning,Computational modeling,learning (artificial intelligence),pattern classification,Predictive models,pattern clustering,deep learning,Visualization,Machine learning algorithms,feedforward neural nets,interpretability,model interpretability,Convolutional Neural Network,Prediction algorithms,convolution,Training data,base learning algorithms,clustering algorithm,CNN-INTE,deep CNN model,interpretable deep convolutional neural networks,machine learning approaches,meta-learning,Meta-learning,meta-level test data,meta-level training data,MNIST image dataset,random forest,random processes,TensorFlow},
  pages = {1-9},
  issn = {2161-4407}
}

@inproceedings{8490433,
  title = {Explainable {{AI}} for {{Designers}}: {{A Human}}-{{Centered Perspective}} on {{Mixed}}-{{Initiative Co}}-{{Creation}}},
  abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  doi = {10.1109/CIG.2018.8490433},
  author = {Zhu, J. and Liapis, A. and Risi, S. and Bidarra, R. and Youngblood, G. M.},
  month = aug,
  year = {2018},
  keywords = {Machine learning,learning (artificial intelligence),Games,Visualization,human computer interaction,Tools,Task analysis,machine learning,computer games,explainable artificial intelligence,game design,XAI,AI machine,AI/ML techniques,explainable AI for designers,game designers,human-centered approach,human-centered perspective,human-computer interaction,mixed-initiative co-creation,Neurons,XAID framework},
  pages = {1-8},
  issn = {2325-4289}
}

@inproceedings{8490530,
  title = {From {{Machine Learning}} to {{Explainable AI}}},
  abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible ``glass-box'' approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
  booktitle = {2018 {{World Symposium}} on {{Digital Intelligence}} for {{Systems}} and {{Machines}} ({{DISA}})},
  doi = {10.1109/DISA.2018.8490530},
  author = {Holzinger, A.},
  month = aug,
  year = {2018},
  keywords = {Data mining,Machine learning,probability,learning (artificial intelligence),Games,ontologies (artificial intelligence),Uncertainty,artificial intelligence,Data visualization,AI winter,big quality data,Cognitive science,computational power,context-adaptive procedures,contextual explanatory models,deep learning approaches,engineering effort,glass-box approaches,high-dimensional vectors,knowledge representations,logical approaches,mathematical principles,ontologies,probabilistic learning methods,rule-based approaches,statistical machine learning methods},
  pages = {55-66},
  note = {ISSN:}
}

@inproceedings{8491501,
  title = {Explainable {{AI}} for {{Understanding Decisions}} and {{Data}}-{{Driven Optimization}} of the {{Choquet Integral}}},
  abstract = {To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future-i.e., new-data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.},
  booktitle = {2018 {{IEEE International Conference}} on {{Fuzzy Systems}} ({{FUZZ}}-{{IEEE}})},
  doi = {10.1109/FUZZ-IEEE.2018.8491501},
  author = {Murray, B. and Islam, M. A. and Pinar, A. J. and Havens, T. C. and Anderson, D. T. and Scott, G.},
  month = jul,
  year = {2018},
  keywords = {optimisation,Optimization,learning (artificial intelligence),Electronic mail,Indexes,Artificial intelligence,Machine Learning,feedforward neural nets,Explainable AI,explainable AI,Choquet integral,Choquet Integral,convolution,Convolutional neural networks,data-centric XAI tools,data-driven optimization,Frequency modulation,fusion solution,Fuzzy Integral,heterogeneous deep convolutional neural networks,parametric nonlinear aggregation function,remote sensing,Remote sensing,sensor fusion,XAI-ChI methods},
  pages = {1-8},
  note = {ISSN:}
}

@article{8494828,
  title = {Seq2seq-{{Vis}}: {{A Visual Debugging Tool}} for {{Sequence}}-to-{{Sequence Models}}},
  volume = {25},
  issn = {1077-2626},
  abstract = {Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and ``what if''-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2018.2865044},
  author = {Strobelt, H. and Gehrmann, S. and Behrisch, M. and Perer, A. and Pfister, H. and Rush, A. M.},
  month = jan,
  year = {2019},
  keywords = {Data models,program debugging,Machine learning,learning (artificial intelligence),Predictive models,Analytical models,Machine Learning,Visualization,Tools,neural nets,Deep Learning,Explainable AI,NLP,data visualisation,Atmosphere,blackbox pipeline,deep learning methods,neural sequence-to-sequence models,seq2seq-Vis,sequences,source sequence,target sequence,vector space,visual analysis tool,Visual Analytics,Visual Debugging,visual debugging tool},
  pages = {353-363}
}

@inproceedings{8519384,
  title = {Target {{Aspect Identification}} in {{SAR Image}}: {{A Machine Learning Approach}}},
  abstract = {Identifying the aspect for a given target is an important issue in synthetic aperture radar (SAR) image interpretation. A new SAR target aspect identification method based on machine learning theory is proposed in this paper. First, the aspect angles of the SAR target are discretized, and the spatial relationships of the neighborhoods of the SAR target samples are established. Then an optimal linear mapping is solved based on the proposed subspace aspect discriminant analysis. The samples will be projected into a low-dimensional space and be of a better aspect identifiability than in their original space. Finally, the projected samples are fed into a multilayer neural network, and the aspects of the SAR targets will be indicated. Experimental results have shown the superiority of the proposed method based on the moving and stationary target acquisition and recognition (MSTAR) data set.},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  doi = {10.1109/IGARSS.2018.8519384},
  author = {Pei, J. and Huang, Y. and Huo, W. and Zhang, Y. and Yang, J.},
  month = jul,
  year = {2018},
  keywords = {Machine learning,learning (artificial intelligence),multilayer perceptrons,Training,machine learning,Neurons,Estimation,aspect angles,aspect identifiability,image sampling,low-dimensional space,machine learning theory,moving-and-stationary target acquisition-and-recognition data set,MSTAR data set,multi-layer neural network,Multi-layer neural network,multilayer neural network,optimal linear mapping,projected samples,radar computing,radar imaging,radar target recognition,SAR image,SAR target aspect identification method,SAR target samples,stationary target acquisition,subspace aspect discriminant analysis,synthetic aperture radar,Synthetic aperture radar,synthetic aperture radar image interpretation,target aspect identification},
  pages = {2310-2313},
  issn = {2153-7003}
}

@inproceedings{8538416,
  title = {Inflectional {{Review}} of {{Deep Learning}} on {{Natural Language Processing}}},
  abstract = {In the age of knowledge, Natural Language Processing (NLP) express its demand by a huge range of utilization. Previously NLP was dealing with statically data. Contemporary time NLP is doing considerably with the corpus, lexicon database, pattern reorganization. Considering Deep Learning (DL) method recognize artificial Neural Network (NN) to nonlinear process, NLP tools become increasingly accurate and efficient that begin a debacle. Multi-Layer Neural Network obtaining the importance of the NLP for its capability including standard speed and resolute output. Hierarchical designs of data operate recurring processing layers to learn and with this arrangement of DL methods manage several practices. In this paper, this resumed striving to reach a review of the tools and the necessary methodology to present a clear understanding of the association of NLP and DL for truly understand in the training. Efficiency and execution both are improved in NLP by Part of speech tagging (POST), Morphological Analysis, Named Entity Recognition (NER), Semantic Role Labeling (SRL), Syntactic Parsing, and Coreference resolution. Artificial Neural Networks (ANN), Time Delay Neural Networks (TDNN), Recurrent Neural Network (RNN), Convolution Neural Networks (CNN), and Long-Short-Term-Memory (LSTM) dealings among Dense Vector (DV), Windows Approach (WA), and Multitask learning (MTL) as a characteristic of Deep Learning. After statically methods, when DL communicate the influence of NLP, the individual form of the NLP process and DL rule collaboration was started a fundamental connection.},
  booktitle = {2018 {{International Conference}} on {{Smart Computing}} and {{Electronic Enterprise}} ({{ICSCEE}})},
  doi = {10.1109/ICSCEE.2018.8538416},
  author = {Fahad, S. A. and Yahya, A. E.},
  month = jul,
  year = {2018},
  keywords = {learning (artificial intelligence),Artificial neural networks,Semantics,deep learning,Task analysis,Natural language processing,text analysis,recurrent neural nets,natural language processing,-Deep Learning,(DL) method,artificial Neural Network,Artificial Neural Networks,Considering Deep,contemporary time NLP,Convolution Neural Networks,Deep nural Network,DL methods,MultiLayer Neural Network,Multitask Learning,NLP process,NLP tools,nonlinear process,processing layers,Recurrent Neural Network,Tagging,Time Delay Neural Networks},
  pages = {1-4},
  note = {ISSN:}
}

@article{8540793,
  title = {Understanding the {{Artificial Intelligence Business Ecosystem}}},
  volume = {46},
  issn = {0360-8581},
  abstract = {This technology manager's note piece identifies the major components in the artificial intelligence (AI) business ecosystem and discusses several implications for managers. Specifically, it emphasizes on the designing of AI user scenarios, data acquisition for AI, and building the AI ecosystem.},
  number = {4},
  journal = {IEEE Engineering Management Review},
  doi = {10.1109/EMR.2018.2882430},
  author = {Quan, X. I. and Sanderson, J.},
  year = {Fourthquarter 2018},
  keywords = {Medical services,Machine learning,Buildings,Business,business data processing,Ecosystems,Artificial intelligence,data acquisition,artificial intelligence,technology management,AI ecosystem,AI user scenarios,artificial intelligence business ecosystem,business ecosystem,competitive intelligence,technology manager},
  pages = {22-25}
}

@inproceedings{8551093,
  title = {Dynamic {{Fuzzy Parser}} to {{Parse English Sentence Using POS Tagger}} and {{Fuzzy Max}}-{{Min Technique}}},
  abstract = {Natural Language (NL) is an essential part of ourlife. Humans use language for communication. NL is a prevailing tool used by the humans to convey the information. Natural Language Understanding (NLU) is a major challenge in Natural Language Processing (NLP). NLP is a part of Artificial Intelligence (AI). NLP provides a significant tool for communication. It attempts to produces noise free data and conversion of noise to text. NLU is having different levels. This paper presents the issue with respect to one of the level such as syntax analysis. To provide a solution for syntax analysis, dynamic fuzzy parser is designed and implemented to parse the English input sentences. Traditional approach of parsing is enhanced by applying fuzzy logic. This helps to know the syntactic correctness of the sentence. Penns tree bank parts of speech tags are used for the Parts of Speech Tagger (POS). POS tagger assigns the parts of speech tags for the input English sentence. Then these tags of the words are parsed using the grammar rules. Finally the result is displayed to represent the number of words parsed in a sentence with its associated fuzzy membership value. This parser produces Precision value of 1(100\%), Recall value of 0.92 (92\%) and F-measure value of 0.9583 for the sample of 50 correct and 50 incorrect sentences.},
  booktitle = {2018 {{International Conference}} on {{Current Trends}} towards {{Converging Technologies}} ({{ICCTCT}})},
  doi = {10.1109/ICCTCT.2018.8551093},
  author = {Kanakaraddi, S. G. and Nandval, S. S.},
  month = mar,
  year = {2018},
  keywords = {Conferences,Market research,Artificial intelligence,artificial intelligence,fuzzy logic,Natural language processing,AI,NLP,text analysis,natural language processing,natural language understanding,CFG,computational linguistics,context-free grammars,dynamic fuzzy parser,F-measure value,FCFG,fuzzy max-min technique,fuzzy membership value,Grammar,grammar rules,natural language,NL,NLU,parse English input sentences,parts of speech tagger,Penns tree bank parts of speech tags,POS,POS tagger,precision value,recall value,Syntactics,syntax analysis},
  pages = {1-5},
  note = {ISSN:}
}

@inproceedings{8588744,
  title = {Sight-{{Seeing}} in the {{Eyes}} of {{Deep Neural Networks}}},
  abstract = {We address the interpretability of convolutional neural networks (CNNs) for predicting a geo-location from an image. In a pilot experiment we classify images of Pittsburgh vs Tokyo and visualize the learned CNN filters. We found that varying the CNN architecture leads to variating in the visualized filters. This calls for further investigation of the effective parameters on the interpretability of CNNs.},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  doi = {10.1109/eScience.2018.00125},
  author = {Khademi, S. and Shi, X. and Mager, T. and Siebes, R. and Hein, C. and {de Boer}, V. and {van Gemert}, J.},
  month = oct,
  year = {2018},
  keywords = {Computer architecture,learning (artificial intelligence),Conferences,Visualization,neural net architecture,Computer science,feedforward neural nets,Neural networks,interpretability,deep neural networks,visualization,classification,convolutional neural networks,Image recognition,Intelligent systems,CNN architecture,CNNs,convolutional neural network (CNN),eyes,geo-location,learned CNN filters,place recognition,sight-seeing,Tokyo,visualized filters},
  pages = {407-408},
  note = {ISSN:}
}

@inproceedings{8591457,
  title = {An {{Adversarial Approach}} for {{Explainable AI}} in {{Intrusion Detection Systems}}},
  abstract = {Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
  booktitle = {{{IECON}} 2018 - 44th {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  doi = {10.1109/IECON.2018.8591457},
  author = {Marino, D. L. and Wickramasinghe, C. S. and Manic, M.},
  month = oct,
  year = {2018},
  keywords = {security of data,Machine learning,learning (artificial intelligence),multilayer perceptrons,pattern classification,Mathematical model,Intrusion detection,cyber-security,Visualization,neural nets,Explainable AI,explainable AI,deep neural networks,machine learning techniques,adversarial approach,adversarial machine learning,Adversarial Machine Learning,Adversarial samples,cyber-security applications,data-driven intrusion detection systems,Estimation,IDSs,multilayer perceptron classifiers},
  pages = {3237-3243},
  issn = {2577-1647}
}

@inproceedings{8613997,
  title = {Applying {{Internet}} of {{Things}} and {{Machine}}-{{Learning}} for {{Personalized Healthcare}}: {{Issues}} and {{Challenges}}},
  abstract = {Personalized Healthcare (PH) is a new patientoriented healthcare approach which expects to improve the traditional healthcare system. The focus of this new advancement is the patient data collected from patient Electronic health records (EHR), Internet of Things (IoT) sensor devices, wearables and mobile devices, web-based information and social media. PH applies Artificial Intelligence (AI) techniques to the collected dataset to improve disease progression technique, disease prediction, patient selfmanagement and clinical intervention. Machine learning techniques are widely used in this regard to develop analytic models. These models are integrated into different healthcare service applications and clinical decision support systems. These models mainly analyse the collected data from sensor devices and other sources to identify behavioral patterns and clinical conditions of the patient. For example, these models analyse the collected data to identify the patient's improvements, habits and anomaly in daily routine, changes in sleeping and mobility, eating, drinking and digestive pattern. Based on those patterns the healthcare applications and the clinical decision support systems recommend lifestyle advice, special treatment and care plans for the patient. The doctors and caregivers can also be engaged in the care plan process to validate lifestyle advice. However, there are many uncertainties and a grey area when it comes to applying machine learning in this context. Clinical, behaviour and lifestyle data in nature are very sensitive. There could be different types of biased involved in the process of data collection and interpretation. The training data model could have an older version of the dataset. All these could lead to an incorrect decision from the system without the user's knowledge. In this paper, some of the standards of the ML models reported in the recent research trends, identify the reliability issues and propose improvements.},
  booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
  doi = {10.1109/iCMLDE.2018.00014},
  author = {Ahamed, F. and Farid, F.},
  month = dec,
  year = {2018},
  keywords = {Internet of Things,Machine learning,Monitoring,learning (artificial intelligence),health care,Machine Learning,mobile devices,data acquisition,healthcare system,electronic health records,Hospitals,machine learning techniques,decision support systems,artificial intelligence techniques,behavioral patterns,clinical conditions,clinical decision support systems,clinical intervention,data collection,disease prediction,disease progression technique,diseases,healthcare applications,healthcare service applications,Internet of Things sensor devices,lifestyle data,ML models,patient data,patient self-management,patient treatment,personalized healthcare,Personalized Healthcare,Sleep apnea,social media,training data model,Web-based information},
  pages = {19-21},
  note = {ISSN:}
}

@inproceedings{8614007,
  title = {Attention {{Visualization}} of {{Gated Convolutional Neural Networks}} with {{Self Attention}} in {{Sentiment Analysis}}},
  abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
  booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
  doi = {10.1109/iCMLDE.2018.00024},
  author = {Yanagimto, H. and Hashimoto, K. and Okada, M.},
  month = dec,
  year = {2018},
  keywords = {Logic gates,Deep learning,learning (artificial intelligence),Sentiment analysis,deep learning,neural net architecture,Task analysis,Natural language processing,natural language processing,Convolutional neural networks,sentiment analysis,convolutional neural nets,Kernel,Amazon reviews,attention visualization,Gated CNN,gated convolutional neural networks,GCNN,neural network architecture,self-attention mechanism,sentient analysis,the self-attention mechanism},
  pages = {77-82},
  note = {ISSN:}
}

@inproceedings{8614130,
  title = {Interpretability and {{Reproducability}} in {{Production Machine Learning Applications}}},
  abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  doi = {10.1109/ICMLA.2018.00105},
  author = {Ghanta, S. and Subramanian, S. and Sundararaman, S. and Khermosh, L. and Sridhar, V. and Arteaga, D. and Luo, Q. and Das, D. and Talagala, N.},
  month = dec,
  year = {2018},
  keywords = {Data models,Production,Machine learning,production engineering computing,learning (artificial intelligence),Predictive models,Load modeling,Training,Pipelines,tracking,interpretability,explainability,human understandable machine learning,interpretability techniques,legal industry requirements,production machine learning applications,production ML reproducibility solution,production scenario,production viability,reproducability,systems},
  pages = {658-664},
  note = {ISSN:}
}

@inproceedings{8621470,
  title = {Prediction of Chromatin Spatial Structure Characteristics Using Machine Learning Methods},
  abstract = {Development of chromosome conformation capture methods boosted progress in the study of the spatial organization of chromatin. Accumulation of large amounts of experimental data provides an opportunity to apply machine learning methods to examine the connection between epigenetics and the three-dimensional structure of chromatin. The aim of this study was to predict the characteristics of the chromatin structure, namely the transitional gamma, from ChIP-Seq experimental data by means of machine learning methods, and also to reveal the properties of epigenetic data influencing prediction. The neural network and the loss function designed for the prediction task are shown to perform with a sufficiently high accuracy. In addition, the genomic size of the chromatin context required for improving the quality of the prediction was assessed. Several neural network visualization techniques were tested as a means for improving interpretability of network, showing the possibility for using visualization to study interrelations in epigenetic data relevant for three-dimensional chromatin structure. To sum up, a close relationship between epigenetic factors and the structure of chromatin has been confirmed.},
  booktitle = {2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  doi = {10.1109/BIBM.2018.8621470},
  author = {Starikov, S. and Khrameeva, E. and Gelfand, M.},
  month = dec,
  year = {2018},
  keywords = {Machine learning,learning (artificial intelligence),Conferences,biology computing,neural nets,machine learning,neural networks,Neural networks,data visualisation,Data visualization,Bioinformatics,Biomedical engineering,ChIP-Seq,ChIP-Seq experimental data,chromatin spatial structure characteristics,chromosome conformation capture methods,epigenetic data influencing prediction,genetics,genomics,Hi-C,Life sciences,machine learning methods,neural network visualization techniques,three-dimensional chromatin structure},
  pages = {2489-2489},
  note = {ISSN:}
}

@inproceedings{8622073,
  title = {Explainable {{Text Classification}} in {{Legal Document Review A Case Study}} of {{Explainable Predictive Coding}}},
  abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2018.8622073},
  author = {Chhatwal, R. and Gronvall, P. and {Huber-Fliflet}, N. and Keeling, R. and Zhang, J. and Zhao, H.},
  month = dec,
  year = {2018},
  keywords = {Law,Machine learning,pattern classification,Predictive models,machine learning,explainable AI,data-focused engagements,document classification,electronically-stored documents,explainable AI system,explainable predictive coding,explainable predictive coding methods,law administration,legal document review,machine learning researchers,predictive coding,Predictive coding,responsive documents,responsive snippets,supervised learning,supervised learning task,technology-based techniques,text analysis,text categorization,Text categorization,text classification,typical legal document review scenarios},
  pages = {1905-1911},
  note = {ISSN:}
}

@inproceedings{8622433,
  title = {Toward {{Efficient Automation}} of {{Interpretable Machine Learning}}},
  abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2018.8622433},
  author = {Kovalerchuk, B. and Neuhaus, N.},
  month = dec,
  year = {2018},
  keywords = {Machine learning,Computational modeling,learning (artificial intelligence),pattern classification,Mathematical model,machine learning,Machine learning algorithms,Neural networks,accuracy,interpretability,interpretable machine learning,visualization,explainability,Prediction algorithms,Classification algorithms,classifier,complex multidimensional relations,DCP algorithm,domain-explainable ML models,dominance classifier and predictor algorithm,dominant intervals,human perceptual limitations,human-understandable machine learning models,interpretable grey box models,interpretable mathematical forms,interpretable methods,noninterpretable models,unintelligible black box models,visual model,white box models},
  pages = {4940-4947},
  note = {ISSN:}
}

@inproceedings{8622439,
  title = {Toward {{Explainable Recommendations}}: {{Generating Review Text}} from {{Multicriteria Evaluation Data}}},
  abstract = {Explaining recommendations helps users to make more accurate and effective decisions and improves system credibility and transparency. Current explainable recommender systems tend to provide fixed statements such as "customers who purchased this item also purchased....". This explanation is generated only on the basis of the purchase history of similar customers, so it does not include the preferences of customers who have purchased the item or a description of the item. Since user-generated reviews generally contain information about the reviewer's preferences and a description of the item, such reviews typically have more effect on purchase decisions. Therefore, using reviews to explain recommendations should be more useful than providing only a fixed statement explanation. Aiming to create a system that provides personalized explanations for recommendations, we have developed a recurrent neural network model that uses multicriteria evaluation data to generate reviews.},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2018.8622439},
  author = {Suzuki, T. and Oyama, S. and Kurihara, M.},
  month = dec,
  year = {2018},
  keywords = {Data models,Computational modeling,History,transparency,Mathematical model,Recurrent neural networks,Recommender systems,recommender systems,explainable recommendation,RNN,explainable recommender systems,Decoding,explainable recommendations,fixed statement explanation,fixed statements,information filtering,information filters,multicriteria evaluation data,personalized explanations,purchase decisions,purchase history,recurrent neural nets,review text,reviewer,system credibility,text generation,user-generated reviews},
  pages = {3549-3551},
  note = {ISSN:}
}

@article{8653995,
  title = {Multi-Scale {{Interpretation Model}} for {{Convolutional Neural Networks}}: {{Building Trust}} Based on {{Hierarchical Interpretation}}},
  issn = {1520-9210},
  abstract = {With the rapid development of deep learning models, their performances in various tasks are improved, while meanwhile their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In the paper, we propose a novel Multi-scale INTerpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.},
  journal = {IEEE Transactions on Multimedia},
  doi = {10.1109/TMM.2019.2902099},
  author = {Cui, X. and Wang, D. and Wang, Z. J.},
  year = {2019},
  keywords = {Computational modeling,Feature extraction,Analytical models,Visualization,Heating systems,Model interpretability,convolutional neural networks,Image segmentation,model-agnostic,multi-scale interpretation,Perturbation methods},
  pages = {1-1}
}

@inproceedings{8679150,
  title = {The {{Structure}} of {{Deep Neural Network}} for {{Interpretable Transfer Learning}}},
  abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  doi = {10.1109/BIGCOMP.2019.8679150},
  author = {Kim, D. and Lim, W. and Hong, M. and Kim, H.},
  month = feb,
  year = {2019},
  keywords = {Data models,Computational modeling,Feature extraction,learning (artificial intelligence),feature extraction,Training,Machine Learning,image classification,neural nets,Task analysis,Neural networks,Interpretability,interpretability,deep neural network,natural language processing,complex model,Convolution,feature extraction layer,high-quality data,interpretable features,interpretable transfer learning,Korean characters classification,pretrained model,Transfer Learning,transfer learning task},
  pages = {1-4},
  issn = {2375-9356}
}

@inproceedings{8679370,
  title = {Deep {{Explanation Model}} for {{Facial Expression Recognition Through Facial Action Coding Unit}}},
  abstract = {Facial expression is the most powerful and natural non-verbal emotional communication method. Facial Expression Recognition(FER) has significance in machine learning tasks. Deep Learning models perform well in FER tasks, but it doesn't provide any justification for its decisions. Based on the hypothesis that facial expression is a combination of facial muscle movements, we find that Facial Action Coding Units(AUs) and Emotion label have a relationship in CK+ Dataset. In this paper, we propose a model which utilises AUs to explain Convolutional Neural Network(CNN) model's classification results. The CNN model is trained with CK+ Dataset and classifies emotion based on extracted features. Explanation model classifies the multiple AUs with the extracted features and emotion classes from the CNN model. Our experiment shows that with only features and emotion classes obtained from the CNN model, Explanation model generates AUs very well.},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  doi = {10.1109/BIGCOMP.2019.8679370},
  author = {Kim, S. and Kim, H.},
  month = feb,
  year = {2019},
  keywords = {Deep learning,Gold,Computational modeling,Feature extraction,learning (artificial intelligence),feature extraction,Task analysis,Justification,Hidden Markov models,face recognition,Face recognition,emotion recognition,CK+ Dataset,CNN model,convolutional neural nets,convolutional neural network model,deep explanation model,Deep Learning models,emotion classes,Explanation Model,Facial Action Coding System,facial action coding unit,facial action coding units,facial expression recognition,Facial Expression Recognition,facial muscle movements,FER tasks,machine learning tasks,nonverbal emotional communication method},
  pages = {1-4},
  issn = {2375-9356}
}

@inproceedings{953778,
  title = {Automated Discovery of Dependencies between Logical Components in Document Image Understanding},
  abstract = {Document image understanding denotes the recognition of semantically relevant components in the layout extracted from a document image. This recognition process is based on some visual models, whose manual specification can be a highly demanding task. In order to automatically acquire these models, we propose the application of machine learning techniques. Problems raised by possible dependencies between concepts to be learned are illustrated and solved with a computational strategy based on the separate-and-parallel-conquer search. The approach is tested on a set of real multi-page documents processed by the system WISDOM++. New results confirm the validity of the proposed strategy and show some limits of the learning system used in this work.},
  booktitle = {Proceedings of {{Sixth International Conference}} on {{Document Analysis}} and {{Recognition}}},
  doi = {10.1109/ICDAR.2001.953778},
  author = {Malerba, D. and Esposito, F. and Lisi, F. A. and Altamura, O.},
  month = sep,
  year = {2001},
  keywords = {learning (artificial intelligence),machine learning,Image analysis,document image processing,Text analysis,Image recognition,Optical character recognition software,optical character recognition,System testing,document image recognition,computational strategy,Digital images,divide and conquer methods,document image understanding,Image databases,logical component dependence discovery,multi-page documents,OCR,Optical devices,Publishing,search problems,separate-and-parallel-conquer search,visual models,WISDOM system,XML},
  pages = {174-178},
  note = {ISSN:}
}

@inproceedings{953860,
  title = {Geometric Method for Document Understanding and Classification Using Online Machine Learning},
  abstract = {We propose a geometric method for document image processing. This research focuses on document understanding and classification by applying the Winnow algorithm, an online machine learning method. This application makes the document image processing more flexible with various kind of documents since the meaningful knowledge can be extracted from training examples and the model for document type can be updated when there is a new example. This research aims to analyze and classify scientific papers. We conduct the experiments on documents from the proceedings of various conferences to show the performance of the proposed method. The experimental results are compared with the WISDOM++ system and also show the advantages of using the online machine learning method.},
  booktitle = {Proceedings of {{Sixth International Conference}} on {{Document Analysis}} and {{Recognition}}},
  doi = {10.1109/ICDAR.2001.953860},
  author = {Nattee, C. and Numao, M.},
  month = sep,
  year = {2001},
  keywords = {Machine learning,pattern classification,Electronic mail,image retrieval,Computer science,real-time systems,machine learning,Machine learning algorithms,Application software,document image processing,Document image processing,document understanding,geometric method,Information analysis,learning systems,Learning systems,Logic,real time systems,scientific papers,Text analysis,Winnow algorithm},
  pages = {602-606},
  note = {ISSN:}
}

@incollection{aakurInherentExplainabilityPattern2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {On the {{Inherent Explainability}} of {{Pattern Theory}}-{{Based Video Event Interpretations}}},
  isbn = {978-3-319-98131-4},
  abstract = {The ability of artificial intelligence systems to offer explanations for its decisions is central to building user confidence and structuring smart human-machine interactions. Expressing the rationale behind such a system's output is an important aspect of human-machine interaction as AI continues to be prominent in general, everyday use-cases. In this paper, we introduce a novel framework integrating Grenander's pattern theory structures to produce inherently explainable, symbolic representations for activity interpretations. These representations provide semantically rich and coherent interpretations of video activity using connected structures of detected (grounded) concepts, such as objects and actions, that are bound by semantics through background concepts not directly observed, i.e. contextualization cues. We use contextualization cues to establish semantic relationships among concepts to infer a deeper interpretation of events than what can be directly sensed. We propose the use of six questions that can be used to gain insight into the models ability to justify its decision and enhance its ability to interact with humans. The six questions are designed to (1) build an understanding of how the model is able to infer interpretations, (2) enable us to walk through its decision-making process, and (3) understand its drawbacks and possibly address them. We demonstrate the viability of this idea on video data using a dialog model that uses interpretations to generate explanations grounded in both video data and semantics.},
  language = {English},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Aakur, Sathyanarayanan N. and {de Souza}, Fillipe D. M. and Sarkar, Sudeep},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Semantics,Explainability,Activity interpretation,ConceptNet},
  pages = {277-299},
  doi = {10.1007/978-3-319-98131-4_11}
}

@inproceedings{Abdollahi:2017:UEC:3109859.3109913,
  series = {{{RecSys}} '17},
  title = {Using {{Explainability}} for {{Constrained Matrix Factorization}}},
  isbn = {978-1-4503-4652-8},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3109859.3109913},
  author = {Abdollahi, Behnoush and Nasraoui, Olfa},
  year = {2017},
  keywords = {explanations,interpretable models,latent factor models,matrix factorization,recommender systems},
  pages = {79-83},
  acmid = {3109913},
  numpages = {5}
}

@inproceedings{Abreu:2009:RSF:1529282.1529374,
  series = {{{SAC}} '09},
  title = {Refining {{Spectrum}}-Based {{Fault Localization Rankings}}},
  isbn = {978-1-60558-166-8},
  booktitle = {Proceedings of the 2009 {{ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/1529282.1529374},
  author = {Abreu, Rui and Mayer, Wolfgang and Stumptner, Markus and {van Gemund}, Arjan J. C.},
  year = {2009},
  keywords = {abstract interpretation,fault localization,program spectra},
  pages = {409-414},
  acmid = {1529374},
  numpages = {6}
}

@article{Abstracts36thAnnual2013,
  title = {Abstracts from the 36th {{Annual Meeting}} of the {{Society}} of {{General Internal Medicine}}},
  volume = {28},
  issn = {1525-1497},
  language = {English},
  number = {1},
  journal = {Journal of General Internal Medicine},
  doi = {10.1007/s11606-013-2436-y},
  month = jun,
  year = {2013},
  pages = {1-489},
  file = {/home/tim/Zotero/storage/AHP36SZJ/2013 - Abstracts from the 36th Annual Meeting of the Soci.pdf}
}

@incollection{AbstractsDocumentsThis1997,
  address = {Boston, MA},
  title = {Abstracts of {{Documents}} in {{This Supplement}}},
  isbn = {978-1-4615-5971-9},
  language = {English},
  booktitle = {Political {{Science Abstracts}}: 1996 {{Annual Supplement}};{{Volume}} 1},
  publisher = {{Springer US}},
  year = {1997},
  pages = {1-817},
  doi = {10.1007/978-1-4615-5971-9_1}
}

@article{AbstractsScientificPapers1999,
  title = {Abstracts {{Scientific Papers Honorary Lectures Categorical Courses Workshops State}}-of-the-{{Art Symposia}}},
  volume = {9},
  issn = {1432-1084},
  language = {English},
  number = {1},
  journal = {European Radiology},
  doi = {10.1007/BF03323585},
  month = mar,
  year = {1999},
  keywords = {Magnetic Resonance Angiography,Magnetic Resonance Imaging,Spiral Compute Tomography,Takayasu Arteritis,Transjugular Intrahepatic Portosystemic Shunt},
  pages = {S1-S362}
}

@incollection{aggarwalModelBasedCollaborativeFiltering2016,
  address = {Cham},
  title = {Model-{{Based Collaborative Filtering}}},
  isbn = {978-3-319-29659-3},
  abstract = {The neighborhood-based methods of the previous chapter can be viewed as generalizations of k-nearest neighbor classifiers, which are commonly used in machine learning.},
  language = {English},
  booktitle = {Recommender {{Systems}}: {{The Textbook}}},
  publisher = {{Springer International Publishing}},
  author = {Aggarwal, Charu C.},
  editor = {Aggarwal, Charu C.},
  year = {2016},
  pages = {71-138},
  doi = {10.1007/978-3-319-29659-3_3}
}

@inproceedings{alonsoZadehComputingWords2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {From {{Zadeh}}'s {{Computing}} with {{Words Towards eXplainable Artificial Intelligence}}},
  isbn = {978-3-030-12544-8},
  abstract = {The European Commission has identified Artificial Intelligence (AI) as the ``most strategic technology of the 21st century'' [7].},
  language = {English},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Alonso, Jose M.},
  editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
  year = {2019},
  keywords = {Fuzzy Logic,Explainable AI,Cointension,Computing with perceptions,Computing with words,Interpretable fuzzy systems},
  pages = {244-248}
}

@inproceedings{Amir:2018:HSA:3237383.3237869,
  series = {{{AAMAS}} '18},
  title = {{{HIGHLIGHTS}}: {{Summarizing Agent Behavior}} to {{People}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Amir, Dan and Amir, Ofra},
  year = {2018},
  keywords = {explainable ai,strategy summarization},
  pages = {1168-1176},
  acmid = {3237869},
  numpages = {9}
}

@incollection{azizMachineLearningAI2019,
  address = {Cham},
  series = {Palgrave {{Studies}} in {{Digital Business}} \& {{Enabling Technologies}}},
  title = {Machine {{Learning}} and {{AI}} for {{Risk Management}}},
  isbn = {978-3-030-02330-0},
  abstract = {We explore how machine learning and artificial intelligence (AI) solutions are transforming risk management. A non-technical overview is first given of the main machine learning and AI techniques of benefit to risk management. Then a review is provided, using current practice and empirical evidence, of the application of these techniques to the risk management fields of credit risk, market risk, operational risk, and compliance (`RegTech'). We conclude with some thoughts on current limitations and views on how the field is likely to develop in the short- to medium-term. Overall, we present an optimistic picture of the role of machine learning and AI in risk management, but note some practical limitations around suitable data management policies, transparency, and lack of necessary skillsets within firms.},
  language = {English},
  booktitle = {Disrupting {{Finance}}: {{FinTech}} and {{Strategy}} in the 21st {{Century}}},
  publisher = {{Springer International Publishing}},
  author = {Aziz, Saqib and Dowling, Michael},
  editor = {Lynn, Theo and Mooney, John G. and Rosati, Pierangelo and Cummins, Mark},
  year = {2019},
  keywords = {Machine learning,Risk management,AI,Credit risk,Market risk,Operational risk,RegTech},
  pages = {33-50},
  file = {/home/tim/Zotero/storage/D23CV3ZY/Aziz and Dowling - 2019 - Machine Learning and AI for Risk Management.pdf},
  doi = {10.1007/978-3-030-02330-0_3}
}

@inproceedings{Balachandran:2009:IRC:1645953.1646227,
  series = {{{CIKM}} '09},
  title = {Interpretable and {{Reconfigurable Clustering}} of {{Document Datasets}} by {{Deriving Word}}-Based {{Rules}}},
  isbn = {978-1-60558-512-3},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/1645953.1646227},
  author = {Balachandran, Vipin and P, Deepak and Khemani, Deepak},
  year = {2009},
  keywords = {interpretable clustering},
  pages = {1773-1776},
  acmid = {1646227},
  numpages = {4}
}

@article{balachandranInterpretableReconfigurableClustering2012,
  title = {Interpretable and {{Reconfigurable Clustering}} of {{Document Datasets}} by {{Deriving Word}}-{{Based Rules}}},
  volume = {32},
  issn = {0219-3116},
  abstract = {Clusters of text documents output by clustering algorithms are often hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clusters and outline the problem of generating clusterings with interpretable and reconfigurable cluster models. We develop two clustering algorithms toward the outlined goal of building interpretable and reconfigurable cluster models. They generate clusters with associated rules that are composed of conditions on word occurrences or nonoccurrences. The proposed approaches vary in the complexity of the format of the rules; RGC employs disjunctions and conjunctions in rule generation whereas RGC-D rules are simple disjunctions of conditions signifying presence of various words. In both the cases, each cluster is comprised of precisely the set of documents that satisfy the corresponding rule. Rules of the latter kind are easy to interpret, whereas the former leads to more accurate clustering. We show that our approaches outperform the unsupervised decision tree approach for rule-generating clustering and also an approach we provide for generating interpretable models for general clusterings, both by significant margins. We empirically show that the purity and f-measure losses to achieve interpretability can be as little as 3 and 5\%, respectively using the algorithms presented herein.},
  language = {English},
  number = {3},
  journal = {Knowledge and Information Systems},
  doi = {10.1007/s10115-011-0446-9},
  author = {Balachandran, Vipin and {Deepak P} and Khemani, Deepak},
  month = sep,
  year = {2012},
  keywords = {Interpretability,Data clustering,Text clustering},
  pages = {475-503},
  file = {/home/tim/Zotero/storage/7XCAXXQM/Balachandran et al. - 2012 - Interpretable and reconfigurable clustering of doc.pdf}
}

@inproceedings{Baral:2018:RRA:3209219.3209237,
  series = {{{UMAP}} '18},
  title = {{{ReEL}}: {{Review Aware Explanation}} of {{Location Recommendation}}},
  isbn = {978-1-4503-5589-6},
  booktitle = {Proceedings of the 26th {{Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  publisher = {{ACM}},
  doi = {10.1145/3209219.3209237},
  author = {Baral, Ramesh and Zhu, XiaoLong and Iyengar, S. S. and Li, Tao},
  year = {2018},
  keywords = {social networks,explainable recommendation,information retrieval},
  pages = {23-32},
  acmid = {3209237},
  numpages = {10}
}

@inproceedings{Bashar:2014:IDP:2682647.2682753,
  series = {{{WI}}-{{IAT}} '14},
  title = {Interpreting {{Discovered Patterns}} in {{Terms}} of {{Ontology Concepts}}},
  isbn = {978-1-4799-4143-8},
  booktitle = {Proceedings of the 2014 {{IEEE}}/{{WIC}}/{{ACM International Joint Conferences}} on {{Web Intelligence}} ({{WI}}) and {{Intelligent Agent Technologies}} ({{IAT}}) - {{Volume}} 01},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WI-IAT.2014.67},
  author = {Bashar, Md Abul and Li, Yuefeng and Shen, Yan and Albathan, Mubarak},
  year = {2014},
  keywords = {Semantic Web,Information Mismatch and Overload,Ontology-based Mining,Pattern Interpretation,Text Mining},
  pages = {432-437},
  acmid = {2682753},
  numpages = {6}
}

@inproceedings{Bellini:2018:KAE:3270323.3270327,
  series = {{{DLRS}} 2018},
  title = {Knowledge-Aware {{Autoencoders}} for {{Explainable Recommender Systems}}},
  isbn = {978-1-4503-6617-5},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Deep Learning}} for {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3270323.3270327},
  author = {Bellini, Vito and Schiavone, Angelo and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
  year = {2018},
  keywords = {Deep Learning,Explanation,Autoencoder Neural Networks,Explainable Models,Recommender Systems},
  pages = {24-31},
  acmid = {3270327},
  numpages = {8}
}

@inproceedings{berzinsInnovationsNaturalLanguage2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Innovations in {{Natural Language Document Processing}} for {{Requirements Engineering}}},
  isbn = {978-3-540-89778-1},
  abstract = {This paper evaluates the potential contributions of natural language processing to requirements engineering. We present a selective history of the relationship between requirements engineering (RE) and natural-language processing (NLP), and briefly summarize relevant recent trends in NLP. The paper outlines basic issues in RE and how they relate to interactions between a NLP front end and system-development processes. We suggest some improvements to NLP that may be possible in the context of RE and conclude with an assessment of what should be done to improve likelihood of practical impact in this direction.},
  language = {English},
  booktitle = {Innovations for {{Requirement Analysis}}. {{From Stakeholders}}' {{Needs}} to {{Formal Designs}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Berzins, Valdis and Martell, Craig and {Luqi} and Adams, Paige},
  editor = {Paech, Barbara and Martell, Craig},
  year = {2008},
  keywords = {Requirements,Natural Language,Ambiguity,Domain-Specific Methods,Gaps},
  pages = {125-146}
}

@article{bharadhwajExplanationsTemporalRecommendations2018,
  title = {Explanations for {{Temporal Recommendations}}},
  volume = {32},
  issn = {1610-1987},
  abstract = {Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
  language = {English},
  number = {4},
  journal = {KI - K\"unstliche Intelligenz},
  doi = {10.1007/s13218-018-0560-x},
  author = {Bharadhwaj, Homanga and Joshi, Shruti},
  month = nov,
  year = {2018},
  keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
  pages = {267-272},
  file = {/home/tim/Zotero/storage/SEKRY7UH/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf}
}

@inproceedings{biermannNaturalLanguageProgramming1983,
  series = {{{NATO Advanced Study Institutes Series}}},
  title = {Natural {{Language Programming}}},
  isbn = {978-94-009-7019-9},
  abstract = {A procedural semantics system is described for English imperative sentences in natural language programming. Issues related to the handling of dialog focus, noun group resolution, quantifier processing, and imperative verb execution are discussed. Sequences of imperative sentences may be assembled to build natural language programs and techniques are given for processing such programs. The final sections include a discussion of related research and a brief overview of the field.},
  language = {English},
  booktitle = {Computer {{Program Synthesis Methodologies}}},
  publisher = {{Springer Netherlands}},
  author = {Biermann, Alan W.},
  editor = {Biermann, Alan W. and Guiho, G\'erard},
  year = {1983},
  keywords = {Natural Language,Focus Mechanism,Head Noun,Naval Postgraduate School,Procedural Representation},
  pages = {335-368}
}

@inproceedings{Bock:2018:VNN:3281505.3281605,
  series = {{{VRST}} '18},
  title = {Visualization of {{Neural Networks}} in {{Virtual Reality Using Unreal Engine}}},
  isbn = {978-1-4503-6086-9},
  booktitle = {Proceedings of the 24th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  publisher = {{ACM}},
  doi = {10.1145/3281505.3281605},
  author = {Bock, Marcel and Schreiber, Andreas},
  year = {2018},
  keywords = {deep learning,neural networks,explainable ai,visualization},
  pages = {132:1-132:2},
  acmid = {3281605},
  articleno = {132},
  numpages = {2}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
  series = {International {{Centre}} for {{Mechanical Sciences}}},
  title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
  isbn = {978-3-7091-2668-4},
  shorttitle = {Machine {{Learning}}},
  abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
  language = {English},
  booktitle = {Learning, {{Networks}} and {{Statistics}}},
  publisher = {{Springer Vienna}},
  author = {Bratko, I.},
  editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
  year = {1997},
  keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
  pages = {163-177}
}

@inproceedings{brideDependableExplainableMachine2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
  isbn = {978-3-030-02450-5},
  abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
  language = {English},
  booktitle = {Formal {{Methods}} and {{Software Engineering}}},
  publisher = {{Springer International Publishing}},
  author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
  editor = {Sun, Jing and Sun, Meng},
  year = {2018},
  pages = {412-416}
}

@inproceedings{Brown:2018:RNN:3217871.3217872,
  series = {{{MLCS}}'18},
  title = {Recurrent {{Neural Network Attention Mechanisms}} for {{Interpretable System Log Anomaly Detection}}},
  isbn = {978-1-4503-5865-1},
  booktitle = {Proceedings of the {{First Workshop}} on {{Machine Learning}} for {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3217871.3217872},
  author = {Brown, Andy and Tuor, Aaron and Hutchinson, Brian and Nichols, Nicole},
  year = {2018},
  keywords = {Anomaly detection,Recurrent Neural Networks,Attention,Interpretable Machine Learning,Online Training,System Log Analysis},
  pages = {1:1-1:8},
  acmid = {3217872},
  articleno = {1},
  numpages = {8}
}

@article{Bultan:1999:MCS:325478.325480,
  title = {Model-Checking {{Concurrent Systems}} with {{Unbounded Integer Variables}}: {{Symbolic Representations}}, {{Approximations}}, and {{Experimental Results}}},
  volume = {21},
  issn = {0164-0925},
  number = {4},
  journal = {ACM Trans. Program. Lang. Syst.},
  doi = {10.1145/325478.325480},
  author = {Bultan, Tevfik and Gerber, Richard and Pugh, William},
  month = jul,
  year = {1999},
  keywords = {abstract interpretation,Presburger arithmetic,symbolic model checking},
  pages = {747-789},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {325480},
  issue_date = {July 1999},
  numpages = {43}
}

@article{butz12thBiannualConference2014,
  title = {12th {{Biannual Conference}} of the {{German Cognitive Science Society}} ({{Gesellschaft F\"ur Kognitionswissenschaft}})},
  volume = {15},
  issn = {1612-4790},
  language = {English},
  number = {1},
  journal = {Cognitive Processing},
  doi = {10.1007/s10339-014-0632-2},
  author = {Butz, Martin V.},
  month = sep,
  year = {2014},
  keywords = {Head Noun,Lateralized Readiness Potential,Mental Rotation,Relative Clause,SNARC Effect},
  pages = {1-158}
}

@inproceedings{Cai:2019:EEE:3301275.3302289,
  series = {{{IUI}} '19},
  title = {The {{Effects}} of {{Example}}-Based {{Explanations}} in a {{Machine Learning Interface}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302289},
  author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
  year = {2019},
  keywords = {machine learning,explainable AI,example-based explanations,human-AI interaction},
  pages = {258-262},
  acmid = {3302289},
  numpages = {5}
}

@inproceedings{carringtonMeasuresModelInterpretability2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Measures of {{Model Interpretability}} for {{Model Selection}}},
  isbn = {978-3-319-99740-7},
  abstract = {The literature lacks definitions for quantitative measures of model interpretability for automatic model selection to achieve high accuracy and interpretability, hence we define inherent model interpretability. We extend the work of Lipton et al. and Liu et al. from qualitative and subjective concepts of model interpretability to objective criteria and quantitative measures. We also develop another new measure called simplicity of sensitivity and illustrate prior, initial and posterior measurement. Measures are tested and validated with some measures recommended for use. It is demonstrated that high accuracy and high interpretability are jointly achievable with little to no sacrifice in either.},
  language = {English},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Carrington, Andr\'e and Fieguth, Paul and Chen, Helen},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Support vector machines,Model interpretability,Kernels,Model transparency},
  pages = {329-349}
}

@article{CARS2016Computer2016,
  title = {{{CARS}} 2016\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 30th {{International Congress}} and {{Exhibition Heidelberg}}, {{Germany}}, {{June}} 21\textendash{}25, 2016},
  volume = {11},
  issn = {1861-6429},
  language = {English},
  number = {1},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  doi = {10.1007/s11548-016-1412-5},
  month = jun,
  year = {2016},
  pages = {1-286}
}

@article{CARS2017Computer2017,
  title = {{{CARS}} 2017\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 31st {{International Congress}} and {{Exhibition Barcelona}}, {{Spain}}, {{June}} 20\textendash{}24, 2017},
  volume = {12},
  issn = {1861-6429},
  language = {English},
  number = {1},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  doi = {10.1007/s11548-017-1588-3},
  month = jun,
  year = {2017},
  pages = {1-286},
  file = {/home/tim/Zotero/storage/8G4UHRGG/2017 - CARS 2017—Computer Assisted Radiology and Surgery .pdf}
}

@article{CARS2018Computer2018,
  title = {{{CARS}} 2018\textemdash{{Computer Assisted Radiology}} and {{Surgery Proceedings}} of the 32nd {{International Congress}} and {{Exhibition Berlin}}, {{Germany}}, {{June}} 20\textendash{}23, 2018},
  volume = {13},
  issn = {1861-6429},
  language = {English},
  number = {1},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  doi = {10.1007/s11548-018-1766-y},
  month = jun,
  year = {2018},
  pages = {1-273},
  file = {/home/tim/Zotero/storage/2FDPTWGX/2018 - CARS 2018—Computer Assisted Radiology and Surgery .pdf}
}

@inproceedings{Chen:2018:NAR:3178876.3186070,
  series = {{{WWW}} '18},
  title = {Neural {{Attentional Rating Regression}} with {{Review}}-Level {{Explanations}}},
  isbn = {978-1-4503-5639-8},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186070},
  author = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
  year = {2018},
  keywords = {recommender systems,explainable recommendation,neural attention network,review usefulness},
  pages = {1583-1592},
  acmid = {3186070},
  numpages = {10}
}

@inproceedings{Chen:2018:PET:3180308.3180362,
  series = {{{IUI}} '18 {{Companion}}},
  title = {How {{Personal Experience}} and {{Technical Knowledge Affect Using Conversational Agents}}},
  isbn = {978-1-4503-5571-1},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3180308.3180362},
  author = {Chen, Mei-Ling and Wang, Hao-Chuan},
  year = {2018},
  keywords = {explainable intelligent user interfaces,Conversational agents,mental models},
  pages = {53:1-53:2},
  acmid = {3180362},
  articleno = {53},
  numpages = {2}
}

@article{Cheng:2019:MER:3306215.3291060,
  title = {{{MMALFM}}: {{Explainable Recommendation}} by {{Leveraging Reviews}} and {{Images}}},
  volume = {37},
  issn = {1046-8188},
  number = {2},
  journal = {ACM Trans. Inf. Syst.},
  doi = {10.1145/3291060},
  author = {Cheng, Zhiyong and Chang, Xiaojun and Zhu, Lei and Kanjirathinkal, Rose C. and Kankanhalli, Mohan},
  month = jan,
  year = {2019},
  keywords = {explainable recommendation,Aspect,latent factor model,multi-modal,rating prediction},
  pages = {16:1-16:28},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {3291060},
  articleno = {16},
  issue_date = {March 2019},
  numpages = {28}
}

@inproceedings{Croitoru:2010:GEN:1838206.1838404,
  series = {{{AAMAS}} '10},
  title = {Graphically {{Explaining Norms}}},
  isbn = {978-0-9826571-1-9},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}: {{Volume}} 1 - {{Volume}} 1},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Croitoru, Madalina and Oren, Nir and Miles, Simon and Luck, Michael},
  year = {2010},
  keywords = {conceptual graphs,norms},
  pages = {1405-1406},
  acmid = {1838404},
  numpages = {2}
}

@incollection{curuksuPrinciplesDataScience2018,
  address = {Cham},
  series = {Management for {{Professionals}}},
  title = {Principles of {{Data Science}}: {{Advanced}}},
  isbn = {978-3-319-70229-2},
  shorttitle = {Principles of {{Data Science}}},
  abstract = {This chapter covers advanced analytics principles and applications. Let us first back up on our objectives and progress so far. In Chap. 6, we defined the key concepts underlying the mathematical science of data analysis. The discussion was structured in two categories: descriptive and inferential statistics. In the context of a data science project, these two categories may be referred to as unsupervised and supervised modeling respectively. These two categories are ubiquitous because the objective of a data science project is always (bear with me please) to better understand some data or else to predict something. Chapter 7 thus again follows this binary structure, although some topics (e.g. computer simulation, Sect. 7.3) may be used to collect and understand data, forecast events, or both.},
  language = {English},
  booktitle = {Data {{Driven}}: {{An Introduction}} to {{Management Consulting}} in the 21st {{Century}}},
  publisher = {{Springer International Publishing}},
  author = {Curuksu, Jeremy David},
  editor = {Curuksu, Jeremy David},
  year = {2018},
  pages = {87-127},
  doi = {10.1007/978-3-319-70229-2_7}
}

@inproceedings{Dalmia:2018:TIN:3184558.3191523,
  series = {{{WWW}} '18},
  title = {Towards {{Interpretation}} of {{Node Embeddings}}},
  isbn = {978-1-4503-5640-4},
  booktitle = {Companion {{Proceedings}} of the {{The Web Conference}} 2018},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3184558.3191523},
  author = {Dalmia, Ayushi and J, Ganesh and Gupta, Manish},
  year = {2018},
  keywords = {neural networks,graph representation,model interpretability},
  pages = {945-952},
  acmid = {3191523},
  numpages = {8}
}

@inproceedings{deGraaf:2018:ERS:3173386.3173568,
  series = {{{HRI}} '18},
  title = {Explainable {{Robotic Systems}}},
  isbn = {978-1-4503-5615-2},
  booktitle = {Companion of the 2018 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3173386.3173568},
  author = {{de Graaf}, Maartje M.A. and Malle, Bertram F. and Dragan, Anca and Ziemke, Tom},
  year = {2018},
  keywords = {transparency,behavior explanation,explainable robotics,intentionality,theory of mind,trust calibration.},
  pages = {387-388},
  acmid = {3173568},
  numpages = {2}
}

@incollection{dengJointIntroductionNatural2018,
  address = {Singapore},
  title = {A {{Joint Introduction}} to {{Natural Language Processing}} and to {{Deep Learning}}},
  isbn = {978-981-10-5209-5},
  abstract = {In this chapter, we set up the fundamental framework for the book. We first provide an introduction to the basics of natural language processing (NLP) as an integral part of artificial intelligence. We then survey the historical development of NLP, spanning over five decades, in terms of three waves. The first two waves arose as rationalism and empiricism, paving ways to the current deep learning wave. The key pillars underlying the deep learning revolution for NLP consist of (1) distributed representations of linguistic entities via embedding, (2) semantic generalization due to the embedding, (3) long-span deep sequence modeling of natural language, (4) hierarchical networks effective for representing linguistic levels from low to high, and (5) end-to-end deep learning methods to jointly solve many NLP tasks. After the survey, several key limitations of current deep learning technology for NLP are analyzed. This analysis leads to five research directions for future advances in NLP.},
  language = {English},
  booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
  publisher = {{Springer Singapore}},
  author = {Deng, Li and Liu, Yang},
  editor = {Deng, Li and Liu, Yang},
  year = {2018},
  pages = {1-22},
  doi = {10.1007/978-981-10-5209-5_1}
}

@inproceedings{Dominguez:2019:EEA:3301275.3302274,
  series = {{{IUI}} '19},
  title = {The {{Effect}} of {{Explanations}} and {{Algorithmic Accuracy}} on {{Visual Recommender Systems}} of {{Artistic Images}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302274},
  author = {Dominguez, Vicente and Messina, Pablo and {Donoso-Guzm\'an}, Ivania and Parra, Denis},
  year = {2019},
  keywords = {explainable AI,art,visual recommender systems},
  pages = {408-416},
  acmid = {3302274},
  numpages = {9}
}

@article{EANM152015,
  title = {{{EANM}}'15},
  volume = {42},
  issn = {1619-7089},
  language = {English},
  number = {1},
  journal = {European Journal of Nuclear Medicine and Molecular Imaging},
  doi = {10.1007/s00259-015-3198-z},
  month = oct,
  year = {2015},
  pages = {1-924}
}

@article{ECR2005Scientific2005,
  title = {{{ECR}} 2005 \textendash{} {{Scientific Programme}} \textendash{} {{Abstracts}}},
  volume = {15},
  issn = {1613-3757},
  language = {English},
  number = {1},
  journal = {European Radiology Supplements},
  doi = {10.1007/s10406-005-0100-2},
  month = mar,
  year = {2005},
  keywords = {Public Health,Scientific Programme},
  pages = {1-688}
}

@inproceedings{Ehsan:2018:RNM:3278721.3278736,
  series = {{{AIES}} '18},
  title = {Rationalization: {{A Neural Machine Translation Approach}} to {{Generating Natural Language Explanations}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278736},
  author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
  year = {2018},
  keywords = {transparency,artificial intelligence,machine learning,interpretability,ai rationalization,explainable ai,user perception},
  pages = {81-87},
  acmid = {3278736},
  numpages = {7}
}

@incollection{elmiedanyArtificialIntelligence2019,
  address = {Cham},
  title = {Artificial {{Intelligence}}},
  isbn = {978-3-319-98213-7},
  abstract = {Artificial intelligence can be defined as computer systems which have been designed to interact with the world through abilities (e.g. visual perception and speech recognition) and intelligent behaviours (e.g. evaluating the available information and then taking the most sensible action to achieve a defined aim) that we would think of as principally humans. Initially, research has focused on letting software do things better, in which computers have always been doing better, such as the analysis of large datasets. However, the use of artificial intelligence in our day-to-day life has increased exponentially. Data forms the basis for the development of artificial intelligent software systems that will not only collect information but is able to learn, understand and interpret information, adapt its behaviour, plan, conclude, solve problems, think abstract, come up with ideas and understand and interpret language. Thanks to AI, a smart phone can detect cancer and a smart watch can detect a stroke. Machine learning is infiltrating and optimizing nearly every aspect of medicine from the way 911 emergency services are dispatched to assisting doctors during surgery. People can even quit smoking or kick opiate addiction with the help of AI. AI scientists are currently developing new approaches in machine learning, computer modelling and probability statistics to improve decision-making processes and are using decision theory and neuroscience to drive the progress of more effective healthcare and education as well as economics. This chapter will discuss the science of AI and explore the importance of big data and AI strategies. It will expand to discuss AI and medicine as well as medical education. It will conclude with discussion of AI and education as well as the future of artificial intelligence.},
  language = {English},
  booktitle = {Rheumatology {{Teaching}}: {{The Art}} and {{Science}} of {{Medical Education}}},
  publisher = {{Springer International Publishing}},
  author = {El Miedany, Yasser},
  editor = {El Miedany, Yasser},
  year = {2019},
  keywords = {Big data,Artificial intelligence,Medical education,Science of artificial intelligence,Virtual reality in education},
  pages = {347-378},
  doi = {10.1007/978-3-319-98213-7_18}
}

@inproceedings{fabra-boludaModellingMachineLearning2018,
  series = {Studies in {{Applied Philosophy}}, {{Epistemology}} and {{Rational Ethics}}},
  title = {Modelling {{Machine Learning Models}}},
  isbn = {978-3-319-96448-5},
  abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
  language = {English},
  booktitle = {Philosophy and {{Theory}} of {{Artificial Intelligence}} 2017},
  publisher = {{Springer International Publishing}},
  author = {{Fabra-Boluda}, Ra\"ul and Ferri, C\`esar and {Hern\'andez-Orallo}, Jos\'e and {Mart\'inez-Plumed}, Fernando and {Ram\'irez-Quintana}, M. Jos\'e},
  editor = {M\"uller, Vincent C.},
  year = {2018},
  pages = {175-186}
}

@inproceedings{Feng:2018:IPE:3206025.3206048,
  series = {{{ICMR}} '18},
  title = {Interpretable {{Partitioned Embedding}} for {{Customized Multi}}-Item {{Fashion Outfit Composition}}},
  isbn = {978-1-4503-5046-4},
  booktitle = {Proceedings of the 2018 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3206025.3206048},
  author = {Feng, Zunlei and Yu, Zhenyun and Yang, Yezhou and Jing, Yongcheng and Jiang, Junxiao and Song, Mingli},
  year = {2018},
  keywords = {embedding,adversarial,interpretable,outfit composition},
  pages = {143-151},
  acmid = {3206048},
  numpages = {9}
}

@inproceedings{Fernandez:2017:MCC:3132515.3132520,
  series = {{{MUSA2}} '17},
  title = {More {{Cat Than Cute}}?: {{Interpretable Prediction}} of {{Adjective}}-{{Noun Pairs}}},
  isbn = {978-1-4503-5509-4},
  booktitle = {Proceedings of the {{Workshop}} on {{Multimodal Understanding}} of {{Social}}, {{Affective}} and {{Subjective Attributes}}},
  publisher = {{ACM}},
  doi = {10.1145/3132515.3132520},
  author = {Fernandez, Delia and Woodward, Alejandro and Campos, Victor and {Giro-i-Nieto}, Xavier and Jou, Brendan and Chang, Shih-Fu},
  year = {2017},
  keywords = {interpretable models,adjective noun pairs,affective computing,compound concepts,convolutional neural networks},
  pages = {61-69},
  acmid = {3132520},
  numpages = {9}
}

@inproceedings{Gad-Elrab:2019:EFE:3289600.3290996,
  series = {{{WSDM}} '19},
  title = {{{ExFaKT}}: {{A Framework}} for {{Explaining Facts}} over {{Knowledge Graphs}} and {{Text}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290996},
  author = {{Gad-Elrab}, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
  year = {2019},
  keywords = {explainable evidence,fact-checking,knowledge graph,reasoning},
  pages = {87-95},
  acmid = {3290996},
  numpages = {9}
}

@inproceedings{Gaura:2003:UAT:944868.944890,
  series = {{{SIGDOC}} '03},
  title = {Using {{AI Techniques}} to {{Aid Hypermedia Design}}},
  isbn = {1-58113-696-X},
  booktitle = {Proceedings of the 21st {{Annual International Conference}} on {{Documentation}}},
  publisher = {{ACM}},
  doi = {10.1145/944868.944890},
  author = {Gaura, Elena I. and Newman, Robert M.},
  year = {2003},
  keywords = {artificial intelligence,artificial neural networks,hypermedia},
  pages = {100-104},
  acmid = {944890},
  numpages = {5}
}

@inproceedings{Ghazimatin:2019:FFU:3289600.3290990,
  series = {{{WSDM}} '19},
  title = {{{FAIRY}}: {{A Framework}} for {{Understanding Relationships Between Users}}' {{Actions}} and {{Their Social Feeds}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290990},
  author = {Ghazimatin, Azin and Saha Roy, Rishiraj and Weikum, Gerhard},
  year = {2019},
  keywords = {interpretability,explanation paths,social feeds,user actions},
  pages = {240-248},
  acmid = {3290990},
  numpages = {9}
}

@inproceedings{Gilpin:2018:RPC:3173386.3176994,
  series = {{{HRI}} '18},
  title = {Reasonable {{Perception}}: {{Connecting Vision}} and {{Language Systems}} for {{Validating Scene Descriptions}}},
  isbn = {978-1-4503-5615-2},
  booktitle = {Companion of the 2018 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3173386.3176994},
  author = {Gilpin, Leilani H. and Zaman, Cagri and Olson, Danielle and Yuan, Ben Z.},
  year = {2018},
  keywords = {explainable ai,commonsense reasoning,explainable robotic systems,virtual reality},
  pages = {115-116},
  acmid = {3176994},
  numpages = {2}
}

@article{Goel:1991:MMT:122344.122358,
  title = {Mental {{Models}}, {{Text Interpretation}}, and {{Knowledge Acquisition}}},
  volume = {2},
  issn = {0163-5719},
  number = {4},
  journal = {SIGART Bull.},
  doi = {10.1145/122344.122358},
  author = {Goel, Ashok K. and Eiselt, Kurt P.},
  month = jul,
  year = {1991},
  pages = {75-78},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {122358},
  issue_date = {Aug. 1991},
  numpages = {4}
}

@inproceedings{Green:2009:GTS:1639714.1639768,
  series = {{{RecSys}} '09},
  title = {Generating {{Transparent}}, {{Steerable Recommendations}} from {{Textual Descriptions}} of {{Items}}},
  isbn = {978-1-60558-435-5},
  booktitle = {Proceedings of the {{Third ACM Conference}} on {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/1639714.1639768},
  author = {Green, Stephen J. and Lamere, Paul and Alexander, Jeffrey and Maillet, Fran{\c c}ois and Kirk, Susanna and Holt, Jessica and Bourque, Jackie and Mak, Xiao-Wen},
  year = {2009},
  keywords = {explainable recommender,steerable recommender},
  pages = {281-284},
  acmid = {1639768},
  numpages = {4}
}

@article{grillHealthExploringComplexity2016,
  title = {Health\textemdash{{Exploring Complexity}}: {{An Interdisciplinary Systems Approach HEC2016}}},
  volume = {31},
  issn = {1573-7284},
  shorttitle = {Health\textemdash{{Exploring Complexity}}},
  language = {English},
  number = {1},
  journal = {European Journal of Epidemiology},
  doi = {10.1007/s10654-016-0183-1},
  author = {Grill, Eva and M\"uller, Martin and Mansmann, Ulrich},
  month = aug,
  year = {2016},
  pages = {1-239},
  file = {/home/tim/Zotero/storage/MJKWWKHT/Grill et al. - 2016 - Health—exploring complexity an interdisciplinary .pdf}
}

@article{guhaInterpretationInterpretabilityQuantitative2008,
  title = {On the {{Interpretation}} and {{Interpretability}} of {{Quantitative Structure}}\textendash{{Activity Relationship Models}}},
  volume = {22},
  issn = {1573-4951},
  abstract = {The goal of a quantitative structure\textendash{}activity relationship (QSAR) model is to encode the relationship between molecular structure and biological activity or physical property. Based on this encoding, such models can be used for predictive purposes. Assuming the use of relevant and meaningful descriptors, and a statistically significant model, extraction of the encoded structure\textendash{}activity relationships (SARs) can provide insight into what makes a molecule active or inactive. Such analyses by QSAR models are useful in a number of scenarios, such as suggesting structural modifications to enhance activity, explanation of outliers and exploratory analysis of novel SARs. In this paper we discuss the need for interpretation and an overview of the factors that affect interpretability of QSAR models. We then describe interpretation protocols for different types of models, highlighting the different types of interpretations, ranging from very broad, global, trends to very specific, case-by-case, descriptions of the SAR, using examples from the training set. Finally, we discuss a number of case studies where workers have provide some form of interpretation of a QSAR model.},
  language = {English},
  number = {12},
  journal = {Journal of Computer-Aided Molecular Design},
  doi = {10.1007/s10822-008-9240-5},
  author = {Guha, Rajarshi},
  month = dec,
  year = {2008},
  keywords = {Neural network,Interpretation,Linear regression,Partial least squares (PLS),Quantitative structure–activity relationship (QSAR)},
  pages = {857-871}
}

@inproceedings{Guo:2018:LED:3243734.3243792,
  series = {{{CCS}} '18},
  title = {{{LEMNA}}: {{Explaining Deep Learning Based Security Applications}}},
  isbn = {978-1-4503-5693-0},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  doi = {10.1145/3243734.3243792},
  author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
  year = {2018},
  keywords = {explainable AI,binary analysis,deep recurrent neural networks},
  pages = {364-379},
  acmid = {3243792},
  numpages = {16}
}

@inproceedings{Hayes:2017:IRC:2909824.3020233,
  series = {{{HRI}} '17},
  title = {Improving {{Robot Controller Transparency Through Autonomous Policy Explanation}}},
  isbn = {978-1-4503-4336-7},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{IEEE International Conference}} on {{Human}}-{{Robot Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/2909824.3020233},
  author = {Hayes, Bradley and Shah, Julie A.},
  year = {2017},
  keywords = {interpretable machine learning,human-robot interaction,human-robot collaboration,human-robot teaming},
  pages = {303-312},
  acmid = {3020233},
  numpages = {10}
}

@inproceedings{Hicks:2018:MAR:3204949.3208129,
  series = {{{MMSys}} '18},
  title = {Mimir: {{An Automatic Reporting}} and {{Reasoning System}} for {{Deep Learning Based Analysis}} in the {{Medical Domain}}},
  isbn = {978-1-4503-5192-8},
  booktitle = {Proceedings of the 9th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM}},
  doi = {10.1145/3204949.3208129},
  author = {Hicks, Steven Alexander and Eskeland, Sigrun and Lux, Mathias and {de Lange}, Thomas and Randel, Kristin Ranheim and Jeppsson, Mattis and Pogorelov, Konstantin and Halvorsen, P\aa{}l and Riegler, Michael},
  year = {2018},
  keywords = {deep learning,automatic disease detection,interpretable neural networks,medical documentation},
  pages = {369-374},
  acmid = {3208129},
  numpages = {6}
}

@inproceedings{Holcomb:2018:ODA:3206157.3206174,
  series = {{{ICBDE}} '18},
  title = {Overview on {{DeepMind}} and {{Its AlphaGo Zero AI}}},
  isbn = {978-1-4503-6358-7},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Big Data}} and {{Education}}},
  publisher = {{ACM}},
  doi = {10.1145/3206157.3206174},
  author = {Holcomb, Sean D. and Porter, William K. and Ault, Shaun V. and Mao, Guifen and Wang, Jin},
  year = {2018},
  keywords = {Deep Learning,AI,Neural Networks,AlphaGo Zero,Deep Mind,Reinforcement Learning},
  pages = {67-71},
  acmid = {3206174},
  numpages = {5}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
  isbn = {978-3-319-99740-7},
  shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
  abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
  language = {English},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  keywords = {Privacy,Machine learning,Artificial intelligence,Explainable AI,Knowledge extraction},
  pages = {1-8}
}

@incollection{holzingerLectureMultimediaData2014,
  address = {Cham},
  title = {Lecture 6 {{Multimedia Data Mining}} and {{Knowledge Discovery}}},
  isbn = {978-3-319-04528-3},
  abstract = {At the end of this sixth lecture you:},
  language = {English},
  booktitle = {Biomedical {{Informatics}}: {{Discovering Knowledge}} in {{Big Data}}},
  publisher = {{Springer International Publishing}},
  author = {Holzinger, Andreas},
  editor = {Holzinger, Andreas},
  year = {2014},
  pages = {251-298},
  doi = {10.1007/978-3-319-04528-3_6}
}

@inproceedings{holzingerMachineLearningKnowledge2017,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Machine {{Learning}} and {{Knowledge Extraction}} in {{Digital Pathology Needs}} an {{Integrative Approach}}},
  isbn = {978-3-319-69775-8},
  abstract = {During the last decade pathology has benefited from the rapid progress of image digitizing technologies, which led to the development of scanners, capable to produce so-called Whole Slide images (WSI) which can be explored by a pathologist on a computer screen comparable to the conventional microscope and can be used for diagnostics, research, archiving and also education and training. Digital pathology is not just the transformation of the classical microscopic analysis of histological slides by pathologists to just a digital visualization. It is a disruptive innovation that will dramatically change medical work-flows in the coming years and help to foster personalized medicine. Really powerful gets a pathologist if she/he is augmented by machine learning, e.g. by support vector machines, random forests and deep learning. The ultimate benefit of digital pathology is to enable to learn, to extract knowledge and to make predictions from a combination of heterogenous data, i.e. the histological image, the patient history and the *omics data. These challenges call for integrated/integrative machine learning approach fostering transparency, trust, acceptance and the ability to explain step-by-step why a decision has been made.},
  language = {English},
  booktitle = {Towards {{Integrative Machine Learning}} and {{Knowledge Extraction}}},
  publisher = {{Springer International Publishing}},
  author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and M\"uller, Heimo and Reihs, Robert and Zatloukal, Kurt},
  editor = {Holzinger, Andreas and Goebel, Randy and Ferri, Massimo and Palade, Vasile},
  year = {2017},
  keywords = {Deep learning,Data integration,Digital pathology,Integrative machine learning,Transfer learning},
  pages = {13-50}
}

@incollection{hoschkeSelforganizingSensingSystem2008,
  address = {London},
  series = {Advanced {{Information}} and {{Knowledge Processing}}},
  title = {A {{Self}}-{{Organizing Sensing System}} for {{Structural Health Monitoring}} of {{Aerospace Vehicles}}},
  isbn = {978-1-84628-982-8},
  language = {English},
  booktitle = {Advances in {{Applied Self}}-{{Organizing Systems}}},
  publisher = {{Springer London}},
  author = {Hoschke, N. and Lewis, C. J. and Price, D. C. and Scott, D. A. and Gerasimov, V. and Wang, P.},
  editor = {Prokopenko, Mikhail},
  year = {2008},
  keywords = {Cellular Automaton,Genetic Programming,Mobile Agent,Multiagent System},
  pages = {51-76},
  doi = {10.1007/978-1-84628-982-8_4}
}

@inproceedings{Hotta:2008:NGT:1486927.1487030,
  series = {{{WI}}-{{IAT}} '08},
  title = {A {{Neural}}-{{Network}}-{{Based Geographic Tendency Visualization}}},
  isbn = {978-0-7695-3496-1},
  booktitle = {Proceedings of the 2008 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}} - {{Volume}} 01},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WIIAT.2008.141},
  author = {Hotta, Hajime and Hagiwara, Masafumi},
  year = {2008},
  keywords = {neural network,visualization,geographic},
  pages = {817-823},
  acmid = {1487030},
  numpages = {7}
}

@article{http://arxiv.org/abs/1401.5390v1,
  title = {Learning to {{Win}} by {{Reading Manuals}} in a {{Monte}}-{{Carlo Framework}}},
  abstract = {Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34\% absolute improvement and winning over 65\% of games when playing against the built-in AI of Civilization.},
  journal = {arxiv},
  author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
  month = jan,
  year = {2014}
}

@inproceedings{http://arxiv.org/abs/1602.04938v3,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  booktitle = {Arxiv},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = feb,
  year = {2016},
  keywords = {interpretability,black box classifier,explaining machine learning,interpretable machine learning}
}

@article{http://arxiv.org/abs/1604.00289v3,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  journal = {arxiv},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  month = apr,
  year = {2016}
}

@article{http://arxiv.org/abs/1606.05386v1,
  title = {Model-{{Agnostic Interpretability}} of {{Machine Learning}}},
  abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
  journal = {arxiv},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = jun,
  year = {2016}
}

@article{http://arxiv.org/abs/1608.08974v2,
  title = {Towards {{Transparent AI Systems}}: {{Interpreting Visual Question Answering Models}}},
  abstract = {Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.},
  journal = {arxiv},
  author = {Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
  month = aug,
  year = {2016}
}

@article{http://arxiv.org/abs/1611.07270v1,
  title = {Investigating the Influence of Noise and Distractors on the Interpretation of Neural Networks},
  abstract = {Understanding neural networks is becoming increasingly important. Over the last few years different types of visualisation and explanation methods have been proposed. However, none of them explicitly considered the behaviour in the presence of noise and distracting elements. In this work, we will show how noise and distracting dimensions can influence the result of an explanation model. This gives a new theoretical insights to aid selection of the most appropriate explanation model within the deep-Taylor decomposition framework.},
  journal = {arxiv},
  author = {Kindermans, Pieter-Jan and Sch\"utt, Kristof and M\"uller, Klaus-Robert and D\"ahne, Sven},
  month = nov,
  year = {2016}
}

@article{http://arxiv.org/abs/1611.07567v1,
  title = {Feature {{Importance Measure}} for {{Non}}-Linear {{Learning Algorithms}}},
  abstract = {Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point).},
  journal = {arxiv},
  author = {Vidovic, Marina M. -C. and G\"ornitz, Nico and M\"uller, Klaus-Robert and Kloft, Marius},
  month = nov,
  year = {2016}
}

@article{http://arxiv.org/abs/1611.07634v1,
  title = {Interpretation of {{Prediction Models Using}} the {{Input Gradient}}},
  abstract = {State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a "black box". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing.},
  journal = {arxiv},
  author = {Hechtlinger, Yotam},
  month = nov,
  year = {2016}
}

@article{http://arxiv.org/abs/1702.08635v1,
  title = {Learning {{What Data}} to {{Learn}}},
  abstract = {Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.},
  journal = {arxiv},
  author = {Fan, Yang and Tian, Fei and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
  month = feb,
  year = {2017}
}

@article{http://arxiv.org/abs/1703.06914v2,
  title = {Applying {{Deep Machine Learning}} for Psycho-Demographic Profiling of {{Internet}} Users Using {{O}}.{{C}}.{{E}}.{{A}}.{{N}}. Model of Personality},
  abstract = {In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it's possible to apply simple machine learning methods to analyze collected digital footprints and to create psycho-demographic profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psycho-demographic profiling may have an enormous impact on the society (for good or worse) and provides means for Artificial Intelligence (AI) systems to better understand humans by creating their psychological profiles. Thus AI agents may achieve the human-like ability to participate in conversation (communication) flow by anticipating human opponents' reactions, expectations, and behavior.},
  journal = {arxiv},
  author = {Omelianenko, Iaroslav},
  month = mar,
  year = {2017}
}

@article{http://arxiv.org/abs/1704.03296v3,
  title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
  abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
  journal = {arxiv},
  author = {Fong, Ruth and Vedaldi, Andrea},
  month = apr,
  year = {2017}
}

@article{http://arxiv.org/abs/1705.06824v2,
  title = {Learning {{Convolutional Text Representations}} for {{Visual Question Answering}}},
  abstract = {Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our "CNN Inception + Gate" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.},
  journal = {arxiv},
  author = {Wang, Zhengyang and Ji, Shuiwang},
  month = may,
  year = {2017}
}

@article{http://arxiv.org/abs/1706.07206v2,
  title = {Explaining {{Recurrent Neural Network Predictions}} in {{Sentiment Analysis}}},
  abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
  journal = {arxiv},
  author = {Arras, Leila and Montavon, Gr\'egoire and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = jun,
  year = {2017}
}

@article{http://arxiv.org/abs/1706.07979v1,
  title = {Methods for {{Interpreting}} and {{Understanding Deep Neural Networks}}},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
  journal = {arxiv},
  author = {Montavon, Gr\'egoire and Samek, Wojciech and M\"uller, Klaus-Robert},
  month = jun,
  year = {2017}
}

@article{http://arxiv.org/abs/1707.09641v2,
  title = {Towards {{Visual Explanations}} for {{Convolutional Neural Networks}} via {{Input Resampling}}},
  abstract = {The predictive power of neural networks often costs model interpretability. Several techniques have been developed for explaining model outputs in terms of input features; however, it is difficult to translate such interpretations into actionable insight. Here, we propose a framework to analyze predictions in terms of the model's internal features by inspecting information flow through the network. Given a trained network and a test image, we select neurons by two metrics, both measured over a set of images created by perturbations to the input image: (1) magnitude of the correlation between the neuron activation and the network output and (2) precision of the neuron activation. We show that the former metric selects neurons that exert large influence over the network output while the latter metric selects neurons that activate on generalizable features. By comparing the sets of neurons selected by these two metrics, our framework suggests a way to investigate the internal attention mechanisms of convolutional neural networks.},
  journal = {arxiv},
  author = {Lengerich, Benjamin J. and Konam, Sandeep and Xing, Eric P. and Rosenthal, Stephanie and Veloso, Manuela},
  month = jul,
  year = {2017}
}

@article{http://arxiv.org/abs/1708.04988v1,
  title = {Warp: A Method for Neural Network Interpretability Applied to Gene Expression Profiles},
  abstract = {We show a proof of principle for warping, a method to interpret the inner working of neural networks in the context of gene expression analysis. Warping is an efficient way to gain insight to the inner workings of neural nets and make them more interpretable. We demonstrate the ability of warping to recover meaningful information for a given class on a samplespecific individual basis. We found warping works well in both linearly and nonlinearly separable datasets. These encouraging results show that warping has a potential to be the answer to neural networks interpretability in computational biology.},
  journal = {arxiv},
  author = {Assya, Trofimov and Sebastien, Lemieux and Claude, Perreault},
  month = aug,
  year = {2017}
}

@article{http://arxiv.org/abs/1708.08296v1,
  title = {Explainable {{Artificial Intelligence}}: {{Understanding}}, {{Visualizing}} and {{Interpreting Deep Learning Models}}},
  abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
  journal = {arxiv},
  author = {Samek, Wojciech and Wiegand, Thomas and M\"uller, Klaus-Robert},
  month = aug,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.04806v2,
  title = {Deep {{Learning}} for {{Case}}-{{Based Reasoning}} through {{Prototypes}}: {{A Neural Network}} That {{Explains Its Predictions}}},
  abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
  journal = {arxiv},
  author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.09511v2,
  title = {{{InterpNET}}: {{Neural Introspection}} for {{Interpretable Deep Learning}}},
  abstract = {Humans are able to explain their reasoning. On the contrary, deep neural networks are not. This paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classification, inspired by physiological evidence of the human visual system's inner-workings. This paper proposes a neural network design paradigm, termed InterpNET, which can be combined with any existing classification architecture to generate natural language explanations of the classifications. The success of the module relies on the assumption that the network's computation and reasoning is represented in its internal layer activations. While in principle InterpNET could be applied to any existing classification architecture, it is evaluated via an image classification and explanation task. Experiments on a CUB bird classification and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations. While the current state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a much higher METEOR score of 37.9.},
  journal = {arxiv},
  author = {Barratt, Shane},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.10777v1,
  title = {Understanding {{Hidden Memories}} of {{Recurrent Neural Networks}}},
  abstract = {Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.},
  journal = {arxiv},
  author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1710.10967v3,
  title = {Artificial {{Intelligence}} as {{Structural Estimation}}: {{Economic Interpretations}} of {{Deep Blue}}, {{Bonanza}}, and {{AlphaGo}}},
  abstract = {Artificial intelligence (AI) has achieved superhuman performance in a growing number of tasks, but understanding and explaining AI remain challenging. This paper clarifies the connections between machine-learning algorithms to develop AIs and the econometrics of dynamic structural models through the case studies of three famous game AIs. Chess-playing Deep Blue is a calibrated value function, whereas shogi-playing Bonanza is an estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy network" is a deep neural network implementation of Hotz and Miller's (1993) conditional choice probability estimation; its "reinforcement-learning value network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional choice simulation method. Relaxing these AIs' implicit econometric assumptions would improve their structural interpretability.},
  journal = {arxiv},
  author = {Igami, Mitsuru},
  month = oct,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.00404v1,
  title = {Building {{Data}}-Driven {{Models}} with {{Microstructural Images}}: {{Generalization}} and {{Interpretability}}},
  abstract = {As data-driven methods rise in popularity in materials science applications, a key question is how these machine learning models can be used to understand microstructure. Given the importance of process-structure-property relations throughout materials science, it seems logical that models that can leverage microstructural data would be more capable of predicting property information. While there have been some recent attempts to use convolutional neural networks to understand microstructural images, these early studies have focused only on which featurizations yield the highest machine learning model accuracy for a single data set. This paper explores the use of convolutional neural networks for classifying microstructure with a more holistic set of objectives in mind: generalization between data sets, number of features required, and interpretability.},
  journal = {arxiv},
  author = {Ling, Julia and Hutchinson, Maxwell and Antono, Erin and DeCost, Brian and Holm, Elizabeth A. and Meredig, Bryce},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.06431v2,
  title = {Using {{KL}}-Divergence to Focus {{Deep Visual Explanation}}},
  abstract = {We present a method for explaining the image classification predictions of deep convolution neural networks, by highlighting the pixels in the image which influence the final class prediction. Our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction, and here we use Kullback-Leibler divergence to provide this focus. Overall, our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks. In this brief paper, our experiments evaluate the performance of two popular networks in this context of interpretability.},
  journal = {arxiv},
  author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1711.09482v2,
  title = {An {{Introduction}} to {{Deep Visual Explanation}}},
  abstract = {The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.},
  journal = {arxiv},
  author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
  month = nov,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.02034v2,
  title = {{{SMILES2Vec}}: {{An Interpretable General}}-{{Purpose Deep Neural Network}} for {{Predicting Chemical Properties}}},
  abstract = {Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88\%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.},
  journal = {arxiv},
  author = {Goh, Garrett B. and Hodas, Nathan O. and Siegel, Charles and Vishnu, Abhinav},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.06302v3,
  title = {Visual {{Explanation}} by {{Interpretation}}: {{Improving Visual Feedback Capabilities}} of {{Deep Neural Networks}}},
  abstract = {Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by stridded operations in deconvNet-based visualizations. Moreover, we introduce an8Flower, a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest},
  journal = {arxiv},
  author = {Oramas, Jose and Wang, Kaili and Tuytelaars, Tinne},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1712.08107v1,
  title = {A {{Deep Learning Interpretable Classifier}} for {{Diabetic Retinopathy Disease Grading}}},
  abstract = {Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.},
  journal = {arxiv},
  author = {de la Torre, Jordi and Valls, Aida and Puig, Domenec},
  month = dec,
  year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
  title = {A {{Human}}-{{Grounded Evaluation Benchmark}} for {{Local Explanations}} of {{Machine Learning}}},
  abstract = {In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.},
  journal = {arxiv},
  author = {Mohseni, Sina and Ragan, Eric D.},
  month = jan,
  year = {2018}
}

@article{http://arxiv.org/abs/1801.06889v3,
  title = {Visual {{Analytics}} in {{Deep Learning}}: {{An Interrogative Survey}} for the {{Next Frontiers}}},
  abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
  journal = {arxiv},
  author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  month = jan,
  year = {2018}
}

@article{http://arxiv.org/abs/1801.09808v1,
  title = {The {{Intriguing Properties}} of {{Model Explanations}}},
  abstract = {Linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions. In this paper, we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks (CENs). We focus on two questions: (i) whether linear explanations are always consistent or can be misleading, and (ii) when integrated into the prediction process, whether and how explanations affect the performance of the model. Our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous.},
  journal = {arxiv},
  author = {{Al-Shedivat}, Maruan and Dubey, Avinava and Xing, Eric P.},
  month = jan,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.00541v1,
  title = {Causal {{Learning}} and {{Explanation}} of {{Deep Neural Networks}} via {{Autoencoded Activations}}},
  abstract = {Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.},
  journal = {arxiv},
  author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.03043v1,
  title = {{{PoTrojan}}: Powerful Neural-Level Trojan Designs in Deep Learning Models},
  abstract = {With the popularity of deep learning (DL), artificial intelligence (AI) has been applied in many areas of human life. Neural network or artificial neural network (NN), the main technique behind DL, has been extensively studied to facilitate computer vision and natural language recognition. However, the more we rely on information technology, the more vulnerable we are. That is, malicious NNs could bring huge threat in the so-called coming AI era. In this paper, for the first time in the literature, we propose a novel approach to design and insert powerful neural-level trojans or PoTrojan in pre-trained NN models. Most of the time, PoTrojans remain inactive, not affecting the normal functions of their host NN models. PoTrojans could only be triggered in very rare conditions. Once activated, however, the PoTrojans could cause the host NN models to malfunction, either falsely predicting or classifying, which is a significant threat to human society of the AI era. We would explain the principles of PoTrojans and the easiness of designing and inserting them in pre-trained deep learning models. PoTrojans doesn't modify the existing architecture or parameters of the pre-trained models, without re-training. Hence, the proposed method is very efficient.},
  journal = {arxiv},
  author = {Zou, Minhui and Shi, Yang and Wang, Chengliang and Li, Fangyu and Song, WenZhan and Wang, Yu},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1802.07384v2,
  title = {Interpreting {{Neural Network Judgments}} via {{Minimal}}, {{Stable}}, and {{Symbolic Corrections}}},
  abstract = {We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.},
  journal = {arxiv},
  author = {Zhang, Xin and {Solar-Lezama}, Armando and Singh, Rishabh},
  month = feb,
  year = {2018}
}

@article{http://arxiv.org/abs/1803.04263v3,
  title = {The {{Challenge}} of {{Crafting Intelligible Intelligence}}},
  abstract = {Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.},
  journal = {arxiv},
  author = {Weld, Daniel S. and Bansal, Gagan},
  month = mar,
  year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
  title = {Explanation {{Methods}} in {{Deep Learning}}: {{Users}}, {{Values}}, {{Concerns}} and {{Challenges}}},
  abstract = {Issues regarding explainable AI involve four components: users, laws \& regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.},
  journal = {arxiv},
  author = {Ras, Gabrielle and van Gerven, Marcel and Haselager, Pim},
  month = mar,
  year = {2018}
}

@article{http://arxiv.org/abs/1804.02527v1,
  title = {Visual {{Analytics}} for {{Explainable Deep Learning}}},
  abstract = {Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. In this paper, we review visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discuss potential challenges and future research directions.},
  journal = {arxiv},
  author = {Choo, Jaegul and Liu, Shixia},
  month = apr,
  year = {2018}
}

@article{http://arxiv.org/abs/1805.07468v1,
  title = {Unsupervised {{Learning}} of {{Neural Networks}} to {{Explain Neural Networks}}},
  abstract = {This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.},
  journal = {arxiv},
  author = {Zhang, Quanshi and Yang, Yu and Liu, Yuchen and Wu, Ying Nian and Zhu, Song-Chun},
  month = may,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.00069v3,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  journal = {arxiv},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  month = may,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.05337v2,
  title = {Hierarchical Interpretations for Neural Network Predictions},
  abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.},
  journal = {arxiv},
  author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.07470v1,
  title = {Contrastive {{Explanations}} with {{Local Foil Trees}}},
  abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like "Why this output (the fact) instead of that output (the foil)?" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.},
  journal = {arxiv},
  author = {van der Waa, Jasper and Robeer, Marcel and van Diggelen, Jurriaan and Brinkhuis, Matthieu and Neerincx, Mark},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.07538v2,
  title = {Towards {{Robust Interpretability}} with {{Self}}-{{Explaining Neural Networks}}},
  abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating \emph{a posteriori} explanations for previously trained models around specific predictions. \emph{Self-explaining} models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
  journal = {arxiv},
  author = {{Alvarez-Melis}, David and Jaakkola, Tommi S.},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.09809v1,
  title = {Generating {{Counterfactual Explanations}} with {{Natural Language}}},
  abstract = {Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classification decision if present in an image (e.g., "This is not a Scarlet Tanager because it does not have black wings.") We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classification decision if present in the image. To demonstrate our method we consider a fine-grained image classification task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.},
  journal = {arxiv},
  author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
  title = {Evaluating {{Feature Importance Estimates}}},
  abstract = {Interpretability methods should be both meaningful to a human and correctly explain model behavior. In this work, we propose a benchmark to evaluate the latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative accuracy of interpretability methods that estimate feature importance in deep neural networks. We evaluate commonly used interpretability methods and a set of recently proposed ensemble-based derivative approaches. Our results across several large-scale image classification datasets are consistent and thought-provoking -- we find that the formal methods we consider produce estimates that are less accurate or on par with a random designation of feature importance. However, certain derivative approaches that ensemble these estimates far outperform such a random guess. The manner of ensembling remains critical, we show that some approaches do no better than the underlying method but carry a far higher computational burden.},
  journal = {arxiv},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  month = jun,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.03418v1,
  title = {Interpreting and {{Explaining Deep Neural Networks}} for {{Classification}} of {{Audio Signals}}},
  abstract = {Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions. In this paper, two neural network architectures are trained on spectrogram and raw waveform data for audio classification tasks on a newly created audio dataset and layer-wise relevance propagation (LRP), a previously proposed interpretability method, is applied to investigate the models' feature selection and decision making. It is demonstrated that the networks are highly reliant on feature marked as relevant by LRP through systematic manipulation of the input data. Our results show that by making deep audio classifiers interpretable, one can analyze and compare the properties and strategies of different models beyond classification accuracy, which potentially opens up new ways for model improvements.},
  journal = {arxiv},
  author = {Becker, S\"oren and Ackermann, Marcel and Lapuschkin, Sebastian and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.04178v1,
  title = {Explainable {{Security}}},
  abstract = {The Defense Advanced Research Projects Agency (DARPA) recently launched the Explainable Artificial Intelligence (XAI) program that aims to create a suite of new AI techniques that enable end users to understand, appropriately trust, and effectively manage the emerging generation of AI systems. In this paper, inspired by DARPA's XAI program, we propose a new paradigm in security research: Explainable Security (XSec). We discuss the ``Six Ws'' of XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and complex characteristics: XSec involves several different stakeholders (i.e., the system's developers, analysts, users and attackers) and is multi-faceted by nature (as it requires reasoning about system model, threat model and properties of security, privacy and trust as well as about concrete attacks, vulnerabilities and countermeasures). We define a roadmap for XSec that identifies several possible research directions.},
  journal = {arxiv},
  author = {Vigan\`o, Luca and Magazzeni, Daniele},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.06978v1,
  title = {Improving {{Explainable Recommendations}} with {{Synthetic Reviews}}},
  abstract = {An important task for a recommender system to provide interpretable explanations for the user. This is important for the credibility of the system. Current interpretable recommender systems tend to focus on certain features known to be important to the user and offer their explanations in a structured form. It is well known that user generated reviews and feedback from reviewers have strong leverage over the users' decisions. On the other hand, recent text generation works have been shown to generate text of similar quality to human written text, and we aim to show that generated text can be successfully used to explain recommendations. In this paper, we propose a framework consisting of popular review-oriented generation models aiming to create personalised explanations for recommendations. The interpretations are generated at both character and word levels. We build a dataset containing reviewers' feedback from the Amazon books review dataset. Our cross-domain experiments are designed to bridge from natural language processing to the recommender system domain. Besides language model evaluation methods, we employ DeepCoNN, a novel review-oriented recommender system using a deep neural network, to evaluate the recommendation performance of generated reviews by root mean square error (RMSE). We demonstrate that the synthetic personalised reviews have better recommendation performance than human written reviews. To our knowledge, this presents the first machine-generated natural language explanations for rating prediction.},
  journal = {arxiv},
  author = {Ouyang, Sixun and Lawlor, Aonghus and Costa, Felipe and Dolog, Peter},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1807.07404v1,
  title = {Analyzing {{Hypersensitive AI}}: {{Instability}} in {{Corporate}}-{{Scale Machine Learning}}},
  abstract = {Predictive geometric models deliver excellent results for many Machine Learning use cases. Despite their undoubted performance, neural predictive algorithms can show unexpected degrees of instability and variance, particularly when applied to large datasets. We present an approach to measure changes in geometric models with respect to both output consistency and topological stability. Considering the example of a recommender system using word2vec, we analyze the influence of single data points, approximation methods and parameter settings. Our findings can help to stabilize models where needed and to detect differences in informational value of data points on a large scale.},
  journal = {arxiv},
  author = {Regneri, Michaela and Hoffmann, Malte and Kost, Jurij and Pietsch, Niklas and Schulz, Timo and Stamm, Sabine},
  month = jul,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.01591v1,
  title = {{{LISA}}: {{Explaining Recurrent Neural Network Judgments}} via {{Layer}}-{{wIse Semantic Accumulation}} and {{Example}} to {{Pattern Transformation}}},
  abstract = {Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: "How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response" (2) Example2pattern: "How the saliency patterns look like for each category in the data according to the network in decision making". We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.},
  journal = {arxiv},
  author = {Gupta, Pankaj and Sch\"utze, Hinrich},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
  title = {Learning {{Explanations}} from {{Language Data}}},
  abstract = {PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.},
  journal = {arxiv},
  author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.05054v1,
  title = {Shedding {{Light}} on {{Black Box Machine Learning Algorithms}}: {{Development}} of an {{Axiomatic Framework}} to {{Assess}} the {{Quality}} of {{Methods}} That {{Explain Individual Predictions}}},
  abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who book our next appointment at the hair salon or at that restaurant for dinner - machine learning systems are becoming increasingly ubiquitous. The main reason for this is that these methods boast remarkable predictive capabilities. However, most of these models remain black boxes, meaning that it is very challenging for humans to follow and understand their intricate inner workings. Consequently, interpretability has suffered under this ever-increasing complexity of machine learning models. Especially with regards to new regulations, such as the General Data Protection Regulation (GDPR), the necessity for plausibility and verifiability of predictions made by these black boxes is indispensable. Driven by the needs of industry and practice, the research community has recognised this interpretability problem and focussed on developing a growing number of so-called explanation methods over the past few years. These methods explain individual predictions made by black box machine learning models and help to recover some of the lost interpretability. With the proliferation of these explanation methods, it is, however, often unclear, which explanation method offers a higher explanation quality, or is generally better-suited for the situation at hand. In this thesis, we thus propose an axiomatic framework, which allows comparing the quality of different explanation methods amongst each other. Through experimental validation, we find that the developed framework is useful to assess the explanation quality of different explanation methods and reach conclusions that are consistent with independent research.},
  journal = {arxiv},
  author = {Honegger, Milo},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.07292v2,
  title = {K-{{meansNet}}: {{When}} k-Means {{Meets Differentiable Programming}}},
  abstract = {In this paper, we study two challenging problems. The first one is how to implement \emph{k}-means in the neural network, which enjoys efficient training based on the stochastic algorithm. The second one is how to enhance the interpretability of network design for clustering. To solve the problems, we propose a neural network which is a novel formulation of the vanilla k-means objective. Our contribution is in twofold. From the view of neural networks, the proposed \emph{k}-meansNet is with explicit interpretability in neural processing. We could understand not only why the network structure is presented like itself but also why it could perform data clustering. Such an interpretable neural network remarkably differs from the existing works that usually employ visualization technique to explain the result of the neural network. From the view of \emph{k}-means, three highly desired properties are achieved, i.e. robustness to initialization, the capability of handling new coming data, and provable convergence. Extensive experimental studies show that our method achieves promising performance comparing with 12 clustering methods on some challenging datasets.},
  journal = {arxiv},
  author = {Peng, Xi and Tsang, Ivor W. and Zhou, Joey Tianyi and Zhu, Hongyuan},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.09551v1,
  title = {Explaining {{Character}}-{{Aware Neural Networks}} for {{Word}}-{{Level Prediction}}: {{Do They Discover Linguistic Rules}}?},
  abstract = {Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules. Our implementation can be found at https://github.com/FredericGodin/ContextualDecomposition-NLP .},
  journal = {arxiv},
  author = {Godin, Fr\'ederic and Demuynck, Kris and Dambre, Joni and Neve, Wesley De and Demeester, Thomas},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1808.09744v1,
  title = {Rule Induction for Global Explanation of Trained Models},
  abstract = {Understanding the behavior of a trained network and finding explanations for its outputs is important for improving the network's performance and generalization ability, and for ensuring trust in automated systems. Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network. However, the relations between different features and classes are lost in most cases. We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network. We first calculate the importance of the features in the trained network. We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and finally fit a rule induction model to explain the model predictions. We find that the output rule-sets can explain the predictions of a neural network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80. We make the code available at https://github.com/clips/interpret\textsubscript{w}ith\textsubscript{r}ules.},
  journal = {arxiv},
  author = {Sushil, Madhumita and {\v S}uster, Simon and Daelemans, Walter},
  month = aug,
  year = {2018}
}

@article{http://arxiv.org/abs/1809.02479v1,
  title = {Convolutional {{Neural Network}}: {{Text Classification Model}} for {{Open Domain Question Answering System}}},
  abstract = {Recently machine learning is being applied to almost every data domain one of which is Question Answering Systems (QAS). A typical Question Answering System is fairly an information retrieval system, which matches documents or text and retrieve the most accurate one. The idea of open domain question answering system put forth, involves convolutional neural network text classifiers. The Classification model presented in this paper is multi-class text classifier. The neural network classifier can be trained on large dataset. We report series of experiments conducted on Convolution Neural Network (CNN) by training it on two different datasets. Neural network model is trained on top of word embedding. Softmax layer is applied to calculate loss and mapping of semantically related words. Gathered results can help justify the fact that proposed hypothetical QAS is feasible. We further propose a method to integrate Convolutional Neural Network Classifier to an open domain question answering system. The idea of Open domain will be further explained, but the generality of it indicates to the system of domain specific trainable models, thus making it an open domain.},
  journal = {arxiv},
  author = {Amin, Muhammad Zain and Nadeem, Noman},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1809.08037v1,
  title = {Understanding {{Convolutional Neural Networks}} for {{Text Classification}}},
  abstract = {We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions).},
  journal = {arxiv},
  author = {Jacovi, Alon and Shalom, Oren Sar and Goldberg, Yoav},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.00024v1,
  title = {Explainable {{Black}}-{{Box Attacks Against Model}}-Based {{Authentication}}},
  abstract = {Establishing unique identities for both humans and end systems has been an active research problem in the security community, giving rise to innovative machine learning-based authentication techniques. Although such techniques offer an automated method to establish identity, they have not been vetted against sophisticated attacks that target their core machine learning technique. This paper demonstrates that mimicking the unique signatures generated by host fingerprinting and biometric authentication systems is possible. We expose the ineffectiveness of underlying machine learning classification models by constructing a blind attack based around the query synthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an attack in under 130 queries on a state-of-the-art face authentication system, and under 100 queries on a host authentication system. We examine how these attacks can be defended against and explore their limitations. XAI provides an effective means for adversaries to infer decision boundaries and provides a new way forward in constructing attacks against systems using machine learning models for authentication.},
  journal = {arxiv},
  author = {Garcia, Washington and Choi, Joseph I. and Adari, Suman K. and Jha, Somesh and Butler, Kevin R. B.},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
  title = {Training {{Machine Learning Models}} by {{Regularizing}} Their {{Explanations}}},
  abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).},
  journal = {arxiv},
  author = {Ross, Andrew Slavin},
  month = sep,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
  title = {Local {{Interpretable Model}}-Agnostic {{Explanations}} of {{Bayesian Predictive Models}} via {{Kullback}}-{{Leibler Projections}}},
  abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation fidelity and complexity. We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.},
  journal = {arxiv},
  author = {Peltola, Tomi},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.09312v1,
  title = {Analyzing and {{Interpreting Convolutional Neural Networks}} in {{NLP}}},
  abstract = {Convolutional neural networks have been successfully applied to various NLP tasks. However, it is not obvious whether they model different linguistic patterns such as negation, intensification, and clause compositionality to help the decision-making process. In this paper, we apply visualization techniques to observe how the model can capture different linguistic features and how these features can affect the performance of the model. Later on, we try to identify the model errors and their sources. We believe that interpreting CNNs is the first step to understand the underlying semantic features which can raise awareness to further improve the performance and explainability of CNN models.},
  journal = {arxiv},
  author = {Koupaee, Mahnaz and Wang, William Yang},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.13192v4,
  title = {Nearly-Tight Bounds on Linear Regions of Piecewise Linear Neural Networks},
  abstract = {The developments of deep neural networks (DNN) in recent years have ushered a brand new era of artificial intelligence. DNNs are proved to be excellent in solving very complex problems, e.g., visual recognition and text understanding, to the extent of competing with or even surpassing people. Despite inspiring and encouraging success of DNNs, thorough theoretical analyses still lack to unravel the mystery of their magics. The design of DNN structure is dominated by empirical results in terms of network depth, number of neurons and activations. A few of remarkable works published recently in an attempt to interpret DNNs have established the first glimpses of their internal mechanisms. Nevertheless, research on exploring how DNNs operate is still at the initial stage with plenty of room for refinement. In this paper, we extend precedent research on neural networks with piecewise linear activations (PLNN) concerning linear regions bounds. We present (i) the exact maximal number of linear regions for single layer PLNNs; (ii) a upper bound for multi-layer PLNNs; and (iii) a tighter upper bound for the maximal number of liner regions on rectifier networks. The derived bounds also indirectly explain why deep models are more powerful than shallow counterparts, and how non-linearity of activation functions impacts on expressiveness of networks.},
  journal = {arxiv},
  author = {Hu, Qiang and Zhang, Hao},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.13373v1,
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations that they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in-silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.},
  journal = {arxiv},
  author = {Barrett, David G. T. and Morcos, Ari S. and Macke, Jakob H.},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1810.13425v2,
  title = {Understanding {{Deep Neural Networks}} through {{Input Uncertainties}}},
  abstract = {Techniques for understanding the functioning of complex machine learning models are becoming increasingly popular, not only to improve the validation process, but also to extract new insights about the data via exploratory analysis. Though a large class of such tools currently exists, most assume that predictions are point estimates and use a sensitivity analysis of these estimates to interpret the model. Using lightweight probabilistic networks we show how including prediction uncertainties in the sensitivity analysis leads to: (i) more robust and generalizable models; and (ii) a new approach for model interpretation through uncertainty decomposition. In particular, we introduce a new regularization that takes both the mean and variance of a prediction into account and demonstrate that the resulting networks provide improved generalization to unseen data. Furthermore, we propose a new technique to explain prediction uncertainties through uncertainties in the input domain, thus providing new ways to validate and interpret deep learning models.},
  journal = {arxiv},
  author = {Thiagarajan, Jayaraman J. and Kim, Irene and Anirudh, Rushil and Bremer, Peer-Timo},
  month = oct,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.00196v1,
  title = {Towards {{Explainable NLP}}: {{A Generative Explanation Framework}} for {{Text Classification}}},
  abstract = {Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information is often ignored, and the systems do not explicitly generate the human-readable explanations. To better alleviate this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.},
  journal = {arxiv},
  author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.02783v1,
  title = {{{YASENN}}: {{Explaining Neural Networks}} via {{Partitioning Activation Sequences}}},
  abstract = {We introduce a novel approach to feed-forward neural network interpretation based on partitioning the space of sequences of neuron activations. In line with this approach, we propose a model-specific interpretation method, called YASENN. Our method inherits many advantages of model-agnostic distillation, such as an ability to focus on the particular input region and to express an explanation in terms of features different from those observed by a neural network. Moreover, examination of distillation error makes the method applicable to the problems with low tolerance to interpretation mistakes. Technically, YASENN distills the network with an ensemble of layer-wise gradient boosting decision trees and encodes the sequences of neuron activations with leaf indices. The finite number of unique codes induces a partitioning of the input space. Each partition may be described in a variety of ways, including examination of an interpretable model (e.g. a logistic regression or a decision tree) trained to discriminate between objects of those partitions. Our experiments provide an intuition behind the method and demonstrate revealed artifacts in neural network decision making.},
  journal = {arxiv},
  author = {Zharov, Yaroslav and Korzhenkov, Denis and Shvechikov, Pavel and Tuzhilin, Alexander},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.06471v2,
  title = {Towards {{Explainable Deep Learning}} for {{Credit Lending}}: {{A Case Study}}},
  abstract = {Deep learning adoption in the financial services industry has been limited due to a lack of model interpretability. However, several techniques have been proposed to explain predictions made by a neural network. We provide an initial investigation into these techniques for the assessment of credit risk with neural networks.},
  journal = {arxiv},
  author = {Modarres, Ceena and Ibrahim, Mark and Louie, Melissa and Paisley, John},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.07253v1,
  title = {Quantifying {{Uncertainties}} in {{Natural Language Processing Tasks}}},
  abstract = {Reliable uncertainty quantification is a first step towards building explainable, transparent, and accountable artificial intelligent systems. Recent progress in Bayesian deep learning has made such quantification realizable. In this paper, we propose novel methods to study the benefits of characterizing model and data uncertainties for natural language processing (NLP) tasks. With empirical experiments on sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models, we show that explicitly modeling uncertainties is not only necessary to measure output confidence levels, but also useful at enhancing model performances in various NLP tasks.},
  journal = {arxiv},
  author = {Xiao, Yijun and Wang, William Yang},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.08120v1,
  title = {Explaining {{Latent Factor Models}} for {{Recommendation}} with {{Influence Functions}}},
  abstract = {Latent factor models (LFMs) such as matrix factorization achieve the state-of-the-art performance among various Collaborative Filtering (CF) approaches for recommendation. Despite the high recommendation accuracy of LFMs, a critical issue to be resolved is the lack of explainability. Extensive efforts have been made in the literature to incorporate explainability into LFMs. However, they either rely on auxiliary information which may not be available in practice, or fail to provide easy-to-understand explanations. In this paper, we propose a fast influence analysis method named FIA, which successfully enforces explicit neighbor-style explanations to LFMs with the technique of influence functions stemmed from robust statistics. We first describe how to employ influence functions to LFMs to deliver neighbor-style explanations. Then we develop a novel influence computation algorithm for matrix factorization with high efficiency. We further extend it to the more general neural collaborative filtering and introduce an approximation algorithm to accelerate influence analysis over neural network models. Experimental results on real datasets demonstrate the correctness, efficiency and usefulness of our proposed method.},
  journal = {arxiv},
  author = {Cheng, Weiyu and Shen, Yanyan and Zhu, Yanmin and Huang, Linpeng},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.09725v1,
  title = {Interpretable {{Convolutional Filters}} with {{SincNet}}},
  abstract = {Deep learning is currently playing a crucial role toward higher levels of artificial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal "black-box" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques. This paper summarizes our recent efforts to develop a more interpretable neural model for directly processing speech from the raw waveform. In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover more meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized filter-bank front-end, that only depends on some parameters with a clear physical meaning. Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more interpretable than standard CNNs.},
  journal = {arxiv},
  author = {Ravanelli, Mirco and Bengio, Yoshua},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.10799v1,
  title = {What Is {{Interpretable}}? {{Using Machine Learning}} to {{Design Interpretable Decision}}-{{Support Systems}}},
  abstract = {Recent efforts in Machine Learning (ML) interpretability have focused on creating methods for explaining black-box ML models. However, these methods rely on the assumption that simple approximations, such as linear models or decision-trees, are inherently human-interpretable, which has not been empirically tested. Additionally, past efforts have focused exclusively on comprehension, neglecting to explore the trust component necessary to convince non-technical experts, such as clinicians, to utilize ML models in practice. In this paper, we posit that reinforcement learning (RL) can be used to learn what is interpretable to different users and, consequently, build their trust in ML models. To validate this idea, we first train a neural network to provide risk assessments for heart failure patients. We then design a RL-based clinical decision-support system (DSS) around the neural network model, which can learn from its interactions with users. We conduct an experiment involving a diverse set of clinicians from multiple institutions in three different countries. Our results demonstrate that ML experts cannot accurately predict which system outputs will maximize clinicians' confidence in the underlying neural network model, and suggest additional findings that have broad implications to the future of research into ML interpretability and the use of ML in medicine.},
  journal = {arxiv},
  author = {Lahav, Owen and Mastronarde, Nicholas and van der Schaar, Mihaela},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
  title = {A {{Survey}} of {{Evaluation Methods}} and {{Measures}} for {{Interpretable Machine Learning}}},
  abstract = {The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.},
  journal = {arxiv},
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1811.12615v1,
  title = {An {{Interpretable Model}} with {{Globally Consistent Explanations}} for {{Credit Risk}}},
  abstract = {We propose a possible solution to a public challenge posed by the Fair Isaac Corporation (FICO), which is to provide an explainable model for credit risk assessment. Rather than present a black box model and explain it afterwards, we provide a globally interpretable model that is as accurate as other neural networks. Our "two-layer additive risk model" is decomposable into subscales, where each node in the second layer represents a meaningful subscale, and all of the nonlinearities are transparent. We provide three types of explanations that are simpler than, but consistent with, the global model. One of these explanation methods involves solving a minimum set cover problem to find high-support globally-consistent explanations. We present a new online visualization tool to allow users to explore the global model and its explanations.},
  journal = {arxiv},
  author = {Chen, Chaofan and Lin, Kangcheng and Rudin, Cynthia and Shaposhnik, Yaron and Wang, Sijia and Wang, Tong},
  month = nov,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
  title = {Sensitivity Based {{Neural Networks Explanations}}},
  abstract = {Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque "black boxes". The difficulty of interpreting the predictions of a neural network often prevents its use in fields where explainability is important, such as the financial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.},
  journal = {arxiv},
  author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.04801v1,
  title = {Can {{I}} Trust You More? {{Model}}-{{Agnostic Hierarchical Explanations}}},
  abstract = {Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mah\'e, a novel approach to provide Model-agnostic hierarchical \'explanations of how powerful machine learning models, such as deep neural networks, capture these interactions as either dependent on or free of the context of data instances. Specifically, Mah\'e provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. Experimental results show that Mah\'e obtains improved local interaction interpretations over state-of-the-art methods and successfully explains interactions that are context-free.},
  journal = {arxiv},
  author = {Tsang, Michael and Sun, Youbang and Ren, Dongxu and Liu, Yan},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.07169v1,
  title = {Explaining {{Neural Networks Semantically}} and {{Quantitatively}}},
  abstract = {This paper presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically. The analysis of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, but it is also of significant practical values in certain applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, so that we can use the explainable model to provide a quantitative explanation for the CNN prediction. We analyze the typical bias-interpreting problem of the explainable model and develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.},
  journal = {arxiv},
  author = {Chen, Runjin and Chen, Hao and Huang, Ge and Ren, Jie and Zhang, Quanshi},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1812.10537v2,
  title = {Prediction of {{Industrial Process Parameters}} Using {{Artificial Intelligence Algorithms}}},
  abstract = {In the present paper, a method of defining the industrial process parameters for a new product using machine learning algorithms will be presented. The study will describe how to go from the product characteristics till the prediction of the suitable machine parameters to produce a good quality of this product, and this is based on an historical training dataset of similar products with their respective process parameters. In the first part of our study, we will focus on the ultrasonic welding process definition, welding parameters and on how it operate. While in second part, we present the design and implementation of the prediction models such multiple linear regression, support vector regression, and we compare them to an artificial neural networks algorithm. In the following part, we present a new application of Convolutional Neural Networks (CNN) to the industrial process parameters prediction. In addition, we will propose the generalization approach of our CNN to any prediction problem of industrial process parameters. Finally the results of the four methods will be interpreted and discussed.},
  journal = {arxiv},
  author = {Khdoudi, Abdelmoula and Masrour, Tawfik},
  month = dec,
  year = {2018}
}

@article{http://arxiv.org/abs/1901.03838v1,
  title = {Enhancing {{Explainability}} of {{Neural Networks}} through {{Architecture Constraints}}},
  abstract = {Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. The neural networks are known to possess good prediction performance, but lack of sufficient model explainability. In this paper, we propose to enhance the explainability of neural networks through the following architecture constraints: a) sparse additive subnetworks; b) orthogonal projection pursuit; and c) smooth function approximation. It leads to a sparse, orthogonal and smooth explainable neural network (SOSxNN). The multiple parameters in the SOSxNN model are simultaneously estimated by a modified mini-batch gradient descent algorithm based on the backpropagation technique for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. The hyperparameters controlling the sparse and smooth constraints are optimized by the grid search. Through simulation studies, we compare the SOSxNN method to several benchmark methods including least absolute shrinkage and selection operator, support vector machine, random forest, and multi-layer perceptron. It is shown that proposed model keeps the flexibility of pursuing prediction accuracy while attaining the improved interpretability, which can be therefore used as a promising surrogate model for complex model approximation. Finally, the real data example from the Lending Club is employed as a showcase of the SOSxNN application.},
  journal = {arxiv},
  author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.06560v1,
  title = {Explaining {{Explanations}} to {{Society}}},
  abstract = {There is a disconnect between explanatory artificial intelligence (XAI) methods and the types of explanations that are useful for and demanded by society (policy makers, government officials, etc.) Questions that experts in artificial intelligence (AI) ask opaque systems provide inside explanations, focused on debugging, reliability, and validation. These are different from those that society will ask of these systems to build trust and confidence in their decisions. Although explanatory AI systems can answer many questions that experts desire, they often don't explain why they made decisions in a way that is precise (true to the model) and understandable to humans. These outside explanations can be used to build trust, comply with regulatory and policy changes, and act as external validation. In this paper, we focus on XAI methods for deep neural networks (DNNs) because of DNNs' use in decision-making and inherent opacity. We explore the types of questions that explanatory DNN systems can answer and discuss challenges in building explanatory systems that provide outside explanations for societal requirements and benefit.},
  journal = {arxiv},
  author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.07538v1,
  title = {Unsupervised {{Learning}} of {{Neural Networks}} to {{Explain Neural Networks}} (Extended Abstract)},
  abstract = {This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., the explainer uses interpretable visual concepts to explain features in middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the explainer performs like an auto-encoder, which decomposes the feature maps into object-part features. The object-part features are learned to reconstruct CNN features without much loss of information. We can consider the disentangled representations of object parts a paraphrase of CNN features, which help people understand the knowledge encoded by the CNN. More crucially, we learn the explainer via knowledge distillation without using any annotations of object parts or textures for supervision. In experiments, our method was widely used to interpret features of different benchmark CNNs, and explainers significantly boosted the feature interpretability without hurting the discrimination power of the CNNs.},
  journal = {arxiv},
  author = {Zhang, Quanshi and Yang, Yu and Wu, Ying Nian},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.08547v1,
  title = {Human-Centric {{Transfer Learning Explanation}} via {{Knowledge Graph}} [{{Extended Abstract}}]},
  abstract = {Transfer learning which aims at utilizing knowledge learned from one problem (source domain) to solve another different but related problem (target domain) has attracted wide research attentions. However, the current transfer learning methods are mostly uninterpretable, especially to people without ML expertise. In this extended abstract, we brief introduce two knowledge graph (KG) based frameworks towards human understandable transfer learning explanation. The first one explains the transferability of features learned by Convolutional Neural Network (CNN) from one domain to another through pre-training and fine-tuning, while the second justifies the model of a target domain predicted by models from multiple source domains in zero-shot learning (ZSL). Both methods utilize KG and its reasoning capability to provide rich and human understandable explanations to the transfer procedure.},
  journal = {arxiv},
  author = {Geng, Yuxia and Chen, Jiaoyan and {Jimenez-Ruiz}, Ernesto and Chen, Huajun},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.09813v1,
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing and show it can be re-interpreted as word transformation, a mathematical description of "wₓ is to w\textsubscript{y}". From these concepts we prove existence of the linear relationship between W2V-type embeddings that underlies the analogical phenomenon, and identify explicit error terms in the relationship.},
  journal = {arxiv},
  author = {Allen, Carl and Hospedales, Timothy},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1901.09839v1,
  title = {Interpreting {{Deep Neural Networks Through Variable Importance}}},
  abstract = {While the success of deep neural networks (DNNs) is well-established across a variety of domains, our ability to explain and interpret these methods is limited. Unlike previously proposed local methods which try to explain particular classification decisions, we focus on global interpretability and ask a universally applicable question: given a trained model, which features are the most important? In the context of neural networks, a feature is rarely important on its own, so our strategy is specifically designed to leverage partial covariance structures and incorporate variable dependence into feature ranking. Our methodological contributions in this paper are two-fold. First, we propose an effect size analogue for DNNs that is appropriate for applications with highly collinear predictors (ubiquitous in computer vision). Second, we extend the recently proposed "RelATive cEntrality" (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting. RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance. We apply our framework to three broad application areas: computer vision, natural language processing, and social science.},
  journal = {arxiv},
  author = {{Ish-Horowicz}, Jonathan and Udwin, Dana and Flaxman, Seth and Filippi, Sarah and Crawford, Lorin},
  month = jan,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.02041v1,
  title = {Fooling {{Neural Network Interpretations}} via {{Adversarial Model Manipulation}}},
  abstract = {We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original model. By incorporating the interpretation results directly in the regularization term of the objective function for fine-tuning, we show that the state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled with our model manipulation. We propose two types of fooling, passive and active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.},
  journal = {arxiv},
  author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.02384v1,
  title = {Global {{Explanations}} of {{Neural Networks}}: {{Mapping}} the {{Landscape}} of {{Predictions}}},
  abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.},
  journal = {arxiv},
  author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.02497v1,
  title = {{{CHIP}}: {{Channel}}-Wise {{Disentangled Interpretation}} of {{Deep Convolutional Neural Networks}}},
  abstract = {With the widespread applications of deep convolutional neural networks (DCNNs), it becomes increasingly important for DCNNs not only to make accurate predictions but also to explain how they make their decisions. In this work, we propose a CHannel-wise disentangled InterPretation (CHIP) model to give the visual interpretation to the predictions of DCNNs. The proposed model distills the class-discriminative importance of channels in networks by utilizing the sparse regularization. Here, we first introduce the network perturbation technique to learn the model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present the class-discriminative visual interpretation for specific predictions of networks. It is noteworthy that the proposed model is able to interpret different layers of networks without re-training. By combining the distilled interpretation knowledge in different layers, we further propose the Refined CHIP visual interpretation that is both high-resolution and class-discriminative. Experimental results on the standard dataset demonstrate that the proposed model provides promising visual interpretation for the predictions of networks in image classification task compared with existing visual interpretation methods. Besides, the proposed method outperforms related approaches in the application of ILSVRC 2015 weakly-supervised localization task.},
  journal = {arxiv},
  author = {Cui, Xinrui and Wang, Dan and Wang, Z. Jane},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1902.03380v2,
  title = {When {{Causal Intervention Meets Image Masking}} and {{Adversarial Perturbation}} for {{Deep Neural Networks}}},
  abstract = {Discovering and exploiting the causality in deep neural networks (DNNs) are crucial challenges for understanding and reasoning causal effects (CE) on an explainable visual model. "Intervention" has been widely used for recognizing a causal relation ontologically. In this paper, we propose a causal inference framework for visual reasoning via do-calculus. To study the intervention effects on pixel-level feature(s) for causal reasoning, we introduce pixel-wise masking and adversarial perturbation. In our framework, CE is calculated using features in a latent space and perturbed prediction from a DNN-based model. We further provide a first look into the characteristics of discovered CE of adversarially perturbed images generated by gradient-based methods. Experimental results show that CE is a competitive and robust index for understanding DNNs when compared with conventional methods such as class-activation mappings (CAMs) on the ChestX-ray 14 dataset for human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds promises for detecting adversarial examples as it possesses distinct characteristics in the presence of adversarial perturbations.},
  journal = {arxiv},
  author = {Yang, Chao-Han Huck and Liu, Yi-Chieh and Chen, Pin-Yu and Ma, Xiaoli and Tsai, Yi-Chang James},
  month = feb,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
  title = {Aggregating Explainability Methods for Neural Networks Stabilizes Explanations},
  abstract = {Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation. In fact, most works rely on manually assessing the explanation to evaluate the quality of a method. This injects uncertainty in the explanation process along several dimensions: Which explanation method to apply? Who should we ask to evaluate it and which criteria should be used for the evaluation? Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. Our findings show that the aggregation is more robust, well-aligned with human explanations and can attribute relevance to a broader set of features (completeness). Second, we propose a novel way of evaluating explanation methods that circumvents the need for manual evaluation and is not reliant on the alignment of neural networks and humans decision processes.},
  journal = {arxiv},
  author = {Rieger, Laura and Hansen, Lars Kai},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.10246v1,
  title = {Computational and {{Robotic Models}} of {{Early Language Development}}: {{A Review}}},
  abstract = {We review computational and robotics models of early language learning and development. We first explain why and how these models are used to understand better how children learn language. We argue that they provide concrete theories of language learning as a complex dynamic system, complementing traditional methods in psychology and linguistics. We review different modeling formalisms, grounded in techniques from machine learning and artificial intelligence such as Bayesian and neural network approaches. We then discuss their role in understanding several key mechanisms of language development: cross-situational statistical learning, embodiment, situated social interaction, intrinsically motivated learning, and cultural evolution. We conclude by discussing future challenges for research, including modeling of large-scale empirical data about language acquisition in real-world environments. Keywords: Early language learning, Computational and robotic models, machine learning, development, embodiment, social interaction, intrinsic motivation, self-organization, dynamical systems, complexity.},
  journal = {arxiv},
  author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
  title = {{{iBreakDown}}: {{Uncertainty}} of {{Model Explanations}} for {{Non}}-Additive {{Predictive Models}}},
  abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently. Explainability is being presented as a remedy for lack of trust in model predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise instance level interpretability for any complex machine learning model. But how certain are these explanations? Can we rely on additive explanations for non-additive models? In this paper, we examine the behavior of model explainers under the presence of interactions. We define two sources of uncertainty, model level uncertainty, and explanation level uncertainty. We show that adding interactions reduces explanation level uncertainty. We introduce a new method iBreakDown that generates non-additive explanations with local interaction.},
  journal = {arxiv},
  author = {Gosiewska, Alicja and Biecek, Przemyslaw},
  month = mar,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.02323v2,
  title = {Summit: {{Scaling Deep Learning Interpretability}} by {{Visualizing Activation}} and {{Attribution Summarizations}}},
  abstract = {Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed based off of millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, the first interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a state-of-the-art image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.},
  journal = {arxiv},
  author = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.04063v1,
  title = {Analyzing and {{Interpreting Neural Networks}} for {{NLP}}: {{A Report}} on the {{First BlackboxNLP Workshop}}},
  abstract = {The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques specifically developed for analyzing and understanding the inner-workings and representations acquired by neural models of language. Approaches included: systematic manipulation of input to neural networks and investigating the impact on their performance, testing whether interpretable knowledge can be decoded from intermediate representations acquired by neural networks, proposing modifications to neural network architectures to make their knowledge state or generated output more explainable, and examining the performance of networks on simplified or formal languages. Here we review a number of representative studies in each category.},
  journal = {arxiv},
  author = {Alishahi, Afra and Chrupa\l{}a, Grzegorz and Linzen, Tal},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.05488v1,
  title = {Deep {{Neural Network Ensembles}}},
  abstract = {Current deep neural networks suffer from two problems; first, they are hard to interpret, and second, they suffer from overfitting. There have been many attempts to define interpretability in neural networks, but they typically lack causality or generality. A myriad of regularization techniques have been developed to prevent overfitting, and this has driven deep learning to become the hot topic it is today; however, while most regularization techniques are justified empirically and even intuitively, there is not much underlying theory. This paper argues that to extract the features used in neural networks to make decisions, it's important to look at the paths between clusters existing in the hidden spaces of neural networks. These features are of particular interest because they reflect the true decision making process of the neural network. This analysis is then furthered to present an ensemble algorithm for arbitrary neural networks which has guarantees for test accuracy. Finally, a discussion detailing the aforementioned guarantees is introduced and the implications to neural networks, including an intuitive explanation for all current regularization methods, are presented. The ensemble algorithm has generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved test accuracy for all models it has been applied to.},
  journal = {arxiv},
  author = {Tao, Sean},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.08939v1,
  title = {Understanding {{Neural Networks}} via {{Feature Visualization}}: {{A}} Survey},
  abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
  journal = {arxiv},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1904.09273v1,
  title = {"{{Why}} Did You Do That?": {{Explaining}} Black Box Models with {{Inductive Synthesis}}},
  abstract = {By their nature, the composition of black box models is opaque. This makes the ability to generate explanations for the response to stimuli challenging. The importance of explaining black box models has become increasingly important given the prevalence of AI and ML systems and the need to build legal and regulatory frameworks around them. Such explanations can also increase trust in these uncertain systems. In our paper we present RICE, a method for generating explanations of the behaviour of black box models by (1) probing a model to extract model output examples using sensitivity analysis; (2) applying CNPInduce, a method for inductive logic program synthesis, to generate logic programs based on critical input-output pairs; and (3) interpreting the target program as a human-readable explanation. We demonstrate the application of our method by generating explanations of an artificial neural network trained to follow simple traffic rules in a hypothetical self-driving car simulation. We conclude with a discussion on the scalability and usability of our approach and its potential applications to explanation-critical scenarios.},
  journal = {arxiv},
  author = {Pa{\c c}ac\i, G\"orkem and Johnson, David and McKeever, Steve and Hamfelt, Andreas},
  month = apr,
  year = {2019}
}

@article{http://arxiv.org/abs/1905.00122v1,
  title = {To Believe or Not to Believe: {{Validating}} Explanation Fidelity for Dynamic Malware Analysis},
  abstract = {Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications.},
  journal = {arxiv},
  author = {Chen, Li and Yagemann, Carter and Downing, Evan},
  month = apr,
  year = {2019}
}

@inproceedings{Hu:2017:IWI:3132847.3133198,
  series = {{{CIKM}} '17},
  title = {{{IDM}} 2017: {{Workshop}} on {{Interpretable Data Mining}} -- {{Bridging}} the {{Gap Between Shallow}} and {{Deep Models}}},
  isbn = {978-1-4503-4918-5},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/3132847.3133198},
  author = {Hu, Xia and Ji, Shuiwang},
  year = {2017},
  keywords = {data mining,machine learning,interpretability,deep models,shallow models},
  pages = {2565-2566},
  acmid = {3133198},
  numpages = {2}
}

@article{Israelsen:2019:XAY:3303862.3267338,
  title = {\&\#{{x201C}};{{Dave}}...{{I Can Assure You}} ...{{That It}}\&\#x2019;s {{Going}} to {{Be All Right}} ...\&\#{{x201D}}; {{A Definition}}, {{Case}} for, and {{Survey}} of {{Algorithmic Assurances}} in {{Human}}-{{Autonomy Trust Relationships}}},
  volume = {51},
  issn = {0360-0300},
  abstract = {People who design, use, and are affected by autonomous artificially intelligent agents want to be able to trust
such agents\textemdash{}that is, to know that these agents will perform correctly, to understand the reasoning behind
their actions, and to know how to use them appropriately. Many techniques have been devised to assess
and influence human trust in artificially intelligent agents. However, these approaches are typically ad hoc
and have not been formally related to each other or to formal trust models. This article presents a survey
of algorithmic assurances, i.e., programmed components of agent operation that are expressly designed to
calibrate user trust in artificially intelligent agents. Algorithmic assurances are first formally defined and
classified from the perspective of formally modeled human-artificially intelligent agent trust relationships.
Building on these definitions, a synthesis of research across communities such as machine learning, human-
computer interaction, robotics, e-commerce, and others reveals that assurance algorithms naturally fall along
a spectrum in terms of their impact on an agent's core functionality, with seven notable classes ranging from
integral assurances (which impact an agent's core functionality) to supplemental assurances (which have
no direct effect on agent performance). Common approaches within each of these classes are identified and
discussed; benefits and drawbacks of different approaches are also investigated.},
  number = {6},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/3267338},
  author = {Israelsen, Brett W. and Ahmed, Nisar R.},
  month = jan,
  year = {2019},
  keywords = {transparency,fairness,interpretable machine learning,explainable artificial intelligence,accountability,algorithmic assurances,Human-computer trust},
  pages = {113:1-113:37},
  file = {/home/tim/Zotero/storage/F7JWPVE2/Israelsen and Ahmed - 2019 - &#x201C\;Dave...I Can Assure You ...That It&#x2019\;.pdf},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {3267338},
  articleno = {113},
  issue_date = {February 2019},
  numpages = {37}
}

@inproceedings{itoTextVisualizingNeuralNetwork2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Text-{{Visualizing Neural Network Model}}: {{Understanding Online Financial Textual Data}}},
  isbn = {978-3-319-93040-4},
  shorttitle = {Text-{{Visualizing Neural Network Model}}},
  abstract = {This study aims to visualize financial documents to swiftly obtain market sentiment information from these documents and determine the reason for which sentiment decisions are made. This type of visualization is considered helpful for nonexperts to easily understand technical documents such as financial reports. To achieve this, we propose a novel interpretable neural network (NN) architecture called gradient interpretable NN (GINN). GINN can visualize both the market sentiment score from a whole financial document and the sentiment gradient scores in concept units. We experimentally demonstrate the validity of text visualization produced by GINN using a real textual dataset.},
  language = {English},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Ito, Tomoki and Sakaji, Hiroki and Tsubouchi, Kota and Izumi, Kiyoshi and Yamashita, Tatsuo},
  editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  year = {2018},
  keywords = {Interpretable neural network,Support system,Text mining},
  pages = {247-259}
}

@incollection{ivancevicIntroductionHumanComputational2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Introduction: {{Human}} and {{Computational Mind}}},
  isbn = {978-3-540-71561-0},
  shorttitle = {Introduction},
  language = {English},
  booktitle = {Computational {{Mind}}: {{A Complex Dynamics Perspective}}},
  publisher = {{Springer Berlin Heidelberg}},
  editor = {Ivancevic, Vladimir G. and Ivancevic, Tijana T.},
  year = {2007},
  keywords = {Adaptive Resonance Theory,Cellular Automaton,Horn Clause,Human Mind},
  pages = {1-269},
  doi = {10.1007/978-3-540-71561-0_1}
}

@inproceedings{Iyer:2018:TED:3278721.3278776,
  series = {{{AIES}} '18},
  title = {Transparency and {{Explanation}} in {{Deep Reinforcement Learning Neural Networks}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278776},
  author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
  year = {2018},
  keywords = {explainable ai,deep reinforcement learning,human factors,human-ai interaction,system transparency},
  pages = {144-150},
  acmid = {3278776},
  numpages = {7}
}

@inproceedings{Jacobson:2018:VNN:3243250.3243266,
  series = {{{PRAI}} 2018},
  title = {Visualizing {{Neural Networks}} for {{Pattern Recognition}}},
  isbn = {978-1-4503-6482-9},
  booktitle = {Proceedings of the {{International Conference}} on {{Pattern Recognition}} and {{Artificial Intelligence}}},
  publisher = {{ACM}},
  doi = {10.1145/3243250.3243266},
  author = {Jacobson, Victor and Li, J. Jenny and Tapia, Kevin and Morreale, Patricia},
  year = {2018},
  keywords = {Neural networks,pattern recognition,CNN,RNN},
  pages = {18-22},
  acmid = {3243266},
  numpages = {5}
}

@incollection{jinSimultaneousGenerationAccurate2006,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Simultaneous {{Generation}} of {{Accurate}} and {{Interpretable Neural Network Classifiers}}},
  isbn = {978-3-540-33019-6},
  abstract = {Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.},
  language = {English},
  booktitle = {Multi-{{Objective Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jin, Yaochu and Sendhoff, Bernhard and K\"orner, Edgar},
  editor = {Jin, Yaochu},
  year = {2006},
  keywords = {Hide Neuron,Mean Square Error,Multiobjective Optimization,Neural Network,Pareto Front},
  pages = {291-312},
  doi = {10.1007/3-540-33019-4_13}
}

@inproceedings{Kasneci:2016:LLW:2983323.2983746,
  series = {{{CIKM}} '16},
  title = {{{LICON}}: {{A Linear Weighting Scheme}} for the {{Contribution ofInput Variables}} in {{Deep Artificial Neural Networks}}},
  isbn = {978-1-4503-4073-1},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  publisher = {{ACM}},
  doi = {10.1145/2983323.2983746},
  author = {Kasneci, Gjergji and Gottron, Thomas},
  year = {2016},
  keywords = {artificial neural networks,contribution,explanation,input variables,linear weighting scheme},
  pages = {45-54},
  acmid = {2983746},
  numpages = {10}
}

@incollection{kayaMultimodalPersonalityTrait2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Multimodal {{Personality Trait Analysis}} for {{Explainable Modeling}} of {{Job Interview Decisions}}},
  isbn = {978-3-319-98131-4},
  abstract = {Automatic analysis of job interview screening decisions is useful for establishing the nature of biases that may play a role in such decisions. In particular, assessment of apparent personality gives insights into the first impressions evoked by a candidate. Such analysis tools can be used for training purposes, if they can be configured to provide appropriate and clear feedback. In this chapter, we describe a multimodal system that analyzes a short video of a job candidate, producing apparent personality scores and a prediction about whether the candidate will be invited for a further job interview or not. This system provides a visual and textual explanation about its decision, and was ranked first in the ChaLearn 2017 Job Candidate Screening Competition. We discuss the application scenario and the considerations from a broad perspective.},
  language = {English},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kaya, Heysem and Salah, Albert Ali},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable machine learning,Job candidate screening,Multimodal affective computing,Personality trait analysis},
  pages = {255-275},
  doi = {10.1007/978-3-319-98131-4_10}
}

@incollection{kimExplainableDeepDriving2018,
  address = {Cham},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  title = {Explainable {{Deep Driving}} by {{Visualizing Causal Attention}}},
  isbn = {978-3-319-98131-4},
  abstract = {Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable\textemdash{}they should provide easy-to-interpret rationales for their behavior\textemdash{}so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here, we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolutional network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 h of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network highlights interpretable features that are used by humans while driving, and causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.},
  language = {English},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  author = {Kim, Jinkyu and Canny, John},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
  year = {2018},
  keywords = {Explainable AI,Self-driving vehicles,Visual attention},
  pages = {173-193},
  doi = {10.1007/978-3-319-98131-4_8}
}

@inproceedings{Kouki:2019:PEH:3301275.3302306,
  series = {{{IUI}} '19},
  title = {Personalized {{Explanations}} for {{Hybrid Recommender Systems}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302306},
  author = {Kouki, Pigi and Schaffer, James and Pujara, Jay and O'Donovan, John and Getoor, Lise},
  year = {2019},
  keywords = {explainable artificial intelligence,explainable intelligent user interfaces,explainable recommender systems,hybrid recommender systems},
  pages = {379-390},
  acmid = {3302306},
  numpages = {12}
}

@inproceedings{Kuo:2008:FEA:1486927.1486968,
  series = {{{WI}}-{{IAT}} '08},
  title = {Finding {{Explanations}} for {{Assisting Pattern Interpretation}}},
  isbn = {978-0-7695-3496-1},
  booktitle = {Proceedings of the 2008 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}} - {{Volume}} 01},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WIIAT.2008.330},
  author = {Kuo, Yen-Ting and Sonenberg, Liz and Lonie, Andrew},
  year = {2008},
  keywords = {explanation generation,pattern interpretation},
  pages = {48-51},
  acmid = {1486968},
  numpages = {4}
}

@article{Lapuschkin:2016:LTA:2946645.3007067,
  title = {The {{LRP Toolbox}} for {{Artificial Neural Networks}}},
  volume = {17},
  issn = {1532-4435},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gr\'egoire and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = jan,
  year = {2016},
  keywords = {deep learning,artificial neural networks,computer vision,explaining classifiers,layer-wise relevance propagation},
  pages = {3938-3942},
  publisher = {{JMLR.org}},
  acmid = {3007067},
  issue_date = {January 2016},
  numpages = {5}
}

@inproceedings{laugelComparisonBasedInverseClassification2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}}},
  isbn = {978-3-319-91473-2},
  abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
  language = {English},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
  publisher = {{Springer International Publishing}},
  author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2018},
  keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
  pages = {100-111}
}

@incollection{liaoMiningHumanInterpretable2006,
  address = {Boston, MA},
  series = {Massive {{Computing}}},
  title = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}: {{An Overview}}},
  isbn = {978-0-387-34296-2},
  shorttitle = {Mining {{Human Interpretable Knowledge}} with {{Fuzzy Modeling Methods}}},
  abstract = {This chapter focuses on one particular class of data mining methodologies that expresses the mined knowledge in the form of fuzzy If-Then rules or fuzzy decision trees that can be easily understood by a human. Past studies on generating fuzzy If-Then rules (mostly from exemplar crisp data and a few from exemplar fuzzy data) are grouped into six major categories: grid partitioning, fuzzy clustering, genetic algorithms, neural networks, hybrid methods, and others. The representative method in each category is detailed. The latest improvements and advancements in each category are also reviewed. Similarly, past studies on generating fuzzy decision trees (from exemplar nominal and/or numeric data as well as from exemplar fuzzy data) are surveyed. The essence of each method is presented. Moreover, we discuss selected studies that address most of the necessary conditions for a fuzzy model to be interpretable and highlight areas for future studies. To give an idea of where fuzzy modeling methods have been applied, major application areas are also summarized.},
  language = {English},
  booktitle = {Data {{Mining}} and {{Knowledge Discovery Approaches Based}} on {{Rule Induction Techniques}}},
  publisher = {{Springer US}},
  author = {Liao, T. Warren},
  editor = {Triantaphyllou, Evangelos and Felici, Giovanni},
  year = {2006},
  keywords = {Data mining,Neural networks,Fuzzy clustering,Fuzzy decision trees,Fuzzy If-Then rules,Fuzzy modeling,Fuzzy-neural networks,Genetic algorithms},
  pages = {495-550},
  doi = {10.1007/0-387-34296-6_15}
}

@inproceedings{Lim:2019:EES:3308557.3313112,
  series = {{{IUI}} '19},
  title = {{{ExSS}}: {{Explainable Smart Systems}} 2019},
  isbn = {978-1-4503-6673-1},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}: {{Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3308557.3313112},
  author = {Lim, Brian and Sarkar, Advait and {Smith-Renner}, Alison and Stumpf, Simone},
  year = {2019},
  keywords = {transparency,machine learning,explanations,intelligibility,intelligent systems,visualizations},
  pages = {125-126},
  acmid = {3313112},
  numpages = {2}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
  isbn = {978-3-319-03200-9},
  abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
  language = {English},
  booktitle = {Fuzzy {{Logic}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Lisboa, P. J. G.},
  editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
  year = {2013},
  keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
  pages = {15-21}
}

@inproceedings{Liu:2018:INE:3219819.3220001,
  series = {{{KDD}} '18},
  title = {On {{Interpretation}} of {{Network Embedding}} via {{Taxonomy Induction}}},
  isbn = {978-1-4503-5552-0},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \&\#38; {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220001},
  author = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
  year = {2018},
  keywords = {taxonomy,machine learning interpretation,network embedding},
  pages = {1812-1820},
  acmid = {3220001},
  numpages = {9}
}

@inproceedings{Liu:2019:RIS:3289600.3290960,
  series = {{{WSDM}} '19},
  title = {Representation {{Interpretation}} with {{Spatial Encoding}} and {{Multimodal Analytics}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290960},
  author = {Liu, Ninghao and Du, Mengnan and Hu, Xia},
  year = {2019},
  keywords = {recommender systems,interpretation,representation learning},
  pages = {60-68},
  acmid = {3290960},
  numpages = {9}
}

@incollection{liuInterpretabilityComputationalModels2016,
  address = {Cham},
  series = {Studies in {{Computational Intelligence}}},
  title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
  isbn = {978-3-319-30319-2},
  abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
  language = {English},
  booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  year = {2016},
  keywords = {Machine learning,Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Rule based networks,Rule based systems,Sentiment prediction},
  pages = {199-220},
  doi = {10.1007/978-3-319-30319-2_9}
}

@incollection{luceBasicsArtificialIntelligence2019,
  address = {Berkeley, CA},
  title = {Basics of {{Artificial Intelligence}}},
  isbn = {978-1-4842-3931-5},
  abstract = {Fashion not only provides functional purpose, but captures mysterious and elusive aspects of being human. Fashion expresses and invokes human emotion and creativity. How we look and sometimes even how we feel is intertwined in this industry. Fashion has always been forward looking, grabbing onto new technologies as they arise. Artificial intelligence is no exception, and it's moving as quickly as fashion does.},
  language = {English},
  booktitle = {Artificial {{Intelligence}} for {{Fashion}}: {{How AI Is Revolutionizing}} the {{Fashion Industry}}},
  publisher = {{Apress}},
  author = {Luce, Leanne},
  editor = {Luce, Leanne},
  year = {2019},
  pages = {3-18},
  doi = {10.1007/978-1-4842-3931-5_1}
}

@incollection{lughoferModelExplanationInterpretation2018,
  address = {Cham},
  series = {Human\textendash{{Computer Interaction Series}}},
  title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
  isbn = {978-3-319-90403-0},
  abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
  language = {English},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  publisher = {{Springer International Publishing}},
  author = {Lughofer, Edwin},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  pages = {177-221},
  doi = {10.1007/978-3-319-90403-0_10}
}

@inproceedings{Mei:2018:IGA:3209280.3229119,
  series = {{{DocEng}} '18},
  title = {Integrating {{Global Attention}} for {{Pairwise Text Comparison}}},
  isbn = {978-1-4503-5769-2},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Document Engineering}} 2018},
  publisher = {{ACM}},
  doi = {10.1145/3209280.3229119},
  author = {Mei, Jie and Jiang, Xiang and Islam, Aminul and Moh'd, Abidalrahman and Milios, Evangelos},
  year = {2018},
  keywords = {Neural Network,Attention Mechanism,Self-Attention},
  pages = {49:1-49:4},
  acmid = {3229119},
  articleno = {49},
  numpages = {4}
}

@incollection{miradiKnowledgeDiscoveryData2009,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Knowledge {{Discovery}} and {{Data Mining Using Artificial Intelligence}} to {{Unravel Porous Asphalt Concrete}} in the {{Netherlands}}},
  isbn = {978-3-642-04586-8},
  abstract = {The main goal of this study was to discover knowledge from data about Porous Asphalt Concrete (PAC) roads to achieve a better understanding of the behavior of them and via this understanding improve pavement quality and enhance its lifespan. The knowledge discovery process includes five steps, being understanding the problem, understanding the data, data preparation, data mining (modeling), and the interpretation/evaluation of the results of the models. At the moment, almost 75\% of the Dutch motorways network has a PAC top layer. The main damage of PAC is raveling, which is when the top layer of the road loses stones. The SHRP-NL databases provided ten years of material property data from PAC roads. The data for climate and traffic were obtained from databases of the Royal Dutch Meteorological Institute (KNMI) and the Ministry of Transport and Water Management, respectively. Due to the low number of data points (74 data points), an extensive variable selection was performed using eight different methods to determine the four or five most influential input variables and consequently reduce the input dimension. These methods were decision trees, genetic polynomial, artificial neural network, rough set theory, correlation based variable selection with bidirectional and genetic search, wrappers of neural network with genetic search, and relief ranking filter. The modeling step resulted in 8 intelligent models which were developed using two prediction techniques, being artificial neural networks and support vector machines and two rule-based techniques, being decision trees and rough set theory. Taking the low number of data points into account, the prediction models showed a good performance (R2 = 0.95). The rule based models were transparent and easy to interpret but performed less.},
  language = {English},
  booktitle = {Intelligent and {{Soft Computing}} in {{Infrastructure Systems Engineering}}: {{Recent Advances}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Miradi, Maryam and Molenaar, Andre A. A. and {van de Ven}, Martin F. C.},
  editor = {Gopalakrishnan, Kasthurirangan and Ceylan, Halil and {Attoh-Okine}, Nii O.},
  year = {2009},
  keywords = {Artificial Neural Network,Data Mining,Knowledge Discovery,Support Vector Machine,Test Section},
  pages = {107-176},
  doi = {10.1007/978-3-642-04586-8_5}
}

@inproceedings{Model:1980:MVI:800087.802805,
  series = {{{LFP}} '80},
  title = {Multiprocessing via {{Intercommunicating LISP Systems}}},
  booktitle = {Proceedings of the 1980 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/800087.802805},
  author = {Model, Mitchell L},
  year = {1980},
  pages = {188-195},
  acmid = {802805},
  numpages = {8}
}

@article{moriBalancingTradeoffAccuracy2019,
  title = {Balancing the {{Trade}}-off between {{Accuracy}} and {{Interpretability}} in {{Software Defect Prediction}}},
  volume = {24},
  issn = {1573-7616},
  abstract = {ContextClassification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required.ObjectiveThe aim of this research is to propose a new classification model, called superposed naive Bayes (SNB), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation.MethodIn order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets.ResultsA trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method (SNB) can produce a balanced output that satisfies both accuracy and interpretability criteria.ConclusionsSNB offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability.},
  language = {English},
  number = {2},
  journal = {Empirical Software Engineering},
  doi = {10.1007/s10664-018-9638-1},
  author = {Mori, Toshiki and Uchihira, Naoshi},
  month = apr,
  year = {2019},
  keywords = {Interpretability,Ensemble learning,Model approximation,Naive Bayes classifier,Predictive accuracy,Software defect prediction,Trade-off analysis,Weights of evidence},
  pages = {779-825}
}

@article{Mulkers:1994:LDA:174662.174664,
  title = {Live-Structure {{Dataflow Analysis}} for {{Prolog}}},
  volume = {16},
  issn = {0164-0925},
  number = {2},
  journal = {ACM Trans. Program. Lang. Syst.},
  doi = {10.1145/174662.174664},
  author = {Mulkers, Anne and Winsborough, William and Bruynooghe, Maurice},
  month = mar,
  year = {1994},
  keywords = {abstract interpretation,compile-time garbage collection,liveness,program analysis,Prolog},
  pages = {205-258},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {174664},
  issue_date = {March 1994},
  numpages = {54}
}

@incollection{neukartReverseEngineeringMind2017,
  address = {Wiesbaden},
  series = {{{AutoUni}} \textendash{} {{Schriftenreihe}}},
  title = {Reverse {{Engineering}} the {{Mind}}},
  isbn = {978-3-658-16176-7},
  abstract = {Within this chapter all the requirements for reverse engineering the mind based on the knowledge imparted in the previous chapters will be discussed, and open questions attempted to be solved. A suitable theory of mind that on one side may not be the whole truth from a philosophical point of view, but serves as a valid foundation from an engineering point of view on the other side is introduced. Furthermore, as I indicated more than once, I am of the opinion that both quantum physics as well as self-organization occupy the most important roles in how our brain works and lets us experience conscious content and again, it is required to plunge into the information theoretical approach to quantum physics, quantum computer science.},
  language = {English},
  booktitle = {Reverse {{Engineering}} the {{Mind}}: {{Consciously Acting Machines}} and {{Accelerated Evolution}}},
  publisher = {{Springer Fachmedien Wiesbaden}},
  author = {Neukart, Florian},
  editor = {Neukart, Florian},
  year = {2017},
  keywords = {Artificial Neural Network,Hide Markov Model,Quantum Computer,Reverse Engineering,Semantic Network},
  pages = {237-354},
  doi = {10.1007/978-3-658-16176-7_10}
}

@article{nguyenMachineLearningDeep2019,
  title = {Machine {{Learning}} and {{Deep Learning Frameworks}} and {{Libraries}} for {{Large}}-{{Scale Data Mining}}: {{A Survey}}},
  issn = {1573-7462},
  shorttitle = {Machine {{Learning}} and {{Deep Learning Frameworks}} and {{Libraries}} for {{Large}}-{{Scale Data Mining}}},
  abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
  language = {English},
  journal = {Artificial Intelligence Review},
  doi = {10.1007/s10462-018-09679-z},
  author = {Nguyen, Giang and Dlugolinsky, Stefan and Bob\'ak, Martin and Tran, Viet and L\'opez Garc\'ia, \'Alvaro and Heredia, Ignacio and Mal\'ik, Peter and Hluch\'y, Ladislav},
  month = jan,
  year = {2019},
  keywords = {Machine Learning,Deep Learning,Artificial Intelligence software,Graphics processing unit (GPU),Intensive computing,Large-scale data mining,Parallel processing},
  file = {/home/tim/Zotero/storage/HFIU8T5S/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and .pdf}
}

@incollection{nissanNarrativesFormalismComputational2014,
  address = {Berlin, Heidelberg},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Narratives, {{Formalism}}, {{Computational Tools}}, and {{Nonlinearity}}},
  isbn = {978-3-642-45324-3},
  abstract = {We recapitulate four decades of computational processing of narratives. Vladimir Propp's work in the 1920s paved the way to both the structuralists' approach to the folktale and to narratives in general, and the story grammars approach to automate story-processing. In the latter domain, grammar-driven processing was overtaken by goal-driven processing, but there has been a comeback of story grammars, in combination with other devices. Propp's concern was with Russian folktales, and some story-generation programs are relevant indeed for folktale studies: such is the case of the programs TALE-SPIN and Joseph, which reportedly generated fables; MINSTREL generated Arthurian tales. Sometimes, bugs reveal more than proper functioning does, about the actual underlying model. Automated story processing, within artificial intelligence, showed important results since the late 1970s. After slowing down during the 1990s, since the turn of the century the field resurged, especially in the perspective of virtual environments and interactive narratives, also benefiting from the popularity of computer models of the emotions.},
  language = {English},
  booktitle = {Language, {{Culture}}, {{Computation}}. {{Computing}} of the {{Humanities}}, {{Law}}, and {{Narratives}}: {{Essays Dedicated}} to {{Yaacov Choueka}} on the {{Occasion}} of {{His}} 75th {{Birthday}}, {{Part II}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Nissan, Ephraim},
  editor = {Dershowitz, Nachum and Nissan, Ephraim},
  year = {2014},
  keywords = {Natural Language Processing,Belief Revision,Computational Linguistics,Computational Tool,Computer Science Department},
  pages = {270-393},
  doi = {10.1007/978-3-642-45324-3_11}
}

@inproceedings{Nobrega:2019:TER:3297280.3297443,
  series = {{{SAC}} '19},
  title = {Towards {{Explaining Recommendations Through Local Surrogate Models}}},
  isbn = {978-1-4503-5933-7},
  booktitle = {Proceedings of the 34th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/3297280.3297443},
  author = {N\'obrega, Caio and Marinho, Leandro},
  year = {2019},
  keywords = {transparency,explanations,recommender systems,factorization machines},
  pages = {1671-1678},
  acmid = {3297443},
  numpages = {8}
}

@inproceedings{Nyawira:2018:UNP:3219104.3229285,
  series = {{{PEARC}} '18},
  title = {Understanding {{Neural Pathways}} in {{Zebrafish Through Deep Learning}} and {{High Resolution Electron Microscope Data}}},
  isbn = {978-1-4503-6446-1},
  booktitle = {Proceedings of the {{Practice}} and {{Experience}} on {{Advanced Research Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/3219104.3229285},
  author = {Nyaw\~ira, Ishtar and Bushman, Kristi and Qian, Iris and Zhang, Annie},
  year = {2018},
  keywords = {Artificial Intelligence,Machine Learning,Deep Learning,Convolutional Neural Networks,Biomedical Image Processing},
  pages = {65:1-65:8},
  acmid = {3229285},
  articleno = {65},
  numpages = {8}
}

@inproceedings{oitaReverseEngineeringCreativity2020,
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  title = {Reverse {{Engineering Creativity}} into {{Interpretable Neural Networks}}},
  isbn = {978-3-030-12385-7},
  abstract = {In the field of AI the ultimate goal is to achieve generic intelligence, also called ``true AI'', but which depends on the successful enablement of imagination and creativity in artificial agents. To address this problem, this paper presents a novel deep learning framework for creativity, called INNGenuity. Pursuing an interdisciplinary implementation of creativity conditions, INNGenuity aims at the resolution of the various flaws of current AI learning architectures, which stem from the opacity of their models. Inspired by the neuroanatomy of the brain during creative cognition, the proposed framework's hybrid architecture blends both symbolic and connectionist AI, inline with Minsky's ``society of mind''. At its core, semantic gates are designed to facilitate an input/output flow of semantic structures and enable the usage of aligning mechanisms between neural activation clusters and semantic graphs. Having as goal alignment maximization, such a system would enable interpretability through the creation of labeled patterns of computation, and propose unaligned but relevant computation patterns as novel and useful, therefore creative.},
  language = {English},
  booktitle = {Advances in {{Information}} and {{Communication}}},
  publisher = {{Springer International Publishing}},
  author = {Oita, Marilena},
  editor = {Arai, Kohei and Bhatia, Rahul},
  year = {2020},
  keywords = {Neural networks,Interpretability,Creativity,Imagination,Knowledge,Neural architecture,Semantic networks},
  pages = {235-247}
}

@incollection{pace-siggeWhereCorpusLinguistics2018,
  address = {Cham},
  title = {Where {{Corpus Linguistics}} and {{Artificial Intelligence}} ({{AI}}) {{Meet}}},
  isbn = {978-3-319-90719-2},
  abstract = {This chapter will provide a platform to showcase the more recent developments that have grown out of the early laid groundwork. The latest theories in the field of linguistics will be presented, based on empirical data taken from naturally occurring language. In particular, the lexical priming theory will be introduced as a way to explain structures of language that corpus linguists have uncovered. Furthermore, the chapter will discuss the development of increasingly sophisticated algorithms that also deal with the use of language. Here, the focus will be on key achievements in the 1980s by IBM which created a solid foundation for applications that are now widely used in mobile and desktop devices\textemdash{}namely ``assistants'' like Amazon's Echo, Apple's SIRI or Google's (and Android's) Google Go.},
  language = {English},
  booktitle = {Spreading {{Activation}}, {{Lexical Priming}} and the {{Semantic Web}}: {{Early Psycholinguistic Theories}}, {{Corpus Linguistics}} and {{AI Applications}}},
  publisher = {{Springer International Publishing}},
  author = {{Pace-Sigge}, Michael},
  editor = {{Pace-Sigge}, Michael},
  year = {2018},
  keywords = {LSTM,Digital translators,Hoey,Lexical priming,N-gram model,Norvig,Quillian},
  pages = {29-82},
  doi = {10.1007/978-3-319-90719-2_3}
}

@inproceedings{Palmirani:2011:FMS:2018358.2018385,
  series = {{{ICAIL}} '11},
  title = {{{FrameNet Model}} of the {{Suspension}} of {{Norms}}},
  isbn = {978-1-4503-0755-0},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  publisher = {{ACM}},
  doi = {10.1145/2018358.2018385},
  author = {Palmirani, Monica and Ceci, Marcello and Radicioni, Daniele and Mazzei, Alessandro},
  year = {2011},
  keywords = {FrameNet,legal knowledge modelling,NLP,semantic interpretation},
  pages = {189-193},
  acmid = {2018385},
  numpages = {5}
}

@incollection{panesarMachineLearningAlgorithms2019,
  address = {Berkeley, CA},
  title = {Machine {{Learning Algorithms}}},
  isbn = {978-1-4842-3799-1},
  abstract = {You do not need a background in algebra and statistics to get started in machine learning. However, be under no illusions, mathematics is a huge part of machine learning. Math is key to understanding how the algorithm works and why coding a machine learning project from scratch is a great way to improve your mathematical and statistical skills. Not understanding the underlying principles behind an algorithm can lead to a limited understanding of methods or adopting limited interpretations of algorithms. If nothing else, it is useful to understand the mathematical principles that algorithms are based on and thus understand best which machine learning techniques are most appropriate.},
  language = {English},
  booktitle = {Machine {{Learning}} and {{AI}} for {{Healthcare}} : {{Big Data}} for {{Improved Health Outcomes}}},
  publisher = {{Apress}},
  author = {Panesar, Arjun},
  editor = {Panesar, Arjun},
  year = {2019},
  pages = {119-188},
  doi = {10.1007/978-1-4842-3799-1_4}
}

@inproceedings{Pastor:2019:EBB:3297280.3297328,
  series = {{{SAC}} '19},
  title = {Explaining {{Black Box Models}} by {{Means}} of {{Local Rules}}},
  isbn = {978-1-4503-5933-7},
  booktitle = {Proceedings of the 34th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/3297280.3297328},
  author = {Pastor, Eliana and Baralis, Elena},
  year = {2019},
  keywords = {interpretability,local model,prediction explanation},
  pages = {510-517},
  acmid = {3297328},
  numpages = {8}
}

@inproceedings{Popat:2017:TLE:3041021.3055133,
  series = {{{WWW}} '17 {{Companion}}},
  title = {Where the {{Truth Lies}}: {{Explaining}} the {{Credibility}} of {{Emerging Claims}} on the {{Web}} and {{Social Media}}},
  isbn = {978-1-4503-4914-7},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web Companion}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3041021.3055133},
  author = {Popat, Kashyap and Mukherjee, Subhabrata and Str\"otgen, Jannik and Weikum, Gerhard},
  year = {2017},
  keywords = {credibility analysis,rumor and hoax detection,text mining},
  pages = {1003-1012},
  acmid = {3055133},
  numpages = {10}
}

@article{Posters2009,
  title = {Posters},
  volume = {13},
  issn = {1760-4788},
  language = {English},
  number = {1},
  journal = {JNHA - The Journal of Nutrition, Health and Aging},
  doi = {10.1007/s12603-009-0095-9},
  month = jun,
  year = {2009},
  pages = {210-723}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
  isbn = {978-3-319-71746-3},
  shorttitle = {Interpretable {{Probabilistic Embeddings}}},
  abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
  language = {English},
  booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
  publisher = {{Springer International Publishing}},
  author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
  editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
  year = {2018},
  pages = {167-180}
}

@article{qureshiEVEExplainableVector2018,
  title = {{{EVE}}: {{Explainable Vector Based Embedding Technique Using Wikipedia}}},
  issn = {1573-7675},
  shorttitle = {{{EVE}}},
  abstract = {We present an unsupervised explainable vector embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a concept using human-readable labels, thereby it is readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed model, we consider its usefulness in three fundamental tasks: 1) intruder detection\textemdash{}to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster\textemdash{}to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first\textemdash{}to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.},
  language = {English},
  journal = {Journal of Intelligent Information Systems},
  doi = {10.1007/s10844-018-0511-x},
  author = {Qureshi, M. Atif and Greene, Derek},
  month = jun,
  year = {2018},
  keywords = {Distributional semantics,Unsupervised learning,Wikipedia},
  file = {/home/tim/Zotero/storage/LGBSR7QF/Qureshi and Greene - 2018 - EVE explainable vector based embedding technique .pdf}
}

@incollection{schetininAdvancedFeatureRecognition2007,
  address = {Berlin, Heidelberg},
  series = {Studies in {{Computational Intelligence}}},
  title = {Advanced {{Feature Recognition}} and {{Classification Using Artificial Intelligence Paradigms}}},
  isbn = {978-3-540-47518-7},
  language = {English},
  booktitle = {Artificial {{Intelligence}} in {{Recognition}} and {{Classification}} of {{Astrophysical}} and {{Medical Images}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Schetinin, V. and Zharkova, Valentina and Brazhnikov, A. and Zharkov, S. I. and Salerno, Emanuele and Bedini, Luigi and Kuruoglu, Ercan E. and Tonazzini, Anna and Zazula, Damjan and Cigale, Boris and Yoshida, Hiroyuki},
  editor = {Zharkova, Valentina and Jain, Lakhmi C.},
  year = {2007},
  keywords = {Cellular Neural Network,Compute Tomography Colonography,Independent Component Analysis,Source Separation},
  pages = {151-338},
  doi = {10.1007/978-3-540-47518-7_4}
}

@article{schubbachJudgingMachinesPhilosophical2019,
  title = {Judging {{Machines}}: {{Philosophical Aspects}} of {{Deep Learning}}},
  issn = {1573-0964},
  shorttitle = {Judging {{Machines}}},
  abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
  language = {English},
  journal = {Synthese},
  doi = {10.1007/s11229-019-02167-z},
  author = {Schubbach, Arno},
  month = mar,
  year = {2019},
  keywords = {Algorithm,Deep learning,Machine learning,Artificial intelligence,Computation,Explanation,Judgment,Justification,Kant}
}

@inproceedings{Schuessler:2019:MEC:3290607.3312823,
  series = {{{CHI EA}} '19},
  title = {Minimalistic {{Explanations}}: {{Capturing}} the {{Essence}} of {{Decisions}}},
  isbn = {978-1-4503-5971-9},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3290607.3312823},
  author = {Schuessler, Martin and Wei\ss, Philipp},
  year = {2019},
  keywords = {image classification,interpretable machine learning,explanations,deep neural networks},
  pages = {LBW2810:1-LBW2810:6},
  acmid = {3312823},
  articleno = {LBW2810},
  numpages = {6}
}

@article{SCIENTIFICABSTRACTS2018,
  title = {{{SCIENTIFIC ABSTRACTS}}},
  volume = {33},
  issn = {1525-1497},
  language = {English},
  number = {2},
  journal = {Journal of General Internal Medicine},
  doi = {10.1007/s11606-018-4413-y},
  month = apr,
  year = {2018},
  pages = {83-840},
  file = {/home/tim/Zotero/storage/VQ8B5SB2/2018 - SCIENTIFIC ABSTRACTS.pdf}
}

@inproceedings{Seo:2017:ICN:3109859.3109890,
  series = {{{RecSys}} '17},
  title = {Interpretable {{Convolutional Neural Networks}} with {{Dual Local}} and {{Global Attention}} for {{Review Rating Prediction}}},
  isbn = {978-1-4503-4652-8},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3109859.3109890},
  author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
  year = {2017},
  keywords = {attention model,convolutional neural network,deep learning for recommender systems},
  pages = {297-305},
  acmid = {3109890},
  numpages = {9}
}

@inproceedings{Sha:2017:IPC:3107411.3107445,
  series = {{{ACM}}-{{BCB}} '17},
  title = {Interpretable {{Predictions}} of {{Clinical Outcomes}} with {{An Attention}}-Based {{Recurrent Neural Network}}},
  isbn = {978-1-4503-4722-8},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}},and {{Health Informatics}}},
  publisher = {{ACM}},
  doi = {10.1145/3107411.3107445},
  author = {Sha, Ying and Wang, May D.},
  year = {2017},
  keywords = {health care,deep learning,electronic health records,interpretability,visualization,attention,recurrent neural networks},
  pages = {233-240},
  acmid = {3107445},
  numpages = {8}
}

@article{Shao:2013:ICS:2461912.2462003,
  title = {Interpreting {{Concept Sketches}}},
  volume = {32},
  issn = {0730-0301},
  number = {4},
  journal = {ACM Trans. Graph.},
  doi = {10.1145/2461912.2462003},
  author = {Shao, Tianjia and Li, Wilmot and Zhou, Kun and Xu, Weiwei and Guo, Baining and Mitra, Niloy J.},
  month = jul,
  year = {2013},
  keywords = {concept sketch,NPR,part relations,product design,shape analysis},
  pages = {56:1-56:10},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {2462003},
  articleno = {56},
  issue_date = {July 2013},
  numpages = {10}
}

@article{sharpee25thAnnualComputational2016,
  title = {25th {{Annual Computational Neuroscience Meeting}}: {{CNS}}-2016},
  volume = {17},
  issn = {1471-2202},
  shorttitle = {25th {{Annual Computational Neuroscience Meeting}}},
  abstract = {Table of contentsA1 Functional advantages of cell-type heterogeneity in neural circuitsTatyana O. SharpeeA2 Mesoscopic modeling of propagating waves in visual cortexAlain DestexheA3 Dynamics and biomarkers of mental disordersMitsuo KawatoF1 Precise recruitment of spiking output at theta frequencies requires dendritic h-channels in multi-compartment models of oriens-lacunosum/moleculare hippocampal interneuronsVladislav Sekuli\'c, Frances K. SkinnerF2 Kernel methods in reconstruction of current sources from extracellular potentials for single cells and the whole brainsDaniel K. W\'ojcik, Chaitanya Chintaluri, Dorottya Cserp\'an, Zolt\'an Somogyv\'ariF3 The synchronized periods depend on intracellular transcriptional repression mechanisms in circadian clocks.Jae Kyoung Kim, Zachary P. Kilpatrick, Matthew R. Bennett, Kresimir Josi\'cO1 Assessing irregularity and coordination of spiking-bursting rhythms in central pattern generatorsIrene Elices, David Arroyo, Rafael Levi, Francisco B. Rodriguez, Pablo VaronaO2 Regulation of top-down processing by cortically-projecting parvalbumin positive neurons in basal forebrainEunjin Hwang, Bowon Kim, Hio-Been Han, Tae Kim, James T. McKenna, Ritchie E. Brown, Robert W. McCarley, Jee Hyun ChoiO3 Modeling auditory stream segregation, build-up and bistabilityJames Rankin, Pamela Osborn Popp, John RinzelO4 Strong competition between tonotopic neural ensembles explains pitch-related dynamics of auditory cortex evoked fieldsAlejandro Tabas, Andr\'e Rupp, Emili Balaguer-BallesterO5 A simple model of retinal response to multi-electrode stimulationMatias I. Maturana, David B. Grayden, Shaun L. Cloherty, Tatiana Kameneva, Michael R. Ibbotson, Hamish MeffinO6 Noise correlations in V4 area correlate with behavioral performance in visual discrimination taskVeronika Koren, Timm Lochmann, Valentin Dragoi, Klaus ObermayerO7 Input-location dependent gain modulation in cerebellar nucleus neuronsMaria Psarrou, Maria Schilstra, Neil Davey, Benjamin Torben-Nielsen, Volker SteuberO8 Analytic solution of cable energy function for cortical axons and dendritesHuiwen Ju, Jiao Yu, Michael L. Hines, Liang Chen, Yuguo YuO9 C. elegans interactome: interactive visualization of Caenorhabditis elegans worm neuronal networkJimin Kim, Will Leahy, Eli ShlizermanO10 Is the model any good? Objective criteria for computational neuroscience model selectionJustas Birgiolas, Richard C. Gerkin, Sharon M. CrookO11 Cooperation and competition of gamma oscillation mechanismsAtthaphon Viriyopase, Raoul-Martin Memmesheimer, Stan GielenO12 A discrete structure of the brain wavesYuri Dabaghian, Justin DeVito, Luca PerottiO13 Direction-specific silencing of the Drosophila gaze stabilization systemAnmo J. Kim, Lisa M. Fenk, Cheng Lyu, Gaby MaimonO14 What does the fruit fly think about values? A model of olfactory associative learningChang Zhao, Yves Widmer, Simon Sprecher,Walter SennO15 Effects of ionic diffusion on power spectra of local field potentials (LFP)Geir Halnes, Tuomo M\"aki-Marttunen, Daniel Keller, Klas H. Pettersen,Ole A. Andreassen, Gaute T. EinevollO16 Large-scale cortical models towards understanding relationship between brain structure abnormalities and cognitive deficitsYasunori YamadaO17 Spatial coarse-graining the brain: origin of minicolumnsMoira L. Steyn-Ross, D. Alistair Steyn-RossO18 Modeling large-scale cortical networks with laminar structureJorge F. Mejias, John D. Murray, Henry Kennedy, Xiao-Jing WangO19 Information filtering by partial synchronous spikes in a neural populationAlexandra Kruscha, Jan Grewe, Jan Benda, Benjamin LindnerO20 Decoding context-dependent olfactory valence in Drosophila Laurent Badel, Kazumi Ohta, Yoshiko Tsuchimoto, Hokto KazamaP1 Neural network as a scale-free network: the role of a hubB. KahngP2 Hemodynamic responses to emotions and decisions using near-infrared spectroscopy optical imagingNicoladie D. TamP3 Phase space analysis of hemodynamic responses to intentional movement directions using functional near-infrared spectroscopy (fNIRS) optical imaging techniqueNicoladie D.Tam, Luca Pollonini, George ZouridakisP4 Modeling jamming avoidance of weakly electric fishJaehyun Soh, DaeEun KimP5 Synergy and redundancy of retinal ganglion cells in predictionMinsu Yoo, S. E. PalmerP6 A neural field model with a third dimension representing cortical depthViviana Culmone, Ingo BojakP7 Network analysis of a probabilistic connectivity model of the Xenopus tadpole spinal cordAndrea Ferrario, Robert Merrison-Hort, Roman BorisyukP8 The recognition dynamics in the brainChang Sub KimP9 Multivariate spike train analysis using a positive definite kernelTaro TezukaP10 Synchronization of burst periods may govern slow brain dynamics during general anesthesiaPangyu JooP11 The ionic basis of heterogeneity affects stochastic synchronyYoung-Ah Rho, Shawn D. Burton, G. Bard Ermentrout, Jaeseung Jeong, Nathaniel N. UrbanP12 Circular statistics of noise in spike trains with a periodic componentPetr MarsalekP14 Representations of directions in EEG-BCI using Gaussian readoutsHoon-Hee Kim, Seok-hyun Moon, Do-won Lee, Sung-beom Lee, Ji-yong Lee, Jaeseung JeongP15 Action selection and reinforcement learning in basal ganglia during reaching movementsYaroslav I. Molkov, Khaldoun Hamade, Wondimu Teka, William H. Barnett, Taegyo Kim, Sergey Markin, Ilya A. RybakP17 Axon guidance: modeling axonal growth in T-Junction assayCsaba Forro, Harald Dermutz, L\'aszl\'o Demk\'o, J\'anos V\"or\"osP19 Transient cell assembly networks encode persistent spatial memoriesYuri Dabaghian, Andrey BabichevP20 Theory of population coupling and applications to describe high order correlations in large populations of interacting neuronsHaiping HuangP21 Design of biologically-realistic simulations for motor controlSergio Verduzco-FloresP22 Towards understanding the functional impact of the behavioural variability of neuronsFilipa Dos Santos, Peter AndrasP23 Different oscillatory dynamics underlying gamma entrainment deficits in schizophreniaChristoph Metzner, Achim Schweikard, Bartosz ZurowskiP24 Memory recall and spike frequency adaptationJames P. Roach, Leonard M. Sander, Michal R. ZochowskiP25 Stability of neural networks and memory consolidation preferentially occur near criticalityQuinton M. Skilling, Nicolette Ognjanovski, Sara J. Aton, Michal ZochowskiP26 Stochastic Oscillation in Self-Organized Critical States of Small Systems: Sensitive Resting State in Neural SystemsSheng-Jun Wang, Guang Ouyang, Jing Guang, Mingsha Zhang, K. Y. Michael Wong, Changsong ZhouP27 Neurofield: a C++ library for fast simulation of 2D neural field modelsPeter A. Robinson, Paula Sanz-Leon, Peter M. Drysdale, Felix Fung, Romesh G. Abeysuriya, Chris J. Rennie, Xuelong ZhaoP28 Action-based grounding: Beyond encoding/decoding in neural codeYoonsuck Choe, Huei-Fang YangP29 Neural computation in a dynamical system with multiple time scalesYuanyuan Mi, Xiaohan Lin, Si WuP30 Maximum entropy models for 3D layouts of orientation selectivityJoscha Liedtke, Manuel Schottdorf, Fred WolfP31 A behavioral assay for probing computations underlying curiosity in rodentsYoriko Yamamura, Jeffery R. WickensP32 Using statistical sampling to balance error function contributions to optimization of conductance-based modelsTimothy Rumbell, Julia Ramsey, Amy Reyes, Danel Dragulji\'c, Patrick R. Hof, Jennifer Luebke, Christina M. WeaverP33 Exploration and implementation of a self-growing and self-organizing neuron network building algorithmHu He, Xu Yang, Hailin Ma, Zhiheng Xu, Yuzhe WangP34 Disrupted resting state brain network in obese subjects: a data-driven graph theory analysisKwangyeol Baek, Laurel S. Morris, Prantik Kundu, Valerie VoonP35 Dynamics of cooperative excitatory and inhibitory plasticityEverton J. Agnes, Tim P. VogelsP36 Frequency-dependent oscillatory signal gating in feed-forward networks of integrate-and-fire neuronsWilliam F. Podlaski, Tim P. VogelsP37 Phenomenological neural model for adaptation of neurons in area ITMartin Giese, Pradeep Kuravi, Rufin VogelsP38 ICGenealogy: towards a common topology of neuronal ion channel function and genealogy in model and experimentAlexander Seeholzer, William Podlaski, Rajnish Ranjan, Tim VogelsP39 Temporal input discrimination from the interaction between dynamic synapses and neural subthreshold oscillationsJoaquin J. Torres, Fabiano Baroni, Roberto Latorre, Pablo VaronaP40 Different roles for transient and sustained activity during active visual processingBart Gips, Eric Lowet, Mark J. Roberts, Peter de Weerd, Ole Jensen, Jan van der EerdenP41 Scale-free functional networks of 2D Ising model are highly robust against structural defects: neuroscience implicationsAbdorreza Goodarzinick, Mohammad D. Niry, Alireza ValizadehP42 High frequency neuron can facilitate propagation of signal in neural networksAref Pariz, Shervin S. Parsi, Alireza ValizadehP43 Investigating the effect of Alzheimer's disease related amyloidopathy on gamma oscillations in the CA1 region of the hippocampusJulia M. Warburton, Lucia Marucci, Francesco Tamagnini, Jon Brown, Krasimira Tsaneva-AtanasovaP44 Long-tailed distributions of inhibitory and excitatory weights in a balanced network with eSTDP and iSTDPFlorence I. Kleberg, Jochen TrieschP45 Simulation of EMG recording from hand muscle due to TMS of motor cortexBahar Moezzi, Nicolangelo Iannella, Natalie Schaworonkow, Lukas Plogmacher, Mitchell R. Goldsworthy, Brenton Hordacre, Mark D. McDonnell, Michael C. Ridding, Jochen TrieschP46 Structure and dynamics of axon network formed in primary cell cultureMartin Zapotocky, Daniel Smit, Coralie Fouquet, Alain TrembleauP47 Efficient signal processing and sampling in random networks that generate variabilitySakyasingha Dasgupta, Isao Nishikawa, Kazuyuki Aihara, Taro ToyoizumiP48 Modeling the effect of riluzole on bursting in respiratory neural networksDaniel T. Robb, Nick Mellen, Natalia ToporikovaP49 Mapping relaxation training using effective connectivity analysisRongxiang Tang, Yi-Yuan TangP50 Modeling neuron oscillation of implicit sequence learningGuangsheng Liang, Seth A. Kiser, James H. Howard, Jr., Yi-Yuan TangP51 The role of cerebellar short-term synaptic plasticity in the pathology and medication of downbeat nystagmusJulia Goncharenko, Neil Davey, Maria Schilstra, Volker SteuberP52 Nonlinear response of noisy neuronsSergej O. Voronenko, Benjamin LindnerP53 Behavioral embedding suggests multiple chaotic dimensions underlie C. elegans locomotionTosif Ahamed, Greg StephensP54 Fast and scalable spike sorting for large and dense multi-electrodes recordingsPierre Yger, Baptiste Lefebvre, Giulia Lia Beatrice Spampinato, Elric Esposito, Marcel Stimberg et Olivier MarreP55 Sufficient sampling rates for fast hand motion trackingHansol Choi, Min-Ho SongP56 Linear readout of object manifoldsSueYeon Chung, Dan D. Lee, Haim SompolinskyP57 Differentiating models of intrinsic bursting and rhythm generation of the respiratory pre-B\"otzinger complex using phase response curvesRyan S. Phillips, Jeffrey SmithP58 The effect of inhibitory cell network interactions during theta rhythms on extracellular field potentials in CA1 hippocampusAlexandra Pierri Chatzikalymniou, Katie Ferguson, Frances K. SkinnerP59 Expansion recoding through sparse sampling in the cerebellar input layer speeds learningN. Alex Cayco Gajic, Claudia Clopath, R. Angus SilverP60 A set of curated cortical models at multiple scales on Open Source BrainPadraig Gleeson, Boris Marin, Sadra Sadeh, Adrian Quintana, Matteo Cantarelli, Salvador Dura-Bernal, William W. Lytton, Andrew Davison, R. Angus SilverP61 A synaptic story of dynamical information encoding in neural adaptationLuozheng Li, Wenhao Zhang, Yuanyuan Mi, Dahui Wang, Si WuP62 Physical modeling of rule-observant rodent behaviorYoungjo Song, Sol Park, Ilhwan Choi, Jaeseung Jeong, Hee-sup ShinP64 Predictive coding in area V4 and prefrontal cortex explains dynamic discrimination of partially occluded shapesHannah Choi, Anitha Pasupathy, Eric Shea-BrownP65 Stability of FORCE learning on spiking and rate-based networksDongsung Huh, Terrence J. SejnowskiP66 Stabilising STDP in striatal neurons for reliable fast state recognition in noisy environmentsSimon M. Vogt, Arvind Kumar, Robert SchmidtP67 Electrodiffusion in one- and two-compartment neuron models for characterizing cellular effects of electrical stimulationStephen Van Wert, Steven J. SchiffP68 STDP improves speech recognition capabilities in spiking recurrent circuits parameterized via differential evolution Markov Chain Monte CarloRichard Veale, Matthias ScheutzP69 Bidirectional transformation between dominant cortical neural activities and phase difference distributionsSang Wan LeeP70 Maturation of sensory networks through homeostatic structural plasticityJ\'ulia Gallinaro, Stefan RotterP71 Corticothalamic dynamics: structure, number of solutions and stability of steady-state solutions in the space of synaptic couplingsPaula Sanz-Leon, Peter A. RobinsonP72 Optogenetic versus electrical stimulation of the parkinsonian basal ganglia. Computational studyLeonid L. Rubchinsky, Chung Ching Cheung, Shivakeshavan Ratnadurai-GiridharanP73 Exact spike-timing distribution reveals higher-order interactions of neuronsSafura Rashid Shomali, Majid Nili Ahmadabadi, Hideaki Shimazaki, S. Nader RasuliP74 Neural mechanism of visual perceptual learning using a multi-layered neural networkXiaochen Zhao, Malte J. RaschP75 Inferring collective spiking dynamics from mostly unobserved systemsJens Wilting, Viola PriesemannP76 How to infer distributions in the brain from subsampled observationsAnna Levina, Viola PriesemannP77 Influences of embedding and estimation strategies on the inferred memory of single spiking neuronsLucas Rudelt, Joseph T. Lizier, Viola PriesemannP78 A nearest-neighbours based estimator for transfer entropy between spike trainsJoseph T. Lizier, Richard E. Spinney, Mikail Rubinov, Michael Wibral, Viola PriesemannP79 Active learning of psychometric functions with multinomial logistic modelsJi Hyun Bak, Jonathan PillowP81 Inferring low-dimensional network dynamics with variational latent Gaussian processYuan Zaho, Il Memming ParkP82 Computational investigation of energy landscapes in the resting state subcortical brain networkJiyoung Kang, Hae-Jeong ParkP83 Local repulsive interaction between retinal ganglion cells can generate a consistent spatial periodicity of orientation mapJaeson Jang, Se-Bum PaikP84 Phase duration of bistable perception reveals intrinsic time scale of perceptual decision under noisy conditionWoochul Choi, Se-Bum PaikP85 Feedforward convergence between retina and primary visual cortex can determine the structure of orientation mapChangju Lee, Jaeson Jang, Se-Bum PaikP86 Computational method classifying neural network activity patterns for imaging dataMin Song, Hyeonsu Lee, Se-Bum PaikP87 Symmetry of spike-timing-dependent-plasticity kernels regulates volatility of memoryYoungjin Park, Woochul Choi, Se-Bum PaikP88 Effects of time-periodic coupling strength on the first-spike latency dynamics of a scale-free network of stochastic Hodgkin-Huxley neuronsErgin Yilmaz, Veli Baysal, Mahmut OzerP89 Spectral properties of spiking responses in V1 and V4 change within the trial and are highly relevant for behavioral performanceVeronika Koren, Klaus ObermayerP90 Methods for building accurate models of individual neuronsDaniel Saska, Thomas NowotnyP91 A full size mathematical model of the early olfactory system of honeybeesHo Ka Chan, Alan Diamond, Thomas NowotnyP92 Stimulation-induced tuning of ongoing oscillations in spiking neural networksChristoph S. Herrmann, Micah M. Murray, Silvio Ionta, Axel Hutt, J\'er\'emie LefebvreP93 Decision-specific sequences of neural activity in balanced random networks driven by structured sensory inputPhilipp Weidel, Renato Duarte, Abigail MorrisonP94 Modulation of tuning induced by abrupt reduction of SST cell activityJung H. Lee, Ramakrishnan Iyer, Stefan MihalasP95 The functional role of VIP cell activation during locomotionJung H. Lee, Ramakrishnan Iyer, Christof Koch, Stefan MihalasP96 Stochastic inference with spiking neural networksMihai A. Petrovici, Luziwei Leng, Oliver Breitwieser, David St\"ockel, Ilja Bytschok, Roman Martel, Johannes Bill, Johannes Schemmel, Karlheinz MeierP97 Modeling orientation-selective electrical stimulation with retinal prosthesesTimothy B. Esler, Anthony N. Burkitt, David B. Grayden, Robert R. Kerr, Bahman Tahayori, Hamish MeffinP98 Ion channel noise can explain firing correlation in auditory nervesBahar Moezzi, Nicolangelo Iannella, Mark D. McDonnellP99 Limits of temporal encoding of thalamocortical inputs in a neocortical microcircuitMax Nolte, Michael W. Reimann, Eilif Muller, Henry MarkramP100 On the representation of arm reaching movements: a computational modelAntonio Parziale, Rosa Senatore, Angelo MarcelliP101 A computational model for investigating the role of cerebellum in acquisition and retention of motor behaviorRosa Senatore, Antonio Parziale, Angelo MarcelliP102 The emergence of semantic categories from a large-scale brain network of semantic knowledgeK. Skiker, M. MaoueneP103 Multiscale modeling of M1 multitarget pharmacotherapy for dystoniaSamuel A. Neymotin, Salvador Dura-Bernal, Alexandra Seidenstein, Peter Lakatos, Terence D. Sanger, William W. LyttonP104 Effect of network size on computational capacitySalvador Dura-Bernal, Rosemary J. Menzies, Campbell McLauchlan, Sacha J. van Albada, David J. Kedziora, Samuel Neymotin, William W. Lytton, Cliff C. KerrP105 NetPyNE: a Python package for NEURON to facilitate development and parallel simulation of biological neuronal networksSalvador Dura-Bernal, Benjamin A. Suter, Samuel A. Neymotin, Cliff C. Kerr, Adrian Quintana, Padraig Gleeson, Gordon M. G. Shepherd, William W. LyttonP107 Inter-areal and inter-regional inhomogeneity in co-axial anisotropy of Cortical Point Spread in human visual areasJuhyoung Ryu, Sang-Hun LeeP108 Two bayesian quanta of uncertainty explain the temporal dynamics of cortical activity in the non-sensory areas during bistable perceptionJoonwon Lee, Sang-Hun LeeP109 Optimal and suboptimal integration of sensory and value information in perceptual decision makingHyang Jung Lee, Sang-Hun LeeP110 A Bayesian algorithm for phoneme Perception and its neural implementationDaeseob Lim, Sang-Hun LeeP111 Complexity of EEG signals is reduced during unconsciousness induced by ketamine and propofolJisung Wang, Heonsoo LeeP112 Self-organized criticality of neural avalanche in a neural model on complex networksNam Jung, Le Anh Quang, Seung Eun Maeng, Tae Ho Lee, Jae Woo LeeP113 Dynamic alterations in connection topology of the hippocampal network during ictal-like epileptiform activity in an in vitro rat modelChang-hyun Park, Sora Ahn, Jangsup Moon, Yun Seo Choi, Juhee Kim, Sang Beom Jun, Seungjun Lee, Hyang Woon LeeP114 Computational model to replicate seizure suppression effect by electrical stimulationSora Ahn, Sumin Jo, Eunji Jun, Suin Yu, Hyang Woon Lee, Sang Beom Jun, Seungjun LeeP115 Identifying excitatory and inhibitory synapses in neuronal networks from spike trains using sorted local transfer entropyFelix Goetze, Pik-Yin LaiP116 Neural network model for obstacle avoidance based on neuromorphic computational model of boundary vector cell and head direction cellSeonghyun Kim, Jeehyun KwagP117 Dynamic gating of spike pattern propagation by Hebbian and anti-Hebbian spike timing-dependent plasticity in excitatory feedforward network modelHyun Jae Jang, Jeehyun KwagP118 Inferring characteristics of input correlations of cells exhibiting up-down state transitions in the rat striatumMarko Filipovi\'c, Ramon Reig, Ad Aertsen, Gilad Silberberg, Arvind KumarP119 Graph properties of the functional connected brain under the influence of Alzheimer's diseaseClaudia Bachmann, Simone Buttler, Heidi Jacobs, Kim Dillen, Gereon R. Fink, Juraj Kukolja, Abigail MorrisonP120 Learning sparse representations in the olfactory bulbDaniel Kepple, Hamza Giaffar, Dima Rinberg, Steven Shea, Alex KoulakovP121 Functional classification of homologous basal-ganglia networksJyotika Bahuguna,Tom Tetzlaff, Abigail Morrison, Arvind Kumar, Jeanette Hellgren KotaleskiP122 Short term memory based on multistabilityTim Kunze, Andre Peterson, Thomas Kn\"oscheP123 A physiologically plausible, computationally efficient model and simulation software for mammalian motor unitsMinjung Kim, Hojeong KimP125 Decoding laser-induced somatosensory information from EEGJi Sung Park, Ji Won Yeon, Sung-Phil KimP126 Phase synchronization of alpha activity for EEG-based personal authenticationJae-Hwan Kang, Chungho Lee, Sung-Phil KimP129 Investigating phase-lags in sEEG data using spatially distributed time delays in a large-scale brain network modelAndreas Spiegler, Spase Petkoski, Matias J. Palva, Viktor K. JirsaP130 Epileptic seizures in the unfolding of a codimension-3 singularityMaria L. Saggio, Silvan F. Siep, Andreas Spiegler, William C. Stacey, Christophe Bernard, Viktor K. JirsaP131 Incremental dimensional exploratory reasoning under multi-dimensional environmentOh-hyeon Choung, Yong JeongP132 A low-cost model of eye movements and memory in personal visual cognitionYong-il Lee, Jaeseung JeongP133 Complex network analysis of structural connectome of autism spectrum disorder patientsSu Hyun Kim, Mir Jeong, Jaeseung JeongP134 Cognitive motives and the neural correlates underlying human social information transmission, gossipJeungmin Lee, Jaehyung Kwon, Jerald D. Kralik, Jaeseung JeongP135 EEG hyperscanning detects neural oscillation for the social interaction during the economic decision-makingJaehwan Jahng, Dong-Uk Hwang, Jaeseung JeongP136 Detecting purchase decision based on hyperfrontality of the EEGJae-Hyung Kwon, Sang-Min Park, Jaeseung JeongP137 Vulnerability-based critical neurons, synapses, and pathways in the Caenorhabditis elegans connectomeSeongkyun Kim, Hyoungkyu Kim, Jerald D. Kralik, Jaeseung JeongP138 Motif analysis reveals functionally asymmetrical neurons in C. elegans Pyeong Soo Kim, Seongkyun Kim, Hyoungkyu Kim, Jaeseung JeongP139 Computational approach to preference-based serial decision dynamics: do temporal discounting and working memory affect it?Sangsup Yoon, Jaehyung Kwon, Sewoong Lim, Jaeseung JeongP141 Social stress induced neural network reconfiguration affects decision making and learning in zebrafishChoongseok Park, Thomas Miller, Katie Clements, Sungwoo Ahn, Eoon Hye Ji, Fadi A. IssaP142 Descriptive, generative, and hybrid approaches for neural connectivity inference from neural activity dataJeongHun Baek, Shigeyuki Oba, Junichiro Yoshimoto, Kenji Doya, Shin IshiiP145 Divergent-convergent synaptic connectivities accelerate coding in multilayered sensory systemsThiago S. Mosqueiro, Martin F. Strube-Bloss, Brian Smith, Ramon HuertaP146 Swinging networksMichal Hadrava, Jaroslav HlinkaP147 Inferring dynamically relevant motifs from oscillatory stimuli: challenges, pitfalls, and solutionsHannah Bos, Moritz HeliasP148 Spatiotemporal mapping of brain network dynamics during cognitive tasks using magnetoencephalography and deep learningCharles M. Welzig, Zachary J. HarperP149 Multiscale complexity analysis for the segmentation of MRI imagesWon Sup Kim, In-Seob Shin, Hyeon-Man Baek, Seung Kee HanP150 A neuro-computational model of emotional attentionRen\'e Richter, Julien Vitay, Frederick Beuth, Fred H. HamkerP151 Multi-site delayed feedback stimulation in parkinsonian networksKelly Toppin, Yixin GuoP152 Bistability in Hodgkin\textendash{}Huxley-type equationsTatiana Kameneva, Hamish Meffin, Anthony N. Burkitt, David B. GraydenP153 Phase changes in postsynaptic spiking due to synaptic connectivity and short term plasticity: mathematical analysis of frequency dependencyMark D. McDonnell, Bruce P. GrahamP154 Quantifying resilience patterns in brain networks: the importance of directionalityPenelope J. Kale, Leonardo L. GolloP155 Dynamics of rate-model networks with separate excitatory and inhibitory populationsMerav Stern, L. F. AbbottP156 A model for multi-stable dynamics in action recognition modulated by integration of silhouette and shading cuesLeonid A. Fedorov, Martin A. GieseP157 Spiking model for the interaction between action recognition and action executionMohammad Hovaidi Ardestani, Martin GieseP158 Surprise-modulated belief update: how to learn within changing environments?Mohammad Javad Faraji, Kerstin Preuschoff, Wulfram GerstnerP159 A fast, stochastic and adaptive model of auditory nerve responses to cochlear implant stimulationMargriet J. van Gendt, Jeroen J. Briaire, Randy K. Kalkman, Johan H. M. FrijnsP160 Quantitative comparison of graph theoretical measures of simulated and empirical functional brain networksWon Hee Lee, Sophia FrangouP161 Determining discriminative properties of fMRI signals in schizophrenia using highly comparative time-series analysisBen D. Fulcher, Patricia H. P. Tran, Alex FornitoP162 Emergence of narrowband LFP oscillations from completely asynchronous activity during seizures and high-frequency oscillationsStephen V. Gliske, William C. Stacey, Eugene Lim, Katherine A. Holman, Christian G. FinkP163 Neuronal diversity in structure and function: cross-validation of anatomical and physiological classification of retinal ganglion cells in the mouseJinseop S. Kim, Shang Mu, Kevin L. Briggman, H. Sebastian Seung, the EyeWirersP164 Analysis and modelling of transient firing rate changes in area MT in response to rapid stimulus feature changesDetlef Wegener, Lisa Bohnenkamp, Udo A. ErnstP165 Step-wise model fitting accounting for high-resolution spatial measurements: construction of a layer V pyramidal cell model with reduced morphologyTuomo M\"aki-Marttunen, Geir Halnes, Anna Devor, Christoph Metzner, Anders M. Dale, Ole A. Andreassen, Gaute T. EinevollP166 Contributions of schizophrenia-associated genes to neuron firing and cardiac pacemaking: a polygenic modeling approachTuomo M\"aki-Marttunen, Glenn T. Lines, Andy Edwards, Aslak Tveito, Anders M. Dale, Gaute T. Einevoll, Ole A. AndreassenP167 Local field potentials in a 4 \texttimes{} 4 mm2 multi-layered network modelEspen Hagen, Johanna Senk, Sacha J. van Albada, Markus DiesmannP168 A spiking network model explains multi-scale properties of cortical dynamicsMaximilian Schmidt, Rembrandt Bakker, Kelly Shen, Gleb Bezgin, Claus-Christian Hilgetag, Markus Diesmann, Sacha Jennifer van AlbadaP169 Using joint weight-delay spike-timing dependent plasticity to find polychronous neuronal groupsHaoqi Sun, Olga Sourina, Guang-Bin Huang, Felix Klanner, Cornelia DenkP170 Tensor decomposition reveals RSNs in simulated resting state fMRIKatharina Glomb, Adri\'an Ponce-Alvarez, Matthieu Gilson, Petra Ritter, Gustavo DecoP171 Getting in the groove: testing a new model-based method for comparing task-evoked vs resting-state activity in fMRI data on music listeningMatthieu Gilson, Maria AG Witek, Eric F. Clarke, Mads Hansen, Mikkel Wallentin, Gustavo Deco, Morten L. Kringelbach, Peter VuustP172 STochastic engine for pathway simulation (STEPS) on massively parallel processorsGuido Klingbeil, Erik De SchutterP173 Toolkit support for complex parallel spatial stochastic reaction\textendash{}diffusion simulation in STEPSWeiliang Chen, Erik De SchutterP174 Modeling the generation and propagation of Purkinje cell dendritic spikes caused by parallel fiber synaptic inputYunliang Zang, Erik De SchutterP175 Dendritic morphology determines how dendrites are organized into functional subunitsSungho Hong, Akira Takashima, Erik De SchutterP176 A model of Ca2+/calmodulin-dependent protein kinase II activity in long term depression at Purkinje cellsCriseida Zamora, Andrew R. Gallimore, Erik De SchutterP177 Reward-modulated learning of population-encoded vectors for insect-like navigation in embodied agentsDennis Goldschmidt, Poramate Manoonpong, Sakyasingha DasguptaP178 Data-driven neural models part II: connectivity patterns of human seizuresPhilippa J. Karoly, Dean R. Freestone, Daniel Soundry, Levin Kuhlmann, Liam Paninski, Mark CookP179 Data-driven neural models part I: state and parameter estimationDean R. Freestone, Philippa J. Karoly, Daniel Soundry, Levin Kuhlmann, Mark CookP180 Spectral and spatial information processing in human auditory streamingJaejin Lee, Yonatan I. Fishman, Yale E. CohenP181 A tuning curve for the global effects of local perturbations in neural activity: Mapping the systems-level susceptibility of the brainLeonardo L. Gollo, James A. Roberts, Luca CocchiP182 Diverse homeostatic responses to visual deprivation mediated by neural ensemblesYann Sweeney, Claudia ClopathP183 Opto-EEG: a novel method for investigating functional connectome in mouse brain based on optogenetics and high density electroencephalographySoohyun Lee, Woo-Sung Jung, Jee Hyun ChoiP184 Biphasic responses of frontal gamma network to repetitive sleep deprivation during REM sleepBowon Kim, Youngsoo Kim, Eunjin Hwang, Jee Hyun ChoiP185 Brain-state correlate and cortical connectivity for frontal gamma oscillations in top-down fashion assessed by auditory steady-state responseYounginha Jung, Eunjin Hwang, Yoon-Kyu Song, Jee Hyun ChoiP186 Neural field model of localized orientation selective activation in V1James Rankin, Fr\'ed\'eric ChavaneP187 An oscillatory network model of Head direction and Grid cells using locomotor inputsKarthik Soman, Vignesh Muralidharan, V. Srinivasa ChakravarthyP188 A computational model of hippocampus inspired by the functional architecture of basal gangliaKarthik Soman, Vignesh Muralidharan, V. Srinivasa ChakravarthyP189 A computational architecture to model the microanatomy of the striatum and its functional propertiesSabyasachi Shivkumar, Vignesh Muralidharan, V. Srinivasa ChakravarthyP190 A scalable cortico-basal ganglia model to understand the neural dynamics of targeted reachingVignesh Muralidharan, Alekhya Mandali, B. Pragathi Priyadharsini, Hima Mehta, V. Srinivasa ChakravarthyP191 Emergence of radial orientation selectivity from synaptic plasticityCatherine E. Davey, David B. Grayden, Anthony N. BurkittP192 How do hidden units shape effective connections between neurons?Braden A. W. Brinkman, Tyler Kekona, Fred Rieke, Eric Shea-Brown, Michael BuiceP193 Characterization of neural firing in the presence of astrocyte-synapse signalingMaurizio De Pitt\`a, Hugues Berry, Nicolas BrunelP194 Metastability of spatiotemporal patterns in a large-scale network model of brain dynamicsJames A. Roberts, Leonardo L. Gollo, Michael BreakspearP195 Comparison of three methods to quantify detection and discrimination capacity estimated from neural population recordingsGary Marsat, Jordan Drew, Phillip D. Chapman, Kevin C. Daly, Samual P. BradleyP196 Quantifying the constraints for independent evoked and spontaneous NMDA receptor mediated synaptic transmission at individual synapsesSat Byul Seo, Jianzhong Su, Ege T. Kavalali, Justin BlackwellP199 Gamma oscillation via adaptive exponential integrate-and-fire neuronsLieJune Shiau, Laure Buhry, Kanishka BasnayakeP200 Visual face representations during memory retrieval compared to perceptionSue-Hyun Lee, Brandon A. Levy, Chris I. BakerP201 Top-down modulation of sequential activity within packets modeled using avalanche dynamicsTimoth\'ee Leleu, Kazuyuki AiharaQ28 An auto-encoder network realizes sparse features under the influence of desynchronized vascular dynamicsRyan T. Philips, Karishma Chhabria, V. Srinivasa Chakravarthy},
  language = {English},
  number = {1},
  journal = {BMC Neuroscience},
  doi = {10.1186/s12868-016-0283-6},
  author = {Sharpee, Tatyana O. and Destexhe, Alain and Kawato, Mitsuo and Sekuli\'c, Vladislav and Skinner, Frances K. and W\'ojcik, Daniel K. and Chintaluri, Chaitanya and Cserp\'an, Dorottya and Somogyv\'ari, Zolt\'an and Kim, Jae Kyoung and Kilpatrick, Zachary P. and Bennett, Matthew R. and Josi\'c, Kresimir and Elices, Irene and Arroyo, David and Levi, Rafael and Rodriguez, Francisco B. and Varona, Pablo and Hwang, Eunjin and Kim, Bowon and Han, Hio-Been and Kim, Tae and McKenna, James T. and Brown, Ritchie E. and McCarley, Robert W. and Choi, Jee Hyun and Rankin, James and Popp, Pamela Osborn and Rinzel, John and Tabas, Alejandro and Rupp, Andr\'e and {Balaguer-Ballester}, Emili and Maturana, Matias I. and Grayden, David B. and Cloherty, Shaun L. and Kameneva, Tatiana and Ibbotson, Michael R. and Meffin, Hamish and Koren, Veronika and Lochmann, Timm and Dragoi, Valentin and Obermayer, Klaus and Psarrou, Maria and Schilstra, Maria and Davey, Neil and {Torben-Nielsen}, Benjamin and Steuber, Volker and Ju, Huiwen and Yu, Jiao and Hines, Michael L. and Chen, Liang and Yu, Yuguo and Kim, Jimin and Leahy, Will and Shlizerman, Eli and Birgiolas, Justas and Gerkin, Richard C. and Crook, Sharon M. and Viriyopase, Atthaphon and Memmesheimer, Raoul-Martin and Gielen, Stan and Dabaghian, Yuri and DeVito, Justin and Perotti, Luca and Kim, Anmo J. and Fenk, Lisa M. and Cheng, Cheng and Maimon, Gaby and Zhao, Chang and Widmer, Yves and Sprecher, Simon and Senn, Walter and Halnes, Geir and {M\"aki-Marttunen}, Tuomo and Keller, Daniel and Pettersen, Klas H. and Andreassen, Ole A. and Einevoll, Gaute T. and Yamada, Yasunori and {Steyn-Ross}, Moira L. and {Alistair Steyn-Ross}, D. and Mejias, Jorge F. and Murray, John D. and Kennedy, Henry and Wang, Xiao-Jing and Kruscha, Alexandra and Grewe, Jan and Benda, Jan and Lindner, Benjamin and Badel, Laurent and Ohta, Kazumi and Tsuchimoto, Yoshiko and Kazama, Hokto and Kahng, B. and Tam, Nicoladie D. and Pollonini, Luca and Zouridakis, George and Soh, Jaehyun and Kim, DaeEun and Yoo, Minsu and Palmer, S. E. and Culmone, Viviana and Bojak, Ingo and Ferrario, Andrea and {Merrison-Hort}, Robert and Borisyuk, Roman and Kim, Chang Sub and Tezuka, Taro and Joo, Pangyu and Rho, Young-Ah and Burton, Shawn D. and Bard Ermentrout, G. and Jeong, Jaeseung and Urban, Nathaniel N. and Marsalek, Petr and Kim, Hoon-Hee and Moon, Seok-hyun and Lee, Do-won and Lee, Sung-beom and Lee, Ji-yong and Molkov, Yaroslav I. and Hamade, Khaldoun and Teka, Wondimu and Barnett, William H. and Kim, Taegyo and Markin, Sergey and Rybak, Ilya A. and Forro, Csaba and Dermutz, Harald and Demk\'o, L\'aszl\'o and V\"or\"os, J\'anos and Babichev, Andrey and Huang, Haiping and {Verduzco-Flores}, Sergio and Dos Santos, Filipa and Andras, Peter and Metzner, Christoph and Schweikard, Achim and Zurowski, Bartosz and Roach, James P. and Sander, Leonard M. and Zochowski, Michal R. and Skilling, Quinton M. and Ognjanovski, Nicolette and Aton, Sara J. and Zochowski, Michal and Wang, Sheng-Jun and Ouyang, Guang and Guang, Jing and Zhang, Mingsha and Michael Wong, K. Y. and Zhou, Changsong and Robinson, Peter A. and {Sanz-Leon}, Paula and Drysdale, Peter M. and Fung, Felix and Abeysuriya, Romesh G. and Rennie, Chris J. and Zhao, Xuelong and Choe, Yoonsuck and Yang, Huei-Fang and Mi, Yuanyuan and Lin, Xiaohan and Wu, Si and Liedtke, Joscha and Schottdorf, Manuel and Wolf, Fred and Yamamura, Yoriko and Wickens, Jeffery R. and Rumbell, Timothy and Ramsey, Julia and Reyes, Amy and Dragulji\'c, Danel and Hof, Patrick R. and Luebke, Jennifer and Weaver, Christina M. and He, Hu and Yang, Xu and Ma, Hailin and Xu, Zhiheng and Wang, Yuzhe and Baek, Kwangyeol and Morris, Laurel S. and Kundu, Prantik and Voon, Valerie and Agnes, Everton J. and Vogels, Tim P. and Podlaski, William F. and Giese, Martin and Kuravi, Pradeep and Vogels, Rufin and Seeholzer, Alexander and Podlaski, William and Ranjan, Rajnish and Vogels, Tim and Torres, Joaquin J. and Baroni, Fabiano and Latorre, Roberto and Gips, Bart and Lowet, Eric and Roberts, Mark J. and {de Weerd}, Peter and Jensen, Ole and {van der Eerden}, Jan and Goodarzinick, Abdorreza and Niry, Mohammad D. and Valizadeh, Alireza and Pariz, Aref and Parsi, Shervin S. and Warburton, Julia M. and Marucci, Lucia and Tamagnini, Francesco and Brown, Jon and {Tsaneva-Atanasova}, Krasimira and Kleberg, Florence I. and Triesch, Jochen and Moezzi, Bahar and Iannella, Nicolangelo and Schaworonkow, Natalie and Plogmacher, Lukas and Goldsworthy, Mitchell R. and Hordacre, Brenton and McDonnell, Mark D. and Ridding, Michael C. and Zapotocky, Martin and Smit, Daniel and Fouquet, Coralie and Trembleau, Alain and Dasgupta, Sakyasingha and Nishikawa, Isao and Aihara, Kazuyuki and Toyoizumi, Taro and Robb, Daniel T. and Mellen, Nick and Toporikova, Natalia and Tang, Rongxiang and Tang, Yi-Yuan and Liang, Guangsheng and Kiser, Seth A. and Howard, James H. and Goncharenko, Julia and Voronenko, Sergej O. and Ahamed, Tosif and Stephens, Greg and Yger, Pierre and Lefebvre, Baptiste and Spampinato, Giulia Lia Beatrice and Esposito, Elric and {et Olivier Marre}, Marcel Stimberg and Choi, Hansol and Song, Min-Ho and Chung, SueYeon and Lee, Dan D. and Sompolinsky, Haim and Phillips, Ryan S. and Smith, Jeffrey and Chatzikalymniou, Alexandra Pierri and Ferguson, Katie and Alex Cayco Gajic, N. and Clopath, Claudia and Angus Silver, R. and Gleeson, Padraig and Marin, Boris and Sadeh, Sadra and Quintana, Adrian and Cantarelli, Matteo and {Dura-Bernal}, Salvador and Lytton, William W. and Davison, Andrew and Li, Luozheng and Zhang, Wenhao and Wang, Dahui and Song, Youngjo and Park, Sol and Choi, Ilhwan and Shin, Hee-sup and Choi, Hannah and Pasupathy, Anitha and {Shea-Brown}, Eric and Huh, Dongsung and Sejnowski, Terrence J. and Vogt, Simon M. and Kumar, Arvind and Schmidt, Robert and Van Wert, Stephen and Schiff, Steven J. and Veale, Richard and Scheutz, Matthias and Lee, Sang Wan and Gallinaro, J\'ulia and Rotter, Stefan and Rubchinsky, Leonid L. and Cheung, Chung Ching and {Ratnadurai-Giridharan}, Shivakeshavan and Shomali, Safura Rashid and Ahmadabadi, Majid Nili and Shimazaki, Hideaki and Nader Rasuli, S. and Zhao, Xiaochen and Rasch, Malte J.},
  month = aug,
  year = {2016},
  pages = {54},
  file = {/home/tim/Zotero/storage/M8QKAWM5/Sharpee et al. - 2016 - 25th Annual Computational Neuroscience Meeting CN.pdf}
}

@article{Shin:2016:ITD:2946645.3007060,
  title = {Interleaved {{Text}}/{{Image Deep Mining}} on a {{Large}}-Scale {{Radiology Database}} for {{Automated Image Interpretation}}},
  volume = {17},
  issn = {1532-4435},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  author = {Shin, Hoo-Chang and Lu, Le and Kim, Lauren and Seff, Ari and Yao, Jianhua and Summers, Ronald M.},
  month = jan,
  year = {2016},
  keywords = {deep learning,topic models,convolutional neural networks,natural language processing,medical Imaging},
  pages = {3729-3759},
  publisher = {{JMLR.org}},
  acmid = {3007060},
  issue_date = {January 2016},
  numpages = {31}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
  isbn = {978-3-030-02628-8},
  abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
  language = {English},
  booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
  editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
  year = {2018},
  keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
  pages = {133-140}
}

@inproceedings{Simmons:2009:LLA:1480945.1480949,
  series = {{{PEPM}} '09},
  title = {Linear {{Logical Approximations}}},
  isbn = {978-1-60558-327-3},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/1480945.1480949},
  author = {Simmons, Robert J. and Pfenning, Frank},
  year = {2009},
  keywords = {abstract interpretation,bottom-up linear logic programming,operational semantics},
  pages = {9-20},
  acmid = {1480949},
  numpages = {12}
}

@inproceedings{Singh:2019:EES:3289600.3290620,
  series = {{{WSDM}} '19},
  title = {{{EXS}}: {{Explainable Search Using Local Model Agnostic Interpretability}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3290620},
  author = {Singh, Jaspreet and Anand, Avishek},
  year = {2019},
  keywords = {interpretability,explainable search,neural ranking models},
  pages = {770-773},
  acmid = {3290620},
  numpages = {4}
}

@incollection{skienaMachineLearning2017,
  address = {Cham},
  series = {Texts in {{Computer Science}}},
  title = {Machine {{Learning}}},
  isbn = {978-3-319-55444-0},
  abstract = {For much of my career, I was highly suspicious of the importance of machine learning. I sat through many talks over the years, with grandiose claims and very meager results. But it is clear that the tide has turned. The most interesting work in computer science today revolves around machine learning, both powerful new algorithms and exciting new applications.},
  language = {English},
  booktitle = {The {{Data Science Design Manual}}},
  publisher = {{Springer International Publishing}},
  author = {Skiena, Steven S.},
  editor = {Skiena, Steven S.},
  year = {2017},
  pages = {351-390},
  doi = {10.1007/978-3-319-55444-0_11}
}

@inproceedings{Sklar:2018:ETA:3284432.3284470,
  series = {{{HAI}} '18},
  title = {Explanation {{Through Argumentation}}},
  isbn = {978-1-4503-5953-5},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Human}}-{{Agent Interaction}}},
  publisher = {{ACM}},
  doi = {10.1145/3284432.3284470},
  author = {Sklar, Elizabeth I. and Azhar, Mohammad Q.},
  year = {2018},
  keywords = {explainable ai,human-robot interaction,computational argumentation},
  pages = {277-285},
  acmid = {3284470},
  numpages = {9}
}

@inproceedings{Sridhara:2010:TAG:1858996.1859006,
  series = {{{ASE}} '10},
  title = {Towards {{Automatically Generating Summary Comments}} for {{Java Methods}}},
  isbn = {978-1-4503-0116-9},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  publisher = {{ACM}},
  doi = {10.1145/1858996.1859006},
  author = {Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and {Vijay-Shanker}, K.},
  year = {2010},
  keywords = {comment generation,method summarization,natural language program analysis},
  pages = {43-52},
  acmid = {1859006},
  numpages = {10}
}

@inproceedings{Tamagnini:2017:IBC:3077257.3077260,
  series = {{{HILDA}}'17},
  title = {Interpreting {{Black}}-{{Box Classifiers Using Instance}}-{{Level Visual Explanations}}},
  isbn = {978-1-4503-5029-7},
  booktitle = {Proceedings of the {{2Nd Workshop}} on {{Human}}-{{In}}-the-{{Loop Data Analytics}}},
  publisher = {{ACM}},
  doi = {10.1145/3077257.3077260},
  author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
  year = {2017},
  keywords = {machine learning,explanation,classification,visual analytics},
  pages = {6:1-6:6},
  acmid = {3077260},
  articleno = {6},
  numpages = {6}
}

@inproceedings{Tan:2018:IAD:3278721.3278802,
  series = {{{AIES}} '18},
  title = {Interpretable {{Approaches}} to {{Detect Bias}} in {{Black}}-{{Box Models}}},
  isbn = {978-1-4503-6012-8},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278802},
  author = {Tan, Sarah},
  year = {2018},
  keywords = {transparency,interpretability,algorithmic fairness,black-box models,model distillation},
  pages = {382-383},
  acmid = {3278802},
  numpages = {2}
}

@article{tienInternetThingsRealTime2017,
  title = {Internet of {{Things}}, {{Real}}-{{Time Decision Making}}, and {{Artificial Intelligence}}},
  volume = {4},
  issn = {2198-5812},
  abstract = {In several earlier papers, the author defined and detailed the concept of a servgood, which can be thought of as a physical good or product enveloped by a services-oriented layer that makes the good smarter or more adaptable and customizable for a particular use. Adding another layer of physical sensors could then enhance its smartness and intelligence, especially if it were to be connected with other servgoods\textemdash{}thus, constituting an Internet of Things (IoT) or servgoods. More importantly, real-time decision making is central to the Internet of Things; it is about decision informatics and embraces the advanced technologies of sensing (i.e., Big Data), processing (i.e., real-time analytics), reacting (i.e., real-time decision-making), and learning (i.e., deep learning). Indeed, real-time decision making (RTDM) is becoming an integral aspect of IoT and artificial intelligence (AI), including its improving abilities at voice and video recognition, speech and predictive synthesis, and language and social-media understanding. These three key and mutually supportive technologies\textemdash{}IoT, RTDM, and AI\textemdash{}are considered herein, including their progress to date.},
  language = {English},
  number = {2},
  journal = {Annals of Data Science},
  doi = {10.1007/s40745-017-0112-5},
  author = {Tien, James M.},
  month = jun,
  year = {2017},
  keywords = {Internet of things,Artificial intelligence,Services,Goods,Real-time decision making,Servgoods},
  pages = {149-178}
}

@inproceedings{Treanor:2012:MG:2282338.2282347,
  series = {{{FDG}} '12},
  title = {The {{Micro}}-Rhetorics of {{Game}}-o-{{Matic}}},
  isbn = {978-1-4503-1333-9},
  booktitle = {Proceedings of the {{International Conference}} on the {{Foundations}} of {{Digital Games}}},
  publisher = {{ACM}},
  doi = {10.1145/2282338.2282347},
  author = {Treanor, Mike and Schweizer, Bobby and Bogost, Ian and Mateas, Michael},
  year = {2012},
  keywords = {game design,game interpretation,procedural rhetoric},
  pages = {18-25},
  acmid = {2282347},
  numpages = {8}
}

@incollection{turnerControllingCreations2019,
  address = {Cham},
  title = {Controlling the {{Creations}}},
  isbn = {978-3-319-96235-1},
  abstract = {Turner explains how in order to implement constraints into AI directly, we will need to address both moral and technical questions: Which norms should be chosen? How can these be implemented? Potential basic laws for robots include: a law of identification, requiring that AI makes its status clear; a law of explanation, requiring that at least some parts of AI's reasoning be divulged; a laws on avoiding bias; and a law setting out any limits to areas where AI can operate. Finally, a kill switch law might make it mandatory that AI systems include a mechanism for safely interrupting their processes or operations, either temporarily or permanently.},
  language = {English},
  booktitle = {Robot {{Rules}} : {{Regulating Artificial Intelligence}}},
  publisher = {{Springer International Publishing}},
  author = {Turner, Jacob},
  editor = {Turner, Jacob},
  year = {2019},
  pages = {319-369},
  doi = {10.1007/978-3-319-96235-1_8}
}

@inproceedings{Vartak:2018:MSS:3183713.3196934,
  series = {{{SIGMOD}} '18},
  title = {{{MISTIQUE}}: {{A System}} to {{Store}} and {{Query Model Intermediates}} for {{Model Diagnosis}}},
  isbn = {978-1-4503-4703-7},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  doi = {10.1145/3183713.3196934},
  author = {Vartak, Manasi and {F. da Trindade}, Joana M. and Madden, Samuel and Zaharia, Matei},
  year = {2018},
  keywords = {machine learning,model interpretability,model diagnosis,systems for machine learning},
  pages = {1285-1300},
  acmid = {3196934},
  numpages = {16}
}

@inproceedings{Wang:2016:TCW:2906831.2906852,
  series = {{{HRI}} '16},
  title = {Trust {{Calibration Within}} a {{Human}}-{{Robot Team}}: {{Comparing Automatically Generated Explanations}}},
  isbn = {978-1-4673-8370-7},
  booktitle = {The {{Eleventh ACM}}/{{IEEE International Conference}} on {{Human Robot Interaction}}},
  publisher = {{IEEE Press}},
  author = {Wang, Ning and Pynadath, David V. and Hill, Susan G.},
  year = {2016},
  keywords = {trust,explainable a.i.,human-robot interaction,pomdp},
  pages = {109-116},
  acmid = {2906852},
  numpages = {8}
}

@inproceedings{Wang:2018:TTE:3178876.3186066,
  series = {{{WWW}} '18},
  title = {{{TEM}}: {{Tree}}-Enhanced {{Embedding Model}} for {{Explainable Recommendation}}},
  isbn = {978-1-4503-5639-8},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186066},
  author = {Wang, Xiang and He, Xiangnan and Feng, Fuli and Nie, Liqiang and Chua, Tat-Seng},
  year = {2018},
  keywords = {explainable recommendation,embedding-based model,neural attention network,tree-based model},
  pages = {1543-1552},
  acmid = {3186066},
  numpages = {10}
}

@article{wengMedicalSubdomainClassification2017,
  title = {Medical {{Subdomain Classification}} of {{Clinical Notes Using}} a {{Machine Learning}}-{{Based Natural Language Processing Approach}}},
  volume = {17},
  issn = {1472-6947},
  abstract = {BackgroundThe medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note.MethodsWe constructed the pipeline using the clinical NLP system, clinical Text Analysis and Knowledge Extraction System (cTAKES), the Unified Medical Language System (UMLS) Metathesaurus, Semantic Network, and learning algorithms to extract features from two datasets \textemdash{} clinical notes from Integrating Data for Analysis, Anonymization, and Sharing (iDASH) data repository (n = 431) and Massachusetts General Hospital (MGH) (n = 91,237), and built medical subdomain classifiers with different combinations of data representation methods and supervised learning algorithms. We evaluated the performance of classifiers and their portability across the two datasets.ResultsThe convolutional recurrent neural network with neural word embeddings trained-medical subdomain classifier yielded the best performance measurement on iDASH and MGH datasets with area under receiver operating characteristic curve (AUC) of 0.975 and 0.991, and F1 scores of 0.845 and 0.870, respectively. Considering better clinical interpretability, linear support vector machine-trained medical subdomain classifier using hybrid bag-of-words and clinically relevant UMLS concepts as the feature representation, with term frequency-inverse document frequency (tf-idf)-weighting, outperformed other shallow learning classifiers on iDASH and MGH datasets with AUC of 0.957 and 0.964, and F1 scores of 0.932 and 0.934 respectively. We trained classifiers on one dataset, applied to the other dataset and yielded the threshold of F1 score of 0.7 in classifiers for half of the medical subdomains we studied.ConclusionOur study shows that a supervised learning-based NLP approach is useful to develop medical subdomain classifiers. The deep learning algorithm with distributed word representation yields better performance yet shallow learning algorithms with the word and concept representation achieves comparable performance with better clinical interpretability. Portable classifiers may also be used across datasets from different institutions.},
  language = {English},
  number = {1},
  journal = {BMC Medical Informatics and Decision Making},
  doi = {10.1186/s12911-017-0556-8},
  author = {Weng, Wei-Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
  month = dec,
  year = {2017},
  keywords = {Machine Learning,Deep Learning,Computer-assisted,Distributed Representation,Medical Decision Making,Natural Language Processing,Unified Medical Language System},
  pages = {155},
  file = {/home/tim/Zotero/storage/N3MHX2RB/Weng et al. - 2017 - Medical subdomain classification of clinical notes.pdf}
}

@inproceedings{Wiegand:2019:IDY:3290607.3312817,
  series = {{{CHI EA}} '19},
  title = {I {{Drive}} - {{You Trust}}: {{Explaining Driving Behavior Of Autonomous Cars}}},
  isbn = {978-1-4503-5971-9},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3290607.3312817},
  author = {Wiegand, Gesa and Schmidmaier, Matthias and Weber, Thomas and Liu, Yuanting and Hussmann, Heinrich},
  year = {2019},
  keywords = {autonomous driving,explainability,mental model,situation awareness},
  pages = {LBW0163:1-LBW0163:6},
  acmid = {3312817},
  articleno = {LBW0163},
  numpages = {6}
}

@incollection{wodeckiArtificialIntelligenceMethods2019,
  address = {Cham},
  title = {Artificial {{Intelligence Methods}} and {{Techniques}}},
  isbn = {978-3-319-91596-8},
  abstract = {Artificial intelligence (AI) is a fascinating concept whose origins can be found in the mid-twentieth century. It is an interdisciplinary field, integrating the efforts of logicians, mathematicians, computer scientists, psychologists and, more recently, managers and ethicists. Developing dynamically in the dimension of methods as well as technology, on the one hand, raises many hopes; on the other hand, it raises many fears and controversies (compare e.g. Bostrom 2014), particularly among investors who are interested in ventures with high development potential, yet they are afraid to invest in projects they simply do not understand.},
  language = {English},
  booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
  publisher = {{Springer International Publishing}},
  author = {Wodecki, Andrzej},
  editor = {Wodecki, Andrzej},
  year = {2019},
  pages = {71-132},
  doi = {10.1007/978-3-319-91596-8_2}
}

@inproceedings{Wolf:2019:EST:3301275.3302317,
  series = {{{IUI}} '19},
  title = {Explainability {{Scenarios}}: {{Towards Scenario}}-Based {{XAI Design}}},
  isbn = {978-1-4503-6272-6},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302317},
  author = {Wolf, Christine T.},
  year = {2019},
  keywords = {aging-in-place,explainability scenarios,scenario-based design,XAI},
  pages = {252-257},
  acmid = {3302317},
  numpages = {6}
}

@inproceedings{Wu:2016:ERR:2872518.2889400,
  series = {{{WWW}} '16 {{Companion}}},
  title = {Explaining {{Reviews}} and {{Ratings}} with {{PACO}}: {{Poisson Additive Co}}-{{Clustering}}},
  isbn = {978-1-4503-4144-8},
  booktitle = {Proceedings of the 25th {{International Conference Companion}} on {{World Wide Web}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/2872518.2889400},
  author = {Wu, Chao-Yuan and Beutel, Alex and Ahmed, Amr and Smola, Alexander J.},
  year = {2016},
  keywords = {recommendation systems,co-clustering,joint modeling},
  pages = {127-128},
  acmid = {2889400},
  numpages = {2}
}

@inproceedings{Wu:2018:SDN:3178876.3185995,
  series = {{{WWW}} '18},
  title = {Sharing {{Deep Neural Network Models}} with {{Interpretation}}},
  isbn = {978-1-4503-5639-8},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3185995},
  author = {Wu, Huijun and Wang, Chen and Yin, Jie and Lu, Kai and Zhu, Liming},
  year = {2018},
  keywords = {interpretability,deep neural networks,decision boundary,model sharing},
  pages = {177-186},
  acmid = {3185995},
  numpages = {10}
}

@article{Wyner:2017:ESA:3122009.3153004,
  title = {Explaining the {{Success}} of {{Adaboost}} and {{Random Forests As Interpolating Classifiers}}},
  volume = {18},
  issn = {1532-4435},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  author = {Wyner, Abraham J. and Olson, Matthew and Bleich, Justin and Mease, David},
  month = jan,
  year = {2017},
  keywords = {classification,adaboost,overfitting,random forests,tree-ensembles},
  pages = {1558-1590},
  publisher = {{JMLR.org}},
  acmid = {3153004},
  issue_date = {January 2017},
  numpages = {33}
}

@article{yankovskayaTradeoffSearchMethods2017,
  title = {Tradeoff {{Search Methods}} between {{Interpretability}} and {{Accuracy}} of the {{Identification Fuzzy Systems Based}} on {{Rules}}},
  volume = {27},
  issn = {1555-6212},
  abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
  language = {English},
  number = {2},
  journal = {Pattern Recognition and Image Analysis},
  doi = {10.1134/S1054661817020134},
  author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
  month = apr,
  year = {2017},
  keywords = {machine learning,accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,metaheuristic,pattern recognition,synergy},
  pages = {243-265}
}

@inproceedings{Zaheer:2019:UHS:3289600.3291036,
  series = {{{WSDM}} '19},
  title = {Uncovering {{Hidden Structure}} in {{Sequence Data}} via {{Threading Recurrent Models}}},
  isbn = {978-1-4503-5940-5},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3289600.3291036},
  author = {Zaheer, Manzil and Ahmed, Amr and Wang, Yuan and Silva, Daniel and Najork, Marc and Wu, Yuchen and Sanan, Shibani and Chatterjee, Surojit},
  year = {2019},
  keywords = {interpretable recurrent neural network,sequence clustering,topic models},
  pages = {186-194},
  acmid = {3291036},
  numpages = {9}
}

@article{zerilliTransparencyAlgorithmicHuman2018,
  title = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}: {{Is There}} a {{Double Standard}}?},
  issn = {2210-5441},
  shorttitle = {Transparency in {{Algorithmic}} and {{Human Decision}}-{{Making}}},
  abstract = {We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems, show in what respects AI fares little worse or better and argue that at least some regulatory proposals for explainable AI could end up setting the bar higher than is necessary or indeed helpful. The demands of practical reason require the justification of action to be pitched at the level of practical reason. Decision tools that support or supplant practical reasoning should not be expected to aim higher than this. We cast this desideratum in terms of Daniel Dennett's theory of the ``intentional stance'' and argue that since the justification of action for human purposes takes the form of intentional stance explanation, the justification of algorithmic decisions should take the same form. In practice, this means that the sorts of explanations for algorithmic decisions that are analogous to intentional stance explanations should be preferred over ones that aim at the architectural innards of a decision tool.},
  language = {English},
  journal = {Philosophy \& Technology},
  doi = {10.1007/s13347-018-0330-6},
  author = {Zerilli, John and Knott, Alistair and Maclaurin, James and Gavaghan, Colin},
  month = sep,
  year = {2018},
  keywords = {Transparency,Explainable AI,Algorithmic decision-making,Intentional stance}
}

@inproceedings{Zhang:2018:SWE:3209978.3210193,
  series = {{{SIGIR}} '18},
  title = {{{SIGIR}} 2018 {{Workshop}} on {{ExplainAble Recommendation}} and {{Search}} ({{EARS}} 2018)},
  isbn = {978-1-4503-5657-2},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \&\#38; {{Development}} in {{Information Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3209978.3210193},
  author = {Zhang, Yongfeng and Zhang, Yi and Zhang, Min},
  year = {2018},
  keywords = {explainable search,explainable recommendation,information retrieval,recommendation systems},
  pages = {1411-1413},
  acmid = {3210193},
  numpages = {3}
}

@article{Zhang:2019:DLM:3309769.3279952,
  title = {Deep {{Learning}}\&\#x02013;{{Based Multimedia Analytics}}: {{A Review}}},
  volume = {15},
  issn = {1551-6857},
  number = {1s},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  doi = {10.1145/3279952},
  author = {Zhang, Wei and Yao, Ting and Zhu, Shiai and Saddik, Abdulmotaleb El},
  month = jan,
  year = {2019},
  keywords = {deep learning,neural networks,Multimedia analytics},
  pages = {2:1-2:26},
  location = {New York, NY, USA},
  publisher = {{ACM}},
  acmid = {3279952},
  articleno = {2},
  issue_date = {February 2019},
  numpages = {26}
}

@inproceedings{zhangInterpretableNeuralModel2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {An {{Interpretable Neural Model}} with {{Interactive Stepwise Influence}}},
  isbn = {978-3-030-16142-2},
  abstract = {Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.},
  language = {English},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhang, Yin and Liu, Ninghao and Ji, Shuiwang and Caverlee, James and Hu, Xia},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  keywords = {Neural network,Interpretation,Stepwise Influence},
  pages = {528-540}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
  isbn = {978-3-030-04503-6},
  abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
  language = {English},
  booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer International Publishing}},
  author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
  editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
  year = {2018},
  keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
  pages = {295-308}
}

@article{zhuangChallengesOpportunitiesBig2017,
  title = {Challenges and {{Opportunities}}: {{From Big Data}} to {{Knowledge}} in {{AI}} 2.0},
  volume = {18},
  issn = {2095-9230},
  shorttitle = {Challenges and {{Opportunities}}},
  abstract = {In this paper, we review recent emerging theoretical and technological advances of artificial intelligence (AI) in the big data settings. We conclude that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI, as follows: from shallow computation to deep neural reasoning; from merely data-driven model to data-driven with structured logic rules models; from task-oriented (domain-specific) intelligence (adherence to explicit instructions) to artificial general intelligence in a general context (the capability to learn from experience). Motivated by such endeavors, the next generation of AI, namely AI 2.0, is positioned to reinvent computing itself, to transform big data into structured knowledge, and to enable better decision-making for our society.},
  language = {English},
  number = {1},
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  doi = {10.1631/FITEE.1601883},
  author = {Zhuang, Yue-ting and Wu, Fei and Chen, Chun and Pan, Yun-he},
  month = jan,
  year = {2017},
  keywords = {Big data,Artificial general intelligence,Cross media,Deep reasoning,Knowledge base population,TP391.4},
  pages = {3-14},
  file = {/home/tim/Zotero/storage/V53VCNQS/Zhuang et al. - 2017 - Challenges and opportunities from big data to kno.pdf}
}

@article{guidottiSurveyMethodsExplaining2018,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  volume = {51},
  issn = {0360-0300},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  number = {5},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/3236009},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  month = aug,
  year = {2018},
  keywords = {interpretability,explanations,Open the black box,transparent models},
  pages = {93:1--93:42},
  file = {/home/tim/Zotero/storage/P7EGUVPK/Guidotti et al. - 2018 - A Survey of Methods for Explaining Black Box Model.pdf}
}

@inproceedings{liuAdversarialDetectionModel2018,
  address = {New York, NY, USA},
  series = {{{KDD}} '18},
  title = {Adversarial {{Detection}} with {{Model Interpretation}}},
  isbn = {978-1-4503-5552-0},
  abstract = {Machine learning (ML) systems have been increasingly applied in web security applications such as spammer detection, malware detection and fraud detection. These applications have an intrinsic adversarial nature where intelligent attackers can adaptively change their behaviors to avoid being detected by the deployed detectors. Existing efforts against adversaries are usually limited by the type of applied ML models or the specific applications such as image classification. Additionally, the working mechanisms of ML models usually cannot be well understood by users, which in turn impede them from understanding the vulnerabilities of models nor improving their robustness. To bridge the gap, in this paper, we propose to investigate whether model interpretation could potentially help adversarial detection. Specifically, we develop a novel adversary-resistant detection framework by utilizing the interpretation of ML models. The interpretation process explains the mechanism of how the target ML model makes prediction for a given instance, thus providing more insights for crafting adversarial samples. The robustness of detectors is then improved through adversarial training with the adversarial samples. A data-driven method is also developed to empirically estimate costs of adversaries in feature manipulation. Our approach is model-agnostic and can be applied to various types of classification models. Our experimental results on two real-world datasets demonstrate the effectiveness of interpretation-based attacks and how estimated feature manipulation cost would affect the behavior of adversaries.},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220027},
  author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
  year = {2018},
  keywords = {adversarial detection,machine learning interpretation,spammer detection},
  pages = {1803--1811}
}

@inproceedings{amirAgentStrategySummarization2018,
  address = {Richland, SC},
  series = {{{AAMAS}} '18},
  title = {Agent {{Strategy Summarization}}},
  abstract = {Intelligent agents and AI-based systems are becoming increasingly prevalent. They support people in different ways, such as providing users with advice, working with them to achieve goals or acting on users' behalf. One key capability missing in such systems is the ability to present their users with an effective summary of their strategy and expected behaviors under different conditions and scenarios. This capability, which we see as complimentary to those currently under development in the context of "interpretable machine learning'' and "explainable AI'', is critical in various settings. In particular, it is likely to play a key role whenever a user needs to understand the strategy of an agent she is working along with, when having to choose between different available agents to act on her behalf, or when requested to determine the level of autonomy to be granted to the agent or approve its strategy. In this paper, we pose the challenge of developing capabilities for strategy summarization, which is not addressed by current theories and methods in the field. We propose a conceptual framework for strategy summarization, which we envision as a collaborative process that involves both agents and people. Last, we suggest possible testbeds that could be used to evaluate progress in research on strategy summarization.},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Amir, Ofra and {Doshi-Velez}, Finale and Sarne, David},
  year = {2018},
  keywords = {explainable ai,strategy summarization},
  pages = {1203--1207}
}

@inproceedings{strobelAxiomaticApproachExplain2018,
  address = {New York, NY, USA},
  series = {{{AIES}} '18},
  title = {An {{Axiomatic Approach}} to {{Explain Computer Generated Decisions}}: {{Extended Abstract}}},
  isbn = {978-1-4503-6012-8},
  shorttitle = {An {{Axiomatic Approach}} to {{Explain Computer Generated Decisions}}},
  abstract = {Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3278721.3278788},
  author = {Strobel, Martin},
  year = {2018},
  keywords = {axiomatic approach,explainable machine learning},
  pages = {380--381}
}

@inproceedings{haddouchiAssessingInterpretationCapacity2018,
  address = {New York, NY, USA},
  series = {{{SITA}}'18},
  title = {Assessing {{Interpretation Capacity}} in {{Machine Learning}}: {{A Critical Review}}},
  isbn = {978-1-4503-6462-1},
  shorttitle = {Assessing {{Interpretation Capacity}} in {{Machine Learning}}},
  abstract = {Interpretability of Machine Learning (ML) methods and models is a fundamental issue that concerns a wide range of data mining research. This topic is not only an academic concern, but a crucial aspect for public acceptance of ML in practical contexts as well. Indeed, one should know that the lack of interpretability can be a real drawback for various application areas, such as in healthcare, biology, sociology and industrial decision support systems. In fact, an algorithm, which does not give enough information about the learner process and the learned model would be merely discarded in favor of less accurate and more interpretable approaches. Several papers have been proposed to interpret efficient models, such as Neural Networks and Random Forest, but there is still no consensus about what interpretability refers to. Interestingly, the term has been associated with different notions depending on the point of view of each author, as well as the nature of the issue being treated and the users concerned by the explanation. Therefore, this paper primarily aims to provide a painstaking overview of the aspects related to interpretability of ML learning process and resulting models, as reported by the literature, and to organize the aforementioned aspects into metrics that can be used for ML Interpretability scoring.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Intelligent Systems}}: {{Theories}} and {{Applications}}},
  publisher = {{ACM}},
  doi = {10.1145/3289402.3289549},
  author = {Haddouchi, Maissae and Berrado, Abdelaziz},
  year = {2018},
  keywords = {Interpretability,measures,ML,scoring},
  pages = {49:1--49:6}
}

@inproceedings{yangMultiLayerNeuralNetwork2018,
  address = {New York, NY, USA},
  series = {{{ICCAI}} 2018},
  title = {A {{Multi}}-{{Layer Neural Network Model Integrating BiLSTM}} and {{CNN}} for {{Chinese Sentiment Recognition}}},
  isbn = {978-1-4503-6419-5},
  abstract = {Technology of artificial intelligent has become research focus. Natural language understanding (NLU) is regarded as core technology of AI. Sentiment recognition is a difficult task in NLU; however it is advantageous to business market and public opinion analysis. We proposed a multi-layer neural network model through integrating LSTM and CNN to improve the performance of sentiment recognition. The structure of LSTM is appropriate to storage text sequence information, and CNN has ability to extract salient features for sentiment recognition task. We implemented models of LSTM-CNN and BiLSTM-CNN, and conduct experiments on different dataset. In the end, we contrast our proposed method with certain baseline methods. The result shows that the proposed method outperforms single layer model and other statistic learnint method.},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Computing}} and {{Artificial Intelligence}}},
  publisher = {{ACM}},
  doi = {10.1145/3194452.3194473},
  author = {Yang, Shanliang and Sun, Qi and Zhou, Huyong and Gong, Zhengjie},
  year = {2018},
  keywords = {Neural network,BiLSTM,BiLSTM-CNN,CNN,Sentiment recognition,Word embedding},
  pages = {23--29}
}

@inproceedings{ehsanAutomatedRationaleGeneration2019,
  address = {New York, NY, USA},
  series = {{{IUI}} '19},
  title = {Automated {{Rationale Generation}}: {{A Technique}} for {{Explainable AI}} and {{Its Effects}} on {{Human Perceptions}}},
  isbn = {978-1-4503-6272-6},
  shorttitle = {Automated {{Rationale Generation}}},
  abstract = {Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations, (b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated rationales with regard to confidence in the autonomous agent, communicating failure and unexpected behavior. Overall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users. Moreover, context permitting, participants preferred detailed rationales to form a stable mental model of the agent's behavior.},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302316},
  author = {Ehsan, Upol and Tambwekar, Pradyumna and Chan, Larry and Harrison, Brent and Riedl, Mark O.},
  year = {2019},
  keywords = {transparency,artificial intelligence,machine learning,interpretability,user perception,algorithmic decision-making,algorithmic explanation,explainable AI,rationale generation},
  pages = {263--274}
}

@inproceedings{costaAutomaticGenerationNatural2018,
  address = {New York, NY, USA},
  series = {{{IUI}} '18 {{Companion}}},
  title = {Automatic {{Generation}} of {{Natural Language Explanations}}},
  isbn = {978-1-4503-5571-1},
  abstract = {An interesting challenge for explainable recommender systems is to provide successful interpretation of recommendations using structured sentences. It is well known that user-generated reviews, have strong influence on the users' decision. Recent techniques exploit user reviews to generate natural language explanations. In this paper, we propose a character-level attention-enhanced long short-term memory model to generate natural language explanations. We empirically evaluated this network using two real-world review datasets. The generated text present readable and similar to a real user's writing, due to the ability of reproducing negation, misspellings, and domain-specific vocabulary.},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  publisher = {{ACM}},
  doi = {10.1145/3180308.3180366},
  author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
  year = {2018},
  keywords = {Explainability,Explanations,Neural Network,Natural Language Generation,Recommender systems},
  pages = {57:1--57:2},
  file = {/home/tim/Zotero/storage/WWC2MQRX/Costa et al. - 2018 - Automatic Generation of Natural Language Explanati.pdf}
}

@inproceedings{st.amantBalancingEfficiencyInterpretability2003,
  address = {New York, NY, USA},
  series = {{{IUI}} '03},
  title = {Balancing {{Efficiency}} and {{Interpretability}} in an {{Interactive Statistical Assistant}}},
  isbn = {978-1-58113-586-2},
  abstract = {Making an interface more efficient, in a task analysis sense, can make it more difficult for an automated reasoning system to infer user goals, by eliminating some user actions, by presenting information without requiring overt user selection, and so forth. We call the extent to which a system can make such inferences interpretability. In this paper we describe the tradeoff between interpretability and efficiency. We give some general heuristics for improving interpretability in a system and explain how they apply in an implemented system, an assistant for exploratory statistical analysis. Increased interpretability in the system is provided by navigation techniques for data exploration and a data mountain for organizing results; a formative evaluation illustrates some of the potential benefits of applying interpretability heuristics to an intelligent user interface},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/604045.604074},
  author = {St. Amant, Robert and Dinardo, Michael D. and Buckner, Nickie},
  year = {2003},
  keywords = {interpretability,data mountain,efficiency,navigation,user interface design},
  pages = {181--188}
}

@inproceedings{weiszBigBlueBotTeachingStrategies2019,
  address = {New York, NY, USA},
  series = {{{IUI}} '19},
  title = {{{BigBlueBot}}: {{Teaching Strategies}} for {{Successful Human}}-Agent {{Interactions}}},
  isbn = {978-1-4503-6272-6},
  shorttitle = {{{BigBlueBot}}},
  abstract = {Chatbots are becoming quite popular, with many brands developing conversational experiences using platforms such as IBM's Watson Assistant and Facebook Messenger. However, previous research reveals that users' expectations of what conversational agents can understand and do far outpace their actual technical capabilities. Our work seeks to bridge the gap between these expectations and reality by designing a fun learning experience with several goals: explaining how chatbots work by mapping utterances to a set of intents, teaching strategies for avoiding conversational breakdowns, and increasing desire to use chatbots by creating feelings of empathy toward them. Our experience, called BigBlueBot, consists of interactions with two chatbots in which breakdowns occur and the user (or chatbot) must recover using one or more repair strategies. In a Mechanical Turk evaluation (N=88), participants learned strategies for having successful human-agent interactions, reported feelings of empathy toward the chatbots, and expressed a desire to interact with chatbots in the future.},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  doi = {10.1145/3301275.3302290},
  author = {Weisz, Justin D. and Jain, Mohit and Joshi, Narendra Nath and Johnson, James and Lange, Ingrid},
  year = {2019},
  keywords = {explainable AI,conversational agents,mechanical turk},
  pages = {448--459}
}

@inproceedings{pynadathClusteringBehaviorRecognize2018,
  title = {Clustering {{Behavior}} to {{Recognize Subjective Beliefs}} in {{Human}}-{{Agent Teams}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  author = {Pynadath, David V. and Wang, Ning and Rovira, Ericka and Barnes, Michael J.},
  month = sep,
  year = {2018},
  keywords = {trust,explainable ai,affect recognition,human-agent teams},
  pages = {1495-1503},
  file = {/home/tim/Zotero/storage/9JU9JZTV/citation.html}
}

@inproceedings{hicksComprehensibleReasoningAutomated2018,
  title = {Comprehensible Reasoning and Automated Reporting of Medical Examinations Based on Deep Learning Analysis},
  isbn = {978-1-4503-5192-8},
  booktitle = {Proceedings of the 9th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM}},
  doi = {10.1145/3204949.3208113},
  author = {Hicks, Steven Alexander and Pogorelov, Konstantin and {de Lange}, Thomas and Lux, Mathias and Jeppsson, Mattis and Randel, Kristin Ranheim and Eskeland, Sigrun and Halvorsen, P\aa{}l and Riegler, Michael},
  month = dec,
  year = {2018},
  keywords = {deep learning,automatic disease detection,interpretable neural networks,medical documentation},
  pages = {490-493},
  file = {/home/tim/Zotero/storage/TJ8MNBBP/citation.html}
}

@article{biecekDALEXExplainersComplex2018,
  title = {{{DALEX}}: Explainers for Complex Predictive Models in {{R}}},
  volume = {19},
  issn = {1532-4435},
  shorttitle = {{{DALEX}}},
  number = {1},
  journal = {The Journal of Machine Learning Research},
  author = {Biecek, Przemys\l{}aw},
  month = jan,
  year = {2018},
  keywords = {interpretable machine learning,explainable artificial intelligence,model visualization,predictive modelling},
  pages = {3245-3249},
  file = {/home/tim/Zotero/storage/7IRZKSY6/citation.html}
}

@inproceedings{blankDeepLearningClassroom2018,
  title = {Deep {{Learning}} in the {{Classroom}}: ({{Abstract Only}})},
  isbn = {978-1-4503-5103-4},
  shorttitle = {Deep {{Learning}} in the {{Classroom}}},
  booktitle = {Proceedings of the 49th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  publisher = {{ACM}},
  doi = {10.1145/3159450.3162370},
  author = {Blank, Douglas and Meeden, Lisa and Marshall, Jim},
  month = feb,
  year = {2018},
  keywords = {deep learning,artificial intelligence,artificial neural networks,python},
  pages = {1055-1055},
  file = {/home/tim/Zotero/storage/YU97PXVR/citation.html}
}

@inproceedings{farsalDeepLearningOverview2018,
  address = {New York, NY, USA},
  series = {{{SITA}}'18},
  title = {Deep {{Learning}}: {{An Overview}}},
  isbn = {978-1-4503-6462-1},
  shorttitle = {Deep {{Learning}}},
  abstract = {Deep learning is a thriving research area with many successful applications in different fields. The article is written with a view to provide a state of the art review of deep learning. To some extent, we will present a historical overview necessary to understand the concepts that laid the foundations of today's Deep Learning. We will cover different methods that made the successful training of deep learning models possible at a very high scale in various modern practices.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Intelligent Systems}}: {{Theories}} and {{Applications}}},
  publisher = {{ACM}},
  doi = {10.1145/3289402.3289538},
  author = {Farsal, Wissal and Anter, Samir and Ramdani, Mohammed},
  year = {2018},
  keywords = {Artificial Intelligence,Deep Learning,Neural Networks},
  pages = {38:1--38:6}
}

@inproceedings{cardDeepWeightedAveraging2019,
  address = {New York, NY, USA},
  series = {{{FAT}}* '19},
  title = {Deep {{Weighted Averaging Classifiers}}},
  isbn = {978-1-4503-6125-5},
  abstract = {Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  publisher = {{ACM}},
  doi = {10.1145/3287560.3287595},
  author = {Card, Dallas and Zhang, Michael and Smith, Noah A.},
  year = {2019},
  keywords = {conformal methods,interpretability credibility},
  pages = {369--378},
  file = {/home/tim/Zotero/storage/CC2ELJ4P/Card et al. - 2019 - Deep Weighted Averaging Classifiers.pdf}
}

@inproceedings{tangEvaluationVisualizationMethods2018,
  address = {New York, NY, USA},
  series = {{{ACAI}} 2018},
  title = {Evaluation of {{Visualization Methods}}' {{Effect}} on {{Convolutional Neural Networks Research}}},
  isbn = {978-1-4503-6625-0},
  abstract = {In recent years, as an important research hotspot in the field of artificial intelligence and machine learning, the convolutional neural network has made substantial breakthroughs and has been widely used. In order to better explore and understand its structure, more and more researchers have shifted the focus of their research to the visualization of convolutional neural networks. They learned from the neural network what features were studied. They applied it to parameter adjustment and optimization in the convolutional neural networks and achieved good results. In this paper, the basic structure of convolutional neural network is described first. Secondly, some commonly used volume and neural network models are introduced. Finally, the convolutional neural network visualization technology is evaluated.},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Algorithms}}, {{Computing}} and {{Artificial Intelligence}}},
  publisher = {{ACM}},
  doi = {10.1145/3302425.3302476},
  author = {Tang, Haijing and Wang, Yiru and Yang, Xu},
  year = {2018},
  keywords = {Artificial Intelligence,Machine Learning,Visualization,Convolutional Neural Network},
  pages = {52:1--52:5}
}

@inproceedings{sangDeepLearningInterpretation2018,
  title = {Deep {{Learning Interpretation}}},
  isbn = {978-1-4503-5665-7},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  publisher = {{ACM}},
  doi = {10.1145/3240508.3241472},
  author = {Sang, Jitao},
  month = oct,
  year = {2018},
  keywords = {deep learning,interpretable machine learning},
  pages = {2098-2100},
  file = {/home/tim/Zotero/storage/3BNJRZU2/citation.html}
}

@inproceedings{haDesigningExplainabilityArtificial2018,
  address = {New York, NY, USA},
  series = {{{TechMindSociety}} '18},
  title = {Designing {{Explainability}} of an {{Artificial Intelligence System}}},
  isbn = {978-1-4503-5420-2},
  abstract = {Explainability and accuracy of the machine learning algorithms usually laid on a trade-off relationship. Several algorithms such as deep-learning artificial neural networks have high accuracy but low explainability. Since there were only limited ways to access the learning and prediction processes in algorithms, researchers and users were not able to understand how the results were given to them. However, a recent project, explainable artificial intelligence (XAI) by DARPA, showed that AI systems can be highly explainable but also accurate. Several technical reports of XAI suggested ways of extracting explainable features and their positive effects on users; the results showed that explainability of AI was helpful to make users understand and trust the system. However, only a few studies have addressed why the explainability can bring positive effects to users. We suggest theoretical reasons from the attribution theory and anthropomorphism studies. Trough a review, we develop three hypotheses: (1) causal attribution is a human nature and thus a system which provides casual explanation on their process will affect users to attribute the result of system; (2) Based on the attribution results, users will perceive the system as human-like and which will be a motivation of anthropomorphism; (3) The system will be perceived by the users through the anthropomorphism. We provide a research framework for designing causal explainability of an AI system and discuss the expected results of the research.},
  booktitle = {Proceedings of the {{Technology}}, {{Mind}}, and {{Society}}},
  publisher = {{ACM}},
  doi = {10.1145/3183654.3183683},
  author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
  year = {2018},
  keywords = {Explainability,Anthropomorphism,Attribution theory,User perception},
  pages = {14:1--14:1}
}

@inproceedings{zantedeschiEfficientDefensesAdversarial2017,
  address = {New York, NY, USA},
  series = {{{AISec}} '17},
  title = {Efficient {{Defenses Against Adversarial Attacks}}},
  isbn = {978-1-4503-5202-4},
  abstract = {Following the recent adoption of deep neural networks (DNN) accross a wide range of applications, adversarial attacks against these models have proven to be an indisputable threat. Adversarial samples are crafted with a deliberate intention of undermining a system. In the case of DNNs, the lack of better understanding of their working has prevented the development of efficient defenses. In this paper, we propose a new defense method based on practical observations which is easy to integrate into models and performs better than state-of-the-art defenses. Our proposed solution is meant to reinforce the structure of a DNN, making its prediction more stable and less likely to be fooled by adversarial samples. We conduct an extensive experimental study proving the efficiency of our method against multiple attacks, comparing it to numerous defenses, both in white-box and black-box setups. Additionally, the implementation of our method brings almost no overhead to the training procedure, while maintaining the prediction performance of the original model on clean samples.},
  booktitle = {Proceedings of the 10th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}}},
  publisher = {{ACM}},
  doi = {10.1145/3128572.3140449},
  author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
  year = {2017},
  keywords = {deep neural network,adversarial learning,defenses,model security},
  pages = {39--49},
  file = {/home/tim/Zotero/storage/JX3Q5V3C/Zantedeschi et al. - 2017 - Efficient Defenses Against Adversarial Attacks.pdf}
}

@inproceedings{zhangDetectingAdversarialPerturbations2018,
  address = {New York, NY, USA},
  series = {{{ICIT}} 2018},
  title = {Detecting {{Adversarial Perturbations}} with {{Salieny}}},
  isbn = {978-1-4503-6629-8},
  abstract = {In this paper, we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. Saliency In the case of image classification model, saliency simply explain how the model makes decisions by identifying significant pixels for prediction. A model shows wrong classification output always learns wrong features and shows wrong saliency as well. Our approach shows good performance on detecting adversarial perturbations. We quantitatively evaluate generalization ability of the detector, showing that detectors trained with strong adversaries perform well on weak adversaries.},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Information Technology}}: {{IoT}} and {{Smart City}}},
  publisher = {{ACM}},
  doi = {10.1145/3301551.3301588},
  author = {Zhang, Chiliang and Yang, Zhimou and Ye, Zuochang},
  year = {2018},
  keywords = {Adversarial Examples,Convolutional Neural Networks,Model Interpretation,Saliency},
  pages = {25--30}
}

@inproceedings{chuExactConsistentInterpretation2018,
  address = {New York, NY, USA},
  series = {{{KDD}} '18},
  title = {Exact and {{Consistent Interpretation}} for {{Piecewise Linear Neural Networks}}: {{A Closed Form Solution}}},
  isbn = {978-1-4503-5552-0},
  shorttitle = {Exact and {{Consistent Interpretation}} for {{Piecewise Linear Neural Networks}}},
  abstract = {Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretations. In this paper, we propose an elegant closed form solution named \$OpenBox\$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply \$OpenBox\$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220063},
  author = {Chu, Lingyang and Hu, Xia and Hu, Juhua and Wang, Lanjun and Pei, Jian},
  year = {2018},
  keywords = {closed form,deep neural network,exact and consistent interpretation},
  pages = {1244--1253}
}

@inproceedings{xiongEndtoEndNeuralAdhoc2017,
  address = {New York, NY, USA},
  series = {{{SIGIR}} '17},
  title = {End-to-{{End Neural Ad}}-Hoc {{Ranking}} with {{Kernel Pooling}}},
  isbn = {978-1-4503-5022-8},
  abstract = {This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3077136.3080809},
  author = {Xiong, Chenyan and Dai, Zhuyun and Callan, Jamie and Liu, Zhiyuan and Power, Russell},
  year = {2017},
  keywords = {embedding,kernel pooling,neural ir,ranking,relevance model},
  pages = {55--64},
  file = {/home/tim/Zotero/storage/ZDMAMVKD/Xiong et al. - 2017 - End-to-End Neural Ad-hoc Ranking with Kernel Pooli.pdf}
}

@inproceedings{wangDesigningTheoryDrivenUserCentric2019,
  address = {New York, NY, USA},
  series = {{{CHI}} '19},
  title = {Designing {{Theory}}-{{Driven User}}-{{Centric Explainable AI}}},
  isbn = {978-1-4503-5970-2},
  abstract = {" From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3290605.3300831},
  author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
  year = {2019},
  keywords = {decision making,explainable artificial intelligence,explanations,clinical decision making,intelligibility},
  pages = {601:1--601:15},
  file = {/home/tim/Zotero/storage/CS6Q63G9/Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf}
}

@inproceedings{zhengDemystifyingDeepLearning2018,
  title = {Demystifying {{Deep Learning}} in {{Networking}}},
  isbn = {978-1-4503-6395-2},
  booktitle = {Proceedings of the 2nd {{Asia}}-{{Pacific Workshop}} on {{Networking}}},
  publisher = {{ACM}},
  doi = {10.1145/3232565.3232569},
  author = {Zheng, Ying and Liu, Ziyu and You, Xinyu and Xu, Yuedong and Jiang, Junchen},
  month = jan,
  year = {2018},
  keywords = {Neural networks,Interpretability,Resource allocation},
  pages = {1-7},
  file = {/home/tim/Zotero/storage/PB3DPZRG/citation.html}
}

@article{liptonMythosModelInterpretability2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03490},
  primaryClass = {cs, stat},
  title = {The {{Mythos}} of {{Model Interpretability}}},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  journal = {arXiv:1606.03490 [cs, stat]},
  author = {Lipton, Zachary C.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tim/Zotero/storage/K9YF2I9K/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/2ID6BE7W/1606.html},
  annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY}
}

@article{arrasWhatRelevantText2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.07843},
  title = {"{{What}} Is {{Relevant}} in a {{Text Document}}?": {{An Interpretable Machine Learning Approach}}},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {"{{What}} Is {{Relevant}} in a {{Text Document}}?},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  number = {8},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0181142},
  author = {Arras, Leila and Horn, Franziska and Montavon, Gr\'egoire and M\"uller, Klaus-Robert and Samek, Wojciech},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Information Retrieval},
  pages = {e0181142},
  file = {/home/tim/Zotero/storage/G6UDH3EC/Arras et al. - 2017 - What is Relevant in a Text Document An Interpr.pdf;/home/tim/Zotero/storage/8GZJMAT2/1612.html},
  annote = {Comment: 19 pages, 7 figures}
}


