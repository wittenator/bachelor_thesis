@article{774103,
 abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
 author = {I. A. {Taha} and J. {Ghosh}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/69.774103},
 issn = {1041-4347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 keywords = {symbol manipulation;knowledge representation;explanation;truth maintenance;feedforward neural nets;neural net architecture;knowledge based systems;learning (artificial intelligence);symbolic interpretation;hybrid intelligent systems;knowledge-based systems;artificial neural networks;domain knowledge representation;domain knowledge mapping;connectionist architecture;network training;rule extraction;explanation power;output decision validation;knowledge refinement;binary rules;feedforward networks;hidden layer;sigmoidal units;partial rules;embedded knowledge;adjustable detail level;rule evaluation technique;rule ordering;performance measures;iris data set;breast cancer data set;Artificial neural networks;Neural networks;Data mining;Military computing;Knowledge representation;Knowledge based systems;Fuzzy neural networks;Fuzzy sets;Computer networks;Intelligent systems},
 month = {May},
 number = {3},
 pages = {448-463},
 title = {Symbolic interpretation of artificial neural networks},
 volume = {11},
 year = {1999}
}

@inproceedings{8215791,
 abstract = {Automatic creation of polarity dictionaries is an important issue, as explanations of prediction models are often required in the financial industry. This paper proposes a novel method of developing an interpretable and predictable neural network model. The neural network model we built can extract polarity scores of concepts from documents. Furthermore, we can detect pairwise interactions between concepts, and create polarity concept dictionaries using our neural network model. The model was built using vector representations of words and polarity scores for about 100 words provided by financial professionals, and we obtained about a hundred times more polarity scores for unknown words through backpropagation. First, we analyze the properties of our method from a theoretical point of view. We then confirm its capabilities by conducting simulations of assigning polarity scores to unknown words and detecting interactions using artificial data. We subsequently estimate sentiment tags using real financial textual datasets. Compared with other conventional methods, the proposed approach can forecast sentiments with higher F1 scores. Finally, we develop a polarity concept dictionary based on Yahoo! Finance board textual data.},
 author = {T. {Ito} and H. {Sakaji} and K. {Izumi} and K. {Tsubouchi} and T. {Yamashita}},
 booktitle = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/ICDMW.2017.159},
 issn = {2375-9259},
 keywords = {backpropagation;data mining;dictionaries;financial data processing;natural language processing;neural nets;text analysis;interpretable neural network model;polarity concept dictionaries;automatic creation;polarity dictionaries;prediction models;interpretable network model;predictable neural network model;polarity scores;unknown words;detecting interactions;financial industry;vector representations;Yahoo! Finance board textual data;Dictionaries;Artificial neural networks;Predictive models;Data mining;Industries;Analytical models;Neural Network Model;Text-mining;Sentiment analysis},
 month = {Nov},
 number = {},
 pages = {1122-1131},
 title = {Development of an Interpretable Neural Network Model for Creation of Polarity Concept Dictionaries},
 volume = {},
 year = {2017}
}

@inproceedings{8258557,
 abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
 author = {S. {Shirataki} and S. {Yamaguchi}},
 booktitle = {2017 IEEE International Conference on Big Data (Big Data)},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/BigData.2017.8258557},
 issn = {},
 keywords = {Big Data;data analysis;learning (artificial intelligence);pattern classification;support vector machines;text analysis;machine learning;decision reasons;Support Vector Machine;big data analysis;SVM;highly weighted words;black boxes;opinions classification;Support vector machines;DVD;Analytical models;Predictive models;Training;Big Data;Tools;SVM;machine learning;interpretability},
 month = {Dec},
 number = {},
 pages = {4830-4831},
 title = {A study on interpretability of decision of machine learning},
 volume = {},
 year = {2017}
}

@inproceedings{8365991,
 abstract = {Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.},
 author = {S. {Nie} and C. {Healey} and K. {Padia} and S. {Leeman-Munk} and J. {Benson} and D. {Caira} and S. {Sethi} and R. {Devarajan}},
 booktitle = {2018 IEEE Pacific Visualization Symposium (PacificVis)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/PacificVis.2018.00031},
 issn = {2165-8773},
 keywords = {data visualisation;feedforward neural nets;learning (artificial intelligence);natural language processing;text analysis;interaction techniques;DNN models;network activation;training patterns;internal activation patterns;neural network researchers;convolutional connected layers;fully connected layers;deep neural networks;DNNs;visualization system;convolutional layers;visual encodings;text analytics;TNNVis;clustering techniques;summarization techniques;speech models;Visualization;Neurons;Computational modeling;Task analysis;Convolutional neural networks;Biological neural networks;information visualization;deep learning;machine learning;visualization design;human centered computing},
 month = {April},
 number = {},
 pages = {180-189},
 title = {Visualizing Deep Neural Networks for Text Analytics},
 volume = {},
 year = {2018}
}

@article{8440085,
 abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.},
 author = {Y. {Ming} and H. {Qu} and E. {Bertini}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TVCG.2018.2864812},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;interactive systems;knowledge representation;learning (artificial intelligence);matrix algebra;pattern classification;rule matrix;black-box model;matrix-based visualization;standardized rule-based knowledge representation;predictive models;interactive visualization technique;machine learning systems;Machine learning;Data visualization;Visualization;Neural networks;Decision trees;Data models;Support vector machines;explainable machine learning;rule visualization;visual analytics},
 month = {Jan},
 number = {1},
 pages = {342-352},
 title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules},
 volume = {25},
 year = {2019}
}

@article{8440091,
 abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
 author = {J. {Zhang} and Y. {Wang} and P. {Molino} and L. {Li} and D. S. {Ebert}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TVCG.2018.2864499},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data analysis;data visualisation;iterative methods;learning (artificial intelligence);neural nets;regression analysis;model-agnostic framework;machine learning models;visual analysis techniques;internal logic;specific model type;model development;diagnosis process;potential machine learning;regression tasks;classification tasks;feature discrimination;customizable tabular view;models outcome;scatterplot-based visual summary;refinement;explanation;inspection;generic framework;deep neural networks;debugging;interpretation;manifold;Visualization;Analytical models;Task analysis;Machine learning;Manifolds;Data models;Computational modeling;Interactive machine learning;performance analysis;model comparison;model debugging},
 month = {Jan},
 number = {1},
 pages = {364-373},
 title = {Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models},
 volume = {25},
 year = {2019}
}

@article{8466590,
 abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
 author = {A. {Adadi} and M. {Berrada}},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/ACCESS.2018.2870052},
 issn = {2169-3536},
 journal = {IEEE Access},
 keywords = {artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
 month = {},
 number = {},
 pages = {52138-52160},
 title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
 volume = {6},
 year = {2018}
}

@inproceedings{8489172,
 abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for “algorithmic fairness” also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
 author = {X. {Liu} and X. {Wang} and S. {Matwin}},
 booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/IJCNN.2018.8489172},
 issn = {2161-4407},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);pattern classification;pattern clustering;random processes;meta-learning;model interpretability;CNN-INTE;clustering algorithm;meta-level training data;base learning algorithms;meta-level test data;machine learning approaches;interpretable deep convolutional neural networks;MNIST image dataset;random forest;deep CNN model;Prediction algorithms;Machine learning;Machine learning algorithms;Predictive models;Computational modeling;Training data;Visualization;interpretability;Meta-learning;deep learning;Convolutional Neural Network;TensorFlow;big data},
 month = {July},
 number = {},
 pages = {1-9},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 volume = {},
 year = {2018}
}

@inproceedings{8490530,
 abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
 author = {A. {Holzinger}},
 booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (DISA)},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/DISA.2018.8490530},
 issn = {},
 keywords = {learning (artificial intelligence);ontologies (artificial intelligence);probability;statistical machine learning methods;AI winter;deep learning approaches;big quality data;computational power;engineering effort;ontologies;knowledge representations;glass-box approaches;mathematical principles;artificial intelligence;logical approaches;probabilistic learning methods;rule-based approaches;contextual explanatory models;context-adaptive procedures;high-dimensional vectors;Machine learning;Data mining;Data visualization;Uncertainty;Games;Cognitive science},
 month = {Aug},
 number = {},
 pages = {55-66},
 title = {From Machine Learning to Explainable AI},
 volume = {},
 year = {2018}
}

@inproceedings{8491501,
 abstract = {To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future-i.e., new-data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.},
 author = {B. {Murray} and M. A. {Islam} and A. J. {Pinar} and T. C. {Havens} and D. T. {Anderson} and G. {Scott}},
 booktitle = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/FUZZ-IEEE.2018.8491501},
 issn = {},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);optimisation;remote sensing;sensor fusion;data-driven optimization;Choquet integral;fusion solution;parametric nonlinear aggregation function;data-centric XAI tools;XAI-ChI methods;explainable AI;heterogeneous deep convolutional neural networks;remote sensing;Frequency modulation;Indexes;Remote sensing;Optimization;Artificial intelligence;Electronic mail;Convolutional neural networks;Choquet Integral;Fuzzy Integral;Explainable AI;Machine Learning},
 month = {July},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Understanding Decisions and Data-Driven Optimization of the Choquet Integral},
 volume = {},
 year = {2018}
}

@inproceedings{8591457,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
 author = {D. L. {Marino} and C. S. {Wickramasinghe} and M. {Manic}},
 booktitle = {IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/IECON.2018.8591457},
 issn = {2577-1647},
 keywords = {learning (artificial intelligence);multilayer perceptrons;neural nets;pattern classification;security of data;adversarial approach;explainable AI;cyber-security applications;adversarial machine learning;multilayer perceptron classifiers;machine learning techniques;deep neural networks;data-driven intrusion detection systems;IDSs;Machine learning;Intrusion detection;Mathematical model;Visualization;Estimation;Adversarial Machine Learning;Adversarial samples;Explainable AI;cyber-security},
 month = {Oct},
 number = {},
 pages = {3237-3243},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection Systems},
 volume = {},
 year = {2018}
}

@inproceedings{8614007,
 abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
 author = {H. {Yanagimto} and K. {Hashimoto} and M. {Okada}},
 booktitle = {2018 International Conference on Machine Learning and Data Engineering (iCMLDE)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/iCMLDE.2018.00024},
 issn = {},
 keywords = {convolutional neural nets;learning (artificial intelligence);neural net architecture;sentiment analysis;neural network architecture;self-attention mechanism;Amazon reviews;GCNN;sentient analysis;attention visualization;natural language processing;gated convolutional neural networks;deep learning;Logic gates;Convolutional neural networks;Kernel;Sentiment analysis;Task analysis;Deep learning;Natural language processing;Gated CNN;Sentiment analysis;the self-attention mechanism},
 month = {Dec},
 number = {},
 pages = {77-82},
 title = {Attention Visualization of Gated Convolutional Neural Networks with Self Attention in Sentiment Analysis},
 volume = {},
 year = {2018}
}

@inproceedings{8622073,
 abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
 author = {R. {Chhatwal} and P. {Gronvall} and N. {Huber-Fliflet} and R. {Keeling} and J. {Zhang} and H. {Zhao}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/BigData.2018.8622073},
 issn = {},
 keywords = {law administration;pattern classification;supervised learning;text analysis;responsive documents;explainable AI system;typical legal document review scenarios;responsive snippets;text classification;explainable predictive coding methods;data-focused engagements;technology-based techniques;machine learning researchers;document classification;supervised learning task;electronically-stored documents;Predictive coding;Law;Predictive models;Text categorization;Machine learning;machine learning;text categorization;explainable AI;predictive coding;explainable predictive coding;legal document review},
 month = {Dec},
 number = {},
 pages = {1905-1911},
 title = {Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding},
 volume = {},
 year = {2018}
}

@article{8653995,
 abstract = {With the rapid development of deep learning models, their performances in various tasks are improved, while meanwhile their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In the paper, we propose a novel Multi-scale INTerpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.},
 author = {X. {Cui} and D. {Wang} and Z. J. {Wang}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TMM.2019.2902099},
 issn = {1520-9210},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Visualization;Computational modeling;Analytical models;Feature extraction;Perturbation methods;Image segmentation;Heating systems;Model interpretability;multi-scale interpretation;convolutional neural networks;model-agnostic},
 month = {},
 number = {},
 pages = {1-1},
 title = {Multi-scale Interpretation Model for Convolutional Neural Networks: Building Trust based on Hierarchical Interpretation},
 volume = {},
 year = {2019}
}

@inproceedings{8679150,
 abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
 author = {D. {Kim} and W. {Lim} and M. {Hong} and H. {Kim}},
 booktitle = {2019 IEEE International Conference on Big Data and Smart Computing (BigComp)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/BIGCOMP.2019.8679150},
 issn = {2375-9356},
 keywords = {feature extraction;image classification;learning (artificial intelligence);natural language processing;neural nets;deep neural network;interpretable transfer learning;high-quality data;complex model;pretrained model;interpretability;transfer learning task;interpretable features;feature extraction layer;Korean characters classification;Feature extraction;Task analysis;Training;Data models;Convolution;Computational modeling;Neural networks;Interpretability;Transfer Learning;Machine Learning},
 month = {Feb},
 number = {},
 pages = {1-4},
 title = {The Structure of Deep Neural Network for Interpretable Transfer Learning},
 volume = {},
 year = {2019}
}

@inproceedings{Balachandran:2009:IRC:1645953.1646227,
 acmid = {1646227},
 address = {New York, NY, USA},
 author = {Balachandran, Vipin and P, Deepak and Khemani, Deepak},
 booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/1645953.1646227},
 isbn = {978-1-60558-512-3},
 keywords = {interpretable clustering},
 location = {Hong Kong, China},
 numpages = {4},
 pages = {1773--1776},
 publisher = {ACM},
 series = {CIKM '09},
 title = {Interpretable and Reconfigurable Clustering of Document Datasets by Deriving Word-based Rules},
 url = {http://doi.acm.org/10.1145/1645953.1646227},
 year = {2009}
}

@article{balachandranInterpretableReconfigurableClustering2012,
 abstract = {Clusters of text documents output by clustering algorithms are often hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clusters and outline the problem of generating clusterings with interpretable and reconfigurable cluster models. We develop two clustering algorithms toward the outlined goal of building interpretable and reconfigurable cluster models. They generate clusters with associated rules that are composed of conditions on word occurrences or nonoccurrences. The proposed approaches vary in the complexity of the format of the rules; RGC employs disjunctions and conjunctions in rule generation whereas RGC-D rules are simple disjunctions of conditions signifying presence of various words. In both the cases, each cluster is comprised of precisely the set of documents that satisfy the corresponding rule. Rules of the latter kind are easy to interpret, whereas the former leads to more accurate clustering. We show that our approaches outperform the unsupervised decision tree approach for rule-generating clustering and also an approach we provide for generating interpretable models for general clusterings, both by significant margins. We empirically show that the purity and f-measure losses to achieve interpretability can be as little as 3 and 5\%, respectively using the algorithms presented herein.},
 author = {Balachandran, Vipin and {Deepak P} and Khemani, Deepak},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/s10115-011-0446-9},
 file = {/home/tim/Zotero/storage/N4JXINRM/Balachandran et al. - 2012 - Interpretable and reconfigurable clustering of doc.pdf},
 issn = {0219-3116},
 journal = {Knowledge and Information Systems},
 keywords = {Data clustering,Interpretability,Text clustering},
 language = {en},
 month = {September},
 number = {3},
 pages = {475-503},
 title = {Interpretable and Reconfigurable Clustering of Document Datasets by Deriving Word-Based Rules},
 volume = {32},
 year = {2012}
}

@inproceedings{Bashar:2014:IDP:2682647.2682753,
 acmid = {2682753},
 address = {Washington, DC, USA},
 author = {Bashar, Md Abul and Li, Yuefeng and Shen, Yan and Albathan, Mubarak},
 booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/WI-IAT.2014.67},
 isbn = {978-1-4799-4143-8},
 keywords = {Pattern Interpretation, Information Mismatch and Overload, Ontology-based Mining, Text Mining, Semantic Web},
 numpages = {6},
 pages = {432--437},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '14},
 title = {Interpreting Discovered Patterns in Terms of Ontology Concepts},
 url = {http://dx.doi.org/10.1109/WI-IAT.2014.67},
 year = {2014}
}

@article{bharadhwajExplanationsTemporalRecommendations2018,
 abstract = {Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/s13218-018-0560-x},
 file = {/home/tim/Zotero/storage/4J8DH8MG/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf},
 issn = {1610-1987},
 journal = {KI - K\"unstliche Intelligenz},
 keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
 language = {en},
 month = {November},
 number = {4},
 pages = {267-272},
 title = {Explanations for {{Temporal Recommendations}}},
 volume = {32},
 year = {2018}
}

@inproceedings{Bock:2018:VNN:3281505.3281605,
 acmid = {3281605},
 address = {New York, NY, USA},
 articleno = {132},
 author = {Bock, Marcel and Schreiber, Andreas},
 booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3281505.3281605},
 isbn = {978-1-4503-6086-9},
 keywords = {deep learning, explainable ai, neural networks, visualization},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {132:1--132:2},
 publisher = {ACM},
 series = {VRST '18},
 title = {Visualization of Neural Networks in Virtual Reality Using Unreal Engine},
 url = {http://doi.acm.org/10.1145/3281505.3281605},
 year = {2018}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
 abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
 author = {Bratko, I.},
 booktitle = {Learning, {{Networks}} and {{Statistics}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
 isbn = {978-3-7091-2668-4},
 keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
 language = {en},
 pages = {163-177},
 publisher = {{Springer Vienna}},
 series = {International {{Centre}} for {{Mechanical Sciences}}},
 shorttitle = {Machine {{Learning}}},
 title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
 year = {1997}
}

@inproceedings{brideDependableExplainableMachine2018,
 abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
 author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
 booktitle = {Formal {{Methods}} and {{Software Engineering}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Sun, Jing and Sun, Meng},
 isbn = {978-3-030-02450-5},
 language = {en},
 pages = {412-416},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
 year = {2018}
}

@inproceedings{Brown:2018:RNN:3217871.3217872,
 acmid = {3217872},
 address = {New York, NY, USA},
 articleno = {1},
 author = {Brown, Andy and Tuor, Aaron and Hutchinson, Brian and Nichols, Nicole},
 booktitle = {Proceedings of the First Workshop on Machine Learning for Computing Systems},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3217871.3217872},
 isbn = {978-1-4503-5865-1},
 keywords = {Anomaly detection, Attention, Interpretable Machine Learning, Online Training, Recurrent Neural Networks, System Log Analysis},
 location = {Tempe, AZ, USA},
 numpages = {8},
 pages = {1:1--1:8},
 publisher = {ACM},
 series = {MLCS'18},
 title = {Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection},
 url = {http://doi.acm.org/10.1145/3217871.3217872},
 year = {2018}
}

@incollection{browneCriticalChallengesVisual2018,
 abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
 address = {Cham},
 author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1007/978-3-319-90403-0_7},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {119-136},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
 year = {2018}
}

@inproceedings{Cai:2019:EEE:3301275.3302289,
 acmid = {3302289},
 address = {New York, NY, USA},
 author = {Cai, Carrie J. and Jongejan, Jonas and Holbrook, Jess},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3301275.3302289},
 isbn = {978-1-4503-6272-6},
 keywords = {example-based explanations, explainable AI, human-AI interaction, machine learning},
 location = {Marina del Ray, California},
 numpages = {5},
 pages = {258--262},
 publisher = {ACM},
 series = {IUI '19},
 title = {The Effects of Example-based Explanations in a Machine Learning Interface},
 url = {http://doi.acm.org/10.1145/3301275.3302289},
 year = {2019}
}

@inproceedings{Card:2019:DWA:3287560.3287595,
 acmid = {3287595},
 address = {New York, NY, USA},
 author = {Card, Dallas and Zhang, Michael and Smith, Noah A.},
 booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3287560.3287595},
 isbn = {978-1-4503-6125-5},
 keywords = {conformal methods, interpretability credibility},
 location = {Atlanta, GA, USA},
 numpages = {10},
 pages = {369--378},
 publisher = {ACM},
 series = {FAT* '19},
 title = {Deep Weighted Averaging Classifiers},
 url = {http://doi.acm.org/10.1145/3287560.3287595},
 year = {2019}
}

@inproceedings{carringtonMeasuresModelInterpretability2018,
 abstract = {The literature lacks definitions for quantitative measures of model interpretability for automatic model selection to achieve high accuracy and interpretability, hence we define inherent model interpretability. We extend the work of Lipton et al. and Liu et al. from qualitative and subjective concepts of model interpretability to objective criteria and quantitative measures. We also develop another new measure called simplicity of sensitivity and illustrate prior, initial and posterior measurement. Measures are tested and validated with some measures recommended for use. It is demonstrated that high accuracy and high interpretability are jointly achievable with little to no sacrifice in either.},
 author = {Carrington, Andr\'e and Fieguth, Paul and Chen, Helen},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Kernels,Model interpretability,Model transparency,Support vector machines},
 language = {en},
 pages = {329-349},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measures of {{Model Interpretability}} for {{Model Selection}}},
 year = {2018}
}

@inproceedings{Costa:2018:AGN:3180308.3180366,
 acmid = {3180366},
 address = {New York, NY, USA},
 articleno = {57},
 author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
 booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3180308.3180366},
 isbn = {978-1-4503-5571-1},
 keywords = {Explainability, Explanations, Natural Language Generation, Neural Network, Recommender systems},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {57:1--57:2},
 publisher = {ACM},
 series = {IUI '18 Companion},
 title = {Automatic Generation of Natural Language Explanations},
 url = {http://doi.acm.org/10.1145/3180308.3180366},
 year = {2018}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
 abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
 address = {Cham},
 author = {{Doshi-Velez}, Finale and Kim, Been},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1007/978-3-319-98131-4_1},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Accountability,Interpretability,Machine learning,Transparency},
 language = {en},
 pages = {3-17},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
 year = {2018}
}

@inproceedings{fabra-boludaModellingMachineLearning2018,
 abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
 author = {{Fabra-Boluda}, Ra\"ul and Ferri, C\`esar and {Hern\'andez-Orallo}, Jos\'e and {Mart\'inez-Plumed}, Fernando and {Ram\'irez-Quintana}, M. Jos\'e},
 booktitle = {Philosophy and {{Theory}} of {{Artificial Intelligence}} 2017},
 comments = {Reviews the current state of explainability research, },
 editor = {M\"uller, Vincent C.},
 isbn = {978-3-319-96448-5},
 language = {en},
 pages = {175-186},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Applied Philosophy}}, {{Epistemology}} and {{Rational Ethics}}},
 title = {Modelling {{Machine Learning Models}}},
 year = {2018}
}

@inproceedings{Fernandez:2017:MCC:3132515.3132520,
 acmid = {3132520},
 address = {New York, NY, USA},
 author = {Fernandez, Delia and Woodward, Alejandro and Campos, Victor and Giro-i-Nieto, Xavier and Jou, Brendan and Chang, Shih-Fu},
 booktitle = {Proceedings of the Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3132515.3132520},
 isbn = {978-1-4503-5509-4},
 keywords = {adjective noun pairs, affective computing, compound concepts, convolutional neural networks, interpretable models},
 location = {Mountain View, California, USA},
 numpages = {9},
 pages = {61--69},
 publisher = {ACM},
 series = {MUSA2 '17},
 title = {More Cat Than Cute?: Interpretable Prediction of Adjective-Noun Pairs},
 url = {http://doi.acm.org/10.1145/3132515.3132520},
 year = {2017}
}

@inproceedings{goebelExplainableAINew2018,
 abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
 author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning},
 language = {en},
 pages = {295-303},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Explainable {{AI}}},
 title = {Explainable {{AI}}: {{The New}} 42?},
 year = {2018}
}

@inproceedings{Green:2009:GTS:1639714.1639768,
 acmid = {1639768},
 address = {New York, NY, USA},
 author = {Green, Stephen J. and Lamere, Paul and Alexander, Jeffrey and Maillet, Fran\c{c}ois and Kirk, Susanna and Holt, Jessica and Bourque, Jackie and Mak, Xiao-Wen},
 booktitle = {Proceedings of the Third ACM Conference on Recommender Systems},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/1639714.1639768},
 isbn = {978-1-60558-435-5},
 keywords = {explainable recommender, steerable recommender},
 location = {New York, New York, USA},
 numpages = {4},
 pages = {281--284},
 publisher = {ACM},
 series = {RecSys '09},
 title = {Generating Transparent, Steerable Recommendations from Textual Descriptions of Items},
 url = {http://doi.acm.org/10.1145/1639714.1639768},
 year = {2009}
}

@article{Guidotti:2018:SME:3271482.3236009,
 acmid = {3236009},
 address = {New York, NY, USA},
 articleno = {93},
 author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1145/3236009},
 issn = {0360-0300},
 issue_date = {January 2019},
 journal = {ACM Comput. Surv.},
 keywords = {Open the black box, explanations, interpretability, transparent models},
 month = {August},
 number = {5},
 numpages = {42},
 pages = {93:1--93:42},
 publisher = {ACM},
 title = {A Survey of Methods for Explaining Black Box Models},
 url = {http://doi.acm.org/10.1145/3236009},
 volume = {51},
 year = {2018}
}

@inproceedings{Guo:2018:LED:3243734.3243792,
 acmid = {3243792},
 address = {New York, NY, USA},
 author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
 booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3243734.3243792},
 isbn = {978-1-4503-5693-0},
 keywords = {binary analysis, deep recurrent neural networks, explainable AI},
 location = {Toronto, Canada},
 numpages = {16},
 pages = {364--379},
 publisher = {ACM},
 series = {CCS '18},
 title = {LEMNA: Explaining Deep Learning Based Security Applications},
 url = {http://doi.acm.org/10.1145/3243734.3243792},
 year = {2018}
}

@inproceedings{Ha:2018:DEA:3183654.3183683,
 acmid = {3183683},
 address = {New York, NY, USA},
 articleno = {14},
 author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
 booktitle = {Proceedings of the Technology, Mind, and Society},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1145/3183654.3183683},
 isbn = {978-1-4503-5420-2},
 keywords = {Anthropomorphism, Attribution theory, Explainability, User perception},
 location = {Washington, DC, USA},
 numpages = {1},
 pages = {14:1--14:1},
 publisher = {ACM},
 series = {TechMindSociety '18},
 title = {Designing Explainability of an Artificial Intelligence System},
 url = {http://doi.acm.org/10.1145/3183654.3183683},
 year = {2018}
}

@inproceedings{Haddouchi:2018:AIC:3289402.3289549,
 acmid = {3289549},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Haddouchi, Maissae and Berrado, Abdelaziz},
 booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1145/3289402.3289549},
 isbn = {978-1-4503-6462-1},
 keywords = {Interpretability, ML, measures, scoring},
 location = {Rabat, Morocco},
 numpages = {6},
 pages = {49:1--49:6},
 publisher = {ACM},
 series = {SITA'18},
 title = {Assessing Interpretation Capacity in Machine Learning: A Critical Review},
 url = {http://doi.acm.org/10.1145/3289402.3289549},
 year = {2018}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
 abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
 author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainable AI,Knowledge extraction,Machine learning,Privacy},
 language = {en},
 pages = {1-8},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
 title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1602.04938v3,
 abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {2},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {http://arxiv.org/pdf/1602.04938v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1604.00289v3,
 abstract = {Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.},
 author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {4},
 title = {Building Machines That Learn and Think Like People},
 url = {http://arxiv.org/pdf/1604.00289v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.03490v3,
 abstract = {Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.},
 author = {Lipton, Zachary C.},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {6},
 title = {The Mythos of Model Interpretability},
 url = {http://arxiv.org/pdf/1606.03490v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.05386v1,
 abstract = {Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {6},
 title = {Model-Agnostic Interpretability of Machine Learning},
 url = {http://arxiv.org/pdf/1606.05386v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1608.08974v2,
 abstract = {Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.},
 author = {Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {8},
 title = {Towards Transparent AI Systems: Interpreting Visual Question Answering
Models},
 url = {http://arxiv.org/pdf/1608.08974v2},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07270v1,
 abstract = {Understanding neural networks is becoming increasingly important. Over the
last few years different types of visualisation and explanation methods have
been proposed. However, none of them explicitly considered the behaviour in the
presence of noise and distracting elements. In this work, we will show how
noise and distracting dimensions can influence the result of an explanation
model. This gives a new theoretical insights to aid selection of the most
appropriate explanation model within the deep-Taylor decomposition framework.},
 author = {Kindermans, Pieter-Jan and Schütt, Kristof and Müller, Klaus-Robert and Dähne, Sven},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {11},
 title = {Investigating the influence of noise and distractors on the
interpretation of neural networks},
 url = {http://arxiv.org/pdf/1611.07270v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07567v1,
 abstract = {Complex problems may require sophisticated, non-linear learning methods such
as kernel machines or deep neural networks to achieve state of the art
prediction accuracies. However, high prediction accuracies are not the only
objective to consider when solving problems using machine learning. Instead,
particular scientific applications require some explanation of the learned
prediction function. Unfortunately, most methods do not come with out of the
box straight forward interpretation. Even linear prediction functions are not
straight forward to explain if features exhibit complex correlation structure.
In this paper, we propose the Measure of Feature Importance (MFI). MFI is
general and can be applied to any arbitrary learning machine (including kernel
machines and deep learning). MFI is intrinsically non-linear and can detect
features that by itself are inconspicuous and only impact the prediction
function through their interaction with other features. Lastly, MFI can be used
for both --- model-based feature importance and instance-based feature
importance (i.e, measuring the importance of a feature for a particular data
point).},
 author = {Vidovic, Marina M. -C. and Görnitz, Nico and Müller, Klaus-Robert and Kloft, Marius},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {11},
 title = {Feature Importance Measure for Non-linear Learning Algorithms},
 url = {http://arxiv.org/pdf/1611.07567v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07634v1,
 abstract = {State of the art machine learning algorithms are highly optimized to provide
the optimal prediction possible, naturally resulting in complex models. While
these models often outperform simpler more interpretable models by order of
magnitudes, in terms of understanding the way the model functions, we are often
facing a "black box".
In this paper we suggest a simple method to interpret the behavior of any
predictive model, both for regression and classification. Given a particular
model, the information required to interpret it can be obtained by studying the
partial derivatives of the model with respect to the input. We exemplify this
insight by interpreting convolutional and multi-layer neural networks in the
field of natural language processing.},
 author = {Hechtlinger, Yotam},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {11},
 title = {Interpretation of Prediction Models Using the Input Gradient},
 url = {http://arxiv.org/pdf/1611.07634v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1612.07843v1,
 abstract = {Text documents can be described by a number of abstract concepts such as
semantic category, writing style, or sentiment. Machine learning (ML) models
have been trained to automatically map documents to these abstract concepts,
allowing to annotate very large text collections, more than could be processed
by a human in a lifetime. Besides predicting the text's category very
accurately, it is also highly desirable to understand how and why the
categorization process takes place. In this paper, we demonstrate that such
understanding can be achieved by tracing the classification decision back to
individual words using layer-wise relevance propagation (LRP), a recently
developed technique for explaining predictions of complex non-linear
classifiers. We train two word-based ML models, a convolutional neural network
(CNN) and a bag-of-words SVM classifier, on a topic categorization task and
adapt the LRP method to decompose the predictions of these models onto words.
Resulting scores indicate how much individual words contribute to the overall
classification decision. This enables one to distill relevant information from
text documents without an explicit semantic information extraction step. We
further use the word-wise relevance scores for generating novel vector-based
document representations which capture semantic information. Based on these
document vectors, we introduce a measure of model explanatory power and show
that, although the SVM and CNN models perform similarly in terms of
classification accuracy, the latter exhibits a higher level of explainability
which makes it more comprehensible for humans and potentially more useful for
other applications.},
 author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {12},
 title = {"What is Relevant in a Text Document?": An Interpretable Machine
Learning Approach},
 url = {http://arxiv.org/pdf/1612.07843v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1704.03296v3,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks "look" in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.},
 author = {Fong, Ruth and Vedaldi, Andrea},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {4},
 title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
 url = {http://arxiv.org/pdf/1704.03296v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07206v2,
 abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.},
 author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Explaining Recurrent Neural Network Predictions in Sentiment Analysis},
 url = {http://arxiv.org/pdf/1706.07206v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07979v1,
 abstract = {This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.},
 author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {6},
 title = {Methods for Interpreting and Understanding Deep Neural Networks},
 url = {http://arxiv.org/pdf/1706.07979v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1707.09641v2,
 abstract = {The predictive power of neural networks often costs model interpretability.
Several techniques have been developed for explaining model outputs in terms of
input features; however, it is difficult to translate such interpretations into
actionable insight. Here, we propose a framework to analyze predictions in
terms of the model's internal features by inspecting information flow through
the network. Given a trained network and a test image, we select neurons by two
metrics, both measured over a set of images created by perturbations to the
input image: (1) magnitude of the correlation between the neuron activation and
the network output and (2) precision of the neuron activation. We show that the
former metric selects neurons that exert large influence over the network
output while the latter metric selects neurons that activate on generalizable
features. By comparing the sets of neurons selected by these two metrics, our
framework suggests a way to investigate the internal attention mechanisms of
convolutional neural networks.},
 author = {Lengerich, Benjamin J. and Konam, Sandeep and Xing, Eric P. and Rosenthal, Stephanie and Veloso, Manuela},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {7},
 title = {Towards Visual Explanations for Convolutional Neural Networks via Input
Resampling},
 url = {http://arxiv.org/pdf/1707.09641v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1708.08296v1,
 abstract = {With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.},
 author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {8},
 title = {Explainable Artificial Intelligence: Understanding, Visualizing and
Interpreting Deep Learning Models},
 url = {http://arxiv.org/pdf/1708.08296v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.04806v2,
 abstract = {Deep neural networks are widely used for classification. These deep models
often suffer from a lack of interpretability -- they are particularly difficult
to understand because of their non-linear nature. As a result, neural networks
are often treated as "black box" models, and in the past, have been trained
purely to optimize the accuracy of predictions. In this work, we create a novel
network architecture for deep learning that naturally explains its own
reasoning for each prediction. This architecture contains an autoencoder and a
special prototype layer, where each unit of that layer stores a weight vector
that resembles an encoded training input. The encoder of the autoencoder allows
us to do comparisons within the latent space, while the decoder allows us to
visualize the learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be similar to at least
one encoded input, a term that encourages every encoded input to be close to at
least one prototype, and a term that encourages faithful reconstruction by the
autoencoder. The distances computed in the prototype layer are used as part of
the classification process. Since the prototypes are learned during training,
the learned network naturally comes with explanations for each prediction, and
the explanations are loyal to what the network actually computes.},
 author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {10},
 title = {Deep Learning for Case-Based Reasoning through Prototypes: A Neural
Network that Explains Its Predictions},
 url = {http://arxiv.org/pdf/1710.04806v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.09511v2,
 abstract = {Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.},
 author = {Barratt, Shane},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {10},
 title = {InterpNET: Neural Introspection for Interpretable Deep Learning},
 url = {http://arxiv.org/pdf/1710.09511v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.10777v1,
 abstract = {Recurrent neural networks (RNNs) have been successfully applied to various
natural language processing (NLP) tasks and achieved better results than
conventional methods. However, the lack of understanding of the mechanisms
behind their effectiveness limits further improvements on their architectures.
In this paper, we present a visual analytics method for understanding and
comparing RNN models for NLP tasks. We propose a technique to explain the
function of individual hidden state units based on their expected response to
input texts. We then co-cluster hidden state units and words based on the
expected response and visualize co-clustering results as memory chips and word
clouds to provide more structured knowledge on RNNs' hidden states. We also
propose a glyph-based sequence visualization based on aggregate information to
analyze the behavior of an RNN's hidden state at the sentence-level. The
usability and effectiveness of our method are demonstrated through case studies
and reviews from domain experts.},
 author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {10},
 title = {Understanding Hidden Memories of Recurrent Neural Networks},
 url = {http://arxiv.org/pdf/1710.10777v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.06302v3,
 abstract = {Interpretation and explanation of deep models is critical towards wide
adoption of systems that rely on them. In this paper, we propose a novel scheme
for both interpretation as well as explanation in which, given a pretrained
model, we automatically identify internal features relevant for the set of
classes considered by the model, without relying on additional annotations. We
interpret the model through average visualizations of this reduced set of
features. Then, at test time, we explain the network prediction by accompanying
the predicted class label with supporting visualizations derived from the
identified features. In addition, we propose a method to address the artifacts
introduced by stridded operations in deconvNet-based visualizations. Moreover,
we introduce an8Flower, a dataset specifically designed for objective
quantitative evaluation of methods for visual explanation.Experiments on the
MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces
detailed explanations with good coverage of relevant features of the classes of
interest},
 author = {Oramas, Jose and Wang, Kaili and Tuytelaars, Tinne},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {12},
 title = {Visual Explanation by Interpretation: Improving Visual Feedback
Capabilities of Deep Neural Networks},
 url = {http://arxiv.org/pdf/1712.06302v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
 abstract = {In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.},
 author = {Mohseni, Sina and Ragan, Eric D.},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {1},
 title = {A Human-Grounded Evaluation Benchmark for Local Explanations of Machine
Learning},
 url = {http://arxiv.org/pdf/1801.05075v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.06889v3,
 abstract = {Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W's and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.},
 author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {1},
 title = {Visual Analytics in Deep Learning: An Interrogative Survey for the Next
Frontiers},
 url = {http://arxiv.org/pdf/1801.06889v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.09808v1,
 abstract = {Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we
study such linear explanations produced either post-hoc by a few recent methods
or generated along with predictions with contextual explanation networks
(CENs). We focus on two questions: (i) whether linear explanations are always
consistent or can be misleading, and (ii) when integrated into the prediction
process, whether and how explanations affect the performance of the model. Our
analysis sheds more light on certain properties of explanations produced by
different methods and suggests that learning models that explain and predict
jointly is often advantageous.},
 author = {Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric P.},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {1},
 title = {The Intriguing Properties of Model Explanations},
 url = {http://arxiv.org/pdf/1801.09808v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00560v2,
 abstract = {Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.},
 author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {2},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 url = {http://arxiv.org/pdf/1802.00560v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00614v2,
 abstract = {This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles' heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.},
 author = {Zhang, Quanshi and Zhu, Song-Chun},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {2},
 title = {Visual Interpretability for Deep Learning: a Survey},
 url = {http://arxiv.org/pdf/1802.00614v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.07384v2,
 abstract = {We present a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU activations
to change its output. We argue that such a correction is a useful way to
provide feedback to a user when the network's output is different from a
desired output. Our algorithm generates such a correction by solving a series
of linear constraint satisfaction problems. The technique is evaluated on three
neural network models: one predicting whether an applicant will pay a mortgage,
one predicting whether a first-order theorem can be proved efficiently by a
solver using certain heuristics, and the final one judging whether a drawing is
an accurate rendition of a canonical drawing of a cat.},
 author = {Zhang, Xin and Solar-Lezama, Armando and Singh, Rishabh},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {2},
 title = {Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
Corrections},
 url = {http://arxiv.org/pdf/1802.07384v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.04263v3,
 abstract = {Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research.},
 author = {Weld, Daniel S. and Bansal, Gagan},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {3},
 title = {The Challenge of Crafting Intelligible Intelligence},
 url = {http://arxiv.org/pdf/1803.04263v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
 abstract = {Issues regarding explainable AI involve four components: users, laws &
regulations, explanations and algorithms. Together these components provide a
context in which explanation methods can be evaluated regarding their adequacy.
The goal of this chapter is to bridge the gap between expert users and lay
users. Different kinds of users are identified and their concerns revealed,
relevant statements from the General Data Protection Regulation are analyzed in
the context of Deep Neural Networks (DNNs), a taxonomy for the classification
of existing explanation methods is introduced, and finally, the various classes
of explanation methods are analyzed to verify if user concerns are justified.
Overall, it is clear that (visual) explanations can be given about various
aspects of the influence of the input on the output. However, it is noted that
explanation methods or interfaces for lay users are missing and we speculate
which criteria these methods / interfaces should satisfy. Finally it is noted
that two important concerns are difficult to address with explanation methods:
the concern about bias in datasets that leads to biased DNNs, as well as the
suspicion about unfair outcomes.},
 author = {Ras, Gabrielle and Gerven, Marcel van and Haselager, Pim},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {3},
 title = {Explanation Methods in Deep Learning: Users, Values, Concerns and
Challenges},
 url = {http://arxiv.org/pdf/1803.07517v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1804.02527v1,
 abstract = {Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.},
 author = {Choo, Jaegul and Liu, Shixia},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {4},
 title = {Visual Analytics for Explainable Deep Learning},
 url = {http://arxiv.org/pdf/1804.02527v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1805.07468v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., explaining knowledge representations hidden in middle conv-layers of the
CNN. Given feature maps of a certain conv-layer of the CNN, the explainer
performs like an auto-encoder, which first disentangles the feature maps into
object-part features and then inverts object-part features back to features of
higher conv-layers of the CNN. More specifically, the explainer contains
interpretable conv-layers, where each filter disentangles the representation of
a specific object part from chaotic input feature maps. As a paraphrase of CNN
features, the disentangled representations of object parts help people
understand the logic inside the CNN. We also learn the explainer to use
object-part features to reconstruct features of higher CNN layers, in order to
minimize loss of information during the feature disentanglement. More
crucially, we learn the explainer via network distillation without using any
annotations of sample labels, object parts, or textures for supervision. We
have applied our method to different types of CNNs for evaluation, and
explainers have significantly boosted the interpretability of CNN features.},
 author = {Zhang, Quanshi and Yang, Yu and Liu, Yuchen and Wu, Ying Nian and Zhu, Song-Chun},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {5},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks},
 url = {http://arxiv.org/pdf/1805.07468v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.00069v3,
 abstract = {There has recently been a surge of work in explanatory artificial
intelligence (XAI). This research area tackles the important problem that
complex machines and algorithms often cannot provide insights into their
behavior and thought processes. XAI allows users and parts of the internal
system to be more transparent, providing explanations of their decisions in
some level of detail. These explanations are important to ensure algorithmic
fairness, identify potential bias/problems in the training data, and to ensure
that the algorithms perform as expected. However, explanations produced by
these systems is neither standardized nor systematically assessed. In an effort
to create best practices and identify open challenges, we provide our
definition of explainability and show how it can be used to classify existing
literature. We discuss why current approaches to explanatory methods especially
for deep neural networks are insufficient. Finally, based on our survey, we
conclude with suggested future research directions for explanatory artificial
intelligence.},
 author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {5},
 title = {Explaining Explanations: An Overview of Interpretability of Machine
Learning},
 url = {http://arxiv.org/pdf/1806.00069v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.05337v2,
 abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance
due to their ability to learn complex, non-linear relationships between
variables. However, the inability to effectively visualize these relationships
has led to DNNs being characterized as black boxes and consequently limited
their applications. To ameliorate this problem, we introduce the use of
hierarchical interpretations to explain DNN predictions through our proposed
method, agglomerative contextual decomposition (ACD). Given a prediction from a
trained DNN, ACD produces a hierarchical clustering of the input features,
along with the contribution of each cluster to the final prediction. This
hierarchy is optimized to identify clusters of features that the DNN learned
are predictive. Using examples from Stanford Sentiment Treebank and ImageNet,
we show that ACD is effective at diagnosing incorrect predictions and
identifying dataset bias. Through human experiments, we demonstrate that ACD
enables users both to identify the more accurate of two DNNs and to better
trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to
adversarial perturbations, implying that it captures fundamental aspects of the
input and ignores spurious noise.},
 author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Hierarchical interpretations for neural network predictions},
 url = {http://arxiv.org/pdf/1806.05337v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07470v1,
 abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI
(XAI) construct explanations based on the importance of features in
classification tasks. However, in a high-dimensional feature space this
approach may become unfeasible without restraining the set of important
features. We propose to utilize the human tendency to ask questions like "Why
this output (the fact) instead of that output (the foil)?" to reduce the number
of features to those that play a main role in the asked contrast. Our proposed
method utilizes locally trained one-versus-all decision trees to identify the
disjoint set of rules that causes the tree to classify data points as the foil
and not as the fact. In this study we illustrate this approach on three
benchmark classification tasks.},
 author = {Waa, Jasper van der and Robeer, Marcel and Diggelen, Jurriaan van and Brinkhuis, Matthieu and Neerincx, Mark},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Contrastive Explanations with Local Foil Trees},
 url = {http://arxiv.org/pdf/1806.07470v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07538v2,
 abstract = {Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.},
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {http://arxiv.org/pdf/1806.07538v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.09809v1,
 abstract = {Natural language explanations of deep neural network decisions provide an
intuitive way for a AI agent to articulate a reasoning process. Current textual
explanations learn to discuss class discriminative features in an image.
However, it is also helpful to understand which attributes might change a
classification decision if present in an image (e.g., "This is not a Scarlet
Tanager because it does not have black wings.") We call such textual
explanations counterfactual explanations, and propose an intuitive method to
generate counterfactual explanations by inspecting which evidence in an input
is missing, but might contribute to a different classification decision if
present in the image. To demonstrate our method we consider a fine-grained
image classification task in which we take as input an image and a
counterfactual class and output text which explains why the image does not
belong to a counterfactual class. We then analyze our generated counterfactual
explanations both qualitatively and quantitatively using proposed automatic
metrics.},
 author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Generating Counterfactual Explanations with Natural Language},
 url = {http://arxiv.org/pdf/1806.09809v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
 abstract = {Interpretability methods should be both meaningful to a human and correctly
explain model behavior. In this work, we propose a benchmark to evaluate the
latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative
accuracy of interpretability methods that estimate feature importance in deep
neural networks. We evaluate commonly used interpretability methods and a set
of recently proposed ensemble-based derivative approaches. Our results across
several large-scale image classification datasets are consistent and
thought-provoking -- we find that the formal methods we consider produce
estimates that are less accurate or on par with a random designation of feature
importance. However, certain derivative approaches that ensemble these
estimates far outperform such a random guess. The manner of ensembling remains
critical, we show that some approaches do no better than the underlying method
but carry a far higher computational burden.},
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {6},
 title = {Evaluating Feature Importance Estimates},
 url = {http://arxiv.org/pdf/1806.10758v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06978v1,
 abstract = {An important task for a recommender system to provide interpretable
explanations for the user. This is important for the credibility of the system.
Current interpretable recommender systems tend to focus on certain features
known to be important to the user and offer their explanations in a structured
form. It is well known that user generated reviews and feedback from reviewers
have strong leverage over the users' decisions. On the other hand, recent text
generation works have been shown to generate text of similar quality to human
written text, and we aim to show that generated text can be successfully used
to explain recommendations.
In this paper, we propose a framework consisting of popular review-oriented
generation models aiming to create personalised explanations for
recommendations. The interpretations are generated at both character and word
levels. We build a dataset containing reviewers' feedback from the Amazon books
review dataset. Our cross-domain experiments are designed to bridge from
natural language processing to the recommender system domain. Besides language
model evaluation methods, we employ DeepCoNN, a novel review-oriented
recommender system using a deep neural network, to evaluate the recommendation
performance of generated reviews by root mean square error (RMSE). We
demonstrate that the synthetic personalised reviews have better recommendation
performance than human written reviews. To our knowledge, this presents the
first machine-generated natural language explanations for rating prediction.},
 author = {Ouyang, Sixun and Lawlor, Aonghus and Costa, Felipe and Dolog, Peter},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {7},
 title = {Improving Explainable Recommendations with Synthetic Reviews},
 url = {http://arxiv.org/pdf/1807.06978v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.01591v1,
 abstract = {Recurrent neural networks (RNNs) are temporal networks and cumulative in
nature that have shown promising results in various natural language processing
tasks. Despite their success, it still remains a challenge to understand their
hidden behavior. In this work, we analyze and interpret the cumulative nature
of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation
(LISA) for explaining decisions and detecting the most likely (i.e., saliency)
patterns that the network relies on while decision making. We demonstrate (1)
LISA: "How an RNN accumulates or builds semantics during its sequential
processing for a given text example and expected response" (2) Example2pattern:
"How the saliency patterns look like for each category in the data according to
the network in decision making". We analyse the sensitiveness of RNNs about
different inputs to check the increase or decrease in prediction scores and
further extract the saliency patterns learned by the network. We employ two
relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to
explain RNN predictions via the LISA and example2pattern.},
 author = {Gupta, Pankaj and Schütze, Hinrich},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {8},
 title = {LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse
Semantic Accumulation and Example to Pattern Transformation},
 url = {http://arxiv.org/pdf/1808.01591v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
 abstract = {PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.},
 author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {8},
 title = {Learning Explanations from Language Data},
 url = {http://arxiv.org/pdf/1808.04127v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.05054v1,
 abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.},
 author = {Honegger, Milo},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {8},
 title = {Shedding Light on Black Box Machine Learning Algorithms: Development of
an Axiomatic Framework to Assess the Quality of Methods that Explain
Individual Predictions},
 url = {http://arxiv.org/pdf/1808.05054v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09744v1,
 abstract = {Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network's performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.},
 author = {Sushil, Madhumita and Šuster, Simon and Daelemans, Walter},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {8},
 title = {Rule induction for global explanation of trained models},
 url = {http://arxiv.org/pdf/1808.09744v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
 abstract = {Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).},
 author = {Ross, Andrew Slavin},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {9},
 title = {Training Machine Learning Models by Regularizing their Explanations},
 url = {http://arxiv.org/pdf/1810.00869v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
 abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian
predictive models by projecting the information in the predictive distribution
locally to a simpler, interpretable explanation model. The proposed approach
combines the recent Local Interpretable Model-agnostic Explanations (LIME)
method with ideas from Bayesian projection predictive variable selection
methods. The information theoretic basis helps in navigating the trade-off
between explanation fidelity and complexity. We demonstrate the method in
explaining MNIST digit classifications made by a Bayesian deep convolutional
neural network.},
 author = {Peltola, Tomi},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {10},
 title = {Local Interpretable Model-agnostic Explanations of Bayesian Predictive
Models via Kullback-Leibler Projections},
 url = {http://arxiv.org/pdf/1810.02678v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.09312v1,
 abstract = {Convolutional neural networks have been successfully applied to various NLP
tasks. However, it is not obvious whether they model different linguistic
patterns such as negation, intensification, and clause compositionality to help
the decision-making process. In this paper, we apply visualization techniques
to observe how the model can capture different linguistic features and how
these features can affect the performance of the model. Later on, we try to
identify the model errors and their sources. We believe that interpreting CNNs
is the first step to understand the underlying semantic features which can
raise awareness to further improve the performance and explainability of CNN
models.},
 author = {Koupaee, Mahnaz and Wang, William Yang},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {10},
 title = {Analyzing and Interpreting Convolutional Neural Networks in NLP},
 url = {http://arxiv.org/pdf/1810.09312v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.02783v1,
 abstract = {We introduce a novel approach to feed-forward neural network interpretation
based on partitioning the space of sequences of neuron activations. In line
with this approach, we propose a model-specific interpretation method, called
YASENN. Our method inherits many advantages of model-agnostic distillation,
such as an ability to focus on the particular input region and to express an
explanation in terms of features different from those observed by a neural
network. Moreover, examination of distillation error makes the method
applicable to the problems with low tolerance to interpretation mistakes.
Technically, YASENN distills the network with an ensemble of layer-wise
gradient boosting decision trees and encodes the sequences of neuron
activations with leaf indices. The finite number of unique codes induces a
partitioning of the input space. Each partition may be described in a variety
of ways, including examination of an interpretable model (e.g. a logistic
regression or a decision tree) trained to discriminate between objects of those
partitions. Our experiments provide an intuition behind the method and
demonstrate revealed artifacts in neural network decision making.},
 author = {Zharov, Yaroslav and Korzhenkov, Denis and Shvechikov, Pavel and Tuzhilin, Alexander},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {11},
 title = {YASENN: Explaining Neural Networks via Partitioning Activation Sequences},
 url = {http://arxiv.org/pdf/1811.02783v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.10799v1,
 abstract = {Recent efforts in Machine Learning (ML) interpretability have focused on
creating methods for explaining black-box ML models. However, these methods
rely on the assumption that simple approximations, such as linear models or
decision-trees, are inherently human-interpretable, which has not been
empirically tested. Additionally, past efforts have focused exclusively on
comprehension, neglecting to explore the trust component necessary to convince
non-technical experts, such as clinicians, to utilize ML models in practice. In
this paper, we posit that reinforcement learning (RL) can be used to learn what
is interpretable to different users and, consequently, build their trust in ML
models. To validate this idea, we first train a neural network to provide risk
assessments for heart failure patients. We then design a RL-based clinical
decision-support system (DSS) around the neural network model, which can learn
from its interactions with users. We conduct an experiment involving a diverse
set of clinicians from multiple institutions in three different countries. Our
results demonstrate that ML experts cannot accurately predict which system
outputs will maximize clinicians' confidence in the underlying neural network
model, and suggest additional findings that have broad implications to the
future of research into ML interpretability and the use of ML in medicine.},
 author = {Lahav, Owen and Mastronarde, Nicholas and Schaar, Mihaela van der},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {11},
 title = {What is Interpretable? Using Machine Learning to Design Interpretable
Decision-Support Systems},
 url = {http://arxiv.org/pdf/1811.10799v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11705v1,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g.
Deep Neural Networks) in cyber-security applications, most of these models are
perceived as a black-box for the user. Adversarial machine learning offers an
approach to increase our understanding of these models. In this paper we
present an approach to generate explanations for incorrect classifications made
by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is
used to find the minimum modifications (of the input features) required to
correctly classify a given set of misclassified samples. The magnitude of such
modifications is used to visualize the most relevant features that explain the
reason for the misclassification. The presented methodology generated
satisfactory explanations that describe the reasoning behind the
mis-classifications, with descriptions that match expert knowledge. The
advantages of the presented methodology are: 1) applicable to any classifier
with defined gradients. 2) does not require any modification of the classifier
model. 3) can be extended to perform further diagnosis (e.g. vulnerability
assessment) and gain further understanding of the system. Experimental
evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and
Multilayer perceptron classifiers. The results are shown using intuitive
visualizations in order to improve the interpretability of the results.},
 author = {Marino, Daniel L. and Wickramasinghe, Chathurika S. and Manic, Milos},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {11},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection
Systems},
 url = {http://arxiv.org/pdf/1811.11705v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
 abstract = {The need for interpretable and accountable intelligent system gets sensible
as artificial intelligence plays more role in human life. Explainable
artificial intelligence systems can be a solution by self-explaining the
reasoning behind the decisions and predictions of the intelligent system.
Researchers from different disciplines work together to define, design and
evaluate interpretable intelligent systems for the user. Our work supports the
different evaluation goals in interpretable machine learning research by a
thorough review of evaluation methodologies used in machine-explanation
research across the fields of human-computer interaction, visual analytics, and
machine learning. We present a 2D categorization of interpretable machine
learning evaluation methods and show a mapping between user groups and
evaluation measures. Further, we address the essential factors and steps for a
right evaluation plan by proposing a nested model for design and evaluation of
explainable artificial intelligence systems.},
 author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {11},
 title = {A Survey of Evaluation Methods and Measures for Interpretable Machine
Learning},
 url = {http://arxiv.org/pdf/1811.11839v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
 abstract = {Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque "black boxes". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.},
 author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {12},
 title = {Sensitivity based Neural Networks Explanations},
 url = {http://arxiv.org/pdf/1812.01029v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.04801v1,
 abstract = {Interactions such as double negation in sentences and scene interactions in
images are common forms of complex dependencies captured by state-of-the-art
machine learning models. We propose Mah\'e, a novel approach to provide
Model-agnostic hierarchical \'explanations of how powerful machine learning
models, such as deep neural networks, capture these interactions as either
dependent on or free of the context of data instances. Specifically, Mah\'e
provides context-dependent explanations by a novel local interpretation
algorithm that effectively captures any-order interactions, and obtains
context-free explanations through generalizing context-dependent interactions
to explain global behaviors. Experimental results show that Mah\'e obtains
improved local interaction interpretations over state-of-the-art methods and
successfully explains interactions that are context-free.},
 author = {Tsang, Michael and Sun, Youbang and Ren, Dongxu and Liu, Yan},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {12},
 title = {Can I trust you more? Model-Agnostic Hierarchical Explanations},
 url = {http://arxiv.org/pdf/1812.04801v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.07169v1,
 abstract = {This paper presents a method to explain the knowledge encoded in a
convolutional neural network (CNN) quantitatively and semantically. The
analysis of the specific rationale of each prediction made by the CNN presents
a key issue of understanding neural networks, but it is also of significant
practical values in certain applications. In this study, we propose to distill
knowledge from the CNN into an explainable additive model, so that we can use
the explainable model to provide a quantitative explanation for the CNN
prediction. We analyze the typical bias-interpreting problem of the explainable
model and develop prior losses to guide the learning of the explainable
additive model. Experimental results have demonstrated the effectiveness of our
method.},
 author = {Chen, Runjin and Chen, Hao and Huang, Ge and Ren, Jie and Zhang, Quanshi},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {12},
 title = {Explaining Neural Networks Semantically and Quantitatively},
 url = {http://arxiv.org/pdf/1812.07169v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1901.03838v1,
 abstract = {Prediction accuracy and model explainability are the two most important
objectives when developing machine learning algorithms to solve real-world
problems. The neural networks are known to possess good prediction performance,
but lack of sufficient model explainability. In this paper, we propose to
enhance the explainability of neural networks through the following
architecture constraints: a) sparse additive subnetworks; b) orthogonal
projection pursuit; and c) smooth function approximation. It leads to a sparse,
orthogonal and smooth explainable neural network (SOSxNN). The multiple
parameters in the SOSxNN model are simultaneously estimated by a modified
mini-batch gradient descent algorithm based on the backpropagation technique
for calculating the derivatives and the Cayley transform for preserving the
projection orthogonality. The hyperparameters controlling the sparse and smooth
constraints are optimized by the grid search. Through simulation studies, we
compare the SOSxNN method to several benchmark methods including least absolute
shrinkage and selection operator, support vector machine, random forest, and
multi-layer perceptron. It is shown that proposed model keeps the flexibility
of pursuing prediction accuracy while attaining the improved interpretability,
which can be therefore used as a promising surrogate model for complex model
approximation. Finally, the real data example from the Lending Club is employed
as a showcase of the SOSxNN application.},
 author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {1},
 title = {Enhancing Explainability of Neural Networks through Architecture
Constraints},
 url = {http://arxiv.org/pdf/1901.03838v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.07538v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., the explainer uses interpretable visual concepts to explain features in
middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the
explainer performs like an auto-encoder, which decomposes the feature maps into
object-part features. The object-part features are learned to reconstruct CNN
features without much loss of information. We can consider the disentangled
representations of object parts a paraphrase of CNN features, which help people
understand the knowledge encoded by the CNN. More crucially, we learn the
explainer via knowledge distillation without using any annotations of object
parts or textures for supervision. In experiments, our method was widely used
to interpret features of different benchmark CNNs, and explainers significantly
boosted the feature interpretability without hurting the discrimination power
of the CNNs.},
 author = {Zhang, Quanshi and Yang, Yu and Wu, Ying Nian},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {1},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks
(extended abstract)},
 url = {http://arxiv.org/pdf/1901.07538v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.08547v1,
 abstract = {Transfer learning which aims at utilizing knowledge learned from one problem
(source domain) to solve another different but related problem (target domain)
has attracted wide research attentions. However, the current transfer learning
methods are mostly uninterpretable, especially to people without ML expertise.
In this extended abstract, we brief introduce two knowledge graph (KG) based
frameworks towards human understandable transfer learning explanation. The
first one explains the transferability of features learned by Convolutional
Neural Network (CNN) from one domain to another through pre-training and
fine-tuning, while the second justifies the model of a target domain predicted
by models from multiple source domains in zero-shot learning (ZSL). Both
methods utilize KG and its reasoning capability to provide rich and human
understandable explanations to the transfer procedure.},
 author = {Geng, Yuxia and Chen, Jiaoyan and Jimenez-Ruiz, Ernesto and Chen, Huajun},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {1},
 title = {Human-centric Transfer Learning Explanation via Knowledge Graph
[Extended Abstract]},
 url = {http://arxiv.org/pdf/1901.08547v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09839v1,
 abstract = {While the success of deep neural networks (DNNs) is well-established across a
variety of domains, our ability to explain and interpret these methods is
limited. Unlike previously proposed local methods which try to explain
particular classification decisions, we focus on global interpretability and
ask a universally applicable question: given a trained model, which features
are the most important? In the context of neural networks, a feature is rarely
important on its own, so our strategy is specifically designed to leverage
partial covariance structures and incorporate variable dependence into feature
ranking. Our methodological contributions in this paper are two-fold. First, we
propose an effect size analogue for DNNs that is appropriate for applications
with highly collinear predictors (ubiquitous in computer vision). Second, we
extend the recently proposed "RelATive cEntrality" (RATE) measure (Crawford et
al., 2019) to the Bayesian deep learning setting. RATE applies an information
theoretic criterion to the posterior distribution of effect sizes to assess
feature significance. We apply our framework to three broad application areas:
computer vision, natural language processing, and social science.},
 author = {Ish-Horowicz, Jonathan and Udwin, Dana and Flaxman, Seth and Filippi, Sarah and Crawford, Lorin},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {1},
 title = {Interpreting Deep Neural Networks Through Variable Importance},
 url = {http://arxiv.org/pdf/1901.09839v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02384v1,
 abstract = {A barrier to the wider adoption of neural networks is their lack of
interpretability. While local explanation methods exist for one prediction,
most global attributions still reduce neural network decisions to a single set
of features. In response, we present an approach for generating global
attributions called GAM, which explains the landscape of neural network
predictions across subpopulations. GAM augments global explanations with the
proportion of samples that each attribution best explains and specifies which
samples are described by each attribution. Global explanations also have
tunable granularity to detect more or fewer subpopulations. We demonstrate that
GAM's global explanations 1) yield the known feature importances of simulated
data, 2) match feature weights of interpretable statistical models on real
data, and 3) are intuitive to practitioners through user studies. With more
transparent predictions, GAM can help ensure neural network decisions are
generated for the right reasons.},
 author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {2},
 title = {Global Explanations of Neural Networks: Mapping the Landscape of
Predictions},
 url = {http://arxiv.org/pdf/1902.02384v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02497v1,
 abstract = {With the widespread applications of deep convolutional neural networks
(DCNNs), it becomes increasingly important for DCNNs not only to make accurate
predictions but also to explain how they make their decisions. In this work, we
propose a CHannel-wise disentangled InterPretation (CHIP) model to give the
visual interpretation to the predictions of DCNNs. The proposed model distills
the class-discriminative importance of channels in networks by utilizing the
sparse regularization. Here, we first introduce the network perturbation
technique to learn the model. The proposed model is capable to not only distill
the global perspective knowledge from networks but also present the
class-discriminative visual interpretation for specific predictions of
networks. It is noteworthy that the proposed model is able to interpret
different layers of networks without re-training. By combining the distilled
interpretation knowledge in different layers, we further propose the Refined
CHIP visual interpretation that is both high-resolution and
class-discriminative. Experimental results on the standard dataset demonstrate
that the proposed model provides promising visual interpretation for the
predictions of networks in image classification task compared with existing
visual interpretation methods. Besides, the proposed method outperforms related
approaches in the application of ILSVRC 2015 weakly-supervised localization
task.},
 author = {Cui, Xinrui and Wang, Dan and Wang, Z. Jane},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {2},
 title = {CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional
Neural Networks},
 url = {http://arxiv.org/pdf/1902.02497v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
 abstract = {Despite a growing literature on explaining neural networks, no consensus has
been reached on how to explain a neural network decision or how to evaluate an
explanation. In fact, most works rely on manually assessing the explanation to
evaluate the quality of a method. This injects uncertainty in the explanation
process along several dimensions: Which explanation method to apply? Who should
we ask to evaluate it and which criteria should be used for the evaluation? Our
contributions in this paper are twofold. First, we investigate schemes to
combine explanation methods and reduce model uncertainty to obtain a single
aggregated explanation. Our findings show that the aggregation is more robust,
well-aligned with human explanations and can attribute relevance to a broader
set of features (completeness). Second, we propose a novel way of evaluating
explanation methods that circumvents the need for manual evaluation and is not
reliant on the alignment of neural networks and humans decision processes.},
 author = {Rieger, Laura and Hansen, Lars Kai},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {3},
 title = {Aggregating explainability methods for neural networks stabilizes
explanations},
 url = {http://arxiv.org/pdf/1903.00519v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.02323v2,
 abstract = {Deep learning is increasingly used in decision-making tasks. However,
understanding how neural networks produce final predictions remains a
fundamental challenge. Existing work on interpreting neural network predictions
for images often focuses on explaining predictions for single images or
neurons. As predictions are often computed based off of millions of weights
that are optimized over millions of images, such explanations can easily miss a
bigger picture. We present Summit, the first interactive system that scalably
and systematically summarizes and visualizes what features a deep learning
model has learned and how those features interact to make predictions. Summit
introduces two new scalable summarization techniques: (1) activation
aggregation discovers important neurons, and (2) neuron-influence aggregation
identifies relationships among such neurons. Summit combines these techniques
to create the novel attribution graph that reveals and summarizes crucial
neuron associations and substructures that contribute to a model's outcomes.
Summit scales to large data, such as the ImageNet dataset with 1.2M images, and
leverages neural network feature visualization and dataset examples to help
users distill large, complex neural network models into compact, interactive
visualizations. We present neural network exploration scenarios where Summit
helps us discover multiple surprising insights into a state-of-the-art image
classifier's learned representations and informs future neural network
architecture design. The Summit visualization runs in modern web browsers and
is open-sourced.},
 author = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {4},
 title = {Summit: Scaling Deep Learning Interpretability by Visualizing Activation
and Attribution Summarizations},
 url = {http://arxiv.org/pdf/1904.02323v2},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.04063v1,
 abstract = {The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques
specifically developed for analyzing and understanding the inner-workings and
representations acquired by neural models of language. Approaches included:
systematic manipulation of input to neural networks and investigating the
impact on their performance, testing whether interpretable knowledge can be
decoded from intermediate representations acquired by neural networks,
proposing modifications to neural network architectures to make their knowledge
state or generated output more explainable, and examining the performance of
networks on simplified or formal languages. Here we review a number of
representative studies in each category.},
 author = {Alishahi, Afra and Chrupała, Grzegorz and Linzen, Tal},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {4},
 title = {Analyzing and Interpreting Neural Networks for NLP: A Report on the
First BlackboxNLP Workshop},
 url = {http://arxiv.org/pdf/1904.04063v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.08939v1,
 abstract = {A neuroscience method to understanding the brain is to find and study the
preferred stimuli that highly activate an individual cell or groups of cells.
Recent advances in machine learning enable a family of methods to synthesize
preferred stimuli that cause a neuron in an artificial or biological brain to
fire strongly. Those methods are known as Activation Maximization (AM) or
Feature Visualization via Optimization. In this chapter, we (1) review existing
AM techniques in the literature; (2) discuss a probabilistic interpretation for
AM; and (3) review the applications of AM in debugging and explaining networks.},
 author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
 comments = {Reviews the current state of explainability research, },
 journal = {arxiv},
 month = {4},
 title = {Understanding Neural Networks via Feature Visualization: A survey},
 url = {http://arxiv.org/pdf/1904.08939v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.09273v1,
 abstract = {By their nature, the composition of black box models is opaque. This makes
the ability to generate explanations for the response to stimuli challenging.
The importance of explaining black box models has become increasingly important
given the prevalence of AI and ML systems and the need to build legal and
regulatory frameworks around them. Such explanations can also increase trust in
these uncertain systems. In our paper we present RICE, a method for generating
explanations of the behaviour of black box models by (1) probing a model to
extract model output examples using sensitivity analysis; (2) applying
CNPInduce, a method for inductive logic program synthesis, to generate logic
programs based on critical input-output pairs; and (3) interpreting the target
program as a human-readable explanation. We demonstrate the application of our
method by generating explanations of an artificial neural network trained to
follow simple traffic rules in a hypothetical self-driving car simulation. We
conclude with a discussion on the scalability and usability of our approach and
its potential applications to explanation-critical scenarios.},
 author = {Paçacı, Görkem and Johnson, David and McKeever, Steve and Hamfelt, Andreas},
 comments = {Presents a specific method for enhancing explainability for models, },
 journal = {arxiv},
 month = {4},
 title = {"Why did you do that?": Explaining black box models with Inductive
Synthesis},
 url = {http://arxiv.org/pdf/1904.09273v1},
 year = {2019}
}

@inproceedings{Hu:2017:IWI:3132847.3133198,
 acmid = {3133198},
 address = {New York, NY, USA},
 author = {Hu, Xia and Ji, Shuiwang},
 booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1145/3132847.3133198},
 isbn = {978-1-4503-4918-5},
 keywords = {data mining, deep models, interpretability, machine learning, shallow models},
 location = {Singapore, Singapore},
 numpages = {2},
 pages = {2565--2566},
 publisher = {ACM},
 series = {CIKM '17},
 title = {IDM 2017: Workshop on Interpretable Data Mining -- Bridging the Gap Between Shallow and Deep Models},
 url = {http://doi.acm.org/10.1145/3132847.3133198},
 year = {2017}
}

@inproceedings{huExplainableNeuralComputation2018,
 abstract = {In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.},
 author = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
 booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
 isbn = {978-3-030-01234-2},
 keywords = {Interpretable reasoning,Neural module networks,Visual question answering},
 language = {en},
 pages = {55-71},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Explainable {{Neural Computation}} via {{Stack Neural Module Networks}}},
 year = {2018}
}

@article{itoGINNGradientInterpretable2018,
 abstract = {This study aims to visualize financial documents in such a way that even nonexperts can understand the sentiments contained therein. To achieve this, we propose a novel text visualization method using an interpretable neural network (NN) architecture, called a gradient interpretable NN (GINN). A GINN can visualize a market sentiment score from an entire financial document and the sentiment gradient scores in both word and concept units. Moreover, the GINN can visualize important concepts given in various sentence contexts. Such visualization helps nonexperts easily understand financial documents. We theoretically analyze the validity of the GINN and experimentally demonstrate the validity of text visualization produced by the GINN using real financial texts.},
 author = {Ito, Tomoki and Sakaji, Hiroki and Izumi, Kiyoshi and Tsubouchi, Kota and Yamashita, Tatsuo},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/s41060-018-0160-8},
 issn = {2364-4168},
 journal = {International Journal of Data Science and Analytics},
 keywords = {Interpretable neural network,Support system,Text mining},
 language = {en},
 month = {December},
 shorttitle = {{{GINN}}},
 title = {{{GINN}}: Gradient Interpretable Neural Networks for Visualizing Financial Texts},
 year = {2018}
}

@inproceedings{itoTextVisualizingNeuralNetwork2018,
 abstract = {This study aims to visualize financial documents to swiftly obtain market sentiment information from these documents and determine the reason for which sentiment decisions are made. This type of visualization is considered helpful for nonexperts to easily understand technical documents such as financial reports. To achieve this, we propose a novel interpretable neural network (NN) architecture called gradient interpretable NN (GINN). GINN can visualize both the market sentiment score from a whole financial document and the sentiment gradient scores in concept units. We experimentally demonstrate the validity of text visualization produced by GINN using a real textual dataset.},
 author = {Ito, Tomoki and Sakaji, Hiroki and Tsubouchi, Kota and Izumi, Kiyoshi and Yamashita, Tatsuo},
 booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
 isbn = {978-3-319-93040-4},
 keywords = {Interpretable neural network,Support system,Text mining},
 language = {en},
 pages = {247-259},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Text-{{Visualizing Neural Network Model}}},
 title = {Text-{{Visualizing Neural Network Model}}: {{Understanding Online Financial Textual Data}}},
 year = {2018}
}

@incollection{jinSimultaneousGenerationAccurate2006,
 abstract = {Generating machine learning models is inherently a multi-objective optimization problem. Two most common objectives are accuracy and interpretability, which are very likely conflicting with each other. While in most cases we are interested only in the model accuracy, interpretability of the model becomes the major concern if the model is used for data mining or if the model is applied to critical applications. In this chapter, we present a method for simultaneously generating accurate and interpretable neural network models for classification using an evolutionary multi-objective optimization algorithm. Lifetime learning is embedded to fine-tune the weights in the evolution that mutates the structure and weights of the neural networks. The efficiency of Baldwin effect and Lamarckian evolution are compared. It is found that the Lamarckian evolution outperforms the Baldwin effect in evolutionary multi-objective optimization of neural networks. Simulation results on two benchmark problems demonstrate that the evolutionary multi-objective approach is able to generate both accurate and understandable neural network models, which can be used for different purpose.},
 address = {Berlin, Heidelberg},
 author = {Jin, Yaochu and Sendhoff, Bernhard and K\"orner, Edgar},
 booktitle = {Multi-{{Objective Machine Learning}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/3-540-33019-4_13},
 editor = {Jin, Yaochu},
 isbn = {978-3-540-33019-6},
 keywords = {Hide Neuron,Mean Square Error,Multiobjective Optimization,Neural Network,Pareto Front},
 language = {en},
 pages = {291-312},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Simultaneous {{Generation}} of {{Accurate}} and {{Interpretable Neural Network Classifiers}}},
 year = {2006}
}

@inproceedings{Kasneci:2016:LLW:2983323.2983746,
 acmid = {2983746},
 address = {New York, NY, USA},
 author = {Kasneci, Gjergji and Gottron, Thomas},
 booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/2983323.2983746},
 isbn = {978-1-4503-4073-1},
 keywords = {artificial neural networks, contribution, explanation, input variables, linear weighting scheme},
 location = {Indianapolis, Indiana, USA},
 numpages = {10},
 pages = {45--54},
 publisher = {ACM},
 series = {CIKM '16},
 title = {LICON: A Linear Weighting Scheme for the Contribution ofInput Variables in Deep Artificial Neural Networks},
 url = {http://doi.acm.org/10.1145/2983323.2983746},
 year = {2016}
}

@article{Lapuschkin:2016:LTA:2946645.3007067,
 acmid = {3007067},
 author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and M\"{u}ller, Klaus-Robert and Samek, Wojciech},
 comments = {Presents a specific method for enhancing explainability for models, },
 issn = {1532-4435},
 issue_date = {January 2016},
 journal = {J. Mach. Learn. Res.},
 keywords = {artificial neural networks, computer vision, deep learning, explaining classifiers, layer-wise relevance propagation},
 month = {January},
 number = {1},
 numpages = {5},
 pages = {3938--3942},
 publisher = {JMLR.org},
 title = {The LRP Toolbox for Artificial Neural Networks},
 url = {http://dl.acm.org/citation.cfm?id=2946645.3007067},
 volume = {17},
 year = {2016}
}

@inproceedings{laugelComparisonBasedInverseClassification2018,
 abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
 author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
 booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge}}-{{Based Systems}}. {{Theory}} and {{Foundations}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Medina, Jes\'us and {Ojeda-Aciego}, Manuel and Verdegay, Jos\'e Luis and Pelta, David A. and Cabrera, Inma P. and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
 isbn = {978-3-319-91473-2},
 keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
 language = {en},
 pages = {100-111},
 publisher = {{Springer International Publishing}},
 series = {Communications in {{Computer}} and {{Information Science}}},
 title = {Comparison-{{Based Inverse Classification}} for {{Interpretability}} in {{Machine Learning}}},
 year = {2018}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
 abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
 author = {Lisboa, P. J. G.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
 isbn = {978-3-319-03200-9},
 keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
 language = {en},
 pages = {15-21},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
 year = {2013}
}

@inproceedings{Liu:2018:INE:3219819.3220001,
 acmid = {3220001},
 address = {New York, NY, USA},
 author = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3219819.3220001},
 isbn = {978-1-4503-5552-0},
 keywords = {machine learning interpretation, network embedding, taxonomy},
 location = {London, United Kingdom},
 numpages = {9},
 pages = {1812--1820},
 publisher = {ACM},
 series = {KDD '18},
 title = {On Interpretation of Network Embedding via Taxonomy Induction},
 url = {http://doi.acm.org/10.1145/3219819.3220001},
 year = {2018}
}

@inproceedings{Liu:2019:RIS:3289600.3290960,
 acmid = {3290960},
 address = {New York, NY, USA},
 author = {Liu, Ninghao and Du, Mengnan and Hu, Xia},
 booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3289600.3290960},
 isbn = {978-1-4503-5940-5},
 keywords = {interpretation, recommender systems, representation learning},
 location = {Melbourne VIC, Australia},
 numpages = {9},
 pages = {60--68},
 publisher = {ACM},
 series = {WSDM '19},
 title = {Representation Interpretation with Spatial Encoding and Multimodal Analytics},
 url = {http://doi.acm.org/10.1145/3289600.3290960},
 year = {2019}
}

@incollection{liuInterpretabilityComputationalModels2016,
 abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
 address = {Cham},
 author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
 booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/978-3-319-30319-2_9},
 editor = {Pedrycz, Witold and Chen, Shyi-Ming},
 isbn = {978-3-319-30319-2},
 keywords = {Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Machine learning,Rule based networks,Rule based systems,Sentiment prediction},
 language = {en},
 pages = {199-220},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
 year = {2016}
}

@inproceedings{Mei:2018:IGA:3209280.3229119,
 acmid = {3229119},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Mei, Jie and Jiang, Xiang and Islam, Aminul and Moh'd, Abidalrahman and Milios, Evangelos},
 booktitle = {Proceedings of the ACM Symposium on Document Engineering 2018},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3209280.3229119},
 isbn = {978-1-4503-5769-2},
 keywords = {Attention Mechanism, Neural Network, Self-Attention},
 location = {Halifax, NS, Canada},
 numpages = {4},
 pages = {49:1--49:4},
 publisher = {ACM},
 series = {DocEng '18},
 title = {Integrating Global Attention for Pairwise Text Comparison},
 url = {http://doi.acm.org/10.1145/3209280.3229119},
 year = {2018}
}

@inproceedings{otteSafeInterpretableMachine2013,
 abstract = {When learning models from data, the interpretability of the resulting model is often mandatory. For example, safety-related applications for automation and control require that the correctness of the model must be ensured not only for the available data but for all possible input combinations. Thus, understanding what the model has learned and in particular how it will extrapolate to unseen data is a crucial concern. The paper discusses suitable learning methods for classification and regression. For classification problems, we review an approach based on an ensemble of nonlinear low-dimensional submodels, where each submodel is simple enough to be completely verified by domain experts. For regression problems, we review related approaches that try to achieve interpretability by using low-dimensional submodels (for instance, MARS and tree-growing methods). We compare them with symbolic regression, which is a different approach based on genetic algorithms. Finally, a novel approach is proposed for combining a symbolic regression model, which is shown to be easily interpretable, with a Gaussian Process. The combined model has an improved accuracy and provides error bounds in the sense that the deviation from the verified symbolic model is always kept below a defined limit.},
 author = {Otte, Clemens},
 booktitle = {Computational {{Intelligence}} in {{Intelligent Data Analysis}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Moewes, Christian and N\"urnberger, Andreas},
 isbn = {978-3-642-32378-2},
 keywords = {Input Space,Methodological Review,Multivariate Adaptive Regression Spline,Symbolic Model,Symbolic Regression},
 language = {en},
 pages = {111-122},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Studies in {{Computational Intelligence}}},
 shorttitle = {Safe and {{Interpretable Machine Learning}}},
 title = {Safe and {{Interpretable Machine Learning}}: {{A~Methodological Review}}},
 year = {2013}
}

@inproceedings{Pastor:2019:EBB:3297280.3297328,
 acmid = {3297328},
 address = {New York, NY, USA},
 author = {Pastor, Eliana and Baralis, Elena},
 booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3297280.3297328},
 isbn = {978-1-4503-5933-7},
 keywords = {interpretability, local model, prediction explanation},
 location = {Limassol, Cyprus},
 numpages = {8},
 pages = {510--517},
 publisher = {ACM},
 series = {SAC '19},
 title = {Explaining Black Box Models by Means of Local Rules},
 url = {http://doi.acm.org/10.1145/3297280.3297328},
 year = {2019}
}

@inproceedings{pomarlanMeaningfulClusteringsRecurrent2018,
 abstract = {Recurrent neural networks have found applications in NLP, but their operation is difficult to interpret. A state automaton that approximates the network would be more interpretable, but for this one needs a method to group network activation states by their behavior. In this paper we propose such a method, and compare it to an existing dimensionality reduction and clustering approach. Our method is better able to group together neural states of similar behavior.},
 author = {Pomarlan, Mihai and Bateman, John},
 booktitle = {Mining {{Intelligence}} and {{Knowledge Exploration}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Groza, Adrian and Prasath, Rajendra},
 isbn = {978-3-030-05918-7},
 keywords = {Interpretability,Natural language processing,Recurrent neural networks},
 language = {en},
 pages = {11-20},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Meaningful {{Clusterings}} of {{Recurrent Neural Network Activations}} for {{NLP}}},
 year = {2018}
}

@inproceedings{pomarlanUnderstandingNLPNeural2018,
 abstract = {Recurrent neural networks have proven useful in natural language processing. For example, they can be trained to predict, and even generate plausible text with few or no spelling and syntax errors. However, it is not clear what grammar a network has learned, or how it keeps track of the syntactic structure of its input. In this paper, we present a new method to extract a finite state machine from a recurrent neural network. A FSM is in principle a more interpretable representation of a grammar than a neural net would be, however the extracted FSMs for realistic neural networks will also be large. Therefore, we also look at ways to group the states and paths through the extracted FSM so as to get a smaller, easier to understand model of the neural network. To illustrate our methods, we use them to investigate how a neural network learns noun-verb agreement from a simple grammar where relative clauses may appear between noun and verb.},
 author = {Pomarlan, Mihai and Bateman, John},
 booktitle = {{{KI}} 2018: {{Advances}} in {{Artificial Intelligence}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Trollmann, Frank and Turhan, Anni-Yasmin},
 isbn = {978-3-030-00111-7},
 keywords = {Interpretability,Natural language processing,Recurrent neural networks},
 language = {en},
 pages = {284-296},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Understanding {{NLP Neural Networks}} by the {{Texts They Generate}}},
 year = {2018}
}

@article{qureshiEVEExplainableVector2018,
 abstract = {We present an unsupervised explainable vector embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a concept using human-readable labels, thereby it is readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed model, we consider its usefulness in three fundamental tasks: 1) intruder detection\textemdash{}to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster\textemdash{}to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first\textemdash{}to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.},
 author = {Qureshi, M. Atif and Greene, Derek},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1007/s10844-018-0511-x},
 file = {/home/tim/Zotero/storage/9BHZ5UXW/Qureshi and Greene - 2018 - EVE explainable vector based embedding technique .pdf},
 issn = {1573-7675},
 journal = {Journal of Intelligent Information Systems},
 keywords = {Distributional semantics,Unsupervised learning,Wikipedia},
 language = {en},
 month = {June},
 shorttitle = {{{EVE}}},
 title = {{{EVE}}: Explainable Vector Based Embedding Technique Using {{Wikipedia}}},
 year = {2018}
}

@inproceedings{Ribeiro:2016:WIT:2939672.2939778,
 acmid = {2939778},
 address = {New York, NY, USA},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1145/2939672.2939778},
 isbn = {978-1-4503-4232-2},
 keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
 location = {San Francisco, California, USA},
 numpages = {10},
 pages = {1135--1144},
 publisher = {ACM},
 series = {KDD '16},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {http://doi.acm.org/10.1145/2939672.2939778},
 year = {2016}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
 abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
 author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
 booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
 isbn = {978-3-030-02628-8},
 keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
 language = {en},
 pages = {133-140},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
 year = {2018}
}

@inproceedings{Tamagnini:2017:IBC:3077257.3077260,
 acmid = {3077260},
 address = {New York, NY, USA},
 articleno = {6},
 author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
 booktitle = {Proceedings of the 2Nd Workshop on Human-In-the-Loop Data Analytics},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3077257.3077260},
 isbn = {978-1-4503-5029-7},
 keywords = {classification, explanation, machine learning, visual analytics},
 location = {Chicago, IL, USA},
 numpages = {6},
 pages = {6:1--6:6},
 publisher = {ACM},
 series = {HILDA'17},
 title = {Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations},
 url = {http://doi.acm.org/10.1145/3077257.3077260},
 year = {2017}
}

@inproceedings{weberInvestigatingTextualCaseBased2018,
 abstract = {This paper demonstrates how case-based reasoning (CBR) can be used for an explainable artificial intelligence (XAI) approach to justify solutions produced by an opaque learning method (i.e., target method), particularly in the context of unstructured textual data. Our general hypothesis is twofold: (1) There exists patterns in the relationship between problems and solutions and there should be data or a body of knowledge that describes how problems and solutions relate; and (2) the identification, manipulation, and learning of such patterns through case features can help create and reuse explanations for solutions produced by the target method. When the target method relies on neural network architectures (e.g., deep learning), the resulting latent space (i.e., word embeddings) becomes useful for finding patterns and semantic relatedness in textual data. In the proposed approach, case problems are input-output pairs from the target method, and case solutions are explanations. We exemplify our approach by explaining recommended citations from Citeomatic - a multi-layer neural-network architecture from the Allen Institute for Artificial Intelligence. Citation analysis is the body of knowledge that describes how query documents (i.e., inputs) relate to recommended citations (i.e., outputs). We build cases and similarity assessment to learn features that represent patterns between problems and solutions that can lead to the reuse of corresponding explanations. The illustrative implementation we present becomes an explanation-augmented citation recommender that targets human-computer trust.},
 author = {Weber, Rosina O. and Johs, Adam J. and Li, Jianfei and Huang, Kent},
 booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Cox, Michael T. and Funk, Peter and Begum, Shahina},
 isbn = {978-3-030-01081-2},
 keywords = {Case-Based reasoning,Citation recommendation,Explainable artificial intelligence,Human-Computer trust,Semantic relatedness,Textual Case-Based reasoning,Word embeddings},
 language = {en},
 pages = {431-447},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Investigating {{Textual Case}}-{{Based XAI}}},
 year = {2018}
}

@inproceedings{Wu:2018:SDN:3178876.3185995,
 acmid = {3185995},
 address = {Republic and Canton of Geneva, Switzerland},
 author = {Wu, Huijun and Wang, Chen and Yin, Jie and Lu, Kai and Zhu, Liming},
 booktitle = {Proceedings of the 2018 World Wide Web Conference},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1145/3178876.3185995},
 isbn = {978-1-4503-5639-8},
 keywords = {decision boundary, deep neural networks, interpretability, model sharing},
 location = {Lyon, France},
 numpages = {10},
 pages = {177--186},
 publisher = {International World Wide Web Conferences Steering Committee},
 series = {WWW '18},
 title = {Sharing Deep Neural Network Models with Interpretation},
 url = {https://doi.org/10.1145/3178876.3185995},
 year = {2018}
}

@article{Wyner:2017:ESA:3122009.3153004,
 acmid = {3153004},
 author = {Wyner, Abraham J. and Olson, Matthew and Bleich, Justin and Mease, David},
 comments = {Presents a specific method for enhancing explainability for models, },
 issn = {1532-4435},
 issue_date = {January 2017},
 journal = {J. Mach. Learn. Res.},
 keywords = {adaboost, classification, overfitting, random forests, tree-ensembles},
 month = {January},
 number = {1},
 numpages = {33},
 pages = {1558--1590},
 publisher = {JMLR.org},
 title = {Explaining the Success of Adaboost and Random Forests As Interpolating Classifiers},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3153004},
 volume = {18},
 year = {2017}
}

@article{yankovskayaTradeoffSearchMethods2017,
 abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
 author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1134/S1054661817020134},
 issn = {1555-6212},
 journal = {Pattern Recognition and Image Analysis},
 keywords = {accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,machine learning,metaheuristic,pattern recognition,synergy},
 language = {en},
 month = {April},
 number = {2},
 pages = {243-265},
 title = {Tradeoff Search Methods between Interpretability and Accuracy of the Identification Fuzzy Systems Based on Rules},
 volume = {27},
 year = {2017}
}

@inproceedings{zhangInterpretableNeuralModel2019,
 abstract = {Deep neural networks have achieved promising prediction performance, but are often criticized for the lack of interpretability, which is essential in many real-world applications such as health informatics and political science. Meanwhile, it has been observed that many shallow models, such as linear models or tree-based models, are fairly interpretable though not accurate enough. Motivated by these observations, in this paper, we investigate how to fully take advantage of the interpretability of shallow models in neural networks. To this end, we propose a novel interpretable neural model with Interactive Stepwise Influence (ISI) framework. Specifically, in each iteration of the learning process, ISI interactively trains a shallow model with soft labels computed from a neural network, and the learned shallow model is then used to influence the neural network to gain interpretability. Thus ISI could achieve interpretability in three aspects: importance of features, impact of feature value changes, and adaptability of feature weights in the neural network learning process. Experiments on both synthetic and two real-world datasets demonstrate that ISI could generate reliable interpretation with respect to the three aspects, as well as preserve prediction accuracy by comparing with other state-of-the-art methods.},
 author = {Zhang, Yin and Liu, Ninghao and Ji, Shuiwang and Caverlee, James and Hu, Xia},
 booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
 comments = {Presents a specific method for enhancing explainability for models, },
 editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
 isbn = {978-3-030-16142-2},
 keywords = {Interpretation,Neural network,Stepwise Influence},
 language = {en},
 pages = {528-540},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {An {{Interpretable Neural Model}} with {{Interactive Stepwise Influence}}},
 year = {2019}
}

@article{zhangVisualInterpretabilityDeep2018,
 abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human\textendash{}computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
 author = {Zhang, Quan-shi and Zhu, Song-chun},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1631/FITEE.1700808},
 file = {/home/tim/Zotero/storage/RDRDKZXC/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf},
 issn = {2095-9230},
 journal = {Frontiers of Information Technology \& Electronic Engineering},
 keywords = {Artificial intelligence,Deep learning,Interpretable model,TP391},
 language = {en},
 month = {January},
 number = {1},
 pages = {27-39},
 shorttitle = {Visual Interpretability for Deep Learning},
 title = {Visual Interpretability for Deep Learning: A Survey},
 volume = {19},
 year = {2018}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
 abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
 author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
 booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
 comments = {Reviews the current state of explainability research, },
 editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
 isbn = {978-3-030-04503-6},
 keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
 language = {en},
 pages = {295-308},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
 year = {2018}
}

