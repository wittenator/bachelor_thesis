@inproceedings{118383,
 abstract = {Summary form only given, as follows. The information in a trained neural network is stored as numerical weights in the neural elements and the connectivity pattern of the network. For many applications, it is desirable to have this neural network information converted into symbolic knowledge form for communication with human or machine experts. Techniques are presented for converting the information in a trained network into symbolic form as a set of rules and for obtaining explanations from the network for specific inputs. These two techniques provide the neurocomputer with one advantage of expert systems while retaining the learning and generalization capability of the neural network.{$<$}{$>$}},
 booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
 doi = {10.1109/IJCNN.1989.118383},
 keywords = {connectivity pattern,expert systems,Expert systems,explanation,Explanation,generalization capability,interrogation system,learning,neural nets,neural networks,Neural networks,neurocomputer,numerical weights,symbolic knowledge form},
 note = {ISSN:},
 pages = {594 vol.2-},
 title = {A General Explanation and Interrogation System for Neural Networks},
 year = {1989}
}

@inproceedings{1380384,
 abstract = {Support vector machine works well in classifying populations characterized by abrupt decreases in density functions. Its generalization accuracy, however, is not always optimal in dealing with real world problems with neither Gaussian distributions nor sharp boundaries. Incorporating domain theory about problems and excellent intelligent techniques in machine learning into SVM becomes one of promising alternatives. A novel approach, explanation based generalized /spl epsi/-SVM, which synthesizes SVM, prior knowledge, fuzzy logic and neural network, is proposed. Prior knowledge is expressed as a trained fuzzy neural network. An optimal subset of features is obtained by dynamically reducing feature space dimensionality according to the training derivatives extracted from network. By examining a subset of the practical data sampled from Guangdong Natural Science Foundation and testing the remaining set of data, application shows that explanation based generalized /spl epsi/-SVM performs better than that pure SVM and other traditional classifiers.},
 booktitle = {Proceedings of 2004 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}} ({{IEEE Cat}}. {{No}}.{{04EX826}})},
 doi = {10.1109/ICMLC.2004.1380384},
 keywords = {Density functional theory,domain theory,explanation,fuzzy logic,Fuzzy logic,fuzzy neural nets,Gaussian distribution,generalisation (artificial intelligence),generalized /spl epsi/-SVM,intelligent project management,learning (artificial intelligence),Learning systems,machine learning,Machine learning,Network synthesis,neural network,Neural networks,pattern classification,prior knowledge,project management,Project management,support vector machine,Support vector machine classification,support vector machines,Support vector machines,trained fuzzy neural network},
 month = {August},
 note = {ISSN:},
 pages = {3454-3459 vol.6},
 title = {Explanation Based Generalized /Spl Epsi/-{{SVM}} and Its Application in Intelligent Project Management},
 volume = {6},
 year = {2004}
}

@inproceedings{190514,
 abstract = {Outlines an experimental machine learning implementation, called 'FM', that applies both explanation based learning and similarity-based learning to AI planners. The system shell of FM contains techniques for learning application-dependent heuristics, through the experience of using a performance component (a planner) in that application. An application domain is supplied by specifying a set of action schemas, and environmental facts and rules. FM is then fed an initial state, and a sequence of tasks within this application, roughly in ascending order of complexity, which it is expected to solve. After each task has been solved, the system analyses the planning trace, allowing it to learn from experience.{$<$}{$>$}},
 author = {McCluskey, T. L.},
 booktitle = {{{IEE Colloquium}} on {{Machine Learning}}},
 keywords = {AI planners,application-dependent heuristics,artificial intelligence,Artificial intelligence,explanation based learning,learning systems,Learning systems,machine learning,planning,similarity-based learning},
 month = {June},
 note = {ISSN:},
 pages = {6/1-6/3},
 title = {Empirical Results from Applying Machine Learning Techniques to Planning},
 year = {1990}
}

@inproceedings{4761819,
 abstract = {Interpretation accuracy of current handwriting applications can be improved by providing contextual information about an ink samplepsilas expected type. We have developed a novel approach that uses a classic machine learning technique to predict this expected type from an ink sample. With this approach, we can create a ldquodynamic dispatch interpreterrdquo by biasing interpretation differently according to the predicted expected types of the ink samples. When evaluated in the domain of introductory computer science, our interpreter achieves high interpretation accuracy (87\%), an improvement from Microsoftpsilas default interpreter (62\%), and comparable with other previous interpreters (87-89\%), which, unlike ours, require additional user-specified expected type information for each ink sample.},
 author = {Tay, K. S. and Koile, K.},
 booktitle = {2008 19th {{International Conference}} on {{Pattern Recognition}}},
 doi = {10.1109/ICPR.2008.4761819},
 issn = {1051-4651},
 keywords = {Application software,Artificial intelligence,Artificial neural networks,computer aided instruction,Computer science,contextual information,digital ink interpretation,dynamic dispatch interpreter,expected type prediction,Feature extraction,handwriting applications,Handwriting recognition,Hidden Markov models,Ink,Laboratories,learning (artificial intelligence),Machine learning,machine learning technique,Microsoft default interpreter},
 month = {December},
 pages = {1-4},
 title = {Improving Digital Ink Interpretation through Expected Type Prediction and Dynamic Dispatch},
 year = {2008}
}

@article{5993545,
 abstract = {Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.},
 author = {Razavi, S. and Tolson, B. A.},
 doi = {10.1109/TNN.2011.2163169},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {Algorithms,Artificial Intelligence,Biological neural networks,black box model,derivative free optimization algorithm,error response surface,Feedback,feedforward neural nets,feedforward neural network,Feedforward neural networks,function approximation,Function approximation,function approximation techniques,generalisation (artificial intelligence),generalization,generalization ability,geometrical interpretation,internal behavior,learning (artificial intelligence),learning ability,measure of regularization,Models,neural network functional geometry,Neural Networks (Computer),Neurological,Neurons,Nickel,optimisation,Optimization,reformulated neural network,ReNN approach,Software Design,training,Training,training method},
 month = {October},
 number = {10},
 pages = {1588-1598},
 title = {A {{New Formulation}} for {{Feedforward Neural Networks}}},
 volume = {22},
 year = {2011}
}

@article{7063914,
 abstract = {Cloud contamination is a big obstacle when processing satellite images retrieved from visible and infrared spectral ranges for application. Although computational techniques including interpolation and substitution have been applied to recover missing information caused by cloud contamination, these algorithms are subject to many limitations. In this paper, a novel smart information reconstruction (SMIR) method is proposed, in order to reconstruct cloud contaminated pixel values from the time-space-spectrum continuum with the aid of a machine learning tool, namely extreme learning machine (ELM). For the purpose of demonstration, the performance of SMIR is evaluated by reconstructing the missing remote sensing reflectance values derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) on board the Terra satellite over Lake Nicaragua, where is a very cloudy area year round. For comparison, the traditional backpropagation neural network algorithms will also be implemented to reconstruct the missing values. Experimental results show that the ELM outperforms the BP algorithms by an enhanced machine learning capacity with simulated memory effect embedded in MODIS due to linking the complex time-space-spectrum continuum between cloud-free and cloudy pixels. The ELM-based SMIR practice presents a correlation coefficient of 0.88 with root mean squared error of 7.4E - 04sr\textsuperscript{-1} between simulated and observed reflectance values. Finding suggests that the SMIR method is effective to reconstruct all the missing information providing visually logical and quantitatively assured images for further image processing and interpretation in environmental applications.},
 author = {Chang, N. and Bai, K. and Chen, C.},
 doi = {10.1109/JSTARS.2015.2400636},
 issn = {1939-1404},
 journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
 keywords = {Artificial neural network,backpropagation neural network algorithm,cloud contaminated pixel reconstruction,cloud contamination,cloud removal,cloud-free,clouds,Clouds,cloudy area,cloudy pixel,computational intelligence,computational technique,Contamination,ELM-based SMIR practice,enhanced machine learning capacity,environmental application,extreme learning machine,geophysical image processing,image reconstruction,Image reconstruction,infrared spectral range,Lake Nicaragua,lakes,learning (artificial intelligence),machine learning,machine learning tool,missing information recovery,missing remote sensing reflectance value,moderate resolution imaging spectroradiometer,neural nets,Neural networks,reflectivity,remote sensing,Remote sensing,root mean squared error,satellite image processing,satellite images,Satellites,smart information reconstruction,SMIR method,Terra satellite,time-space-spectrum continuum,Training,visible spectral range},
 month = {May},
 number = {5},
 pages = {1898-1912},
 title = {Smart {{Information Reconstruction}} via {{Time}}-{{Space}}-{{Spectrum Continuum}} for {{Cloud Removal}} in {{Satellite Images}}},
 volume = {8},
 year = {2015}
}

@inproceedings{7099955,
 abstract = {This paper discusses the potential benefits of artificial intelligence techniques. First, the role of alarm interpretation in complex process monitoring is examined followed by an analysis of the alarm interpretation task. The techniques used in several industrial applications are described. They are compared and the common features are highlighted. This paper has been written so as to provide the adequate introductory material to the ECC'99 invited session on Artificial Intelligence for Industrial Process Supervision.},
 author = {{Trav\'e-Massuy\`es}, L. and Gentil, S.},
 booktitle = {1999 {{European Control Conference}} ({{ECC}})},
 doi = {10.23919/ECC.1999.7099955},
 keywords = {alarm interpretation,alarm interpretation task,alarm systems,artificial intelligence,artificial intelligence approach,artificial intelligence for industrial process supervision,Artificial intelligence techniques,Automata,Cognition,complex process monitoring,diagnosis,Expert systems,industrial applications,industrial environments,Monitoring,Numerical models,process monitoring,Sensors,supervision interpretation},
 month = {August},
 note = {ISSN:},
 pages = {3989-3994},
 title = {Artificial Intelligence Approaches for Supervision and Alarm Interpretation in Industrial Environments},
 year = {1999}
}

@article{774103,
 abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
 author = {Taha, I. A. and Ghosh, J.},
 doi = {10.1109/69.774103},
 issn = {1041-4347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 keywords = {adjustable detail level,artificial neural networks,Artificial neural networks,binary rules,breast cancer data set,Computer networks,connectionist architecture,Data mining,domain knowledge mapping,domain knowledge representation,embedded knowledge,explanation,explanation power,feedforward networks,feedforward neural nets,Fuzzy neural networks,Fuzzy sets,hidden layer,hybrid intelligent systems,Intelligent systems,iris data set,knowledge based systems,Knowledge based systems,knowledge refinement,knowledge representation,Knowledge representation,knowledge-based systems,learning (artificial intelligence),Military computing,network training,neural net architecture,Neural networks,output decision validation,partial rules,performance measures,rule evaluation technique,rule extraction,rule ordering,sigmoidal units,symbol manipulation,symbolic interpretation,truth maintenance},
 month = {May},
 number = {3},
 pages = {448-463},
 title = {Symbolic Interpretation of Artificial Neural Networks},
 volume = {11},
 year = {1999}
}

@inproceedings{8204039,
 abstract = {With the advancements in Internet of Things (IoT), we could efficiently improve our daily life activities like health care, monitoring, transportation, smart homes etc. Artificial Intelligence along with Machine learning has played a very supportive role to analyze various situations and take decisions accordingly. Maneuver anticipation supplements existing Advance Driver Assistance Systems (ADAS) by anticipating mishaps and giving drivers more opportunity to respond to road circumstances proactively. The capacity to sort the driver conduct is extremely beneficial for advance driver assistance system (ADAS). Deep learning solutions would further be an endeavor of for driving conduct recognition. A technique for distinguishing driver's conduct is imperative to help operative mode transition between the driver and independent vehicles. We propose a novel approach of dissecting driver's conduct by using Convolutional Neural Network (CNN), Recurrent Neural Network(RNN) and a combination of Convolutional Neural Network with Long-Short Term Memory (LSTM) that would give better results in less response time. We are likewise proposing to concentrate high level features and interpretable features depicting complex driving examples by trying CNN, RNN and then CNN with LSTM. We could improve the system accuracy to 95\% by combining CNN with LSTM.},
 author = {Virmani, S. and Gite, S.},
 booktitle = {2017 8th {{International Conference}} on {{Computing}}, {{Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
 doi = {10.1109/ICCCNT.2017.8204039},
 keywords = {ADAS,advance driver assistance system,Advance Driver Assistance Systems,artificial intelligence,Automobiles,CNN,convolutional neural network,daily life activities,Deep learning CNN,deep learning solutions,driver conduct,driver information systems,driving conduct recognition,feedforward neural nets,Hidden Markov models,high level features,independent vehicles,Internet of Things,IoT,ITS,learning (artificial intelligence),long-short term memory,LSTM,machine learning,Machine learning,maneuver anticipation,operative mode transition,recurrent neural nets,Recurrent Neural Network,Recurrent neural networks,RNN,road circumstances,road vehicles,Technological innovation},
 month = {July},
 note = {ISSN:},
 pages = {1-8},
 title = {Performance of Convolutional Neural Network and Recurrent Neural Network for Anticipation of Driver's Conduct},
 year = {2017}
}

@inproceedings{8258557,
 abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
 author = {Shirataki, S. and Yamaguchi, S.},
 booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
 doi = {10.1109/BigData.2017.8258557},
 keywords = {Analytical models,Big Data,big data analysis,black boxes,data analysis,decision reasons,DVD,highly weighted words,interpretability,learning (artificial intelligence),machine learning,opinions classification,pattern classification,Predictive models,Support Vector Machine,support vector machines,Support vector machines,SVM,text analysis,Tools,Training},
 month = {December},
 note = {ISSN:},
 pages = {4830-4831},
 title = {A Study on Interpretability of Decision of Machine Learning},
 year = {2017}
}

@article{8489172,
 abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for ``algorithmic fairness'' also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
 author = {Liu, X. and Wang, X. and Matwin, S.},
 doi = {10.1109/IJCNN.2018.8489172},
 issn = {2161-4407},
 journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
 keywords = {base learning algorithms,big data,clustering algorithm,CNN-INTE,Computational modeling,convolution,Convolutional Neural Network,deep CNN model,deep learning,feedforward neural nets,interpretability,interpretable deep convolutional neural networks,learning (artificial intelligence),Machine learning,Machine learning algorithms,machine learning approaches,meta-learning,Meta-learning,meta-level test data,meta-level training data,MNIST image dataset,model interpretability,pattern classification,pattern clustering,Prediction algorithms,Predictive models,random forest,random processes,TensorFlow,Training data,Visualization},
 month = {July},
 pages = {1-9},
 title = {Interpretable {{Deep Convolutional Neural Networks}} via {{Meta}}-Learning},
 year = {2018}
}

@inproceedings{8489490,
 abstract = {Deep neural network architectures have redefined benchmark machine learning challenges, from classification to anomaly detection, and have become popular in the time series domain. However, deep learning techniques fall short in time series classification (TSC) because the explainability of deep learning is still abstract, and the training requires vast amounts of data, which utilizes computational power. These obstacles are not the case with Shepard Interpolation Neural Networks (SINN), a shallow learning architecture approach for deep learning tasks. Based on a statistical interpolation technique rather than a biological brain, SINN require little data to achieve high accuracy in its training. Additionally, its explainability can be equated to feature mapping onto hyper surfaces in the feature space. Our proposed algorithm outperforms the other state-of-the-art algorithms on the popular UCR time series classification benchmark data set and outperforms LSTMs on data sets which have significantly smaller training data than testing.},
 author = {Smith, K. E. and Williams, P. and Bryan, K. J. and Solomon, M. and Ble, M. and Haber, R.},
 booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
 doi = {10.1109/IJCNN.2018.8489490},
 issn = {2161-4407},
 keywords = {benchmark machine learning challenges,deep learning tasks,deep learning techniques,interpolation,Interpolation,K-means,learning (artificial intelligence),Machine learning,Measurement,neural nets,neural network architectures,Neural networks,Neural Networks,Neurons,pattern classification,Shallow and Deep Learning,shallow learning method,Shepard Interpolation,Shepard interpolation neural networks,Shepard Interpolation Neural Networks,SINN,statistical interpolation technique,Task analysis,time series,Time series analysis,Time Series Classification,time series domain,UCR time series classification benchmark data,Unsupervised Clustering},
 month = {July},
 pages = {1-6},
 title = {Shepard {{Interpolation Neural Networks}} with {{K}}-{{Means}}: {{A Shallow Learning Method}} for {{Time Series Classification}}},
 year = {2018}
}

@inproceedings{8490433,
 abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
 author = {Zhu, J. and Liapis, A. and Risi, S. and Bidarra, R. and Youngblood, G. M.},
 booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
 doi = {10.1109/CIG.2018.8490433},
 issn = {2325-4289},
 keywords = {AI machine,AI/ML techniques,computer games,explainable AI for designers,explainable artificial intelligence,game design,game designers,Games,human computer interaction,human-centered approach,human-centered perspective,human-computer interaction,learning (artificial intelligence),machine learning,Machine learning,mixed-initiative co-creation,Neurons,Task analysis,Tools,Visualization,XAI,XAID framework},
 month = {August},
 pages = {1-8},
 title = {Explainable {{AI}} for {{Designers}}: {{A Human}}-{{Centered Perspective}} on {{Mixed}}-{{Initiative Co}}-{{Creation}}},
 year = {2018}
}

@inproceedings{8529552,
 abstract = {This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labeled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.},
 author = {Karnewar, A.},
 booktitle = {2018 3rd {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
 doi = {10.1109/I2CT.2018.8529552},
 keywords = {AANN,absolute artificial neural network,absolute valued functions,activation functions,artificial neural networks,Artificial neural networks,Backpropagation,data structures,Generative adversarial networks,input data classification,input data representations,knowledge representation,learning (artificial intelligence),mathematical abs functions,neural net architecture,neural nets,neural network architecture,Neurons,pattern classification,representation vectors,supervised learning,Task analysis,Training,transfer functions},
 month = {April},
 note = {ISSN:},
 pages = {1-6},
 title = {{{AANN}}: {{Absolute Artificial Neural Network}}},
 year = {2018}
}

@inproceedings{8614007,
 abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
 author = {Yanagimto, H. and Hashimoto, K. and Okada, M.},
 booktitle = {2018 {{International Conference}} on {{Machine Learning}} and {{Data Engineering}} ({{iCMLDE}})},
 doi = {10.1109/iCMLDE.2018.00024},
 keywords = {Amazon reviews,attention visualization,convolutional neural nets,Convolutional neural networks,deep learning,Deep learning,Gated CNN,gated convolutional neural networks,GCNN,Kernel,learning (artificial intelligence),Logic gates,natural language processing,Natural language processing,neural net architecture,neural network architecture,self-attention mechanism,sentient analysis,sentiment analysis,Sentiment analysis,Task analysis,the self-attention mechanism},
 month = {December},
 note = {ISSN:},
 pages = {77-82},
 title = {Attention {{Visualization}} of {{Gated Convolutional Neural Networks}} with {{Self Attention}} in {{Sentiment Analysis}}},
 year = {2018}
}

@inproceedings{8614130,
 abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
 author = {Ghanta, S. and Subramanian, S. and Sundararaman, S. and Khermosh, L. and Sridhar, V. and Arteaga, D. and Luo, Q. and Das, D. and Talagala, N.},
 booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
 doi = {10.1109/ICMLA.2018.00105},
 keywords = {Data models,explainability,human understandable machine learning,interpretability,interpretability techniques,learning (artificial intelligence),legal industry requirements,Load modeling,Machine learning,Pipelines,Predictive models,Production,production engineering computing,production machine learning applications,production ML reproducibility solution,production scenario,production viability,reproducability,systems,tracking,Training},
 month = {December},
 note = {ISSN:},
 pages = {658-664},
 title = {Interpretability and {{Reproducability}} in {{Production Machine Learning Applications}}},
 year = {2018}
}

@inproceedings{8621470,
 abstract = {Development of chromosome conformation capture methods boosted progress in the study of the spatial organization of chromatin. Accumulation of large amounts of experimental data provides an opportunity to apply machine learning methods to examine the connection between epigenetics and the three-dimensional structure of chromatin. The aim of this study was to predict the characteristics of the chromatin structure, namely the transitional gamma, from ChIP-Seq experimental data by means of machine learning methods, and also to reveal the properties of epigenetic data influencing prediction. The neural network and the loss function designed for the prediction task are shown to perform with a sufficiently high accuracy. In addition, the genomic size of the chromatin context required for improving the quality of the prediction was assessed. Several neural network visualization techniques were tested as a means for improving interpretability of network, showing the possibility for using visualization to study interrelations in epigenetic data relevant for three-dimensional chromatin structure. To sum up, a close relationship between epigenetic factors and the structure of chromatin has been confirmed.},
 author = {Starikov, S. and Khrameeva, E. and Gelfand, M.},
 booktitle = {2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
 doi = {10.1109/BIBM.2018.8621470},
 keywords = {Bioinformatics,biology computing,Biomedical engineering,ChIP-Seq,ChIP-Seq experimental data,chromatin spatial structure characteristics,chromosome conformation capture methods,Conferences,data visualisation,Data visualization,epigenetic data influencing prediction,genetics,genomics,Hi-C,learning (artificial intelligence),Life sciences,machine learning,Machine learning,machine learning methods,neural nets,neural network visualization techniques,neural networks,Neural networks,three-dimensional chromatin structure},
 month = {December},
 note = {ISSN:},
 pages = {2489-2489},
 title = {Prediction of Chromatin Spatial Structure Characteristics Using Machine Learning Methods},
 year = {2018}
}

@inproceedings{8622073,
 abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
 author = {Chhatwal, R. and Gronvall, P. and {Huber-Fliflet}, N. and Keeling, R. and Zhang, J. and Zhao, H.},
 booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
 doi = {10.1109/BigData.2018.8622073},
 keywords = {data-focused engagements,document classification,electronically-stored documents,explainable AI,explainable AI system,explainable predictive coding,explainable predictive coding methods,Law,law administration,legal document review,machine learning,Machine learning,machine learning researchers,pattern classification,predictive coding,Predictive coding,Predictive models,responsive documents,responsive snippets,supervised learning,supervised learning task,technology-based techniques,text analysis,text categorization,Text categorization,text classification,typical legal document review scenarios},
 month = {December},
 note = {ISSN:},
 pages = {1905-1911},
 title = {Explainable {{Text Classification}} in {{Legal Document Review A Case Study}} of {{Explainable Predictive Coding}}},
 year = {2018}
}

@inproceedings{8661467,
 abstract = {In the present work, the artificial intelligence is used, through neural networks, in the diagnosis of power transformers for the interpretation of the results obtained in the frequency response analysis test. The results of the classification of three types of failures in this technique are exposed. The characteristics of the statistical indicators that function as input variables of the neural network and the reason for implementing a multilayer network with backpropagation algorithm, in the training are exposed. The results of the discrimination of acceptable or not acceptable state of the transformer are presented and if it has any of the following faults: open winding, short circuit winding and winding with a point to ground. The response of the neural network is determined in case of study of real transformers.},
 author = {Ar\'eu, O. H. and Men\'endez, A. M. G. and S\'anchez, J. I. H. and Vald\'es, E. S.},
 booktitle = {2018 {{IEEE International Autumn Meeting}} on {{Power}}, {{Electronics}} and {{Computing}} ({{ROPEC}})},
 doi = {10.1109/ROPEC.2018.8661467},
 issn = {2573-0770},
 keywords = {artificial intelligence,Artificial intelligence,artificial neuronal network,backpropagation,backpropagation algorithm,fault diagnosis,faults diagnosis,FRA testing,frequency response,frequency response analysis test,multilayer network,neural nets,neural network,open winding,power engineering computing,power transformer testing,power transformers,short circuit winding,statistical analysis,statistical indicators,transformer diagnostic,transformer windings},
 month = {November},
 pages = {1-5},
 title = {Diagnosis of Faults in Power Transformers through the Interpretation of {{FRA}} Testing with Artificial Intelligence},
 year = {2018}
}

@inproceedings{8679150,
 abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
 author = {Kim, D. and Lim, W. and Hong, M. and Kim, H.},
 booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
 doi = {10.1109/BIGCOMP.2019.8679150},
 issn = {2375-9356},
 keywords = {complex model,Computational modeling,Convolution,Data models,deep neural network,feature extraction,Feature extraction,feature extraction layer,high-quality data,image classification,interpretability,Interpretability,interpretable features,interpretable transfer learning,Korean characters classification,learning (artificial intelligence),Machine Learning,natural language processing,neural nets,Neural networks,pretrained model,Task analysis,Training,Transfer Learning,transfer learning task},
 month = {February},
 pages = {1-4},
 title = {The {{Structure}} of {{Deep Neural Network}} for {{Interpretable Transfer Learning}}},
 year = {2019}
}

@incollection{abdollahiTransparencyFairMachine2018,
 abstract = {Machine Learning (ML) models are increasingly being used in many sectors, ranging from health and education to justice and criminal investigation. Therefore, building a fair and transparent model which conveys the reasoning behind its predictions is of great importance. This chapter discusses the role of explanation mechanisms in building fair machine learning models and explainable ML technique. We focus on the special case of recommender systems because they are a prominent example of a ML model that interacts directly with humans. This is in contrast to many other traditional decision making systems that interact with experts (e.g. in the health-care domain). In addition, we discuss the main sources of bias that can lead to biased and unfair models. We then review the taxonomy of explanation styles for recommender systems and review models that can provide explanations for their recommendations. We conclude by reviewing evaluation metrics for assessing the power of explainability in recommender systems.},
 address = {Cham},
 author = {Abdollahi, Behnoush and Nasraoui, Olfa},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 doi = {10.1007/978-3-319-90403-0_2},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {21-35},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 shorttitle = {Transparency in {{Fair Machine Learning}}},
 title = {Transparency in {{Fair Machine Learning}}: The {{Case}} of {{Explainable Recommender Systems}}},
 year = {2018}
}

@inproceedings{abdollahiUsingExplainabilityConstrained2017,
 abstract = {Accurate model-based Collaborative Filtering (CF) approaches, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. Yet explanations have been shown to improve the transparency of a recommender system by justifying recommendations, and this in turn can enhance the user's trust in the recommendations. Hence, one main challenge in designing a recommender system is mitigating the trade-off between an explainable technique with moderate prediction accuracy and a more accurate technique with no explainable recommendations. In this paper, we focus on factorization models and further assume the absence of any additional data source, such as item content or user attributes. We propose an explainability constrained MF technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations.},
 address = {New York, NY, USA},
 author = {Abdollahi, Behnoush and Nasraoui, Olfa},
 booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
 doi = {10.1145/3109859.3109913},
 file = {/home/tim/Zotero/storage/M76U3KM7/Abdollahi and Nasraoui - 2017 - Using Explainability for Constrained Matrix Factor.pdf},
 isbn = {978-1-4503-4652-8},
 keywords = {explanations,interpretable models,latent factor models,matrix factorization,recommender systems},
 pages = {79--83},
 publisher = {{ACM}},
 series = {{{RecSys}} '17},
 title = {Using {{Explainability}} for {{Constrained Matrix Factorization}}},
 year = {2017}
}

@inproceedings{brideDependableExplainableMachine2018,
 abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
 author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H\'ou, Zh\'e},
 booktitle = {Formal {{Methods}} and {{Software Engineering}}},
 editor = {Sun, Jing and Sun, Meng},
 file = {/home/tim/Zotero/storage/8Y3ZXFCV/Bride et al. - 2018 - Towards Dependable and Explainable Machine Learnin.pdf},
 isbn = {978-3-030-02450-5},
 language = {en},
 pages = {412-416},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Dependable}} and {{Explainable Machine Learning Using Automated Reasoning}}},
 year = {2018}
}

@incollection{dengEpilogueFrontiersNLP2018,
 abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
 address = {Singapore},
 author = {Deng, Li and Liu, Yang},
 booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
 doi = {10.1007/978-981-10-5209-5_11},
 editor = {Deng, Li and Liu, Yang},
 isbn = {978-981-10-5209-5},
 language = {en},
 pages = {309-326},
 publisher = {{Springer Singapore}},
 shorttitle = {Epilogue},
 title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
 year = {2018}
}

@inproceedings{guoLEMNAExplainingDeep2018,
 abstract = {While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks ( e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications ( e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.},
 address = {New York, NY, USA},
 author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
 booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
 doi = {10.1145/3243734.3243792},
 isbn = {978-1-4503-5693-0},
 keywords = {binary analysis,deep recurrent neural networks,explainable AI},
 pages = {364--379},
 publisher = {{ACM}},
 series = {{{CCS}} '18},
 shorttitle = {{{LEMNA}}},
 title = {{{LEMNA}}: {{Explaining Deep Learning Based Security Applications}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1604.00289v3,
 abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
 author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
 journal = {arxiv},
 month = {April},
 title = {Building {{Machines That Learn}} and {{Think Like People}}},
 year = {2016}
}

@article{http://arxiv.org/abs/1704.03296v3,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
 author = {Fong, Ruth and Vedaldi, Andrea},
 journal = {arxiv},
 month = {April},
 title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
 year = {2017}
}

@article{http://arxiv.org/abs/1803.04263v3,
 abstract = {Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.},
 author = {Weld, Daniel S. and Bansal, Gagan},
 journal = {arxiv},
 month = {March},
 title = {The {{Challenge}} of {{Crafting Intelligible Intelligence}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for recommendation systems (RS) provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network (RNN) architecture for recommendation and a neighbourhood-based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 file = {/home/tim/Zotero/storage/TLKPAEUT/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf},
 journal = {arxiv},
 keywords = {Explainable AI,Recommendation systems,Recurrent Neural Networks},
 month = {July},
 title = {Explanations for {{Temporal Recommendations}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.00033v2,
 abstract = {Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.},
 author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
 journal = {arxiv},
 month = {July},
 title = {Techniques for {{Interpretable Machine Learning}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.04053v1,
 abstract = {In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.},
 author = {Chauvet, Jean-Marie},
 journal = {arxiv},
 month = {October},
 title = {The 30-{{Year Cycle In The AI Debate}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.09725v1,
 abstract = {Deep learning is currently playing a crucial role toward higher levels of artificial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal "black-box" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques. This paper summarizes our recent efforts to develop a more interpretable neural model for directly processing speech from the raw waveform. In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover more meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized filter-bank front-end, that only depends on some parameters with a clear physical meaning. Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more interpretable than standard CNNs.},
 author = {Ravanelli, Mirco and Bengio, Yoshua},
 journal = {arxiv},
 month = {November},
 title = {Interpretable {{Convolutional Filters}} with {{SincNet}}},
 year = {2018}
}

@inproceedings{http://arxiv.org/abs/1811.11705v1,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g. Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
 author = {Marino, Daniel L. and Wickramasinghe, Chathurika S. and Manic, Milos},
 booktitle = {Arxiv},
 keywords = {adversarial approach,adversarial machine learning,Adversarial Machine Learning,Adversarial samples,cyber-security,cyber-security applications,data-driven intrusion detection systems,deep neural networks,Estimation,explainable AI,Explainable AI,IDSs,Intrusion detection,learning (artificial intelligence),Machine learning,machine learning techniques,Mathematical model,multilayer perceptron classifiers,multilayer perceptrons,neural nets,pattern classification,security of data,Visualization},
 month = {November},
 title = {An {{Adversarial Approach}} for {{Explainable AI}} in {{Intrusion Detection Systems}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
 abstract = {The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. Researchers from different disciplines work together to define, design and evaluate interpretable intelligent systems for the user. Our work supports the different evaluation goals in interpretable machine learning research by a thorough review of evaluation methodologies used in machine-explanation research across the fields of human-computer interaction, visual analytics, and machine learning. We present a 2D categorization of interpretable machine learning evaluation methods and show a mapping between user groups and evaluation measures. Further, we address the essential factors and steps for a right evaluation plan by proposing a nested model for design and evaluation of explainable artificial intelligence systems.},
 author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
 journal = {arxiv},
 month = {November},
 title = {A {{Survey}} of {{Evaluation Methods}} and {{Measures}} for {{Interpretable Machine Learning}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
 abstract = {Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque "black boxes". The difficulty of interpreting the predictions of a neural network often prevents its use in fields where explainability is important, such as the financial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.},
 author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
 journal = {arxiv},
 month = {December},
 title = {Sensitivity Based {{Neural Networks Explanations}}},
 year = {2018}
}

@article{http://arxiv.org/abs/1904.08939v1,
 abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
 author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
 journal = {arxiv},
 month = {April},
 title = {Understanding {{Neural Networks}} via {{Feature Visualization}}: {{A}} Survey},
 year = {2019}
}

@article{http://arxiv.org/abs/1905.00122v1,
 abstract = {Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications.},
 author = {Chen, Li and Yagemann, Carter and Downing, Evan},
 journal = {arxiv},
 month = {April},
 title = {To Believe or Not to Believe: {{Validating}} Explanation Fidelity for Dynamic Malware Analysis},
 year = {2019}
}

@article{liptonMythosModelInterpretability2016,
 abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
 annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY},
 archiveprefix = {arXiv},
 author = {Lipton, Zachary C.},
 eprint = {1606.03490},
 eprinttype = {arxiv},
 file = {/home/tim/Zotero/storage/G56B9GE2/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/tim/Zotero/storage/CP3V8CD5/1606.html},
 journal = {arXiv:1606.03490 [cs, stat]},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
 month = {June},
 primaryclass = {cs, stat},
 title = {The {{Mythos}} of {{Model Interpretability}}},
 year = {2016}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
 abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
 author = {Lisboa, P. J. G.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
 file = {/home/tim/Zotero/storage/LRZ3MW87/Lisboa - 2013 - Interpretability in Machine Learning – Principles .pdf},
 isbn = {978-3-319-03200-9},
 keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
 language = {en},
 pages = {15-21},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
 year = {2013}
}

@inproceedings{munkhdalaiAdvancedNeuralNetwork2019,
 abstract = {Neural network models have achieved a human-level performance in many application domains, including image classification, speech recognition and machine translation. However, in credit scoring application, neural network approach has been useless because of its black box nature that the relationship between contextual input and output cannot be completely understood. In this study, we investigate the advanced neural network approach and its' explanation for credit scoring. We use the LIME technique to interpret the black box of such neural network and verify its' trustworthiness by comparing a high interpretable logistic model. The results show that neural network models give higher accuracy and equivalent explanation with the logistic model.},
 author = {Munkhdalai, Lkhagvadorj and Wang, Ling and Park, Hyun Woo and Ryu, Keun Ho},
 booktitle = {Intelligent {{Information}} and {{Database Systems}}},
 editor = {Nguyen, Ngoc Thanh and Gaol, Ford Lumban and Hong, Tzung-Pei and Trawi\'nski, Bogdan},
 file = {/home/tim/Zotero/storage/7UDYI2UH/Munkhdalai et al. - 2019 - Advanced Neural Network Approach, Its Explanation .pdf},
 isbn = {978-3-030-14802-7},
 keywords = {Credit scoring,LIME,Neural network},
 language = {en},
 pages = {407-419},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Advanced {{Neural Network Approach}}, {{Its Explanation}} with {{LIME}} for {{Credit Scoring Application}}},
 year = {2019}
}

@article{pop00003,
 abstract = {Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.},
 author = {Choo, J and Liu, S},
 journal = {IEEE computer graphics and applications},
 note = {Query date: 2019-04-22},
 publisher = {{ieeexplore.ieee.org}},
 title = {Visual Analytics for Explainable Deep Learning},
 year = {2018}
}

@article{pop00033,
 abstract = {With the advent of digitization on the shopfloor and the developments of Industry 4.0, companies are faced with opportunities and challenges alike. This can be illustrated by the example of AI-based process predictions, which can be valuable for real-time process management in a smart factory. However, to constructively collaborate with such a prediction, users need to establish confidence in its decisions. Explainable artificial intelligence (XAI) has emerged as a new research area to enable humans to understand, trust, and manage the AI they work with. In this contribution, we illustrate the opportunities and challenges of process predictions and XAI for Industry 4.0 with the DFKI-Smart-Lego-Factory. This fully automated factory prototype built out of LEGO\textregistered{} bricks demonstrates the potentials of Industry 4.0 in an innovative, yet easily accessible way. It includes a showcase that predicts likely process outcomes and uses state-of-the-art XAI techniques to explain them to its workers and visitors.},
 author = {Rehse, JR and Mehdiyev, N and Fettke, P},
 file = {/home/tim/Zotero/storage/LBKGX9G9/Rehse et al. - 2019 - Towards Explainable Process Predictions for Indust.pdf},
 journal = {KI-K\"unstliche Intelligenz},
 keywords = {Explainable artificial Intelligence,Industry 4.0,Process prediction,Smart factories},
 note = {Query date: 2019-04-22},
 publisher = {{Springer}},
 title = {Towards {{Explainable Process Predictions}} for {{Industry}} 4.0 in the {{DFKI}}-{{Smart}}-{{Lego}}-{{Factory}}}
}

@article{pop00040,
 abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
 author = {Adadi, A and Berrada, M},
 journal = {IEEE Access},
 keywords = {AI-based systems,artificial intelligence,Biological system modeling,black-box models,black-box nature,Conferences,explainable AI,explainable artificial intelligence,Explainable artificial intelligence,fourth industrial revolution,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms,XAI},
 note = {Query date: 2019-04-22},
 publisher = {{ieeexplore.ieee.org}},
 title = {Peeking inside the Black-Box: {{A}} Survey on {{Explainable Artificial Intelligence}} ({{XAI}})},
 year = {2018}
}

@inproceedings{pop00108,
 abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
 author = {Wang, D and Yang, Q and Abdul, A and Lim, BY},
 booktitle = {Proceedings of the {{SIGCHI}}~\ldots{}},
 keywords = {clinical decision making,decision making,explainable artificial intelligence,explanations,intelligibility},
 note = {Query date: 2019-04-22},
 publisher = {{brianlim.net}},
 title = {Designing {{Theory}}-{{Driven User}}-{{Centric Explainable AI}}},
 type = {PDF},
 year = {2019}
}

@inproceedings{ribeiroWhyShouldTrust2016,
 abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
 address = {New York, NY, USA},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
 doi = {10.1145/2939672.2939778},
 isbn = {978-1-4503-4232-2},
 keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning},
 pages = {1135--1144},
 publisher = {{ACM}},
 series = {{{KDD}} '16},
 shorttitle = {"{{Why Should I Trust You}}?},
 title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
 year = {2016}
}

@article{shehDefiningExplainableAI2018,
 abstract = {Explainable artificial intelligence (XAI) has become popular in the last few years. The artificial intelligence (AI) community in general, and the machine learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements? In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.},
 author = {Sheh, Raymond and Monteath, Isaac},
 doi = {10.1007/s13218-018-0559-3},
 file = {/home/tim/Zotero/storage/K9GUDZBX/Sheh and Monteath - 2018 - Defining Explainable AI for Requirements Analysis.pdf},
 issn = {1610-1987},
 journal = {KI - K\"unstliche Intelligenz},
 keywords = {Decision trees,Explainable AI,Machine learning,Neural networks,Requirements analysis},
 language = {en},
 month = {November},
 number = {4},
 pages = {261-266},
 title = {Defining {{Explainable AI}} for {{Requirements Analysis}}},
 volume = {32},
 year = {2018}
}

@article{vellidoImportanceInterpretabilityVisualization2019,
 abstract = {In a short period of time, many areas of science have made a sharp transition towards data-dependent methods. In some cases, this process has been enabled by simultaneous advances in data acquisition and the development of networked system technologies. This new situation is particularly clear in the life sciences, where data overabundance has sparked a flurry of new methodologies for data management and analysis. This can be seen as a perfect scenario for the use of machine learning and computational intelligence techniques to address problems in which more traditional data analysis approaches might struggle. But, this scenario also poses some serious challenges. One of them is model interpretability and explainability, especially for complex nonlinear models. In some areas such as medicine and health care, not addressing such challenge might seriously limit the chances of adoption, in real practice, of computer-based systems that rely on machine learning and computational intelligence methods for data analysis. In this paper, we reflect on recent investigations about the interpretability and explainability of machine learning methods and discuss their impact on medicine and health care. We pay specific attention to one of the ways in which interpretability and explainability in this context can be addressed, which is through data and model visualization. We argue that, beyond improving model interpretability as a goal in itself, we need to integrate the medical experts in the design of data analysis interpretation strategies. Otherwise, machine learning is unlikely to become a part of routine clinical and health care practice.},
 author = {Vellido, Alfredo},
 doi = {10.1007/s00521-019-04051-w},
 file = {/home/tim/Zotero/storage/SI2SPSZ7/Vellido - 2019 - The importance of interpretability and visualizati.pdf},
 issn = {1433-3058},
 journal = {Neural Computing and Applications},
 keywords = {Explainability,Health care,Interpretability,Machine learning,Medicine,Visualization},
 language = {en},
 month = {February},
 title = {The Importance of Interpretability and Visualization in Machine Learning for Applications in Medicine and Health Care},
 year = {2019}
}

@article{zhongArtificialIntelligenceDrug2018,
 abstract = {Thanks to the fast improvement of the computing power and the rapid development of the computational chemistry and biology, the computer-aided drug design techniques have been successfully applied in almost every stage of the drug discovery and development pipeline to speed up the process of research and reduce the cost and risk related to preclinical and clinical trials. Owing to the development of machine learning theory and the accumulation of pharmacological data, the artificial intelligence (AI) technology, as a powerful data mining tool, has cut a figure in various fields of the drug design, such as virtual screening, activity scoring, quantitative structure-activity relationship (QSAR) analysis, de novo drug design, and in silico evaluation of absorption, distribution, metabolism, excretion and toxicity (ADME/T) properties. Although it is still challenging to provide a physical explanation of the AI-based models, it indeed has been acting as a great power to help manipulating the drug discovery through the versatile frameworks. Recently, due to the strong generalization ability and powerful feature extraction capability, deep learning methods have been employed in predicting the molecular properties as well as generating the desired molecules, which will further promote the application of AI technologies in the field of drug design.},
 author = {Zhong, Feisheng and Xing, Jing and Li, Xutong and Liu, Xiaohong and Fu, Zunyun and Xiong, Zhaoping and Lu, Dong and Wu, Xiaolong and Zhao, Jihui and Tan, Xiaoqin and Li, Fei and Luo, Xiaomin and Li, Zhaojun and Chen, Kaixian and Zheng, Mingyue and Jiang, Hualiang},
 doi = {10.1007/s11427-018-9342-2},
 file = {/home/tim/Zotero/storage/TM3SYSZ9/Zhong et al. - 2018 - Artificial intelligence in drug design.pdf},
 issn = {1869-1889},
 journal = {Science China Life Sciences},
 keywords = {ADME/T,artificial intelligence,deep learning,drug design,QSAR},
 language = {en},
 month = {October},
 number = {10},
 pages = {1191-1204},
 title = {Artificial Intelligence in Drug Design},
 volume = {61},
 year = {2018}
}

