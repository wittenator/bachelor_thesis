@article{774103,
 abstract = {Hybrid intelligent systems that combine knowledge-based and artificial neural network systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of neural network. The other two techniques are specific to feedforward networks, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.},
 author = {I. A. {Taha} and J. {Ghosh}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/69.774103},
 issn = {1041-4347},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 keywords = {symbol manipulation;knowledge representation;explanation;truth maintenance;feedforward neural nets;neural net architecture;knowledge based systems;learning (artificial intelligence);symbolic interpretation;hybrid intelligent systems;knowledge-based systems;artificial neural networks;domain knowledge representation;domain knowledge mapping;connectionist architecture;network training;rule extraction;explanation power;output decision validation;knowledge refinement;binary rules;feedforward networks;hidden layer;sigmoidal units;partial rules;embedded knowledge;adjustable detail level;rule evaluation technique;rule ordering;performance measures;iris data set;breast cancer data set;Artificial neural networks;Neural networks;Data mining;Military computing;Knowledge representation;Knowledge based systems;Fuzzy neural networks;Fuzzy sets;Computer networks;Intelligent systems},
 month = {May},
 number = {3},
 pages = {448-463},
 title = {Symbolic interpretation of artificial neural networks},
 volume = {11},
 year = {1999}
}

@inproceedings{8215791,
 abstract = {Automatic creation of polarity dictionaries is an important issue, as explanations of prediction models are often required in the financial industry. This paper proposes a novel method of developing an interpretable and predictable neural network model. The neural network model we built can extract polarity scores of concepts from documents. Furthermore, we can detect pairwise interactions between concepts, and create polarity concept dictionaries using our neural network model. The model was built using vector representations of words and polarity scores for about 100 words provided by financial professionals, and we obtained about a hundred times more polarity scores for unknown words through backpropagation. First, we analyze the properties of our method from a theoretical point of view. We then confirm its capabilities by conducting simulations of assigning polarity scores to unknown words and detecting interactions using artificial data. We subsequently estimate sentiment tags using real financial textual datasets. Compared with other conventional methods, the proposed approach can forecast sentiments with higher F1 scores. Finally, we develop a polarity concept dictionary based on Yahoo! Finance board textual data.},
 author = {T. {Ito} and H. {Sakaji} and K. {Izumi} and K. {Tsubouchi} and T. {Yamashita}},
 booktitle = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/ICDMW.2017.159},
 issn = {2375-9259},
 keywords = {backpropagation;data mining;dictionaries;financial data processing;natural language processing;neural nets;text analysis;interpretable neural network model;polarity concept dictionaries;automatic creation;polarity dictionaries;prediction models;interpretable network model;predictable neural network model;polarity scores;unknown words;detecting interactions;financial industry;vector representations;Yahoo! Finance board textual data;Dictionaries;Artificial neural networks;Predictive models;Data mining;Industries;Analytical models;Neural Network Model;Text-mining;Sentiment analysis},
 month = {Nov},
 number = {},
 pages = {1122-1131},
 title = {Development of an Interpretable Neural Network Model for Creation of Polarity Concept Dictionaries},
 volume = {},
 year = {2017}
}

@inproceedings{8258557,
 abstract = {Machine learning is one of the most important fields in recent improvement in big data analysis. Many people apply machine learning for a variety of domains for various purposes, such as classification of opinions. However, the constructed models of machine learning are black boxes. They cannot understand the background reason for their decisions. In many cases, understanding the reasons important. In this paper, we focus on interpretation of models and understanding of decision reasons. First, we introduce the results of an opinions classification of the reviews with Support Vector Machine (SVM). Second, we interpret the model by analyzing weights of the model. Third, we introduce a method for helping to understand the reasons for a decision by SVM by providing a simplified information of the highly weighted words.},
 author = {S. {Shirataki} and S. {Yamaguchi}},
 booktitle = {2017 IEEE International Conference on Big Data (Big Data)},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/BigData.2017.8258557},
 issn = {},
 keywords = {Big Data;data analysis;learning (artificial intelligence);pattern classification;support vector machines;text analysis;machine learning;decision reasons;Support Vector Machine;big data analysis;SVM;highly weighted words;black boxes;opinions classification;Support vector machines;DVD;Analytical models;Predictive models;Training;Big Data;Tools;SVM;machine learning;interpretability},
 month = {Dec},
 number = {},
 pages = {4830-4831},
 title = {A study on interpretability of decision of machine learning},
 volume = {},
 year = {2017}
}

@inproceedings{8365991,
 abstract = {Deep neural networks (DNNs) have made tremendous progress in many different areas in recent years. How these networks function internally, however, is often not well understood. Advances in under-standing DNNs will benefit and accelerate the development of the field. We present TNNVis, a visualization system that supports un-derstanding of deep neural networks specifically designed to analyze text. TNNVis focuses on DNNs composed of fully connected and convolutional layers. It integrates visual encodings and interaction techniques chosen specifically for our tasks. The tool allows users to: (1) visually explore DNN models with arbitrary input using a combination of node-link diagrams and matrix representation; (2) quickly identify activation values, weights, and feature map patterns within a network; (3) flexibly focus on visual information of interest with threshold, inspection, insight query, and tooltip operations; (4) discover network activation and training patterns through animation; and (5) compare differences between internal activation patterns for different inputs to the DNN. These functions allow neural network researchers to examine their DNN models from new perspectives, producing insights on how these models function. Clustering and summarization techniques are employed to support large convolutional and fully connected layers. Based on several part of speech models with different structure and size, we present multiple use cases where visualization facilitates an understanding of the models.},
 author = {S. {Nie} and C. {Healey} and K. {Padia} and S. {Leeman-Munk} and J. {Benson} and D. {Caira} and S. {Sethi} and R. {Devarajan}},
 booktitle = {2018 IEEE Pacific Visualization Symposium (PacificVis)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/PacificVis.2018.00031},
 issn = {2165-8773},
 keywords = {data visualisation;feedforward neural nets;learning (artificial intelligence);natural language processing;text analysis;interaction techniques;DNN models;network activation;training patterns;internal activation patterns;neural network researchers;convolutional connected layers;fully connected layers;deep neural networks;DNNs;visualization system;convolutional layers;visual encodings;text analytics;TNNVis;clustering techniques;summarization techniques;speech models;Visualization;Neurons;Computational modeling;Task analysis;Convolutional neural networks;Biological neural networks;information visualization;deep learning;machine learning;visualization design;human centered computing},
 month = {April},
 number = {},
 pages = {180-189},
 title = {Visualizing Deep Neural Networks for Text Analytics},
 volume = {},
 year = {2018}
}

@article{8440085,
 abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.},
 author = {Y. {Ming} and H. {Qu} and E. {Bertini}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TVCG.2018.2864812},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data visualisation;interactive systems;knowledge representation;learning (artificial intelligence);matrix algebra;pattern classification;rule matrix;black-box model;matrix-based visualization;standardized rule-based knowledge representation;predictive models;interactive visualization technique;machine learning systems;Machine learning;Data visualization;Visualization;Neural networks;Decision trees;Data models;Support vector machines;explainable machine learning;rule visualization;visual analytics},
 month = {Jan},
 number = {1},
 pages = {342-352},
 title = {RuleMatrix: Visualizing and Understanding Classifiers with Rules},
 volume = {25},
 year = {2019}
}

@article{8440091,
 abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
 author = {J. {Zhang} and Y. {Wang} and P. {Molino} and L. {Li} and D. S. {Ebert}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TVCG.2018.2864499},
 issn = {1077-2626},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 keywords = {data analysis;data visualisation;iterative methods;learning (artificial intelligence);neural nets;regression analysis;model-agnostic framework;machine learning models;visual analysis techniques;internal logic;specific model type;model development;diagnosis process;potential machine learning;regression tasks;classification tasks;feature discrimination;customizable tabular view;models outcome;scatterplot-based visual summary;refinement;explanation;inspection;generic framework;deep neural networks;debugging;interpretation;manifold;Visualization;Analytical models;Task analysis;Machine learning;Manifolds;Data models;Computational modeling;Interactive machine learning;performance analysis;model comparison;model debugging},
 month = {Jan},
 number = {1},
 pages = {364-373},
 title = {Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models},
 volume = {25},
 year = {2019}
}

@article{8466590,
 abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
 author = {A. {Adadi} and M. {Berrada}},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/ACCESS.2018.2870052},
 issn = {2169-3536},
 journal = {IEEE Access},
 keywords = {artificial intelligence;AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution;Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
 month = {},
 number = {},
 pages = {52138-52160},
 title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
 volume = {6},
 year = {2018}
}

@inproceedings{8489172,
 abstract = {Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model's outputs. The recent movement for “algorithmic fairness” also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.},
 author = {X. {Liu} and X. {Wang} and S. {Matwin}},
 booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/IJCNN.2018.8489172},
 issn = {2161-4407},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);pattern classification;pattern clustering;random processes;meta-learning;model interpretability;CNN-INTE;clustering algorithm;meta-level training data;base learning algorithms;meta-level test data;machine learning approaches;interpretable deep convolutional neural networks;MNIST image dataset;random forest;deep CNN model;Prediction algorithms;Machine learning;Machine learning algorithms;Predictive models;Computational modeling;Training data;Visualization;interpretability;Meta-learning;deep learning;Convolutional Neural Network;TensorFlow;big data},
 month = {July},
 number = {},
 pages = {1-9},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 volume = {},
 year = {2018}
}

@inproceedings{8490530,
 abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
 author = {A. {Holzinger}},
 booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (DISA)},
 comments = {Reviews the current state of explainability research, },
 doi = {10.1109/DISA.2018.8490530},
 issn = {},
 keywords = {learning (artificial intelligence);ontologies (artificial intelligence);probability;statistical machine learning methods;AI winter;deep learning approaches;big quality data;computational power;engineering effort;ontologies;knowledge representations;glass-box approaches;mathematical principles;artificial intelligence;logical approaches;probabilistic learning methods;rule-based approaches;contextual explanatory models;context-adaptive procedures;high-dimensional vectors;Machine learning;Data mining;Data visualization;Uncertainty;Games;Cognitive science},
 month = {Aug},
 number = {},
 pages = {55-66},
 title = {From Machine Learning to Explainable AI},
 volume = {},
 year = {2018}
}

@inproceedings{8491501,
 abstract = {To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future-i.e., new-data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.},
 author = {B. {Murray} and M. A. {Islam} and A. J. {Pinar} and T. C. {Havens} and D. T. {Anderson} and G. {Scott}},
 booktitle = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/FUZZ-IEEE.2018.8491501},
 issn = {},
 keywords = {convolution;feedforward neural nets;learning (artificial intelligence);optimisation;remote sensing;sensor fusion;data-driven optimization;Choquet integral;fusion solution;parametric nonlinear aggregation function;data-centric XAI tools;XAI-ChI methods;explainable AI;heterogeneous deep convolutional neural networks;remote sensing;Frequency modulation;Indexes;Remote sensing;Optimization;Artificial intelligence;Electronic mail;Convolutional neural networks;Choquet Integral;Fuzzy Integral;Explainable AI;Machine Learning},
 month = {July},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Understanding Decisions and Data-Driven Optimization of the Choquet Integral},
 volume = {},
 year = {2018}
}

@inproceedings{8591457,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.},
 author = {D. L. {Marino} and C. S. {Wickramasinghe} and M. {Manic}},
 booktitle = {IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/IECON.2018.8591457},
 issn = {2577-1647},
 keywords = {learning (artificial intelligence);multilayer perceptrons;neural nets;pattern classification;security of data;adversarial approach;explainable AI;cyber-security applications;adversarial machine learning;multilayer perceptron classifiers;machine learning techniques;deep neural networks;data-driven intrusion detection systems;IDSs;Machine learning;Intrusion detection;Mathematical model;Visualization;Estimation;Adversarial Machine Learning;Adversarial samples;Explainable AI;cyber-security},
 month = {Oct},
 number = {},
 pages = {3237-3243},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection Systems},
 volume = {},
 year = {2018}
}

@inproceedings{8614007,
 abstract = {Deep learning is applied to many research topics; Natural Language Processing, Image Processing, and Acoustic Recognition. In deep learning, neural networks have a very complex and deep structure and it is difficult to discuss why they work well or not. So you have to take a trial-and-error to improve their performances. We develop a mechanism to show how neural networks predict final results and help you to design a new neural network architecture based on its prediction criteria. Speaking concrete, we visualize important features to predict the final results with an attentional mechanism. In this paper, we take up sentient analysis, which is one of natural language processing tasks. In image processing visualizing weights of a neural network is a major approach and you can obtain intuitive results; object outlines and object components. However, in natural language processing, the approach is not interpretable because a discriminate function constructed by a neural network is a complex and nonlinear one and it is very difficult to correlate weights and words in a text. We employ Gated Convolutional Neural Network (GCNN) and introduce a self-attention mechanism to understand how GCNN determines sentiment polarities from raw reviews. GCNN can simulate an n-gram model and the self-attention mechanism can make correspondence between weights of a neural network and words clear. In experiments, we used Amazon reviews and evaluated the performance of the proposed method. Especially, the proposed method was able to emphasize some words in the review to determine sentiment polarity. Moreover, when the prediction was wrong, we were able to understand why the proposed method made mistakes because we found what words the proposed method emphasized.},
 author = {H. {Yanagimto} and K. {Hashimoto} and M. {Okada}},
 booktitle = {2018 International Conference on Machine Learning and Data Engineering (iCMLDE)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/iCMLDE.2018.00024},
 issn = {},
 keywords = {convolutional neural nets;learning (artificial intelligence);neural net architecture;sentiment analysis;neural network architecture;self-attention mechanism;Amazon reviews;GCNN;sentient analysis;attention visualization;natural language processing;gated convolutional neural networks;deep learning;Logic gates;Convolutional neural networks;Kernel;Sentiment analysis;Task analysis;Deep learning;Natural language processing;Gated CNN;Sentiment analysis;the self-attention mechanism},
 month = {Dec},
 number = {},
 pages = {77-82},
 title = {Attention Visualization of Gated Convolutional Neural Networks with Self Attention in Sentiment Analysis},
 volume = {},
 year = {2018}
}

@inproceedings{8622073,
 abstract = {In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.},
 author = {R. {Chhatwal} and P. {Gronvall} and N. {Huber-Fliflet} and R. {Keeling} and J. {Zhang} and H. {Zhao}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/BigData.2018.8622073},
 issn = {},
 keywords = {law administration;pattern classification;supervised learning;text analysis;responsive documents;explainable AI system;typical legal document review scenarios;responsive snippets;text classification;explainable predictive coding methods;data-focused engagements;technology-based techniques;machine learning researchers;document classification;supervised learning task;electronically-stored documents;Predictive coding;Law;Predictive models;Text categorization;Machine learning;machine learning;text categorization;explainable AI;predictive coding;explainable predictive coding;legal document review},
 month = {Dec},
 number = {},
 pages = {1905-1911},
 title = {Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding},
 volume = {},
 year = {2018}
}

@article{8653995,
 abstract = {With the rapid development of deep learning models, their performances in various tasks are improved, while meanwhile their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In the paper, we propose a novel Multi-scale INTerpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.},
 author = {X. {Cui} and D. {Wang} and Z. J. {Wang}},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/TMM.2019.2902099},
 issn = {1520-9210},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Visualization;Computational modeling;Analytical models;Feature extraction;Perturbation methods;Image segmentation;Heating systems;Model interpretability;multi-scale interpretation;convolutional neural networks;model-agnostic},
 month = {},
 number = {},
 pages = {1-1},
 title = {Multi-scale Interpretation Model for Convolutional Neural Networks: Building Trust based on Hierarchical Interpretation},
 volume = {},
 year = {2019}
}

@inproceedings{8679150,
 abstract = {Training a deep neural network requires a large amount of high-quality data and time. However, most of the real tasks don't have enough labeled data to train each complex model. To solve this problem, transfer learning reuses the pretrained model on a new task. However, one weakness of transfer learning is that it applies a pretrained model to a new task without understanding the output of an existing model. This may cause a lack of interpretability in training deep neural network. In this paper, we propose a technique to improve the interpretability in transfer learning tasks. We define the interpretable features and use it to train model to a new task. Thus, we will be able to explain the relationship between the source and target domain in a transfer learning task. Feature Network (FN) consists of Feature Extraction Layer and a single mapping layer that connects the features extracted from the source domain to the target domain. We examined the interpretability of the transfer learning by applying pretrained model with defined features to Korean characters classification.},
 author = {D. {Kim} and W. {Lim} and M. {Hong} and H. {Kim}},
 booktitle = {2019 IEEE International Conference on Big Data and Smart Computing (BigComp)},
 comments = {Presents a specific method for enhancing explainability for models, },
 doi = {10.1109/BIGCOMP.2019.8679150},
 issn = {2375-9356},
 keywords = {feature extraction;image classification;learning (artificial intelligence);natural language processing;neural nets;deep neural network;interpretable transfer learning;high-quality data;complex model;pretrained model;interpretability;transfer learning task;interpretable features;feature extraction layer;Korean characters classification;Feature extraction;Task analysis;Training;Data models;Convolution;Computational modeling;Neural networks;Interpretability;Transfer Learning;Machine Learning},
 month = {Feb},
 number = {},
 pages = {1-4},
 title = {The Structure of Deep Neural Network for Interpretable Transfer Learning},
 volume = {},
 year = {2019}
}

