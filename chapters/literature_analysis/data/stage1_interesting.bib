@article{750572,
 abstract = {Presents a methodology for detection of neural-network gaps (NNGs) based on error analysis and the visualization that is applicable to the n-dimensional I/O domain. The generalization problem in artificial neural networks (ANN) training is analyzed and the concept of NNGs is introduced. The NNGs are highly undesirable in ANN generalization and methods for detecting, analyzing, and eliminating them are necessary. Previous methods for NNG detection, based on two-dimensional (2-D) and three dimensional (3-D) visualization, were not applicable for ANNs with more than three inputs. Experiments demonstrate advantages of this new methodology, which allows better understanding of the NNG phenomena using a quantitative approach.},
 author = {M. M. {Kantardzic} and A. A. {Aly} and A. S. {Elmaghraby}},
 comments = {, },
 doi = {10.1109/72.750572},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {neural nets;generalisation (artificial intelligence);learning (artificial intelligence);error analysis;neural-network gaps;error analysis;visualization;n-dimensional I/O domain;generalization problem;quantitative approach;Visualization;Error analysis;Artificial neural networks;Testing;Neural networks;Two dimensional displays;Particle measurements;Computer networks;Performance evaluation;Supervised learning},
 month = {March},
 number = {2},
 pages = {419-426},
 title = {Visualization of neural-network gaps based on error analysis},
 volume = {10},
 year = {1999}
}

@article{7552539,
 abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
 author = {W. {Samek} and A. {Binder} and G. {Montavon} and S. {Lapuschkin} and K. {Müller}},
 comments = {Reviews the current state of explainability research, The described method is neither general, nor focused on NLP},
 doi = {10.1109/TNNLS.2016.2599820},
 issn = {2162-237X},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;sensitivity-based approach;deconvolution method;heatmap;deep neural network;complex machine learning tasks;multilayer nonlinear structure;DNN;MIT Places data sets;SUN397;ILSVRC2012;relevance propagation algorithm;data visualization;Heating;Neurons;Biological neural networks;Deconvolution;Sensitivity;Learning systems;Algorithm design and analysis;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
 month = {Nov},
 number = {11},
 pages = {2660-2673},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 volume = {28},
 year = {2017}
}

@inproceedings{7557899,
 abstract = {Artificial Intelligence (AI) has infiltrated almost every scientific and social endeavour, including everything from medical research to the sociology of crowd control. But the foundation of AI continues to be based on digital representations of knowledge, and computational reasoning therewith. Because so much of modern knowledge infrastructure and social behaviour is connected to AI, understanding the role of AI in each such endeavour not only helps accelerate progress in those fields in which it applies, but also creates the challenges to extend the foundation for modern AI methods. The simple hypothesis herein is that so-called AI-complete problems have a role in helping to articulate the appropriate integration of AI within other disciplines. With the current growth of interest in "big data" and visualization, we argue that relatively simple formal structures provide a basis for the claim that visualization is an AI-complete problem. The value of confirming this claim is largely to encourage stronger formalizations of the visualization process in terms of the AI foundations of representation and reasoning. This connection will help ensure that relevant components of AI are appropriately applied and integrated, to provide value for a basis of a theory of visualization. The sketch of this claim here is based on the simple idea that visualization is an abstraction process, and that abstractions from partial information, however voluminous, directly confronts the non monotonic reasoning challenge, thus the need for caution in engineering visualization systems without carefully considering the consequences of visual abstraction. This is particularly important with interactive visualization, which has recently formed the basis for such fields as visual analytics.},
 author = {R. {Goebel}},
 booktitle = {2016 20th International Conference Information Visualisation (IV)},
 comments = {, },
 doi = {10.1109/IV.2016.53},
 issn = {2375-0138},
 keywords = {artificial intelligence;Big Data;data analysis;data structures;data visualisation;AI-complete problem;artificial intelligence;digital knowledge representations;computational reasoning;modern knowledge infrastructure;social behaviour;AI methods;AI integration;Big Data;formal structures;visualization process formalization;partial information abstraction process;monotonic reasoning challenge;engineering visualization systems;visual abstraction;interactive visualization;visual analytics;Data visualization;Visualization;Artificial intelligence;Context;Complexity theory;Cognition;Semantics;AI-complete visualization incomplete knowledge},
 month = {July},
 number = {},
 pages = {27-32},
 title = {Why Visualization is an AI-complete Problem (and Why That Matters)},
 volume = {},
 year = {2016}
}

@inproceedings{7801719,
 abstract = {Multi-document summarization is to create summaries covering the major information that multiple documents tell in common. For this point, the existing methods are based on hand-crafted features for word and sentence. However, it is difficult to figure out the core contents of each document with the hand-crafted features because they have the limited information presented the given documents. Moreover, there exists a limit to figure out the major information because documents with the same meaning used to be paraphrased depending on their writers. Therefore, it is necessary to represent the semantic meanings of documents as well as sentences through understanding natural language. In this paper, we propose a new multi-document summarization system by creating a synthetic document vector covering the whole documents based on Language Model, whose is well-known for learning the semantic features in text. We experimented with DUC 2004 dataset provided by Document Understanding Conference (DUC) and the results show that our method summarizes multiple documents effectively based on their core contents.},
 author = {D. {Kim} and J. {Lee}},
 booktitle = {2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)},
 comments = {, },
 doi = {10.1109/SCIS-ISIS.2016.0132},
 issn = {},
 keywords = {document handling;natural language processing;word processing;multidocument summarization system;semantic text feature learning;DUC 2004 dataset;document understanding conference;natural language model;semantic document meanings;hand-crafted features;synthetic document vector;Semantics;Context;Hidden Markov models;Computational modeling;Redundancy;Intelligent systems;Natural languages;Multi-document summarization;Core content;Major Information;Synthetic document vector;Language model},
 month = {Aug},
 number = {},
 pages = {605-609},
 title = {Multi-document Summarization by Creating Synthetic Document Vector Based on Language Model},
 volume = {},
 year = {2016}
}

@inproceedings{7846290,
 abstract = {Machine Learning (ML) techniques have allowed a great performance improvement of different challenging Spoken Language Understanding (SLU) tasks. Among these methods, Neural Networks (NN), or Multilayer Perceptron (MLP), recently received a great interest from researchers due to their representation capability of complex internal structures in a low dimensional subspace. However, MLPs employ document representations based on basic word level or topic-based features. Therefore, these basic representations reveal little in way of document statistical structure by only considering words or topics contained in the document as a “bag-of-words”, ignoring relations between them. We propose to remedy this weakness by extending the complex features based on Quaternion algebra presented in [1] to neural networks called QMLP. This original QMLP approach is based on hyper-complex algebra to take into consideration features dependencies in documents. New document features, based on the document structure itself, used as input of the QMLP, are also investigated in this paper, in comparison to those initially proposed in [1]. Experiments made on a SLU task from a real framework of human spoken dialogues showed that our QMLP approach associated with the proposed document features outperforms other approaches, with an accuracy gain of 2% with respect to the MLP based on real numbers and more than 3% with respect to the first Quaternion-based features proposed in [1]. We finally demonstrated that less iterations are needed by our QMLP architecture to be efficient and to reach promising accuracies.},
 author = {T. {Parcollet} and M. {Morchid} and P. {Bousquet} and R. {Dufour} and G. {Linarès} and R. {De Mori}},
 booktitle = {2016 IEEE Spoken Language Technology Workshop (SLT)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SLT.2016.7846290},
 issn = {},
 keywords = {algebra;document handling;learning (artificial intelligence);multilayer perceptrons;natural language processing;quaternion neural networks;machine learning;spoken language understanding;SLU;multilayer perceptron;MLP;document representations;document statistical structure;quaternion algebra;Quaternions;Computational modeling;Neurons;Artificial neural networks;Algebra;Natural language processing;Quaternion;Neural Network;Spoken Language Understanding;Machine Learning},
 month = {Dec},
 number = {},
 pages = {362-368},
 title = {Quaternion Neural Networks for Spoken Language Understanding},
 volume = {},
 year = {2016}
}

@inproceedings{7960721,
 abstract = {The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.},
 author = {İ. Ü. {Oğul} and C. {Özcan} and Ö. {Hakdağlı}},
 booktitle = {2017 25th Signal Processing and Communications Applications Conference (SIU)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SIU.2017.7960721},
 issn = {},
 keywords = {Bayes methods;data analysis;distributed processing;Internet of Things;learning (artificial intelligence);pattern classification;public domain software;social networking (online);text analysis;IoT;open source social news site;Reddit;interpretation data;data analysis;fast data storage;Hadoop architecture;distributed data structure;distributed in-memory data collection;Apache Spark architecture;Naive Bayes method;machine learning;meaning extraction;irrelevant data deletion;Internet of Things;fast text classification;Sparks;Java;Internet of Things;Standards;Text categorization;Art;Machine learning;Text mining;Big data;Apache Spark;Classification;Naive Bayes},
 month = {May},
 number = {},
 pages = {1-4},
 title = {Fast text classification with Naive Bayes method on Apache Spark},
 volume = {},
 year = {2017}
}

@inproceedings{8260658,
 abstract = {Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
 author = {K. {Kowsari} and D. E. {Brown} and M. {Heidarysafa} and K. {Jafari Meimandi} and M. S. {Gerber} and L. E. {Barnes}},
 booktitle = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2017.0-134},
 issn = {},
 keywords = {data mining;learning (artificial intelligence);neural nets;pattern classification;text analysis;document hierarchy;HDLTex;Hierarchical Deep;Text classification;document collections;information processing methods;supervised learning;multiclass classification;hierarchical classification;deep learning architectures;supervised classifiers;text organization;document classification methods;Machine learning;Support vector machines;Computer architecture;Kernel;Mathematical model;Recurrent neural networks;Text Mining;Document Classification;Deep Neural Networks;Hierarchical Learning;Deep Learning},
 month = {Dec},
 number = {},
 pages = {364-371},
 title = {HDLTex: Hierarchical Deep Learning for Text Classification},
 volume = {},
 year = {2017}
}

@inproceedings{8268978,
 abstract = {Deep Neural Networks (DNN) received a great interest from researchers due to their capability to construct robust abstract representations of heterogeneous documents in a latent subspace. Nonetheless, mere real-valued deep neural networks require an appropriate adaptation, such as the convolution process, to capture latent relations between input features. Moreover, real-valued deep neural networks reveal little in way of document internal dependencies, by only considering words or topics contained in the document as an isolate basic element. Quaternion-valued multi-layer perceptrons (QMLP), and autoencoders (QAE) have been introduced to capture such latent dependencies, alongside to represent multidimensional data. Nonetheless, a three-layered neural network does not benefit from the high abstraction capability of DNNs. The paper proposes first to extend the hyper-complex algebra to deep neural networks (QDNN) and, then, introduces pre-trained deep quaternion neural networks (QDNN-AE) with dedicated quaternion encoder-decoders (QAE). The experiments conduced on a theme identification task of spoken dialogues from the DECODA data set show, inter alia, that the QDNN-AE reaches a promising gain of 2.2% compared to the standard real-valued DNN-AE.},
 author = {T. {Parcollet} and M. {Morchid} and G. {Linarès}},
 booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 comments = {, },
 doi = {10.1109/ASRU.2017.8268978},
 issn = {},
 keywords = {learning (artificial intelligence);multilayer perceptrons;natural language processing;speech recognition;real-valued deep neural networks;document internal dependencies;neural network;deep quaternion neural networks;spoken language understanding;quaternion-valued multilayer perceptrons;QMLP;autoencoders;QAE;spoken dialogues;Quaternions;Speech;Task analysis;Telephone sets;Algebra;Biological neural networks;Quaternions;deep neural networks;spoken language understanding;autoencoders;machine learning},
 month = {Dec},
 number = {},
 pages = {504-511},
 title = {Deep quaternion neural networks for spoken language understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8275810,
 abstract = {In text categorization, feature representation for dimensionality reduction is a key step. Usually, some commonly used methods, e.g., latent semantic analysis (LSA), yield a dense representation or a dense transformation matrix, which is difficult to precisely characterize the document-topic or the topic-word relationship. This paper proposes a novel discriminative topic sparse representation (DTSR) approach for text categorization, in which two stages are included: the topic dictionary construction and sparse representation. Firstly, a discriminative and interpretable dictionary is constructed to characterize the topic-word relationship. The dictionary contains all category center vectors as well as some semantic topic vectors generated by a latent Dirichlet allocation (LDA) model. Furthermore, each document can be represented with a sparse form to obtain a good document-topic relationship. Experimental results on well-known benchmark datasets indicate that the proposed method not only achieves a satisfactory classification performance but also provides a reasonable sparse semantic meaningful.},
 author = {W. {Zheng} and Y. {Liu} and H. {Lu} and H. {Tang}},
 booktitle = {2017 10th International Symposium on Computational Intelligence and Design (ISCID)},
 comments = {, },
 doi = {10.1109/ISCID.2017.54},
 issn = {2473-3547},
 keywords = {learning (artificial intelligence);pattern classification;text analysis;vectors;semantic topic vectors;latent Dirichlet allocation model;text categorization;feature representation;dimensionality reduction;latent semantic analysis;dense transformation matrix;topic-word relationship;topic dictionary construction;discriminative dictionary;interpretable dictionary;document-topic relationship;discriminative topic sparse representation approach;Semantics;Dictionaries;Text categorization;Training;Sparse matrices;Large scale integration;Resource management;Text Categorization;Sparse Representation;Topic;Discriminative;Semantic},
 month = {Dec},
 number = {},
 pages = {454-457},
 title = {Discriminative Topic Sparse Representation for Text Categorization},
 volume = {1},
 year = {2017}
}

@inproceedings{8310088,
 abstract = {As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.},
 author = {R. {Raoui-Outach} and C. {Million-Rousseau} and A. {Benoit} and P. {Lambert}},
 booktitle = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
 comments = {, },
 doi = {10.1109/IPTA.2017.8310088},
 issn = {2154-512X},
 keywords = {data analysis;document image processing;feedforward neural nets;learning (artificial intelligence);sales management;text analysis;Deep learning;automatic sale receipt understanding;data analytics;scanned document analysis;paper damages;scanning quality;industrial context;automatic understanding;essential consumption statistics;accurate consumption statistics;smart-phone;tool chain;essential information;store brand;purchased products;related prices;highest possible confidence;high confidence level;tool-chain;Deep Convolutional Neural Networks;classical image;text processings;double check processing;text analysis;Optical character recognition software;Character recognition;Semantics;Text analysis;Task analysis;Object detection;Machine learning;Receipt image understanding;Deep Convolutional Neural Networks;Object Detection;Semantic Analysis},
 month = {Nov},
 number = {},
 pages = {1-6},
 title = {Deep learning for automatic sale receipt understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8490433,
 abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
 author = {J. {Zhu} and A. {Liapis} and S. {Risi} and R. {Bidarra} and G. M. {Youngblood}},
 booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
 comments = {, },
 doi = {10.1109/CIG.2018.8490433},
 issn = {2325-4289},
 keywords = {computer games;human computer interaction;learning (artificial intelligence);game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework;Games;Task analysis;Machine learning;Neurons;Visualization;Tools;explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design},
 month = {Aug},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},
 volume = {},
 year = {2018}
}

@inproceedings{8538416,
 abstract = {In the age of knowledge, Natural Language Processing (NLP) express its demand by a huge range of utilization. Previously NLP was dealing with statically data. Contemporary time NLP is doing considerably with the corpus, lexicon database, pattern reorganization. Considering Deep Learning (DL) method recognize artificial Neural Network (NN) to nonlinear process, NLP tools become increasingly accurate and efficient that begin a debacle. Multi-Layer Neural Network obtaining the importance of the NLP for its capability including standard speed and resolute output. Hierarchical designs of data operate recurring processing layers to learn and with this arrangement of DL methods manage several practices. In this paper, this resumed striving to reach a review of the tools and the necessary methodology to present a clear understanding of the association of NLP and DL for truly understand in the training. Efficiency and execution both are improved in NLP by Part of speech tagging (POST), Morphological Analysis, Named Entity Recognition (NER), Semantic Role Labeling (SRL), Syntactic Parsing, and Coreference resolution. Artificial Neural Networks (ANN), Time Delay Neural Networks (TDNN), Recurrent Neural Network (RNN), Convolution Neural Networks (CNN), and Long-Short-Term-Memory (LSTM) dealings among Dense Vector (DV), Windows Approach (WA), and Multitask learning (MTL) as a characteristic of Deep Learning. After statically methods, when DL communicate the influence of NLP, the individual form of the NLP process and DL rule collaboration was started a fundamental connection.},
 author = {S. A. {Fahad} and A. E. {Yahya}},
 booktitle = {2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
 comments = {, },
 doi = {10.1109/ICSCEE.2018.8538416},
 issn = {},
 keywords = {learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;natural language processing;contemporary time NLP;Considering Deep;(DL) method;artificial Neural Network;nonlinear process;NLP tools;MultiLayer Neural Network;processing layers;DL methods;Artificial Neural Networks;Time Delay Neural Networks;Recurrent Neural Network;Convolution Neural Networks;deep learning;NLP process;Natural language processing;Artificial neural networks;Tagging;Semantics;Task analysis;-Deep Learning;Natural language processing;Deep nural Network;Multitask Learning},
 month = {July},
 number = {},
 pages = {1-4},
 title = {Inflectional Review of Deep Learning on Natural Language Processing},
 volume = {},
 year = {2018}
}

@inproceedings{8614130,
 abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
 author = {S. {Ghanta} and S. {Subramanian} and S. {Sundararaman} and L. {Khermosh} and V. {Sridhar} and D. {Arteaga} and Q. {Luo} and D. {Das} and N. {Talagala}},
 booktitle = {2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2018.00105},
 issn = {},
 keywords = {learning (artificial intelligence);production engineering computing;production scenario;interpretability techniques;production ML reproducibility solution;production viability;reproducability;production machine learning applications;legal industry requirements;human understandable machine learning;Production;Pipelines;Predictive models;Machine learning;Training;Data models;Load modeling;reproducability;explainability;interpretability;systems;tracking},
 month = {Dec},
 number = {},
 pages = {658-664},
 title = {Interpretability and Reproducability in Production Machine Learning Applications},
 volume = {},
 year = {2018}
}

@inproceedings{8622433,
 abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
 author = {B. {Kovalerchuk} and N. {Neuhaus}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 comments = {, },
 doi = {10.1109/BigData.2018.8622433},
 issn = {},
 keywords = {learning (artificial intelligence);pattern classification;domain-explainable ML models;dominance classifier and predictor algorithm;DCP algorithm;unintelligible black box models;interpretable machine learning;interpretable mathematical forms;noninterpretable models;interpretable methods;human-understandable machine learning models;complex multidimensional relations;human perceptual limitations;white box models;interpretable grey box models;Classification algorithms;Prediction algorithms;Machine learning;Mathematical model;Machine learning algorithms;Computational modeling;Neural networks;machine learning;explainability;interpretability;accuracy;classifier;visualization;visual model;dominant intervals},
 month = {Dec},
 number = {},
 pages = {4940-4947},
 title = {Toward Efficient Automation of Interpretable Machine Learning},
 volume = {},
 year = {2018}
}

@inproceedings{953860,
 abstract = {We propose a geometric method for document image processing. This research focuses on document understanding and classification by applying the Winnow algorithm, an online machine learning method. This application makes the document image processing more flexible with various kind of documents since the meaningful knowledge can be extracted from training examples and the model for document type can be updated when there is a new example. This research aims to analyze and classify scientific papers. We conduct the experiments on documents from the proceedings of various conferences to show the performance of the proposed method. The experimental results are compared with the WISDOM++ system and also show the advantages of using the online machine learning method.},
 author = {C. {Nattee} and M. {Numao}},
 booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
 comments = {, },
 doi = {10.1109/ICDAR.2001.953860},
 issn = {},
 keywords = {document image processing;image retrieval;pattern classification;real-time systems;learning systems;document image processing;document understanding;pattern classification;geometric method;Winnow algorithm;machine learning;scientific papers;image retrieval;real time systems;Machine learning;Machine learning algorithms;Learning systems;Document image processing;Text analysis;Information analysis;Logic;Computer science;Electronic mail;Application software},
 month = {Sep.},
 number = {},
 pages = {602-606},
 title = {Geometric method for document understanding and classification using online machine learning},
 volume = {},
 year = {2001}
}

@inproceedings{alonsoZadehComputingWords2019,
 abstract = {The European Commission has identified Artificial Intelligence (AI) as the ``most strategic technology of the 21st century'' [7].},
 author = {Alonso, Jose M.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 comments = {, },
 editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
 isbn = {978-3-030-12544-8},
 keywords = {Cointension,Computing with perceptions,Computing with words,Explainable AI,Fuzzy Logic,Interpretable fuzzy systems},
 language = {en},
 pages = {244-248},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {From {{Zadeh}}'s {{Computing}} with {{Words Towards eXplainable Artificial Intelligence}}},
 year = {2019}
}

@inproceedings{Bellini:2018:KAE:3270323.3270327,
 acmid = {3270327},
 address = {New York, NY, USA},
 author = {Bellini, Vito and Schiavone, Angelo and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
 booktitle = {Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems},
 comments = {, },
 doi = {10.1145/3270323.3270327},
 isbn = {978-1-4503-6617-5},
 keywords = {Autoencoder Neural Networks, Deep Learning, Explainable Models, Explanation, Recommender Systems},
 location = {Vancouver, BC, Canada},
 numpages = {8},
 pages = {24--31},
 publisher = {ACM},
 series = {DLRS 2018},
 title = {Knowledge-aware Autoencoders for Explainable Recommender Systems},
 url = {http://doi.acm.org/10.1145/3270323.3270327},
 year = {2018}
}

@inproceedings{Chen:2018:PET:3180308.3180362,
 acmid = {3180362},
 address = {New York, NY, USA},
 articleno = {53},
 author = {Chen, Mei-Ling and Wang, Hao-Chuan},
 booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
 comments = {, },
 doi = {10.1145/3180308.3180362},
 isbn = {978-1-4503-5571-1},
 keywords = {Conversational agents, explainable intelligent user interfaces, mental models},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {53:1--53:2},
 publisher = {ACM},
 series = {IUI '18 Companion},
 title = {How Personal Experience and Technical Knowledge Affect Using Conversational Agents},
 url = {http://doi.acm.org/10.1145/3180308.3180362},
 year = {2018}
}

@incollection{dengEpilogueFrontiersNLP2018,
 abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
 address = {Singapore},
 author = {Deng, Li and Liu, Yang},
 booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
 comments = {, },
 doi = {10.1007/978-981-10-5209-5_11},
 editor = {Deng, Li and Liu, Yang},
 isbn = {978-981-10-5209-5},
 language = {en},
 pages = {309-326},
 publisher = {{Springer Singapore}},
 shorttitle = {Epilogue},
 title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
 year = {2018}
}

@inproceedings{Ehsan:2018:RNM:3278721.3278736,
 acmid = {3278736},
 address = {New York, NY, USA},
 author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 comments = {, The described method is neither general, nor focused on NLP},
 doi = {10.1145/3278721.3278736},
 isbn = {978-1-4503-6012-8},
 keywords = {ai rationalization, artificial intelligence, explainable ai, interpretability, machine learning, transparency, user perception},
 location = {New Orleans, LA, USA},
 numpages = {7},
 pages = {81--87},
 publisher = {ACM},
 series = {AIES '18},
 title = {Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations},
 url = {http://doi.acm.org/10.1145/3278721.3278736},
 year = {2018}
}

@article{Goel:1991:MMT:122344.122358,
 acmid = {122358},
 address = {New York, NY, USA},
 author = {Goel, Ashok K. and Eiselt, Kurt P.},
 comments = {, },
 doi = {10.1145/122344.122358},
 issn = {0163-5719},
 issue_date = {Aug. 1991},
 journal = {SIGART Bull.},
 month = {July},
 number = {4},
 numpages = {4},
 pages = {75--78},
 publisher = {ACM},
 title = {Mental Models, Text Interpretation, and Knowledge Acquisition},
 url = {http://doi.acm.org/10.1145/122344.122358},
 volume = {2},
 year = {1991}
}

@inproceedings{Hotta:2008:NGT:1486927.1487030,
 acmid = {1487030},
 address = {Washington, DC, USA},
 author = {Hotta, Hajime and Hagiwara, Masafumi},
 booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
 comments = {, },
 doi = {10.1109/WIIAT.2008.141},
 isbn = {978-0-7695-3496-1},
 keywords = {neural network, visualization, geographic},
 numpages = {7},
 pages = {817--823},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '08},
 title = {A Neural-Network-Based Geographic Tendency Visualization},
 url = {http://dx.doi.org/10.1109/WIIAT.2008.141},
 year = {2008}
}

@article{http://arxiv.org/abs/1705.06824v2,
 abstract = {Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
"CNN Inception + Gate" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.},
 author = {Wang, Zhengyang and Ji, Shuiwang},
 comments = {, },
 journal = {arxiv},
 month = {5},
 title = {Learning Convolutional Text Representations for Visual Question
Answering},
 url = {http://arxiv.org/pdf/1705.06824v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 comments = {, },
 journal = {arxiv},
 month = {7},
 title = {Explanations for Temporal Recommendations},
 url = {http://arxiv.org/pdf/1807.06161v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.07292v2,
 abstract = {In this paper, we study two challenging problems. The first one is how to
implement \textit{k}-means in the neural network, which enjoys efficient
training based on the stochastic algorithm. The second one is how to enhance
the interpretability of network design for clustering. To solve the problems,
we propose a neural network which is a novel formulation of the vanilla
$k$-means objective. Our contribution is in twofold. From the view of neural
networks, the proposed \textit{k}-meansNet is with explicit interpretability in
neural processing. We could understand not only why the network structure is
presented like itself but also why it could perform data clustering. Such an
interpretable neural network remarkably differs from the existing works that
usually employ visualization technique to explain the result of the neural
network. From the view of \textit{k}-means, three highly desired properties are
achieved, i.e. robustness to initialization, the capability of handling new
coming data, and provable convergence. Extensive experimental studies show that
our method achieves promising performance comparing with 12 clustering methods
on some challenging datasets.},
 author = {Peng, Xi and Tsang, Ivor W. and Zhou, Joey Tianyi and Zhu, Hongyuan},
 comments = {, The publication does not focus on explainability},
 journal = {arxiv},
 month = {8},
 title = {k-meansNet: When k-means Meets Differentiable Programming},
 url = {http://arxiv.org/pdf/1808.07292v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09551v1,
 abstract = {Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .},
 author = {Godin, Fréderic and Demuynck, Kris and Dambre, Joni and Neve, Wesley De and Demeester, Thomas},
 comments = {, The publication does not focus on explainability},
 journal = {arxiv},
 month = {8},
 title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
They Discover Linguistic Rules?},
 url = {http://arxiv.org/pdf/1808.09551v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1901.06560v1,
 abstract = {There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don't explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs' use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.},
 author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
 comments = {, },
 journal = {arxiv},
 month = {1},
 title = {Explaining Explanations to Society},
 url = {http://arxiv.org/pdf/1901.06560v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09813v1,
 abstract = {Word embeddings generated by neural network methods such as word2vec (W2V)
are well known to exhibit seemingly linear behaviour, e.g. the embeddings of
analogy "woman is to queen as man is to king" approximately describe a
parallelogram. This property is particularly intriguing since the embeddings
are not trained to achieve it. Several explanations have been proposed, but
each introduces assumptions that do not hold in practice. We derive a
probabilistically grounded definition of paraphrasing and show it can be
re-interpreted as word transformation, a mathematical description of "$w_x$ is
to $w_y$". From these concepts we prove existence of the linear relationship
between W2V-type embeddings that underlies the analogical phenomenon, and
identify explicit error terms in the relationship.},
 author = {Allen, Carl and Hospedales, Timothy},
 comments = {, },
 journal = {arxiv},
 month = {1},
 title = {Analogies Explained: Towards Understanding Word Embeddings},
 url = {http://arxiv.org/pdf/1901.09813v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02041v1,
 abstract = {We ask whether the neural network interpretation methods can be fooled via
adversarial model manipulation, which is defined as a model fine-tuning step
that aims to radically alter the explanations without hurting the accuracy of
the original model. By incorporating the interpretation results directly in the
regularization term of the objective function for fine-tuning, we show that the
state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled
with our model manipulation. We propose two types of fooling, passive and
active, and demonstrate such foolings generalize well to the entire validation
set as well as transfer to other interpretation methods. Our results are
validated by both visually showing the fooled explanations and reporting
quantitative metrics that measure the deviations from the original
explanations. We claim that the stability of neural network interpretation
method with respect to our adversarial model manipulation is an important
criterion to check for developing robust and reliable neural network
interpretation method.},
 author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
 comments = {, },
 journal = {arxiv},
 month = {2},
 title = {Fooling Neural Network Interpretations via Adversarial Model
Manipulation},
 url = {http://arxiv.org/pdf/1902.02041v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
 abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently.
Explainability is being presented as a remedy for lack of trust in model
predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise
instance level interpretability for any complex machine learning model. But how
certain are these explanations? Can we rely on additive explanations for
non-additive models? In this paper, we examine the behavior of model explainers
under the presence of interactions. We define two sources of uncertainty, model
level uncertainty, and explanation level uncertainty. We show that adding
interactions reduces explanation level uncertainty. We introduce a new method
iBreakDown that generates non-additive explanations with local interaction.},
 author = {Gosiewska, Alicja and Biecek, Przemyslaw},
 comments = {, The described method is neither general, nor focused on NLP},
 journal = {arxiv},
 month = {3},
 title = {iBreakDown: Uncertainty of Model Explanations for Non-additive
Predictive Models},
 url = {http://arxiv.org/pdf/1903.11420v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.05488v1,
 abstract = {Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved
test accuracy for all models it has been applied to.},
 author = {Tao, Sean},
 comments = {, },
 journal = {arxiv},
 month = {4},
 title = {Deep Neural Network Ensembles},
 url = {http://arxiv.org/pdf/1904.05488v1},
 year = {2019}
}

@article{Israelsen:2019:XAY:3303862.3267338,
 acmid = {3267338},
 address = {New York, NY, USA},
 articleno = {113},
 author = {Israelsen, Brett W. and Ahmed, Nisar R.},
 comments = {, },
 doi = {10.1145/3267338},
 issn = {0360-0300},
 issue_date = {February 2019},
 journal = {ACM Comput. Surv.},
 keywords = {Human-computer trust, accountability, algorithmic assurances, explainable artificial intelligence, fairness, interpretable machine learning, transparency},
 month = {January},
 number = {6},
 numpages = {37},
 pages = {113:1--113:37},
 publisher = {ACM},
 title = {\&\#x201C;Dave...I Can Assure You ...That It\&\#x2019;s Going to Be All Right ...\&\#x201D; A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships},
 url = {http://doi.acm.org/10.1145/3267338},
 volume = {51},
 year = {2019}
}

@incollection{kochGroupCognitionCollaborative2018,
 abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
 address = {Cham},
 author = {Koch, Janin and Oulasvirta, Antti},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 comments = {, },
 doi = {10.1007/978-3-319-90403-0_15},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {293-312},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Group {{Cognition}} and {{Collaborative AI}}},
 year = {2018}
}

@incollection{lughoferModelExplanationInterpretation2018,
 abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
 address = {Cham},
 author = {Lughofer, Edwin},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 comments = {, },
 doi = {10.1007/978-3-319-90403-0_10},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {177-221},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
 year = {2018}
}

@incollection{neukartReverseEngineeringMind2017,
 abstract = {Within this chapter all the requirements for reverse engineering the mind based on the knowledge imparted in the previous chapters will be discussed, and open questions attempted to be solved. A suitable theory of mind that on one side may not be the whole truth from a philosophical point of view, but serves as a valid foundation from an engineering point of view on the other side is introduced. Furthermore, as I indicated more than once, I am of the opinion that both quantum physics as well as self-organization occupy the most important roles in how our brain works and lets us experience conscious content and again, it is required to plunge into the information theoretical approach to quantum physics, quantum computer science.},
 address = {Wiesbaden},
 author = {Neukart, Florian},
 booktitle = {Reverse {{Engineering}} the {{Mind}}: {{Consciously Acting Machines}} and {{Accelerated Evolution}}},
 comments = {, },
 doi = {10.1007/978-3-658-16176-7_10},
 editor = {Neukart, Florian},
 isbn = {978-3-658-16176-7},
 keywords = {Artificial Neural Network,Hide Markov Model,Quantum Computer,Reverse Engineering,Semantic Network},
 language = {en},
 pages = {237-354},
 publisher = {{Springer Fachmedien Wiesbaden}},
 series = {{{AutoUni}} \textendash{} {{Schriftenreihe}}},
 title = {Reverse Engineering the Mind},
 year = {2017}
}

@incollection{nissanNarrativesFormalismComputational2014,
 abstract = {We recapitulate four decades of computational processing of narratives. Vladimir Propp's work in the 1920s paved the way to both the structuralists' approach to the folktale and to narratives in general, and the story grammars approach to automate story-processing. In the latter domain, grammar-driven processing was overtaken by goal-driven processing, but there has been a comeback of story grammars, in combination with other devices. Propp's concern was with Russian folktales, and some story-generation programs are relevant indeed for folktale studies: such is the case of the programs TALE-SPIN and Joseph, which reportedly generated fables; MINSTREL generated Arthurian tales. Sometimes, bugs reveal more than proper functioning does, about the actual underlying model. Automated story processing, within artificial intelligence, showed important results since the late 1970s. After slowing down during the 1990s, since the turn of the century the field resurged, especially in the perspective of virtual environments and interactive narratives, also benefiting from the popularity of computer models of the emotions.},
 address = {Berlin, Heidelberg},
 author = {Nissan, Ephraim},
 booktitle = {Language, {{Culture}}, {{Computation}}. {{Computing}} of the {{Humanities}}, {{Law}}, and {{Narratives}}: {{Essays Dedicated}} to {{Yaacov Choueka}} on the {{Occasion}} of {{His}} 75th {{Birthday}}, {{Part II}}},
 comments = {, },
 doi = {10.1007/978-3-642-45324-3_11},
 editor = {Dershowitz, Nachum and Nissan, Ephraim},
 isbn = {978-3-642-45324-3},
 keywords = {Belief Revision,Computational Linguistics,Computational Tool,Computer Science Department,Natural Language Processing},
 language = {en},
 pages = {270-393},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Narratives, {{Formalism}}, {{Computational Tools}}, and {{Nonlinearity}}},
 year = {2014}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
 abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
 author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
 booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
 comments = {, },
 editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
 isbn = {978-3-319-71746-3},
 language = {en},
 pages = {167-180},
 publisher = {{Springer International Publishing}},
 series = {Communications in {{Computer}} and {{Information Science}}},
 shorttitle = {Interpretable {{Probabilistic Embeddings}}},
 title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
 year = {2018}
}

@article{schubbachJudgingMachinesPhilosophical2019,
 abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
 author = {Schubbach, Arno},
 comments = {, },
 doi = {10.1007/s11229-019-02167-z},
 issn = {1573-0964},
 journal = {Synthese},
 keywords = {Algorithm,Artificial intelligence,Computation,Deep learning,Explanation,Judgment,Justification,Kant,Machine learning},
 language = {en},
 month = {March},
 shorttitle = {Judging Machines},
 title = {Judging Machines: Philosophical Aspects of Deep Learning},
 year = {2019}
}

@inproceedings{Schuessler:2019:MEC:3290607.3312823,
 acmid = {3312823},
 address = {New York, NY, USA},
 articleno = {LBW2810},
 author = {Schuessler, Martin and Wei\ss, Philipp},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 comments = {, },
 doi = {10.1145/3290607.3312823},
 isbn = {978-1-4503-5971-9},
 keywords = {deep neural networks, explanations, image classification, interpretable machine learning},
 location = {Glasgow, Scotland Uk},
 numpages = {6},
 pages = {LBW2810:1--LBW2810:6},
 publisher = {ACM},
 series = {CHI EA '19},
 title = {Minimalistic Explanations: Capturing the Essence of Decisions},
 url = {http://doi.acm.org/10.1145/3290607.3312823},
 year = {2019}
}

@inproceedings{Strobel:2018:AAE:3278721.3278788,
 acmid = {3278788},
 address = {New York, NY, USA},
 author = {Strobel, Martin},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3278721.3278788},
 isbn = {978-1-4503-6012-8},
 keywords = {axiomatic approach, explainable machine learning},
 location = {New Orleans, LA, USA},
 numpages = {2},
 pages = {380--381},
 publisher = {ACM},
 series = {AIES '18},
 title = {An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract},
 url = {http://doi.acm.org/10.1145/3278721.3278788},
 year = {2018}
}

@inproceedings{Wang:2019:DTU:3290605.3300831,
 acmid = {3300831},
 address = {New York, NY, USA},
 articleno = {601},
 author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 comments = {, The described method is neither general, nor focused on NLP},
 doi = {10.1145/3290605.3300831},
 isbn = {978-1-4503-5970-2},
 keywords = {clinical decision making, decision making, explainable artificial intelligence, explanations, intelligibility},
 location = {Glasgow, Scotland Uk},
 numpages = {15},
 pages = {601:1--601:15},
 publisher = {ACM},
 series = {CHI '19},
 title = {Designing Theory-Driven User-Centric Explainable AI},
 url = {http://doi.acm.org/10.1145/3290605.3300831},
 year = {2019}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
 abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
 address = {Cham},
 author = {Wodecki, Andrzej},
 booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
 comments = {, The publication does not focus on explainability},
 doi = {10.1007/978-3-319-91596-8_3},
 editor = {Wodecki, Andrzej},
 isbn = {978-3-319-91596-8},
 language = {en},
 pages = {133-246},
 publisher = {{Springer International Publishing}},
 title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
 year = {2019}
}

@inproceedings{Wolf:2019:EST:3301275.3302317,
 acmid = {3302317},
 address = {New York, NY, USA},
 author = {Wolf, Christine T.},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3301275.3302317},
 isbn = {978-1-4503-6272-6},
 keywords = {XAI, aging-in-place, explainability scenarios, scenario-based design},
 location = {Marina del Ray, California},
 numpages = {6},
 pages = {252--257},
 publisher = {ACM},
 series = {IUI '19},
 title = {Explainability Scenarios: Towards Scenario-based XAI Design},
 url = {http://doi.acm.org/10.1145/3301275.3302317},
 year = {2019}
}

