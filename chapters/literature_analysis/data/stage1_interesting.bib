@article{750572,
 abstract = {Presents a methodology for detection of neural-network gaps (NNGs) based on error analysis and the visualization that is applicable to the n-dimensional I/O domain. The generalization problem in artificial neural networks (ANN) training is analyzed and the concept of NNGs is introduced. The NNGs are highly undesirable in ANN generalization and methods for detecting, analyzing, and eliminating them are necessary. Previous methods for NNG detection, based on two-dimensional (2-D) and three dimensional (3-D) visualization, were not applicable for ANNs with more than three inputs. Experiments demonstrate advantages of this new methodology, which allows better understanding of the NNG phenomena using a quantitative approach.},
 author = {M. M. {Kantardzic} and A. A. {Aly} and A. S. {Elmaghraby}},
 comments = {, },
 doi = {10.1109/72.750572},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {neural nets;generalisation (artificial intelligence);learning (artificial intelligence);error analysis;neural-network gaps;error analysis;visualization;n-dimensional I/O domain;generalization problem;quantitative approach;Visualization;Error analysis;Artificial neural networks;Testing;Neural networks;Two dimensional displays;Particle measurements;Computer networks;Performance evaluation;Supervised learning},
 month = {March},
 number = {2},
 pages = {419-426},
 title = {Visualization of neural-network gaps based on error analysis},
 volume = {10},
 year = {1999}
}

@article{7552539,
 abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
 author = {W. {Samek} and A. {Binder} and G. {Montavon} and S. {Lapuschkin} and K. {Müller}},
 comments = {Reviews the current state of explainability research, The described method is neither general, nor focused on NLP},
 doi = {10.1109/TNNLS.2016.2599820},
 issn = {2162-237X},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;sensitivity-based approach;deconvolution method;heatmap;deep neural network;complex machine learning tasks;multilayer nonlinear structure;DNN;MIT Places data sets;SUN397;ILSVRC2012;relevance propagation algorithm;data visualization;Heating;Neurons;Biological neural networks;Deconvolution;Sensitivity;Learning systems;Algorithm design and analysis;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
 month = {Nov},
 number = {11},
 pages = {2660-2673},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 volume = {28},
 year = {2017}
}

@inproceedings{7557899,
 abstract = {Artificial Intelligence (AI) has infiltrated almost every scientific and social endeavour, including everything from medical research to the sociology of crowd control. But the foundation of AI continues to be based on digital representations of knowledge, and computational reasoning therewith. Because so much of modern knowledge infrastructure and social behaviour is connected to AI, understanding the role of AI in each such endeavour not only helps accelerate progress in those fields in which it applies, but also creates the challenges to extend the foundation for modern AI methods. The simple hypothesis herein is that so-called AI-complete problems have a role in helping to articulate the appropriate integration of AI within other disciplines. With the current growth of interest in "big data" and visualization, we argue that relatively simple formal structures provide a basis for the claim that visualization is an AI-complete problem. The value of confirming this claim is largely to encourage stronger formalizations of the visualization process in terms of the AI foundations of representation and reasoning. This connection will help ensure that relevant components of AI are appropriately applied and integrated, to provide value for a basis of a theory of visualization. The sketch of this claim here is based on the simple idea that visualization is an abstraction process, and that abstractions from partial information, however voluminous, directly confronts the non monotonic reasoning challenge, thus the need for caution in engineering visualization systems without carefully considering the consequences of visual abstraction. This is particularly important with interactive visualization, which has recently formed the basis for such fields as visual analytics.},
 author = {R. {Goebel}},
 booktitle = {2016 20th International Conference Information Visualisation (IV)},
 comments = {, },
 doi = {10.1109/IV.2016.53},
 issn = {2375-0138},
 keywords = {artificial intelligence;Big Data;data analysis;data structures;data visualisation;AI-complete problem;artificial intelligence;digital knowledge representations;computational reasoning;modern knowledge infrastructure;social behaviour;AI methods;AI integration;Big Data;formal structures;visualization process formalization;partial information abstraction process;monotonic reasoning challenge;engineering visualization systems;visual abstraction;interactive visualization;visual analytics;Data visualization;Visualization;Artificial intelligence;Context;Complexity theory;Cognition;Semantics;AI-complete visualization incomplete knowledge},
 month = {July},
 number = {},
 pages = {27-32},
 title = {Why Visualization is an AI-complete Problem (and Why That Matters)},
 volume = {},
 year = {2016}
}

@inproceedings{7801719,
 abstract = {Multi-document summarization is to create summaries covering the major information that multiple documents tell in common. For this point, the existing methods are based on hand-crafted features for word and sentence. However, it is difficult to figure out the core contents of each document with the hand-crafted features because they have the limited information presented the given documents. Moreover, there exists a limit to figure out the major information because documents with the same meaning used to be paraphrased depending on their writers. Therefore, it is necessary to represent the semantic meanings of documents as well as sentences through understanding natural language. In this paper, we propose a new multi-document summarization system by creating a synthetic document vector covering the whole documents based on Language Model, whose is well-known for learning the semantic features in text. We experimented with DUC 2004 dataset provided by Document Understanding Conference (DUC) and the results show that our method summarizes multiple documents effectively based on their core contents.},
 author = {D. {Kim} and J. {Lee}},
 booktitle = {2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)},
 comments = {, },
 doi = {10.1109/SCIS-ISIS.2016.0132},
 issn = {},
 keywords = {document handling;natural language processing;word processing;multidocument summarization system;semantic text feature learning;DUC 2004 dataset;document understanding conference;natural language model;semantic document meanings;hand-crafted features;synthetic document vector;Semantics;Context;Hidden Markov models;Computational modeling;Redundancy;Intelligent systems;Natural languages;Multi-document summarization;Core content;Major Information;Synthetic document vector;Language model},
 month = {Aug},
 number = {},
 pages = {605-609},
 title = {Multi-document Summarization by Creating Synthetic Document Vector Based on Language Model},
 volume = {},
 year = {2016}
}

@inproceedings{7846290,
 abstract = {Machine Learning (ML) techniques have allowed a great performance improvement of different challenging Spoken Language Understanding (SLU) tasks. Among these methods, Neural Networks (NN), or Multilayer Perceptron (MLP), recently received a great interest from researchers due to their representation capability of complex internal structures in a low dimensional subspace. However, MLPs employ document representations based on basic word level or topic-based features. Therefore, these basic representations reveal little in way of document statistical structure by only considering words or topics contained in the document as a “bag-of-words”, ignoring relations between them. We propose to remedy this weakness by extending the complex features based on Quaternion algebra presented in [1] to neural networks called QMLP. This original QMLP approach is based on hyper-complex algebra to take into consideration features dependencies in documents. New document features, based on the document structure itself, used as input of the QMLP, are also investigated in this paper, in comparison to those initially proposed in [1]. Experiments made on a SLU task from a real framework of human spoken dialogues showed that our QMLP approach associated with the proposed document features outperforms other approaches, with an accuracy gain of 2% with respect to the MLP based on real numbers and more than 3% with respect to the first Quaternion-based features proposed in [1]. We finally demonstrated that less iterations are needed by our QMLP architecture to be efficient and to reach promising accuracies.},
 author = {T. {Parcollet} and M. {Morchid} and P. {Bousquet} and R. {Dufour} and G. {Linarès} and R. {De Mori}},
 booktitle = {2016 IEEE Spoken Language Technology Workshop (SLT)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SLT.2016.7846290},
 issn = {},
 keywords = {algebra;document handling;learning (artificial intelligence);multilayer perceptrons;natural language processing;quaternion neural networks;machine learning;spoken language understanding;SLU;multilayer perceptron;MLP;document representations;document statistical structure;quaternion algebra;Quaternions;Computational modeling;Neurons;Artificial neural networks;Algebra;Natural language processing;Quaternion;Neural Network;Spoken Language Understanding;Machine Learning},
 month = {Dec},
 number = {},
 pages = {362-368},
 title = {Quaternion Neural Networks for Spoken Language Understanding},
 volume = {},
 year = {2016}
}

@inproceedings{7960721,
 abstract = {The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.},
 author = {İ. Ü. {Oğul} and C. {Özcan} and Ö. {Hakdağlı}},
 booktitle = {2017 25th Signal Processing and Communications Applications Conference (SIU)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SIU.2017.7960721},
 issn = {},
 keywords = {Bayes methods;data analysis;distributed processing;Internet of Things;learning (artificial intelligence);pattern classification;public domain software;social networking (online);text analysis;IoT;open source social news site;Reddit;interpretation data;data analysis;fast data storage;Hadoop architecture;distributed data structure;distributed in-memory data collection;Apache Spark architecture;Naive Bayes method;machine learning;meaning extraction;irrelevant data deletion;Internet of Things;fast text classification;Sparks;Java;Internet of Things;Standards;Text categorization;Art;Machine learning;Text mining;Big data;Apache Spark;Classification;Naive Bayes},
 month = {May},
 number = {},
 pages = {1-4},
 title = {Fast text classification with Naive Bayes method on Apache Spark},
 volume = {},
 year = {2017}
}

@inproceedings{8260658,
 abstract = {Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
 author = {K. {Kowsari} and D. E. {Brown} and M. {Heidarysafa} and K. {Jafari Meimandi} and M. S. {Gerber} and L. E. {Barnes}},
 booktitle = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2017.0-134},
 issn = {},
 keywords = {data mining;learning (artificial intelligence);neural nets;pattern classification;text analysis;document hierarchy;HDLTex;Hierarchical Deep;Text classification;document collections;information processing methods;supervised learning;multiclass classification;hierarchical classification;deep learning architectures;supervised classifiers;text organization;document classification methods;Machine learning;Support vector machines;Computer architecture;Kernel;Mathematical model;Recurrent neural networks;Text Mining;Document Classification;Deep Neural Networks;Hierarchical Learning;Deep Learning},
 month = {Dec},
 number = {},
 pages = {364-371},
 title = {HDLTex: Hierarchical Deep Learning for Text Classification},
 volume = {},
 year = {2017}
}

@inproceedings{8268978,
 abstract = {Deep Neural Networks (DNN) received a great interest from researchers due to their capability to construct robust abstract representations of heterogeneous documents in a latent subspace. Nonetheless, mere real-valued deep neural networks require an appropriate adaptation, such as the convolution process, to capture latent relations between input features. Moreover, real-valued deep neural networks reveal little in way of document internal dependencies, by only considering words or topics contained in the document as an isolate basic element. Quaternion-valued multi-layer perceptrons (QMLP), and autoencoders (QAE) have been introduced to capture such latent dependencies, alongside to represent multidimensional data. Nonetheless, a three-layered neural network does not benefit from the high abstraction capability of DNNs. The paper proposes first to extend the hyper-complex algebra to deep neural networks (QDNN) and, then, introduces pre-trained deep quaternion neural networks (QDNN-AE) with dedicated quaternion encoder-decoders (QAE). The experiments conduced on a theme identification task of spoken dialogues from the DECODA data set show, inter alia, that the QDNN-AE reaches a promising gain of 2.2% compared to the standard real-valued DNN-AE.},
 author = {T. {Parcollet} and M. {Morchid} and G. {Linarès}},
 booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 comments = {, },
 doi = {10.1109/ASRU.2017.8268978},
 issn = {},
 keywords = {learning (artificial intelligence);multilayer perceptrons;natural language processing;speech recognition;real-valued deep neural networks;document internal dependencies;neural network;deep quaternion neural networks;spoken language understanding;quaternion-valued multilayer perceptrons;QMLP;autoencoders;QAE;spoken dialogues;Quaternions;Speech;Task analysis;Telephone sets;Algebra;Biological neural networks;Quaternions;deep neural networks;spoken language understanding;autoencoders;machine learning},
 month = {Dec},
 number = {},
 pages = {504-511},
 title = {Deep quaternion neural networks for spoken language understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8275810,
 abstract = {In text categorization, feature representation for dimensionality reduction is a key step. Usually, some commonly used methods, e.g., latent semantic analysis (LSA), yield a dense representation or a dense transformation matrix, which is difficult to precisely characterize the document-topic or the topic-word relationship. This paper proposes a novel discriminative topic sparse representation (DTSR) approach for text categorization, in which two stages are included: the topic dictionary construction and sparse representation. Firstly, a discriminative and interpretable dictionary is constructed to characterize the topic-word relationship. The dictionary contains all category center vectors as well as some semantic topic vectors generated by a latent Dirichlet allocation (LDA) model. Furthermore, each document can be represented with a sparse form to obtain a good document-topic relationship. Experimental results on well-known benchmark datasets indicate that the proposed method not only achieves a satisfactory classification performance but also provides a reasonable sparse semantic meaningful.},
 author = {W. {Zheng} and Y. {Liu} and H. {Lu} and H. {Tang}},
 booktitle = {2017 10th International Symposium on Computational Intelligence and Design (ISCID)},
 comments = {, },
 doi = {10.1109/ISCID.2017.54},
 issn = {2473-3547},
 keywords = {learning (artificial intelligence);pattern classification;text analysis;vectors;semantic topic vectors;latent Dirichlet allocation model;text categorization;feature representation;dimensionality reduction;latent semantic analysis;dense transformation matrix;topic-word relationship;topic dictionary construction;discriminative dictionary;interpretable dictionary;document-topic relationship;discriminative topic sparse representation approach;Semantics;Dictionaries;Text categorization;Training;Sparse matrices;Large scale integration;Resource management;Text Categorization;Sparse Representation;Topic;Discriminative;Semantic},
 month = {Dec},
 number = {},
 pages = {454-457},
 title = {Discriminative Topic Sparse Representation for Text Categorization},
 volume = {1},
 year = {2017}
}

@inproceedings{8310088,
 abstract = {As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.},
 author = {R. {Raoui-Outach} and C. {Million-Rousseau} and A. {Benoit} and P. {Lambert}},
 booktitle = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
 comments = {, },
 doi = {10.1109/IPTA.2017.8310088},
 issn = {2154-512X},
 keywords = {data analysis;document image processing;feedforward neural nets;learning (artificial intelligence);sales management;text analysis;Deep learning;automatic sale receipt understanding;data analytics;scanned document analysis;paper damages;scanning quality;industrial context;automatic understanding;essential consumption statistics;accurate consumption statistics;smart-phone;tool chain;essential information;store brand;purchased products;related prices;highest possible confidence;high confidence level;tool-chain;Deep Convolutional Neural Networks;classical image;text processings;double check processing;text analysis;Optical character recognition software;Character recognition;Semantics;Text analysis;Task analysis;Object detection;Machine learning;Receipt image understanding;Deep Convolutional Neural Networks;Object Detection;Semantic Analysis},
 month = {Nov},
 number = {},
 pages = {1-6},
 title = {Deep learning for automatic sale receipt understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8490433,
 abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
 author = {J. {Zhu} and A. {Liapis} and S. {Risi} and R. {Bidarra} and G. M. {Youngblood}},
 booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
 comments = {, },
 doi = {10.1109/CIG.2018.8490433},
 issn = {2325-4289},
 keywords = {computer games;human computer interaction;learning (artificial intelligence);game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework;Games;Task analysis;Machine learning;Neurons;Visualization;Tools;explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design},
 month = {Aug},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},
 volume = {},
 year = {2018}
}

@inproceedings{8490530,
 abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
 author = {A. {Holzinger}},
 booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (DISA)},
 comments = {, Does not describe the used explainability method},
 doi = {10.1109/DISA.2018.8490530},
 issn = {},
 keywords = {learning (artificial intelligence);ontologies (artificial intelligence);probability;statistical machine learning methods;AI winter;deep learning approaches;big quality data;computational power;engineering effort;ontologies;knowledge representations;glass-box approaches;mathematical principles;artificial intelligence;logical approaches;probabilistic learning methods;rule-based approaches;contextual explanatory models;context-adaptive procedures;high-dimensional vectors;Machine learning;Data mining;Data visualization;Uncertainty;Games;Cognitive science},
 month = {Aug},
 number = {},
 pages = {55-66},
 title = {From Machine Learning to Explainable AI},
 volume = {},
 year = {2018}
}

@inproceedings{8538416,
 abstract = {In the age of knowledge, Natural Language Processing (NLP) express its demand by a huge range of utilization. Previously NLP was dealing with statically data. Contemporary time NLP is doing considerably with the corpus, lexicon database, pattern reorganization. Considering Deep Learning (DL) method recognize artificial Neural Network (NN) to nonlinear process, NLP tools become increasingly accurate and efficient that begin a debacle. Multi-Layer Neural Network obtaining the importance of the NLP for its capability including standard speed and resolute output. Hierarchical designs of data operate recurring processing layers to learn and with this arrangement of DL methods manage several practices. In this paper, this resumed striving to reach a review of the tools and the necessary methodology to present a clear understanding of the association of NLP and DL for truly understand in the training. Efficiency and execution both are improved in NLP by Part of speech tagging (POST), Morphological Analysis, Named Entity Recognition (NER), Semantic Role Labeling (SRL), Syntactic Parsing, and Coreference resolution. Artificial Neural Networks (ANN), Time Delay Neural Networks (TDNN), Recurrent Neural Network (RNN), Convolution Neural Networks (CNN), and Long-Short-Term-Memory (LSTM) dealings among Dense Vector (DV), Windows Approach (WA), and Multitask learning (MTL) as a characteristic of Deep Learning. After statically methods, when DL communicate the influence of NLP, the individual form of the NLP process and DL rule collaboration was started a fundamental connection.},
 author = {S. A. {Fahad} and A. E. {Yahya}},
 booktitle = {2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
 comments = {, },
 doi = {10.1109/ICSCEE.2018.8538416},
 issn = {},
 keywords = {learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;natural language processing;contemporary time NLP;Considering Deep;(DL) method;artificial Neural Network;nonlinear process;NLP tools;MultiLayer Neural Network;processing layers;DL methods;Artificial Neural Networks;Time Delay Neural Networks;Recurrent Neural Network;Convolution Neural Networks;deep learning;NLP process;Natural language processing;Artificial neural networks;Tagging;Semantics;Task analysis;-Deep Learning;Natural language processing;Deep nural Network;Multitask Learning},
 month = {July},
 number = {},
 pages = {1-4},
 title = {Inflectional Review of Deep Learning on Natural Language Processing},
 volume = {},
 year = {2018}
}

@inproceedings{8614130,
 abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
 author = {S. {Ghanta} and S. {Subramanian} and S. {Sundararaman} and L. {Khermosh} and V. {Sridhar} and D. {Arteaga} and Q. {Luo} and D. {Das} and N. {Talagala}},
 booktitle = {2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2018.00105},
 issn = {},
 keywords = {learning (artificial intelligence);production engineering computing;production scenario;interpretability techniques;production ML reproducibility solution;production viability;reproducability;production machine learning applications;legal industry requirements;human understandable machine learning;Production;Pipelines;Predictive models;Machine learning;Training;Data models;Load modeling;reproducability;explainability;interpretability;systems;tracking},
 month = {Dec},
 number = {},
 pages = {658-664},
 title = {Interpretability and Reproducability in Production Machine Learning Applications},
 volume = {},
 year = {2018}
}

@inproceedings{8622433,
 abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
 author = {B. {Kovalerchuk} and N. {Neuhaus}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 comments = {, },
 doi = {10.1109/BigData.2018.8622433},
 issn = {},
 keywords = {learning (artificial intelligence);pattern classification;domain-explainable ML models;dominance classifier and predictor algorithm;DCP algorithm;unintelligible black box models;interpretable machine learning;interpretable mathematical forms;noninterpretable models;interpretable methods;human-understandable machine learning models;complex multidimensional relations;human perceptual limitations;white box models;interpretable grey box models;Classification algorithms;Prediction algorithms;Machine learning;Mathematical model;Machine learning algorithms;Computational modeling;Neural networks;machine learning;explainability;interpretability;accuracy;classifier;visualization;visual model;dominant intervals},
 month = {Dec},
 number = {},
 pages = {4940-4947},
 title = {Toward Efficient Automation of Interpretable Machine Learning},
 volume = {},
 year = {2018}
}

@inproceedings{953860,
 abstract = {We propose a geometric method for document image processing. This research focuses on document understanding and classification by applying the Winnow algorithm, an online machine learning method. This application makes the document image processing more flexible with various kind of documents since the meaningful knowledge can be extracted from training examples and the model for document type can be updated when there is a new example. This research aims to analyze and classify scientific papers. We conduct the experiments on documents from the proceedings of various conferences to show the performance of the proposed method. The experimental results are compared with the WISDOM++ system and also show the advantages of using the online machine learning method.},
 author = {C. {Nattee} and M. {Numao}},
 booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
 comments = {, },
 doi = {10.1109/ICDAR.2001.953860},
 issn = {},
 keywords = {document image processing;image retrieval;pattern classification;real-time systems;learning systems;document image processing;document understanding;pattern classification;geometric method;Winnow algorithm;machine learning;scientific papers;image retrieval;real time systems;Machine learning;Machine learning algorithms;Learning systems;Document image processing;Text analysis;Information analysis;Logic;Computer science;Electronic mail;Application software},
 month = {Sep.},
 number = {},
 pages = {602-606},
 title = {Geometric method for document understanding and classification using online machine learning},
 volume = {},
 year = {2001}
}

@inproceedings{alonsoZadehComputingWords2019,
 abstract = {The European Commission has identified Artificial Intelligence (AI) as the ``most strategic technology of the 21st century'' [7].},
 author = {Alonso, Jose M.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 comments = {, },
 editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
 isbn = {978-3-030-12544-8},
 keywords = {Cointension,Computing with perceptions,Computing with words,Explainable AI,Fuzzy Logic,Interpretable fuzzy systems},
 language = {en},
 pages = {244-248},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {From {{Zadeh}}'s {{Computing}} with {{Words Towards eXplainable Artificial Intelligence}}},
 year = {2019}
}

@inproceedings{Bellini:2018:KAE:3270323.3270327,
 acmid = {3270327},
 address = {New York, NY, USA},
 author = {Bellini, Vito and Schiavone, Angelo and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
 booktitle = {Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems},
 comments = {, },
 doi = {10.1145/3270323.3270327},
 isbn = {978-1-4503-6617-5},
 keywords = {Autoencoder Neural Networks, Deep Learning, Explainable Models, Explanation, Recommender Systems},
 location = {Vancouver, BC, Canada},
 numpages = {8},
 pages = {24--31},
 publisher = {ACM},
 series = {DLRS 2018},
 title = {Knowledge-aware Autoencoders for Explainable Recommender Systems},
 url = {http://doi.acm.org/10.1145/3270323.3270327},
 year = {2018}
}

@inproceedings{bratkoMachineLearningAccuracy1997,
 abstract = {Predictive accuracy is the usual measure of success of Machine Learning (ML) applications. However, experience from many ML applications in difficult, domains indicates the importance of interpretability of induced descriptions. Often in such domains, predictive accuracy is hardly of interest to the user. Instead, the users' interest now lies in the interpretion of the induced descriptions and not, in their use for prediction. In such cases, ML is essentially used as a tool for exploring the domain, to generate new, potentially useful ideas about the domain, and thus improve the user's understanding of the domain. The important questions are how to make domain-specific background knowledge usable by the learning system, and how to interpret the results in the light of this background expertise. These questions are discussed and illustrated by relevant example applications of ML, including: medical diagnosis, ecological modelling, and interpreting discrete event simulations. The observations in these applications show that predictive accuracy, the usual measure of success in ML, should be accompanied by a. criterion of interpretability of induced descriptions. The formalisation of interpretability is however a completely new challenge for ML.},
 author = {Bratko, I.},
 booktitle = {Learning, {{Networks}} and {{Statistics}}},
 comments = {, },
 editor = {Della Riccia, Giacomo and Lenz, Hans-Joachim and Kruse, Rudolf},
 isbn = {978-3-7091-2668-4},
 keywords = {Discrete Event Simulation,Ecological Modelling,Machine Learn,Predictive Accuracy,Regression Tree},
 language = {en},
 pages = {163-177},
 publisher = {{Springer Vienna}},
 series = {International {{Centre}} for {{Mechanical Sciences}}},
 shorttitle = {Machine {{Learning}}},
 title = {Machine {{Learning}}: {{Between Accuracy}} and {{Interpretability}}},
 year = {1997}
}

@inproceedings{carringtonMeasuresModelInterpretability2018,
 abstract = {The literature lacks definitions for quantitative measures of model interpretability for automatic model selection to achieve high accuracy and interpretability, hence we define inherent model interpretability. We extend the work of Lipton et al. and Liu et al. from qualitative and subjective concepts of model interpretability to objective criteria and quantitative measures. We also develop another new measure called simplicity of sensitivity and illustrate prior, initial and posterior measurement. Measures are tested and validated with some measures recommended for use. It is demonstrated that high accuracy and high interpretability are jointly achievable with little to no sacrifice in either.},
 author = {Carrington, Andr\'e and Fieguth, Paul and Chen, Helen},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {, Does not describe the used explainability method},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Kernels,Model interpretability,Model transparency,Support vector machines},
 language = {en},
 pages = {329-349},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measures of {{Model Interpretability}} for {{Model Selection}}},
 year = {2018}
}

@inproceedings{Chen:2018:PET:3180308.3180362,
 acmid = {3180362},
 address = {New York, NY, USA},
 articleno = {53},
 author = {Chen, Mei-Ling and Wang, Hao-Chuan},
 booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
 comments = {, },
 doi = {10.1145/3180308.3180362},
 isbn = {978-1-4503-5571-1},
 keywords = {Conversational agents, explainable intelligent user interfaces, mental models},
 location = {Tokyo, Japan},
 numpages = {2},
 pages = {53:1--53:2},
 publisher = {ACM},
 series = {IUI '18 Companion},
 title = {How Personal Experience and Technical Knowledge Affect Using Conversational Agents},
 url = {http://doi.acm.org/10.1145/3180308.3180362},
 year = {2018}
}

@incollection{dengEpilogueFrontiersNLP2018,
 abstract = {In the first part of this epilogue, we summarize the book holistically from two perspectives. The first, task-centric perspective ties together and categories a wide range of NLP techniques discussed in book in terms of general machine learning paradigms. In this way, the majority of sections and chapters of the book can be naturally clustered into four classes: classification, sequence-based prediction, higher-order structured prediction, and sequential decision-making. The second, representation-centric perspective distills insight from holistically analyzed book chapters from cognitive science viewpoints and in terms of two basic types of natural language representations: symbolic and distributed representations. In the second part of the epilogue, we update the most recent progress on deep learning in NLP (mainly during the later part of 2017, not surveyed in earlier chapters). Based on our reviews of these rapid recent advances, we then enrich our earlier writing on the research frontiers of NLP in Chap. 1 by addressing future directions of exploiting compositionality of natural language for generalization, unsupervised and reinforcement learning for NLP and their intricate connections, meta-learning for NLP, and weak-sense and strong-sense interpretability for NLP systems based on deep learning.},
 address = {Singapore},
 author = {Deng, Li and Liu, Yang},
 booktitle = {Deep {{Learning}} in {{Natural Language Processing}}},
 comments = {, },
 doi = {10.1007/978-981-10-5209-5_11},
 editor = {Deng, Li and Liu, Yang},
 isbn = {978-981-10-5209-5},
 language = {en},
 pages = {309-326},
 publisher = {{Springer Singapore}},
 shorttitle = {Epilogue},
 title = {Epilogue: {{Frontiers}} of {{NLP}} in the {{Deep Learning Era}}},
 year = {2018}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
 abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
 address = {Cham},
 author = {{Doshi-Velez}, Finale and Kim, Been},
 booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
 comments = {, Does not describe the used explainability method},
 doi = {10.1007/978-3-319-98131-4_1},
 editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar\'o, Xavier and G\"u{\c c}l\"ut\"urk, Ya{\u g}mur and G\"u{\c c}l\"u, Umut and {van Gerven}, Marcel},
 isbn = {978-3-319-98131-4},
 keywords = {Accountability,Interpretability,Machine learning,Transparency},
 language = {en},
 pages = {3-17},
 publisher = {{Springer International Publishing}},
 series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
 title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
 year = {2018}
}

@inproceedings{Ehsan:2018:RNM:3278721.3278736,
 acmid = {3278736},
 address = {New York, NY, USA},
 author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 comments = {, The described method is neither general, nor focused on NLP},
 doi = {10.1145/3278721.3278736},
 isbn = {978-1-4503-6012-8},
 keywords = {ai rationalization, artificial intelligence, explainable ai, interpretability, machine learning, transparency, user perception},
 location = {New Orleans, LA, USA},
 numpages = {7},
 pages = {81--87},
 publisher = {ACM},
 series = {AIES '18},
 title = {Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations},
 url = {http://doi.acm.org/10.1145/3278721.3278736},
 year = {2018}
}

@inproceedings{fabra-boludaModellingMachineLearning2018,
 abstract = {Machine learning (ML) models make decisions for governments, companies, and individuals. Accordingly, there is the increasing concern of not having a rich explanatory and predictive account of the behaviour of these ML models relative to the users' interests (goals) and (pre-)conceptions (ontologies). We argue that the recent research trends in finding better characterisations of what a ML model does are leading to the view of ML models as complex behavioural systems. A good explanation for a model should depend on how well it describes the behaviour of the model in simpler, more comprehensible, or more understandable terms according to a given context. Consequently, we claim that a more contextual abstraction is necessary (as is done in system theory and psychology), which is very much like building a subjective mind modelling problem. We bring some research evidence of how this partial and subjective modelling of machine learning models can take place, suggesting that more machine learning is the answer.},
 author = {{Fabra-Boluda}, Ra\"ul and Ferri, C\`esar and {Hern\'andez-Orallo}, Jos\'e and {Mart\'inez-Plumed}, Fernando and {Ram\'irez-Quintana}, M. Jos\'e},
 booktitle = {Philosophy and {{Theory}} of {{Artificial Intelligence}} 2017},
 comments = {, Does not describe the used explainability method},
 editor = {M\"uller, Vincent C.},
 isbn = {978-3-319-96448-5},
 language = {en},
 pages = {175-186},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Applied Philosophy}}, {{Epistemology}} and {{Rational Ethics}}},
 title = {Modelling {{Machine Learning Models}}},
 year = {2018}
}

@inproceedings{goebelExplainableAINew2018,
 abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to ``explain'' their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
 author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {, Does not describe the used explainability method},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning},
 language = {en},
 pages = {295-303},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Explainable {{AI}}},
 title = {Explainable {{AI}}: {{The New}} 42?},
 year = {2018}
}

@article{Goel:1991:MMT:122344.122358,
 acmid = {122358},
 address = {New York, NY, USA},
 author = {Goel, Ashok K. and Eiselt, Kurt P.},
 comments = {, },
 doi = {10.1145/122344.122358},
 issn = {0163-5719},
 issue_date = {Aug. 1991},
 journal = {SIGART Bull.},
 month = {July},
 number = {4},
 numpages = {4},
 pages = {75--78},
 publisher = {ACM},
 title = {Mental Models, Text Interpretation, and Knowledge Acquisition},
 url = {http://doi.acm.org/10.1145/122344.122358},
 volume = {2},
 year = {1991}
}

@inproceedings{Ha:2018:DEA:3183654.3183683,
 acmid = {3183683},
 address = {New York, NY, USA},
 articleno = {14},
 author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
 booktitle = {Proceedings of the Technology, Mind, and Society},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3183654.3183683},
 isbn = {978-1-4503-5420-2},
 keywords = {Anthropomorphism, Attribution theory, Explainability, User perception},
 location = {Washington, DC, USA},
 numpages = {1},
 pages = {14:1--14:1},
 publisher = {ACM},
 series = {TechMindSociety '18},
 title = {Designing Explainability of an Artificial Intelligence System},
 url = {http://doi.acm.org/10.1145/3183654.3183683},
 year = {2018}
}

@inproceedings{Haddouchi:2018:AIC:3289402.3289549,
 acmid = {3289549},
 address = {New York, NY, USA},
 articleno = {49},
 author = {Haddouchi, Maissae and Berrado, Abdelaziz},
 booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3289402.3289549},
 isbn = {978-1-4503-6462-1},
 keywords = {Interpretability, ML, measures, scoring},
 location = {Rabat, Morocco},
 numpages = {6},
 pages = {49:1--49:6},
 publisher = {ACM},
 series = {SITA'18},
 title = {Assessing Interpretation Capacity in Machine Learning: A Critical Review},
 url = {http://doi.acm.org/10.1145/3289402.3289549},
 year = {2018}
}

@inproceedings{holzingerCurrentAdvancesTrends2018,
 abstract = {In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.},
 author = {Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A. Min},
 booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
 comments = {, Does not describe the used explainability method},
 editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
 isbn = {978-3-319-99740-7},
 keywords = {Artificial intelligence,Explainable AI,Knowledge extraction,Machine learning,Privacy},
 language = {en},
 pages = {1-8},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 shorttitle = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}},
 title = {Current {{Advances}}, {{Trends}} and {{Challenges}} of {{Machine Learning}} and {{Knowledge Extraction}}: {{From Machine Learning}} to {{Explainable AI}}},
 year = {2018}
}

@inproceedings{Hotta:2008:NGT:1486927.1487030,
 acmid = {1487030},
 address = {Washington, DC, USA},
 author = {Hotta, Hajime and Hagiwara, Masafumi},
 booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
 comments = {, },
 doi = {10.1109/WIIAT.2008.141},
 isbn = {978-0-7695-3496-1},
 keywords = {neural network, visualization, geographic},
 numpages = {7},
 pages = {817--823},
 publisher = {IEEE Computer Society},
 series = {WI-IAT '08},
 title = {A Neural-Network-Based Geographic Tendency Visualization},
 url = {http://dx.doi.org/10.1109/WIIAT.2008.141},
 year = {2008}
}

@article{http://arxiv.org/abs/1604.00289v3,
 abstract = {Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.},
 author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {4},
 title = {Building Machines That Learn and Think Like People},
 url = {http://arxiv.org/pdf/1604.00289v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.03490v3,
 abstract = {Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.},
 author = {Lipton, Zachary C.},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {6},
 title = {The Mythos of Model Interpretability},
 url = {http://arxiv.org/pdf/1606.03490v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1705.06824v2,
 abstract = {Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
"CNN Inception + Gate" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.},
 author = {Wang, Zhengyang and Ji, Shuiwang},
 comments = {, },
 journal = {arxiv},
 month = {5},
 title = {Learning Convolutional Text Representations for Visual Question
Answering},
 url = {http://arxiv.org/pdf/1705.06824v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
 abstract = {In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.},
 author = {Mohseni, Sina and Ragan, Eric D.},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {1},
 title = {A Human-Grounded Evaluation Benchmark for Local Explanations of Machine
Learning},
 url = {http://arxiv.org/pdf/1801.05075v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.06889v3,
 abstract = {Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W's and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.},
 author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {1},
 title = {Visual Analytics in Deep Learning: An Interrogative Survey for the Next
Frontiers},
 url = {http://arxiv.org/pdf/1801.06889v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
 abstract = {Issues regarding explainable AI involve four components: users, laws &
regulations, explanations and algorithms. Together these components provide a
context in which explanation methods can be evaluated regarding their adequacy.
The goal of this chapter is to bridge the gap between expert users and lay
users. Different kinds of users are identified and their concerns revealed,
relevant statements from the General Data Protection Regulation are analyzed in
the context of Deep Neural Networks (DNNs), a taxonomy for the classification
of existing explanation methods is introduced, and finally, the various classes
of explanation methods are analyzed to verify if user concerns are justified.
Overall, it is clear that (visual) explanations can be given about various
aspects of the influence of the input on the output. However, it is noted that
explanation methods or interfaces for lay users are missing and we speculate
which criteria these methods / interfaces should satisfy. Finally it is noted
that two important concerns are difficult to address with explanation methods:
the concern about bias in datasets that leads to biased DNNs, as well as the
suspicion about unfair outcomes.},
 author = {Ras, Gabrielle and Gerven, Marcel van and Haselager, Pim},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {3},
 title = {Explanation Methods in Deep Learning: Users, Values, Concerns and
Challenges},
 url = {http://arxiv.org/pdf/1803.07517v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1804.02527v1,
 abstract = {Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.},
 author = {Choo, Jaegul and Liu, Shixia},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {4},
 title = {Visual Analytics for Explainable Deep Learning},
 url = {http://arxiv.org/pdf/1804.02527v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
 abstract = {Interpretability methods should be both meaningful to a human and correctly
explain model behavior. In this work, we propose a benchmark to evaluate the
latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative
accuracy of interpretability methods that estimate feature importance in deep
neural networks. We evaluate commonly used interpretability methods and a set
of recently proposed ensemble-based derivative approaches. Our results across
several large-scale image classification datasets are consistent and
thought-provoking -- we find that the formal methods we consider produce
estimates that are less accurate or on par with a random designation of feature
importance. However, certain derivative approaches that ensemble these
estimates far outperform such a random guess. The manner of ensembling remains
critical, we show that some approaches do no better than the underlying method
but carry a far higher computational burden.},
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {6},
 title = {Evaluating Feature Importance Estimates},
 url = {http://arxiv.org/pdf/1806.10758v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 comments = {, },
 journal = {arxiv},
 month = {7},
 title = {Explanations for Temporal Recommendations},
 url = {http://arxiv.org/pdf/1807.06161v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.05054v1,
 abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.},
 author = {Honegger, Milo},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {8},
 title = {Shedding Light on Black Box Machine Learning Algorithms: Development of
an Axiomatic Framework to Assess the Quality of Methods that Explain
Individual Predictions},
 url = {http://arxiv.org/pdf/1808.05054v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.07292v2,
 abstract = {In this paper, we study two challenging problems. The first one is how to
implement \textit{k}-means in the neural network, which enjoys efficient
training based on the stochastic algorithm. The second one is how to enhance
the interpretability of network design for clustering. To solve the problems,
we propose a neural network which is a novel formulation of the vanilla
$k$-means objective. Our contribution is in twofold. From the view of neural
networks, the proposed \textit{k}-meansNet is with explicit interpretability in
neural processing. We could understand not only why the network structure is
presented like itself but also why it could perform data clustering. Such an
interpretable neural network remarkably differs from the existing works that
usually employ visualization technique to explain the result of the neural
network. From the view of \textit{k}-means, three highly desired properties are
achieved, i.e. robustness to initialization, the capability of handling new
coming data, and provable convergence. Extensive experimental studies show that
our method achieves promising performance comparing with 12 clustering methods
on some challenging datasets.},
 author = {Peng, Xi and Tsang, Ivor W. and Zhou, Joey Tianyi and Zhu, Hongyuan},
 comments = {, The publication does not focus on explainability},
 journal = {arxiv},
 month = {8},
 title = {k-meansNet: When k-means Meets Differentiable Programming},
 url = {http://arxiv.org/pdf/1808.07292v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09551v1,
 abstract = {Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .},
 author = {Godin, Fréderic and Demuynck, Kris and Dambre, Joni and Neve, Wesley De and Demeester, Thomas},
 comments = {, The publication does not focus on explainability},
 journal = {arxiv},
 month = {8},
 title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
They Discover Linguistic Rules?},
 url = {http://arxiv.org/pdf/1808.09551v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.10799v1,
 abstract = {Recent efforts in Machine Learning (ML) interpretability have focused on
creating methods for explaining black-box ML models. However, these methods
rely on the assumption that simple approximations, such as linear models or
decision-trees, are inherently human-interpretable, which has not been
empirically tested. Additionally, past efforts have focused exclusively on
comprehension, neglecting to explore the trust component necessary to convince
non-technical experts, such as clinicians, to utilize ML models in practice. In
this paper, we posit that reinforcement learning (RL) can be used to learn what
is interpretable to different users and, consequently, build their trust in ML
models. To validate this idea, we first train a neural network to provide risk
assessments for heart failure patients. We then design a RL-based clinical
decision-support system (DSS) around the neural network model, which can learn
from its interactions with users. We conduct an experiment involving a diverse
set of clinicians from multiple institutions in three different countries. Our
results demonstrate that ML experts cannot accurately predict which system
outputs will maximize clinicians' confidence in the underlying neural network
model, and suggest additional findings that have broad implications to the
future of research into ML interpretability and the use of ML in medicine.},
 author = {Lahav, Owen and Mastronarde, Nicholas and Schaar, Mihaela van der},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {11},
 title = {What is Interpretable? Using Machine Learning to Design Interpretable
Decision-Support Systems},
 url = {http://arxiv.org/pdf/1811.10799v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
 abstract = {The need for interpretable and accountable intelligent system gets sensible
as artificial intelligence plays more role in human life. Explainable
artificial intelligence systems can be a solution by self-explaining the
reasoning behind the decisions and predictions of the intelligent system.
Researchers from different disciplines work together to define, design and
evaluate interpretable intelligent systems for the user. Our work supports the
different evaluation goals in interpretable machine learning research by a
thorough review of evaluation methodologies used in machine-explanation
research across the fields of human-computer interaction, visual analytics, and
machine learning. We present a 2D categorization of interpretable machine
learning evaluation methods and show a mapping between user groups and
evaluation measures. Further, we address the essential factors and steps for a
right evaluation plan by proposing a nested model for design and evaluation of
explainable artificial intelligence systems.},
 author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {11},
 title = {A Survey of Evaluation Methods and Measures for Interpretable Machine
Learning},
 url = {http://arxiv.org/pdf/1811.11839v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1901.06560v1,
 abstract = {There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don't explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs' use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.},
 author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
 comments = {, },
 journal = {arxiv},
 month = {1},
 title = {Explaining Explanations to Society},
 url = {http://arxiv.org/pdf/1901.06560v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09813v1,
 abstract = {Word embeddings generated by neural network methods such as word2vec (W2V)
are well known to exhibit seemingly linear behaviour, e.g. the embeddings of
analogy "woman is to queen as man is to king" approximately describe a
parallelogram. This property is particularly intriguing since the embeddings
are not trained to achieve it. Several explanations have been proposed, but
each introduces assumptions that do not hold in practice. We derive a
probabilistically grounded definition of paraphrasing and show it can be
re-interpreted as word transformation, a mathematical description of "$w_x$ is
to $w_y$". From these concepts we prove existence of the linear relationship
between W2V-type embeddings that underlies the analogical phenomenon, and
identify explicit error terms in the relationship.},
 author = {Allen, Carl and Hospedales, Timothy},
 comments = {, },
 journal = {arxiv},
 month = {1},
 title = {Analogies Explained: Towards Understanding Word Embeddings},
 url = {http://arxiv.org/pdf/1901.09813v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02041v1,
 abstract = {We ask whether the neural network interpretation methods can be fooled via
adversarial model manipulation, which is defined as a model fine-tuning step
that aims to radically alter the explanations without hurting the accuracy of
the original model. By incorporating the interpretation results directly in the
regularization term of the objective function for fine-tuning, we show that the
state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled
with our model manipulation. We propose two types of fooling, passive and
active, and demonstrate such foolings generalize well to the entire validation
set as well as transfer to other interpretation methods. Our results are
validated by both visually showing the fooled explanations and reporting
quantitative metrics that measure the deviations from the original
explanations. We claim that the stability of neural network interpretation
method with respect to our adversarial model manipulation is an important
criterion to check for developing robust and reliable neural network
interpretation method.},
 author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
 comments = {, },
 journal = {arxiv},
 month = {2},
 title = {Fooling Neural Network Interpretations via Adversarial Model
Manipulation},
 url = {http://arxiv.org/pdf/1902.02041v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
 abstract = {Despite a growing literature on explaining neural networks, no consensus has
been reached on how to explain a neural network decision or how to evaluate an
explanation. In fact, most works rely on manually assessing the explanation to
evaluate the quality of a method. This injects uncertainty in the explanation
process along several dimensions: Which explanation method to apply? Who should
we ask to evaluate it and which criteria should be used for the evaluation? Our
contributions in this paper are twofold. First, we investigate schemes to
combine explanation methods and reduce model uncertainty to obtain a single
aggregated explanation. Our findings show that the aggregation is more robust,
well-aligned with human explanations and can attribute relevance to a broader
set of features (completeness). Second, we propose a novel way of evaluating
explanation methods that circumvents the need for manual evaluation and is not
reliant on the alignment of neural networks and humans decision processes.},
 author = {Rieger, Laura and Hansen, Lars Kai},
 comments = {, Does not describe the used explainability method},
 journal = {arxiv},
 month = {3},
 title = {Aggregating explainability methods for neural networks stabilizes
explanations},
 url = {http://arxiv.org/pdf/1903.00519v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
 abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently.
Explainability is being presented as a remedy for lack of trust in model
predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise
instance level interpretability for any complex machine learning model. But how
certain are these explanations? Can we rely on additive explanations for
non-additive models? In this paper, we examine the behavior of model explainers
under the presence of interactions. We define two sources of uncertainty, model
level uncertainty, and explanation level uncertainty. We show that adding
interactions reduces explanation level uncertainty. We introduce a new method
iBreakDown that generates non-additive explanations with local interaction.},
 author = {Gosiewska, Alicja and Biecek, Przemyslaw},
 comments = {, The described method is neither general, nor focused on NLP},
 journal = {arxiv},
 month = {3},
 title = {iBreakDown: Uncertainty of Model Explanations for Non-additive
Predictive Models},
 url = {http://arxiv.org/pdf/1903.11420v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.05488v1,
 abstract = {Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved
test accuracy for all models it has been applied to.},
 author = {Tao, Sean},
 comments = {, },
 journal = {arxiv},
 month = {4},
 title = {Deep Neural Network Ensembles},
 url = {http://arxiv.org/pdf/1904.05488v1},
 year = {2019}
}

@article{Israelsen:2019:XAY:3303862.3267338,
 acmid = {3267338},
 address = {New York, NY, USA},
 articleno = {113},
 author = {Israelsen, Brett W. and Ahmed, Nisar R.},
 comments = {, },
 doi = {10.1145/3267338},
 issn = {0360-0300},
 issue_date = {February 2019},
 journal = {ACM Comput. Surv.},
 keywords = {Human-computer trust, accountability, algorithmic assurances, explainable artificial intelligence, fairness, interpretable machine learning, transparency},
 month = {January},
 number = {6},
 numpages = {37},
 pages = {113:1--113:37},
 publisher = {ACM},
 title = {\&\#x201C;Dave...I Can Assure You ...That It\&\#x2019;s Going to Be All Right ...\&\#x201D; A Definition, Case for, and Survey of Algorithmic Assurances in Human-Autonomy Trust Relationships},
 url = {http://doi.acm.org/10.1145/3267338},
 volume = {51},
 year = {2019}
}

@incollection{kochGroupCognitionCollaborative2018,
 abstract = {Significant advances in artificial intelligence suggest that we will be using intelligent agents on a regular basis in the near future. This chapter discusses group cognition as a principle for designing collaborative AI. Group cognition is the ability to relate to other group members' decisions, abilities, and beliefs. It thereby allows participants to adapt their understanding and actions to reach common objectives. Hence, it underpins collaboration. We review two concepts in the context of group cognition that could inform the development of AI and automation in pursuit of natural collaboration with humans: conversational grounding and theory of mind. These concepts are somewhat different from those already discussed in AI research. We outline some new implications for collaborative AI, aimed at extending skills and solution spaces and at improving joint cognitive and creative capacity.},
 address = {Cham},
 author = {Koch, Janin and Oulasvirta, Antti},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 comments = {, },
 doi = {10.1007/978-3-319-90403-0_15},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {293-312},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Group {{Cognition}} and {{Collaborative AI}}},
 year = {2018}
}

@inproceedings{lisboaInterpretabilityMachineLearning2013,
 abstract = {Theoretical advances in machine learning have been reflected in many research implementations including in safety-critical domains such as medicine. However this has not been reflected in a large number of practical applications used by domain experts. This bottleneck is in a significant part due to lack of interpretability of the non-linear models derived from data. This lecture will review five broad categories of interpretability in machine learning - nomograms, rule induction, fuzzy logic, graphical models \& topographic mapping. Links between the different approaches will be made around the common theme of designing interpretability into the structure of machine learning models, then using the armoury of advanced analytical methods to achieve generic non-linear approximation capabilities.},
 author = {Lisboa, P. J. G.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 comments = {Reviews the current state of explainability research, Is not scientific literature},
 editor = {Masulli, Francesco and Pasi, Gabriella and Yager, Ronald},
 isbn = {978-3-319-03200-9},
 keywords = {Fuzzy Logic,Latent Variable Model,Machine Learning Model,Predictive Inference,Rule Induction},
 language = {en},
 pages = {15-21},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Interpretability in {{Machine Learning}} \textendash{} {{Principles}} and {{Practice}}},
 year = {2013}
}

@incollection{liuInterpretabilityComputationalModels2016,
 abstract = {Sentiment analysis, which is also known as opinion mining, has been an increasingly popular research area focusing on sentiment classification/regression. In many studies, computational models have been considered as effective and efficient tools for sentiment analysis . Computational models could be built by using expert knowledge or learning from data. From this viewpoint, the design of computational models could be categorized into expert based design and data based design. Due to the vast and rapid increase in data, the latter approach of design has become increasingly more popular for building computational models. A data based design typically follows machine learning approaches, each of which involves a particular strategy of learning. Therefore, the resulting computational models are usually represented in different forms. For example, neural network learning results in models in the form of multi-layer perceptron network whereas decision tree learning results in a rule set in the form of decision tree. On the basis of above description, interpretability has become a main problem that arises with computational models. This chapter explores the significance of interpretability for computational models as well as analyzes the factors that impact on interpretability. This chapter also introduces several ways to evaluate and improve the interpretability for computational models which are used as sentiment analysis systems. In particular, rule based systems , a special type of computational models, are used as an example for illustration with respects to evaluation and improvements through the use of computational intelligence methodologies.},
 address = {Cham},
 author = {Liu, Han and Cocea, Mihaela and Gegov, Alexander},
 booktitle = {Sentiment {{Analysis}} and {{Ontology Engineering}}: {{An Environment}} of {{Computational Intelligence}}},
 comments = {, Does not describe the used explainability method},
 doi = {10.1007/978-3-319-30319-2_9},
 editor = {Pedrycz, Witold and Chen, Shyi-Ming},
 isbn = {978-3-319-30319-2},
 keywords = {Computational intelligence,Fuzzy computational models,Interpretability analysis,Interpretability evaluation,Machine learning,Rule based networks,Rule based systems,Sentiment prediction},
 language = {en},
 pages = {199-220},
 publisher = {{Springer International Publishing}},
 series = {Studies in {{Computational Intelligence}}},
 title = {Interpretability of {{Computational Models}} for {{Sentiment Analysis}}},
 year = {2016}
}

@incollection{lughoferModelExplanationInterpretation2018,
 abstract = {We propose two directions for stimulating advanced human-machine interaction in machine learning systems. The first direction acts on a local level by suggesting a reasoning process why certain model decisions/predictions have been made for current sample queries. It may help to better understand how the model behaves and to support humans for providing more consistent and certain feedbacks. A practical example from visual inspection of production items underlines higher human labeling consistency. The second direction acts on a global level by addressing several criteria which are necessary for a good interpretability of the whole model. By meeting the criteria, the likelihood increases (1) of gaining more funded insights into the behavior of the system, and (2) of stimulating advanced expert/operators feedback in form of active manipulations of the model structure. Possibilities how to best integrate different types of advanced feedback in combination with (on-line) data using incremental model updates will be discussed. This leads to a new, hybrid interactive model building paradigm, which is based on subjective knowledge versus objective data and thus integrates the ``expert-in-the-loop'' aspect.},
 address = {Cham},
 author = {Lughofer, Edwin},
 booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
 comments = {, },
 doi = {10.1007/978-3-319-90403-0_10},
 editor = {Zhou, Jianlong and Chen, Fang},
 isbn = {978-3-319-90403-0},
 language = {en},
 pages = {177-221},
 publisher = {{Springer International Publishing}},
 series = {Human\textendash{{Computer Interaction Series}}},
 title = {Model {{Explanation}} and {{Interpretation Concepts}} for {{Stimulating Advanced Human}}-{{Machine Interaction}} with ``{{Expert}}-in-the-{{Loop}}''},
 year = {2018}
}

@incollection{neukartReverseEngineeringMind2017,
 abstract = {Within this chapter all the requirements for reverse engineering the mind based on the knowledge imparted in the previous chapters will be discussed, and open questions attempted to be solved. A suitable theory of mind that on one side may not be the whole truth from a philosophical point of view, but serves as a valid foundation from an engineering point of view on the other side is introduced. Furthermore, as I indicated more than once, I am of the opinion that both quantum physics as well as self-organization occupy the most important roles in how our brain works and lets us experience conscious content and again, it is required to plunge into the information theoretical approach to quantum physics, quantum computer science.},
 address = {Wiesbaden},
 author = {Neukart, Florian},
 booktitle = {Reverse {{Engineering}} the {{Mind}}: {{Consciously Acting Machines}} and {{Accelerated Evolution}}},
 comments = {, },
 doi = {10.1007/978-3-658-16176-7_10},
 editor = {Neukart, Florian},
 isbn = {978-3-658-16176-7},
 keywords = {Artificial Neural Network,Hide Markov Model,Quantum Computer,Reverse Engineering,Semantic Network},
 language = {en},
 pages = {237-354},
 publisher = {{Springer Fachmedien Wiesbaden}},
 series = {{{AutoUni}} \textendash{} {{Schriftenreihe}}},
 title = {Reverse Engineering the Mind},
 year = {2017}
}

@incollection{nissanNarrativesFormalismComputational2014,
 abstract = {We recapitulate four decades of computational processing of narratives. Vladimir Propp's work in the 1920s paved the way to both the structuralists' approach to the folktale and to narratives in general, and the story grammars approach to automate story-processing. In the latter domain, grammar-driven processing was overtaken by goal-driven processing, but there has been a comeback of story grammars, in combination with other devices. Propp's concern was with Russian folktales, and some story-generation programs are relevant indeed for folktale studies: such is the case of the programs TALE-SPIN and Joseph, which reportedly generated fables; MINSTREL generated Arthurian tales. Sometimes, bugs reveal more than proper functioning does, about the actual underlying model. Automated story processing, within artificial intelligence, showed important results since the late 1970s. After slowing down during the 1990s, since the turn of the century the field resurged, especially in the perspective of virtual environments and interactive narratives, also benefiting from the popularity of computer models of the emotions.},
 address = {Berlin, Heidelberg},
 author = {Nissan, Ephraim},
 booktitle = {Language, {{Culture}}, {{Computation}}. {{Computing}} of the {{Humanities}}, {{Law}}, and {{Narratives}}: {{Essays Dedicated}} to {{Yaacov Choueka}} on the {{Occasion}} of {{His}} 75th {{Birthday}}, {{Part II}}},
 comments = {, },
 doi = {10.1007/978-3-642-45324-3_11},
 editor = {Dershowitz, Nachum and Nissan, Ephraim},
 isbn = {978-3-642-45324-3},
 keywords = {Belief Revision,Computational Linguistics,Computational Tool,Computer Science Department,Natural Language Processing},
 language = {en},
 pages = {270-393},
 publisher = {{Springer Berlin Heidelberg}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Narratives, {{Formalism}}, {{Computational Tools}}, and {{Nonlinearity}}},
 year = {2014}
}

@inproceedings{potapenkoInterpretableProbabilisticEmbeddings2018,
 abstract = {We consider probabilistic topic models and more recent word embedding techniques from a perspective of learning hidden semantic representations. Inspired by a striking similarity of the two approaches, we merge them and learn probabilistic embeddings with online EM-algorithm on word co-occurrence data. The resulting embeddings perform on par with Skip-Gram Negative Sampling (SGNS) on word similarity tasks and benefit in the interpretability of the components. Next, we learn probabilistic document embeddings that outperform paragraph2vec on a document similarity task and require less memory and time for training. Finally, we employ multimodal Additive Regularization of Topic Models (ARTM) to obtain a high sparsity and learn embeddings for other modalities, such as timestamps and categories. We observe further improvement of word similarity performance and meaningful inter-modality similarities.},
 author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
 booktitle = {Artificial {{Intelligence}} and {{Natural Language}}},
 comments = {, },
 editor = {Filchenkov, Andrey and Pivovarova, Lidia and {\v Z}i{\v z}ka, Jan},
 isbn = {978-3-319-71746-3},
 language = {en},
 pages = {167-180},
 publisher = {{Springer International Publishing}},
 series = {Communications in {{Computer}} and {{Information Science}}},
 shorttitle = {Interpretable {{Probabilistic Embeddings}}},
 title = {Interpretable {{Probabilistic Embeddings}}: {{Bridging}} the {{Gap Between Topic Models}} and {{Neural Networks}}},
 year = {2018}
}

@article{schubbachJudgingMachinesPhilosophical2019,
 abstract = {Although machine learning has been successful in recent years and is increasingly being deployed in the sciences, enterprises or administrations, it has rarely been discussed in philosophy beyond the philosophy of mathematics and machine learning. The present contribution addresses the resulting lack of conceptual tools for an epistemological discussion of machine learning by conceiving of deep learning networks as `judging machines' and using the Kantian analysis of judgments for specifying the type of judgment they are capable of. At the center of the argument is the fact that the functionality of deep learning networks is established by training and cannot be explained and justified by reference to a predefined rule-based procedure. Instead, the computational process of a deep learning network is barely explainable and needs further justification, as is shown in reference to the current research literature. Thus, it requires a new form of justification, that is to be specified with the help of Kant's epistemology.},
 author = {Schubbach, Arno},
 comments = {, },
 doi = {10.1007/s11229-019-02167-z},
 issn = {1573-0964},
 journal = {Synthese},
 keywords = {Algorithm,Artificial intelligence,Computation,Deep learning,Explanation,Judgment,Justification,Kant,Machine learning},
 language = {en},
 month = {March},
 shorttitle = {Judging Machines},
 title = {Judging Machines: Philosophical Aspects of Deep Learning},
 year = {2019}
}

@inproceedings{Schuessler:2019:MEC:3290607.3312823,
 acmid = {3312823},
 address = {New York, NY, USA},
 articleno = {LBW2810},
 author = {Schuessler, Martin and Wei\ss, Philipp},
 booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
 comments = {, },
 doi = {10.1145/3290607.3312823},
 isbn = {978-1-4503-5971-9},
 keywords = {deep neural networks, explanations, image classification, interpretable machine learning},
 location = {Glasgow, Scotland Uk},
 numpages = {6},
 pages = {LBW2810:1--LBW2810:6},
 publisher = {ACM},
 series = {CHI EA '19},
 title = {Minimalistic Explanations: Capturing the Essence of Decisions},
 url = {http://doi.acm.org/10.1145/3290607.3312823},
 year = {2019}
}

@inproceedings{silvaComplementaryExplanationsUsing2018,
 abstract = {Interpretability is a fundamental property for the acceptance of machine learning models in highly regulated areas. Recently, deep neural networks gained the attention of the scientific community due to their high accuracy in vast classification problems. However, they are still seen as black-box models where it is hard to understand the reasons for the labels that they generate. This paper proposes a deep model with monotonic constraints that generates complementary explanations for its decisions both in terms of style and depth. Furthermore, an objective framework for the evaluation of the explanations is presented. Our method is tested on two biomedical datasets and demonstrates an improvement in relation to traditional models in terms of quality of the explanations generated.},
 author = {Silva, Wilson and Fernandes, Kelwin and Cardoso, Maria J. and Cardoso, Jaime S.},
 booktitle = {Understanding and {{Interpreting Machine Learning}} in {{Medical Image Computing Applications}}},
 comments = {, The described method is neither general, nor focused on NLP},
 editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and {Maier-Hein}, Lena and Marquand, Andre F. and Duchesnay, Edouard and L\"ofstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
 isbn = {978-3-030-02628-8},
 keywords = {Aesthetics evaluation,Deep neural networks,Dermoscopy,Explanations,Interpretable machine learning},
 language = {en},
 pages = {133-140},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Towards {{Complementary Explanations Using Deep Neural Networks}}},
 year = {2018}
}

@inproceedings{Strobel:2018:AAE:3278721.3278788,
 acmid = {3278788},
 address = {New York, NY, USA},
 author = {Strobel, Martin},
 booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3278721.3278788},
 isbn = {978-1-4503-6012-8},
 keywords = {axiomatic approach, explainable machine learning},
 location = {New Orleans, LA, USA},
 numpages = {2},
 pages = {380--381},
 publisher = {ACM},
 series = {AIES '18},
 title = {An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract},
 url = {http://doi.acm.org/10.1145/3278721.3278788},
 year = {2018}
}

@inproceedings{Wang:2019:DTU:3290605.3300831,
 acmid = {3300831},
 address = {New York, NY, USA},
 articleno = {601},
 author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 comments = {, The described method is neither general, nor focused on NLP},
 doi = {10.1145/3290605.3300831},
 isbn = {978-1-4503-5970-2},
 keywords = {clinical decision making, decision making, explainable artificial intelligence, explanations, intelligibility},
 location = {Glasgow, Scotland Uk},
 numpages = {15},
 pages = {601:1--601:15},
 publisher = {ACM},
 series = {CHI '19},
 title = {Designing Theory-Driven User-Centric Explainable AI},
 url = {http://doi.acm.org/10.1145/3290605.3300831},
 year = {2019}
}

@incollection{wodeckiInfluenceArtificialIntelligence2019,
 abstract = {The previous chapter was devoted to the most significant concepts, methods and technologies of artificial intelligence (AI). This gives grounds for the presentation of influence which these systems might have on the contemporary organizations and markets.},
 address = {Cham},
 author = {Wodecki, Andrzej},
 booktitle = {Artificial {{Intelligence}} in {{Value Creation}}: {{Improving Competitive Advantage}}},
 comments = {, The publication does not focus on explainability},
 doi = {10.1007/978-3-319-91596-8_3},
 editor = {Wodecki, Andrzej},
 isbn = {978-3-319-91596-8},
 language = {en},
 pages = {133-246},
 publisher = {{Springer International Publishing}},
 title = {Influence of {{Artificial Intelligence}} on {{Activities}} and {{Competitiveness}} of an {{Organization}}},
 year = {2019}
}

@inproceedings{Wolf:2019:EST:3301275.3302317,
 acmid = {3302317},
 address = {New York, NY, USA},
 author = {Wolf, Christine T.},
 booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
 comments = {, Does not describe the used explainability method},
 doi = {10.1145/3301275.3302317},
 isbn = {978-1-4503-6272-6},
 keywords = {XAI, aging-in-place, explainability scenarios, scenario-based design},
 location = {Marina del Ray, California},
 numpages = {6},
 pages = {252--257},
 publisher = {ACM},
 series = {IUI '19},
 title = {Explainability Scenarios: Towards Scenario-based XAI Design},
 url = {http://doi.acm.org/10.1145/3301275.3302317},
 year = {2019}
}

@article{yankovskayaTradeoffSearchMethods2017,
 abstract = {This paper starts a brief historical overview of occurrence and development of fuzzy systems and their applications. Integration methods are proposed to construct a fuzzy system using other AI methods, achieving synergy effect. Accuracy and interpretability are selected as main properties of rule-based fuzzy systems. The tradeoff between interpretability and accuracy is considered to be the actual problem. The purpose of this paper is the in-depth study of the methods and tools to achieve a tradeoff for accuracy and interpretability in rule-based fuzzy systems and to describe our interpretability indexes. A comparison of the existing ways of interpretability estimation has been made We also propose the new way to construct heuristic interpretability indexes as a quantitative measure of interpretability. In the main part of this paper we describe previously used approaches, the current state and original authors' methods for achieving tradeoff between accuracy and complexity.},
 author = {Yankovskaya, A. E. and Gorbunov, I. V. and Hodashinsky, I. A.},
 comments = {, Does not describe the used explainability method},
 doi = {10.1134/S1054661817020134},
 issn = {1555-6212},
 journal = {Pattern Recognition and Image Analysis},
 keywords = {accuracy,fuzzy modelling,fuzzy system,interpretability,interpretability-accuracy tradeoff,machine learning,metaheuristic,pattern recognition,synergy},
 language = {en},
 month = {April},
 number = {2},
 pages = {243-265},
 title = {Tradeoff Search Methods between Interpretability and Accuracy of the Identification Fuzzy Systems Based on Rules},
 volume = {27},
 year = {2017}
}

@inproceedings{zhouMeasuringInterpretabilityDifferent2018,
 abstract = {The interpretability of a machine learning model plays a significant role in practical applications, thus it is necessary to develop a method to compare the interpretability for different models so as to select the most appropriate one. However, model interpretability, a highly subjective concept, is difficult to be accurately measured, not to mention the interpretability comparison of different models. To this end, we develop an interpretability evaluation model to compute model interpretability and compare interpretability for different models. Specifically, first we we present a general form of model interpretability. Second, a questionnaire survey system is developed to collect information about users' understanding of a machine learning model. Next, three structure features are selected to investigate the relationship between interpretability and structural complexity. After this, an interpretability label is build based on the questionnaire survey result and a linear regression model is developed to evaluate the relationship between the structural features and model interpretability. The experiment results demonstrate that our interpretability evaluation model is valid and reliable to evaluate the interpretability of different models.},
 author = {Zhou, Qing and Liao, Fenglu and Mou, Chao and Wang, Ping},
 booktitle = {Trends and {{Applications}} in {{Knowledge Discovery}} and {{Data Mining}}},
 comments = {, Does not describe the used explainability method},
 editor = {Ganji, Mohadeseh and Rashidi, Lida and Fung, Benjamin C. M. and Wang, Can},
 isbn = {978-3-030-04503-6},
 keywords = {Interpretability evaluation model,Machine learning models,Model interpretability,Structural complexity},
 language = {en},
 pages = {295-308},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Measuring {{Interpretability}} for {{Different Types}} of {{Machine Learning Models}}},
 year = {2018}
}

