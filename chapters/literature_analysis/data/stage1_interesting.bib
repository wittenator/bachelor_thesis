@article{750572,
 abstract = {Presents a methodology for detection of neural-network gaps (NNGs) based on error analysis and the visualization that is applicable to the n-dimensional I/O domain. The generalization problem in artificial neural networks (ANN) training is analyzed and the concept of NNGs is introduced. The NNGs are highly undesirable in ANN generalization and methods for detecting, analyzing, and eliminating them are necessary. Previous methods for NNG detection, based on two-dimensional (2-D) and three dimensional (3-D) visualization, were not applicable for ANNs with more than three inputs. Experiments demonstrate advantages of this new methodology, which allows better understanding of the NNG phenomena using a quantitative approach.},
 author = {M. M. {Kantardzic} and A. A. {Aly} and A. S. {Elmaghraby}},
 comments = {, },
 doi = {10.1109/72.750572},
 issn = {1045-9227},
 journal = {IEEE Transactions on Neural Networks},
 keywords = {neural nets;generalisation (artificial intelligence);learning (artificial intelligence);error analysis;neural-network gaps;error analysis;visualization;n-dimensional I/O domain;generalization problem;quantitative approach;Visualization;Error analysis;Artificial neural networks;Testing;Neural networks;Two dimensional displays;Particle measurements;Computer networks;Performance evaluation;Supervised learning},
 month = {March},
 number = {2},
 pages = {419-426},
 title = {Visualization of neural-network gaps based on error analysis},
 volume = {10},
 year = {1999}
}

@article{7552539,
 abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
 author = {W. {Samek} and A. {Binder} and G. {Montavon} and S. {Lapuschkin} and K. {Müller}},
 comments = {Reviews the current state of explainability research, The described method is neither general, nor focused on NLP},
 doi = {10.1109/TNNLS.2016.2599820},
 issn = {2162-237X},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 keywords = {data visualisation;image classification;learning (artificial intelligence);neural nets;sensitivity-based approach;deconvolution method;heatmap;deep neural network;complex machine learning tasks;multilayer nonlinear structure;DNN;MIT Places data sets;SUN397;ILSVRC2012;relevance propagation algorithm;data visualization;Heating;Neurons;Biological neural networks;Deconvolution;Sensitivity;Learning systems;Algorithm design and analysis;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models},
 month = {Nov},
 number = {11},
 pages = {2660-2673},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 volume = {28},
 year = {2017}
}

@inproceedings{7557899,
 abstract = {Artificial Intelligence (AI) has infiltrated almost every scientific and social endeavour, including everything from medical research to the sociology of crowd control. But the foundation of AI continues to be based on digital representations of knowledge, and computational reasoning therewith. Because so much of modern knowledge infrastructure and social behaviour is connected to AI, understanding the role of AI in each such endeavour not only helps accelerate progress in those fields in which it applies, but also creates the challenges to extend the foundation for modern AI methods. The simple hypothesis herein is that so-called AI-complete problems have a role in helping to articulate the appropriate integration of AI within other disciplines. With the current growth of interest in "big data" and visualization, we argue that relatively simple formal structures provide a basis for the claim that visualization is an AI-complete problem. The value of confirming this claim is largely to encourage stronger formalizations of the visualization process in terms of the AI foundations of representation and reasoning. This connection will help ensure that relevant components of AI are appropriately applied and integrated, to provide value for a basis of a theory of visualization. The sketch of this claim here is based on the simple idea that visualization is an abstraction process, and that abstractions from partial information, however voluminous, directly confronts the non monotonic reasoning challenge, thus the need for caution in engineering visualization systems without carefully considering the consequences of visual abstraction. This is particularly important with interactive visualization, which has recently formed the basis for such fields as visual analytics.},
 author = {R. {Goebel}},
 booktitle = {2016 20th International Conference Information Visualisation (IV)},
 comments = {, },
 doi = {10.1109/IV.2016.53},
 issn = {2375-0138},
 keywords = {artificial intelligence;Big Data;data analysis;data structures;data visualisation;AI-complete problem;artificial intelligence;digital knowledge representations;computational reasoning;modern knowledge infrastructure;social behaviour;AI methods;AI integration;Big Data;formal structures;visualization process formalization;partial information abstraction process;monotonic reasoning challenge;engineering visualization systems;visual abstraction;interactive visualization;visual analytics;Data visualization;Visualization;Artificial intelligence;Context;Complexity theory;Cognition;Semantics;AI-complete visualization incomplete knowledge},
 month = {July},
 number = {},
 pages = {27-32},
 title = {Why Visualization is an AI-complete Problem (and Why That Matters)},
 volume = {},
 year = {2016}
}

@inproceedings{7801719,
 abstract = {Multi-document summarization is to create summaries covering the major information that multiple documents tell in common. For this point, the existing methods are based on hand-crafted features for word and sentence. However, it is difficult to figure out the core contents of each document with the hand-crafted features because they have the limited information presented the given documents. Moreover, there exists a limit to figure out the major information because documents with the same meaning used to be paraphrased depending on their writers. Therefore, it is necessary to represent the semantic meanings of documents as well as sentences through understanding natural language. In this paper, we propose a new multi-document summarization system by creating a synthetic document vector covering the whole documents based on Language Model, whose is well-known for learning the semantic features in text. We experimented with DUC 2004 dataset provided by Document Understanding Conference (DUC) and the results show that our method summarizes multiple documents effectively based on their core contents.},
 author = {D. {Kim} and J. {Lee}},
 booktitle = {2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)},
 comments = {, },
 doi = {10.1109/SCIS-ISIS.2016.0132},
 issn = {},
 keywords = {document handling;natural language processing;word processing;multidocument summarization system;semantic text feature learning;DUC 2004 dataset;document understanding conference;natural language model;semantic document meanings;hand-crafted features;synthetic document vector;Semantics;Context;Hidden Markov models;Computational modeling;Redundancy;Intelligent systems;Natural languages;Multi-document summarization;Core content;Major Information;Synthetic document vector;Language model},
 month = {Aug},
 number = {},
 pages = {605-609},
 title = {Multi-document Summarization by Creating Synthetic Document Vector Based on Language Model},
 volume = {},
 year = {2016}
}

@inproceedings{7846290,
 abstract = {Machine Learning (ML) techniques have allowed a great performance improvement of different challenging Spoken Language Understanding (SLU) tasks. Among these methods, Neural Networks (NN), or Multilayer Perceptron (MLP), recently received a great interest from researchers due to their representation capability of complex internal structures in a low dimensional subspace. However, MLPs employ document representations based on basic word level or topic-based features. Therefore, these basic representations reveal little in way of document statistical structure by only considering words or topics contained in the document as a “bag-of-words”, ignoring relations between them. We propose to remedy this weakness by extending the complex features based on Quaternion algebra presented in [1] to neural networks called QMLP. This original QMLP approach is based on hyper-complex algebra to take into consideration features dependencies in documents. New document features, based on the document structure itself, used as input of the QMLP, are also investigated in this paper, in comparison to those initially proposed in [1]. Experiments made on a SLU task from a real framework of human spoken dialogues showed that our QMLP approach associated with the proposed document features outperforms other approaches, with an accuracy gain of 2% with respect to the MLP based on real numbers and more than 3% with respect to the first Quaternion-based features proposed in [1]. We finally demonstrated that less iterations are needed by our QMLP architecture to be efficient and to reach promising accuracies.},
 author = {T. {Parcollet} and M. {Morchid} and P. {Bousquet} and R. {Dufour} and G. {Linarès} and R. {De Mori}},
 booktitle = {2016 IEEE Spoken Language Technology Workshop (SLT)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SLT.2016.7846290},
 issn = {},
 keywords = {algebra;document handling;learning (artificial intelligence);multilayer perceptrons;natural language processing;quaternion neural networks;machine learning;spoken language understanding;SLU;multilayer perceptron;MLP;document representations;document statistical structure;quaternion algebra;Quaternions;Computational modeling;Neurons;Artificial neural networks;Algebra;Natural language processing;Quaternion;Neural Network;Spoken Language Understanding;Machine Learning},
 month = {Dec},
 number = {},
 pages = {362-368},
 title = {Quaternion Neural Networks for Spoken Language Understanding},
 volume = {},
 year = {2016}
}

@inproceedings{7960721,
 abstract = {The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.},
 author = {İ. Ü. {Oğul} and C. {Özcan} and Ö. {Hakdağlı}},
 booktitle = {2017 25th Signal Processing and Communications Applications Conference (SIU)},
 comments = {, The publication does not focus on explainability},
 doi = {10.1109/SIU.2017.7960721},
 issn = {},
 keywords = {Bayes methods;data analysis;distributed processing;Internet of Things;learning (artificial intelligence);pattern classification;public domain software;social networking (online);text analysis;IoT;open source social news site;Reddit;interpretation data;data analysis;fast data storage;Hadoop architecture;distributed data structure;distributed in-memory data collection;Apache Spark architecture;Naive Bayes method;machine learning;meaning extraction;irrelevant data deletion;Internet of Things;fast text classification;Sparks;Java;Internet of Things;Standards;Text categorization;Art;Machine learning;Text mining;Big data;Apache Spark;Classification;Naive Bayes},
 month = {May},
 number = {},
 pages = {1-4},
 title = {Fast text classification with Naive Bayes method on Apache Spark},
 volume = {},
 year = {2017}
}

@inproceedings{8260658,
 abstract = {Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
 author = {K. {Kowsari} and D. E. {Brown} and M. {Heidarysafa} and K. {Jafari Meimandi} and M. S. {Gerber} and L. E. {Barnes}},
 booktitle = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2017.0-134},
 issn = {},
 keywords = {data mining;learning (artificial intelligence);neural nets;pattern classification;text analysis;document hierarchy;HDLTex;Hierarchical Deep;Text classification;document collections;information processing methods;supervised learning;multiclass classification;hierarchical classification;deep learning architectures;supervised classifiers;text organization;document classification methods;Machine learning;Support vector machines;Computer architecture;Kernel;Mathematical model;Recurrent neural networks;Text Mining;Document Classification;Deep Neural Networks;Hierarchical Learning;Deep Learning},
 month = {Dec},
 number = {},
 pages = {364-371},
 title = {HDLTex: Hierarchical Deep Learning for Text Classification},
 volume = {},
 year = {2017}
}

@inproceedings{8268978,
 abstract = {Deep Neural Networks (DNN) received a great interest from researchers due to their capability to construct robust abstract representations of heterogeneous documents in a latent subspace. Nonetheless, mere real-valued deep neural networks require an appropriate adaptation, such as the convolution process, to capture latent relations between input features. Moreover, real-valued deep neural networks reveal little in way of document internal dependencies, by only considering words or topics contained in the document as an isolate basic element. Quaternion-valued multi-layer perceptrons (QMLP), and autoencoders (QAE) have been introduced to capture such latent dependencies, alongside to represent multidimensional data. Nonetheless, a three-layered neural network does not benefit from the high abstraction capability of DNNs. The paper proposes first to extend the hyper-complex algebra to deep neural networks (QDNN) and, then, introduces pre-trained deep quaternion neural networks (QDNN-AE) with dedicated quaternion encoder-decoders (QAE). The experiments conduced on a theme identification task of spoken dialogues from the DECODA data set show, inter alia, that the QDNN-AE reaches a promising gain of 2.2% compared to the standard real-valued DNN-AE.},
 author = {T. {Parcollet} and M. {Morchid} and G. {Linarès}},
 booktitle = {2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 comments = {, },
 doi = {10.1109/ASRU.2017.8268978},
 issn = {},
 keywords = {learning (artificial intelligence);multilayer perceptrons;natural language processing;speech recognition;real-valued deep neural networks;document internal dependencies;neural network;deep quaternion neural networks;spoken language understanding;quaternion-valued multilayer perceptrons;QMLP;autoencoders;QAE;spoken dialogues;Quaternions;Speech;Task analysis;Telephone sets;Algebra;Biological neural networks;Quaternions;deep neural networks;spoken language understanding;autoencoders;machine learning},
 month = {Dec},
 number = {},
 pages = {504-511},
 title = {Deep quaternion neural networks for spoken language understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8275810,
 abstract = {In text categorization, feature representation for dimensionality reduction is a key step. Usually, some commonly used methods, e.g., latent semantic analysis (LSA), yield a dense representation or a dense transformation matrix, which is difficult to precisely characterize the document-topic or the topic-word relationship. This paper proposes a novel discriminative topic sparse representation (DTSR) approach for text categorization, in which two stages are included: the topic dictionary construction and sparse representation. Firstly, a discriminative and interpretable dictionary is constructed to characterize the topic-word relationship. The dictionary contains all category center vectors as well as some semantic topic vectors generated by a latent Dirichlet allocation (LDA) model. Furthermore, each document can be represented with a sparse form to obtain a good document-topic relationship. Experimental results on well-known benchmark datasets indicate that the proposed method not only achieves a satisfactory classification performance but also provides a reasonable sparse semantic meaningful.},
 author = {W. {Zheng} and Y. {Liu} and H. {Lu} and H. {Tang}},
 booktitle = {2017 10th International Symposium on Computational Intelligence and Design (ISCID)},
 comments = {, },
 doi = {10.1109/ISCID.2017.54},
 issn = {2473-3547},
 keywords = {learning (artificial intelligence);pattern classification;text analysis;vectors;semantic topic vectors;latent Dirichlet allocation model;text categorization;feature representation;dimensionality reduction;latent semantic analysis;dense transformation matrix;topic-word relationship;topic dictionary construction;discriminative dictionary;interpretable dictionary;document-topic relationship;discriminative topic sparse representation approach;Semantics;Dictionaries;Text categorization;Training;Sparse matrices;Large scale integration;Resource management;Text Categorization;Sparse Representation;Topic;Discriminative;Semantic},
 month = {Dec},
 number = {},
 pages = {454-457},
 title = {Discriminative Topic Sparse Representation for Text Categorization},
 volume = {1},
 year = {2017}
}

@inproceedings{8310088,
 abstract = {As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.},
 author = {R. {Raoui-Outach} and C. {Million-Rousseau} and A. {Benoit} and P. {Lambert}},
 booktitle = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
 comments = {, },
 doi = {10.1109/IPTA.2017.8310088},
 issn = {2154-512X},
 keywords = {data analysis;document image processing;feedforward neural nets;learning (artificial intelligence);sales management;text analysis;Deep learning;automatic sale receipt understanding;data analytics;scanned document analysis;paper damages;scanning quality;industrial context;automatic understanding;essential consumption statistics;accurate consumption statistics;smart-phone;tool chain;essential information;store brand;purchased products;related prices;highest possible confidence;high confidence level;tool-chain;Deep Convolutional Neural Networks;classical image;text processings;double check processing;text analysis;Optical character recognition software;Character recognition;Semantics;Text analysis;Task analysis;Object detection;Machine learning;Receipt image understanding;Deep Convolutional Neural Networks;Object Detection;Semantic Analysis},
 month = {Nov},
 number = {},
 pages = {1-6},
 title = {Deep learning for automatic sale receipt understanding},
 volume = {},
 year = {2017}
}

@inproceedings{8490433,
 abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.},
 author = {J. {Zhu} and A. {Liapis} and S. {Risi} and R. {Bidarra} and G. M. {Youngblood}},
 booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
 comments = {, },
 doi = {10.1109/CIG.2018.8490433},
 issn = {2325-4289},
 keywords = {computer games;human computer interaction;learning (artificial intelligence);game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework;Games;Task analysis;Machine learning;Neurons;Visualization;Tools;explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design},
 month = {Aug},
 number = {},
 pages = {1-8},
 title = {Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation},
 volume = {},
 year = {2018}
}

@inproceedings{8538416,
 abstract = {In the age of knowledge, Natural Language Processing (NLP) express its demand by a huge range of utilization. Previously NLP was dealing with statically data. Contemporary time NLP is doing considerably with the corpus, lexicon database, pattern reorganization. Considering Deep Learning (DL) method recognize artificial Neural Network (NN) to nonlinear process, NLP tools become increasingly accurate and efficient that begin a debacle. Multi-Layer Neural Network obtaining the importance of the NLP for its capability including standard speed and resolute output. Hierarchical designs of data operate recurring processing layers to learn and with this arrangement of DL methods manage several practices. In this paper, this resumed striving to reach a review of the tools and the necessary methodology to present a clear understanding of the association of NLP and DL for truly understand in the training. Efficiency and execution both are improved in NLP by Part of speech tagging (POST), Morphological Analysis, Named Entity Recognition (NER), Semantic Role Labeling (SRL), Syntactic Parsing, and Coreference resolution. Artificial Neural Networks (ANN), Time Delay Neural Networks (TDNN), Recurrent Neural Network (RNN), Convolution Neural Networks (CNN), and Long-Short-Term-Memory (LSTM) dealings among Dense Vector (DV), Windows Approach (WA), and Multitask learning (MTL) as a characteristic of Deep Learning. After statically methods, when DL communicate the influence of NLP, the individual form of the NLP process and DL rule collaboration was started a fundamental connection.},
 author = {S. A. {Fahad} and A. E. {Yahya}},
 booktitle = {2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)},
 comments = {, },
 doi = {10.1109/ICSCEE.2018.8538416},
 issn = {},
 keywords = {learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;natural language processing;contemporary time NLP;Considering Deep;(DL) method;artificial Neural Network;nonlinear process;NLP tools;MultiLayer Neural Network;processing layers;DL methods;Artificial Neural Networks;Time Delay Neural Networks;Recurrent Neural Network;Convolution Neural Networks;deep learning;NLP process;Natural language processing;Artificial neural networks;Tagging;Semantics;Task analysis;-Deep Learning;Natural language processing;Deep nural Network;Multitask Learning},
 month = {July},
 number = {},
 pages = {1-4},
 title = {Inflectional Review of Deep Learning on Natural Language Processing},
 volume = {},
 year = {2018}
}

@inproceedings{8614130,
 abstract = {Explainability/Interpretability in machine learning applications is becoming critical, with legal and industry requirements demanding human understandable machine learning results. We describe the additional complexities that occur when a known interpretability technique (canary models) is applied to a real production scenario. We furthermore argue that reproducibility is a key feature in practical usages of such interpretability techniques in production scenarios. With this motivation, we present a production ML reproducibility solution, namely a comprehensive time ordered event sequence for machine learning applications. We demonstrate how our approach can bring this known common interpretability technique into production viability. We further present the system design and early performance characteristics of our reproducibility solution.},
 author = {S. {Ghanta} and S. {Subramanian} and S. {Sundararaman} and L. {Khermosh} and V. {Sridhar} and D. {Arteaga} and Q. {Luo} and D. {Das} and N. {Talagala}},
 booktitle = {2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 comments = {, },
 doi = {10.1109/ICMLA.2018.00105},
 issn = {},
 keywords = {learning (artificial intelligence);production engineering computing;production scenario;interpretability techniques;production ML reproducibility solution;production viability;reproducability;production machine learning applications;legal industry requirements;human understandable machine learning;Production;Pipelines;Predictive models;Machine learning;Training;Data models;Load modeling;reproducability;explainability;interpretability;systems;tracking},
 month = {Dec},
 number = {},
 pages = {658-664},
 title = {Interpretability and Reproducability in Production Machine Learning Applications},
 volume = {},
 year = {2018}
}

@inproceedings{8622433,
 abstract = {Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.},
 author = {B. {Kovalerchuk} and N. {Neuhaus}},
 booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
 comments = {, },
 doi = {10.1109/BigData.2018.8622433},
 issn = {},
 keywords = {learning (artificial intelligence);pattern classification;domain-explainable ML models;dominance classifier and predictor algorithm;DCP algorithm;unintelligible black box models;interpretable machine learning;interpretable mathematical forms;noninterpretable models;interpretable methods;human-understandable machine learning models;complex multidimensional relations;human perceptual limitations;white box models;interpretable grey box models;Classification algorithms;Prediction algorithms;Machine learning;Mathematical model;Machine learning algorithms;Computational modeling;Neural networks;machine learning;explainability;interpretability;accuracy;classifier;visualization;visual model;dominant intervals},
 month = {Dec},
 number = {},
 pages = {4940-4947},
 title = {Toward Efficient Automation of Interpretable Machine Learning},
 volume = {},
 year = {2018}
}

@inproceedings{953860,
 abstract = {We propose a geometric method for document image processing. This research focuses on document understanding and classification by applying the Winnow algorithm, an online machine learning method. This application makes the document image processing more flexible with various kind of documents since the meaningful knowledge can be extracted from training examples and the model for document type can be updated when there is a new example. This research aims to analyze and classify scientific papers. We conduct the experiments on documents from the proceedings of various conferences to show the performance of the proposed method. The experimental results are compared with the WISDOM++ system and also show the advantages of using the online machine learning method.},
 author = {C. {Nattee} and M. {Numao}},
 booktitle = {Proceedings of Sixth International Conference on Document Analysis and Recognition},
 comments = {, },
 doi = {10.1109/ICDAR.2001.953860},
 issn = {},
 keywords = {document image processing;image retrieval;pattern classification;real-time systems;learning systems;document image processing;document understanding;pattern classification;geometric method;Winnow algorithm;machine learning;scientific papers;image retrieval;real time systems;Machine learning;Machine learning algorithms;Learning systems;Document image processing;Text analysis;Information analysis;Logic;Computer science;Electronic mail;Application software},
 month = {Sep.},
 number = {},
 pages = {602-606},
 title = {Geometric method for document understanding and classification using online machine learning},
 volume = {},
 year = {2001}
}

@inproceedings{alonsoZadehComputingWords2019,
 abstract = {The European Commission has identified Artificial Intelligence (AI) as the ``most strategic technology of the 21st century'' [7].},
 author = {Alonso, Jose M.},
 booktitle = {Fuzzy {{Logic}} and {{Applications}}},
 comments = {, },
 editor = {Full\'er, Robert and Giove, Silvio and Masulli, Francesco},
 isbn = {978-3-030-12544-8},
 keywords = {Cointension,Computing with perceptions,Computing with words,Explainable AI,Fuzzy Logic,Interpretable fuzzy systems},
 language = {en},
 pages = {244-248},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {From {{Zadeh}}'s {{Computing}} with {{Words Towards eXplainable Artificial Intelligence}}},
 year = {2019}
}

