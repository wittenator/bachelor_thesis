@article{http://arxiv.org/abs/1401.5390v1,
 abstract = {Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization.},
 author = {Branavan, S. R. K. and Silver, David and Barzilay, Regina},
 journal = {arxiv},
 month = {1},
 title = {Learning to Win by Reading Manuals in a Monte-Carlo Framework},
 url = {http://arxiv.org/pdf/1401.5390v1},
 year = {2014}
}

@article{http://arxiv.org/abs/1602.04938v3,
 abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 journal = {arxiv},
 month = {2},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {http://arxiv.org/pdf/1602.04938v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1604.00289v3,
 abstract = {Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.},
 author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
 journal = {arxiv},
 month = {4},
 title = {Building Machines That Learn and Think Like People},
 url = {http://arxiv.org/pdf/1604.00289v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.03490v3,
 abstract = {Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.},
 author = {Lipton, Zachary C.},
 journal = {arxiv},
 month = {6},
 title = {The Mythos of Model Interpretability},
 url = {http://arxiv.org/pdf/1606.03490v3},
 year = {2016}
}

@article{http://arxiv.org/abs/1606.05386v1,
 abstract = {Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 journal = {arxiv},
 month = {6},
 title = {Model-Agnostic Interpretability of Machine Learning},
 url = {http://arxiv.org/pdf/1606.05386v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1608.08974v2,
 abstract = {Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.},
 author = {Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
 journal = {arxiv},
 month = {8},
 title = {Towards Transparent AI Systems: Interpreting Visual Question Answering
  Models},
 url = {http://arxiv.org/pdf/1608.08974v2},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07270v1,
 abstract = {Understanding neural networks is becoming increasingly important. Over the
last few years different types of visualisation and explanation methods have
been proposed. However, none of them explicitly considered the behaviour in the
presence of noise and distracting elements. In this work, we will show how
noise and distracting dimensions can influence the result of an explanation
model. This gives a new theoretical insights to aid selection of the most
appropriate explanation model within the deep-Taylor decomposition framework.},
 author = {Kindermans, Pieter-Jan and Schütt, Kristof and Müller, Klaus-Robert and Dähne, Sven},
 journal = {arxiv},
 month = {11},
 title = {Investigating the influence of noise and distractors on the
  interpretation of neural networks},
 url = {http://arxiv.org/pdf/1611.07270v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07567v1,
 abstract = {Complex problems may require sophisticated, non-linear learning methods such
as kernel machines or deep neural networks to achieve state of the art
prediction accuracies. However, high prediction accuracies are not the only
objective to consider when solving problems using machine learning. Instead,
particular scientific applications require some explanation of the learned
prediction function. Unfortunately, most methods do not come with out of the
box straight forward interpretation. Even linear prediction functions are not
straight forward to explain if features exhibit complex correlation structure.
  In this paper, we propose the Measure of Feature Importance (MFI). MFI is
general and can be applied to any arbitrary learning machine (including kernel
machines and deep learning). MFI is intrinsically non-linear and can detect
features that by itself are inconspicuous and only impact the prediction
function through their interaction with other features. Lastly, MFI can be used
for both --- model-based feature importance and instance-based feature
importance (i.e, measuring the importance of a feature for a particular data
point).},
 author = {Vidovic, Marina M. -C. and Görnitz, Nico and Müller, Klaus-Robert and Kloft, Marius},
 journal = {arxiv},
 month = {11},
 title = {Feature Importance Measure for Non-linear Learning Algorithms},
 url = {http://arxiv.org/pdf/1611.07567v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1611.07634v1,
 abstract = {State of the art machine learning algorithms are highly optimized to provide
the optimal prediction possible, naturally resulting in complex models. While
these models often outperform simpler more interpretable models by order of
magnitudes, in terms of understanding the way the model functions, we are often
facing a "black box".
  In this paper we suggest a simple method to interpret the behavior of any
predictive model, both for regression and classification. Given a particular
model, the information required to interpret it can be obtained by studying the
partial derivatives of the model with respect to the input. We exemplify this
insight by interpreting convolutional and multi-layer neural networks in the
field of natural language processing.},
 author = {Hechtlinger, Yotam},
 journal = {arxiv},
 month = {11},
 title = {Interpretation of Prediction Models Using the Input Gradient},
 url = {http://arxiv.org/pdf/1611.07634v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1612.07843v1,
 abstract = {Text documents can be described by a number of abstract concepts such as
semantic category, writing style, or sentiment. Machine learning (ML) models
have been trained to automatically map documents to these abstract concepts,
allowing to annotate very large text collections, more than could be processed
by a human in a lifetime. Besides predicting the text's category very
accurately, it is also highly desirable to understand how and why the
categorization process takes place. In this paper, we demonstrate that such
understanding can be achieved by tracing the classification decision back to
individual words using layer-wise relevance propagation (LRP), a recently
developed technique for explaining predictions of complex non-linear
classifiers. We train two word-based ML models, a convolutional neural network
(CNN) and a bag-of-words SVM classifier, on a topic categorization task and
adapt the LRP method to decompose the predictions of these models onto words.
Resulting scores indicate how much individual words contribute to the overall
classification decision. This enables one to distill relevant information from
text documents without an explicit semantic information extraction step. We
further use the word-wise relevance scores for generating novel vector-based
document representations which capture semantic information. Based on these
document vectors, we introduce a measure of model explanatory power and show
that, although the SVM and CNN models perform similarly in terms of
classification accuracy, the latter exhibits a higher level of explainability
which makes it more comprehensible for humans and potentially more useful for
other applications.},
 author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {12},
 title = {"What is Relevant in a Text Document?": An Interpretable Machine
  Learning Approach},
 url = {http://arxiv.org/pdf/1612.07843v1},
 year = {2016}
}

@article{http://arxiv.org/abs/1702.08635v1,
 abstract = {Machine learning is essentially the sciences of playing with data. An
adaptive data selection strategy, enabling to dynamically choose different data
at various training stages, can reach a more effective model in a more
efficient way. In this paper, we propose a deep reinforcement learning
framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter}
(\textbf{NDF}), to explore automatic and adaptive data selection in the
training process. In particular, NDF takes advantage of a deep neural network
to adaptively select and filter important data instances from a sequential
stream of training data, such that the future accumulative reward (e.g., the
convergence speed) is maximized. In contrast to previous studies in data
selection that is mainly based on heuristic strategies, NDF is quite generic
and thus can be widely suitable for many machine learning tasks. Taking neural
network training with stochastic gradient descent (SGD) as an example,
comprehensive experiments with respect to various neural network modeling
(e.g., multi-layer perceptron networks, convolutional neural networks and
recurrent neural networks) and several applications (e.g., image classification
and text understanding) demonstrate that NDF powered SGD can achieve comparable
accuracy with standard SGD process by using less data and fewer iterations.},
 author = {Fan, Yang and Tian, Fei and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
 journal = {arxiv},
 month = {2},
 title = {Learning What Data to Learn},
 url = {http://arxiv.org/pdf/1702.08635v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1703.06914v2,
 abstract = {In the modern era, each Internet user leaves enormous amounts of auxiliary
digital residuals (footprints) by using a variety of on-line services. All this
data is already collected and stored for many years. In recent works, it was
demonstrated that it's possible to apply simple machine learning methods to
analyze collected digital footprints and to create psycho-demographic profiles
of individuals. However, while these works clearly demonstrated the
applicability of machine learning methods for such an analysis, created simple
prediction models still lacks accuracy necessary to be successfully applied for
practical needs. We have assumed that using advanced deep machine learning
methods may considerably increase the accuracy of predictions. We started with
simple machine learning methods to estimate basic prediction performance and
moved further by applying advanced methods based on shallow and deep neural
networks. Then we compared prediction power of studied models and made
conclusions about its performance. Finally, we made hypotheses how prediction
accuracy can be further improved. As result of this work, we provide full
source code used in the experiments for all interested researchers and
practitioners in corresponding GitHub repository. We believe that applying deep
machine learning for psycho-demographic profiling may have an enormous impact
on the society (for good or worse) and provides means for Artificial
Intelligence (AI) systems to better understand humans by creating their
psychological profiles. Thus AI agents may achieve the human-like ability to
participate in conversation (communication) flow by anticipating human
opponents' reactions, expectations, and behavior.},
 author = {Omelianenko, Iaroslav},
 journal = {arxiv},
 month = {3},
 title = {Applying Deep Machine Learning for psycho-demographic profiling of
  Internet users using O.C.E.A.N. model of personality},
 url = {http://arxiv.org/pdf/1703.06914v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1704.03296v3,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks "look" in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.},
 author = {Fong, Ruth and Vedaldi, Andrea},
 journal = {arxiv},
 month = {4},
 title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
 url = {http://arxiv.org/pdf/1704.03296v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1705.06824v2,
 abstract = {Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
"CNN Inception + Gate" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.},
 author = {Wang, Zhengyang and Ji, Shuiwang},
 journal = {arxiv},
 month = {5},
 title = {Learning Convolutional Text Representations for Visual Question
  Answering},
 url = {http://arxiv.org/pdf/1705.06824v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07206v2,
 abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.},
 author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {6},
 title = {Explaining Recurrent Neural Network Predictions in Sentiment Analysis},
 url = {http://arxiv.org/pdf/1706.07206v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1706.07979v1,
 abstract = {This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.},
 author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 journal = {arxiv},
 month = {6},
 title = {Methods for Interpreting and Understanding Deep Neural Networks},
 url = {http://arxiv.org/pdf/1706.07979v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1707.09641v2,
 abstract = {The predictive power of neural networks often costs model interpretability.
Several techniques have been developed for explaining model outputs in terms of
input features; however, it is difficult to translate such interpretations into
actionable insight. Here, we propose a framework to analyze predictions in
terms of the model's internal features by inspecting information flow through
the network. Given a trained network and a test image, we select neurons by two
metrics, both measured over a set of images created by perturbations to the
input image: (1) magnitude of the correlation between the neuron activation and
the network output and (2) precision of the neuron activation. We show that the
former metric selects neurons that exert large influence over the network
output while the latter metric selects neurons that activate on generalizable
features. By comparing the sets of neurons selected by these two metrics, our
framework suggests a way to investigate the internal attention mechanisms of
convolutional neural networks.},
 author = {Lengerich, Benjamin J. and Konam, Sandeep and Xing, Eric P. and Rosenthal, Stephanie and Veloso, Manuela},
 journal = {arxiv},
 month = {7},
 title = {Towards Visual Explanations for Convolutional Neural Networks via Input
  Resampling},
 url = {http://arxiv.org/pdf/1707.09641v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1708.04988v1,
 abstract = {We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.},
 author = {Assya, Trofimov and Sebastien, Lemieux and Claude, Perreault},
 journal = {arxiv},
 month = {8},
 title = {Warp: a method for neural network interpretability applied to gene
  expression profiles},
 url = {http://arxiv.org/pdf/1708.04988v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1708.08296v1,
 abstract = {With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.},
 author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
 journal = {arxiv},
 month = {8},
 title = {Explainable Artificial Intelligence: Understanding, Visualizing and
  Interpreting Deep Learning Models},
 url = {http://arxiv.org/pdf/1708.08296v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.04806v2,
 abstract = {Deep neural networks are widely used for classification. These deep models
often suffer from a lack of interpretability -- they are particularly difficult
to understand because of their non-linear nature. As a result, neural networks
are often treated as "black box" models, and in the past, have been trained
purely to optimize the accuracy of predictions. In this work, we create a novel
network architecture for deep learning that naturally explains its own
reasoning for each prediction. This architecture contains an autoencoder and a
special prototype layer, where each unit of that layer stores a weight vector
that resembles an encoded training input. The encoder of the autoencoder allows
us to do comparisons within the latent space, while the decoder allows us to
visualize the learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be similar to at least
one encoded input, a term that encourages every encoded input to be close to at
least one prototype, and a term that encourages faithful reconstruction by the
autoencoder. The distances computed in the prototype layer are used as part of
the classification process. Since the prototypes are learned during training,
the learned network naturally comes with explanations for each prediction, and
the explanations are loyal to what the network actually computes.},
 author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
 journal = {arxiv},
 month = {10},
 title = {Deep Learning for Case-Based Reasoning through Prototypes: A Neural
  Network that Explains Its Predictions},
 url = {http://arxiv.org/pdf/1710.04806v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.09511v2,
 abstract = {Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.},
 author = {Barratt, Shane},
 journal = {arxiv},
 month = {10},
 title = {InterpNET: Neural Introspection for Interpretable Deep Learning},
 url = {http://arxiv.org/pdf/1710.09511v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.10777v1,
 abstract = {Recurrent neural networks (RNNs) have been successfully applied to various
natural language processing (NLP) tasks and achieved better results than
conventional methods. However, the lack of understanding of the mechanisms
behind their effectiveness limits further improvements on their architectures.
In this paper, we present a visual analytics method for understanding and
comparing RNN models for NLP tasks. We propose a technique to explain the
function of individual hidden state units based on their expected response to
input texts. We then co-cluster hidden state units and words based on the
expected response and visualize co-clustering results as memory chips and word
clouds to provide more structured knowledge on RNNs' hidden states. We also
propose a glyph-based sequence visualization based on aggregate information to
analyze the behavior of an RNN's hidden state at the sentence-level. The
usability and effectiveness of our method are demonstrated through case studies
and reviews from domain experts.},
 author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
 journal = {arxiv},
 month = {10},
 title = {Understanding Hidden Memories of Recurrent Neural Networks},
 url = {http://arxiv.org/pdf/1710.10777v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1710.10967v3,
 abstract = {Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy
network" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its "reinforcement-learning value
network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.},
 author = {Igami, Mitsuru},
 journal = {arxiv},
 month = {10},
 title = {Artificial Intelligence as Structural Estimation: Economic
  Interpretations of Deep Blue, Bonanza, and AlphaGo},
 url = {http://arxiv.org/pdf/1710.10967v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.00404v1,
 abstract = {As data-driven methods rise in popularity in materials science applications,
a key question is how these machine learning models can be used to understand
microstructure. Given the importance of process-structure-property relations
throughout materials science, it seems logical that models that can leverage
microstructural data would be more capable of predicting property information.
While there have been some recent attempts to use convolutional neural networks
to understand microstructural images, these early studies have focused only on
which featurizations yield the highest machine learning model accuracy for a
single data set. This paper explores the use of convolutional neural networks
for classifying microstructure with a more holistic set of objectives in mind:
generalization between data sets, number of features required, and
interpretability.},
 author = {Ling, Julia and Hutchinson, Maxwell and Antono, Erin and DeCost, Brian and Holm, Elizabeth A. and Meredig, Bryce},
 journal = {arxiv},
 month = {11},
 title = {Building Data-driven Models with Microstructural Images: Generalization
  and Interpretability},
 url = {http://arxiv.org/pdf/1711.00404v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.06431v2,
 abstract = {We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.},
 author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
 journal = {arxiv},
 month = {11},
 title = {Using KL-divergence to focus Deep Visual Explanation},
 url = {http://arxiv.org/pdf/1711.06431v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1711.09482v2,
 abstract = {The practical impact of deep learning on complex supervised learning problems
has been significant, so much so that almost every Artificial Intelligence
problem, or at least a portion thereof, has been somehow recast as a deep
learning problem. The applications appeal is significant, but this appeal is
increasingly challenged by what some call the challenge of explainability, or
more generally the more traditional challenge of debuggability: if the outcomes
of a deep learning process produce unexpected results (e.g., less than expected
performance of a classifier), then there is little available in the way of
theories or tools to help investigate the potential causes of such unexpected
behavior, especially when this behavior could impact people's lives. We
describe a preliminary framework to help address this issue, which we call
"deep visual explanation" (DVE). "Deep," because it is the development and
performance of deep neural network models that we want to understand. "Visual,"
because we believe that the most rapid insight into a complex multi-dimensional
model is provided by appropriate visualization techniques, and "Explanation,"
because in the spectrum from instrumentation by inserting print statements to
the abductive inference of explanatory hypotheses, we believe that the key to
understanding deep learning relies on the identification and exposure of
hypotheses about the performance behavior of a learned deep model. In the
exposition of our preliminary framework, we use relatively straightforward
image classification examples and a variety of choices on initial configuration
of a deep model building scenario. By careful but not complicated
instrumentation, we expose classification outcomes of deep models using
visualization, and also show initial results for one potential application of
interpretability.},
 author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
 journal = {arxiv},
 month = {11},
 title = {An Introduction to Deep Visual Explanation},
 url = {http://arxiv.org/pdf/1711.09482v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.02034v2,
 abstract = {Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2vec, a deep RNN
that automatically learns features from SMILES to predict chemical properties,
without the need for additional explicit feature engineering. Using Bayesian
optimization methods to tune the network architecture, we show that an
optimized SMILES2vec model can serve as a general-purpose neural network for
predicting distinct chemical properties including toxicity, activity,
solubility and solvation energy, while also outperforming contemporary MLP
neural networks that uses engineered features. Furthermore, we demonstrate
proof-of-concept of interpretability by developing an explanation mask that
localizes on the most important characters used in making a prediction. When
tested on the solubility dataset, it identified specific parts of a chemical
that is consistent with established first-principles knowledge with an accuracy
of 88%. Our work demonstrates that neural networks can learn technically
accurate chemical concept and provide state-of-the-art accuracy, making
interpretable deep neural networks a useful tool of relevance to the chemical
industry.},
 author = {Goh, Garrett B. and Hodas, Nathan O. and Siegel, Charles and Vishnu, Abhinav},
 journal = {arxiv},
 month = {12},
 title = {SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for
  Predicting Chemical Properties},
 url = {http://arxiv.org/pdf/1712.02034v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.06302v3,
 abstract = {Interpretation and explanation of deep models is critical towards wide
adoption of systems that rely on them. In this paper, we propose a novel scheme
for both interpretation as well as explanation in which, given a pretrained
model, we automatically identify internal features relevant for the set of
classes considered by the model, without relying on additional annotations. We
interpret the model through average visualizations of this reduced set of
features. Then, at test time, we explain the network prediction by accompanying
the predicted class label with supporting visualizations derived from the
identified features. In addition, we propose a method to address the artifacts
introduced by stridded operations in deconvNet-based visualizations. Moreover,
we introduce an8Flower, a dataset specifically designed for objective
quantitative evaluation of methods for visual explanation.Experiments on the
MNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces
detailed explanations with good coverage of relevant features of the classes of
interest},
 author = {Oramas, Jose and Wang, Kaili and Tuytelaars, Tinne},
 journal = {arxiv},
 month = {12},
 title = {Visual Explanation by Interpretation: Improving Visual Feedback
  Capabilities of Deep Neural Networks},
 url = {http://arxiv.org/pdf/1712.06302v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1712.08107v1,
 abstract = {Deep neural network models have been proven to be very successful in image
classification tasks, also for medical diagnosis, but their main concern is its
lack of interpretability. They use to work as intuition machines with high
statistical confidence but unable to give interpretable explanations about the
reported results. The vast amount of parameters of these models make difficult
to infer a rationale interpretation from them. In this paper we present a
diabetic retinopathy interpretable classifier able to classify retine images
into the different levels of disease severity and of explaining its results by
assigning a score for every point in the hidden and input space, evaluating its
contribution to the final classification in a linear way. The generated visual
maps can be interpreted by an expert in order to compare its own knowledge with
the interpretation given by the model.},
 author = {Torre, Jordi de la and Valls, Aida and Puig, Domenec},
 journal = {arxiv},
 month = {12},
 title = {A Deep Learning Interpretable Classifier for Diabetic Retinopathy
  Disease Grading},
 url = {http://arxiv.org/pdf/1712.08107v1},
 year = {2017}
}

@article{http://arxiv.org/abs/1801.05075v1,
 abstract = {In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.},
 author = {Mohseni, Sina and Ragan, Eric D.},
 journal = {arxiv},
 month = {1},
 title = {A Human-Grounded Evaluation Benchmark for Local Explanations of Machine
  Learning},
 url = {http://arxiv.org/pdf/1801.05075v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.06889v3,
 abstract = {Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W's and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.},
 author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
 journal = {arxiv},
 month = {1},
 title = {Visual Analytics in Deep Learning: An Interrogative Survey for the Next
  Frontiers},
 url = {http://arxiv.org/pdf/1801.06889v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1801.09808v1,
 abstract = {Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we
study such linear explanations produced either post-hoc by a few recent methods
or generated along with predictions with contextual explanation networks
(CENs). We focus on two questions: (i) whether linear explanations are always
consistent or can be misleading, and (ii) when integrated into the prediction
process, whether and how explanations affect the performance of the model. Our
analysis sheds more light on certain properties of explanations produced by
different methods and suggests that learning models that explain and predict
jointly is often advantageous.},
 author = {Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric P.},
 journal = {arxiv},
 month = {1},
 title = {The Intriguing Properties of Model Explanations},
 url = {http://arxiv.org/pdf/1801.09808v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00541v1,
 abstract = {Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.},
 author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
 journal = {arxiv},
 month = {2},
 title = {Causal Learning and Explanation of Deep Neural Networks via Autoencoded
  Activations},
 url = {http://arxiv.org/pdf/1802.00541v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00560v2,
 abstract = {Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.},
 author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
 journal = {arxiv},
 month = {2},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 url = {http://arxiv.org/pdf/1802.00560v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.00614v2,
 abstract = {This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles' heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.},
 author = {Zhang, Quanshi and Zhu, Song-Chun},
 journal = {arxiv},
 month = {2},
 title = {Visual Interpretability for Deep Learning: a Survey},
 url = {http://arxiv.org/pdf/1802.00614v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.03043v1,
 abstract = {With the popularity of deep learning (DL), artificial intelligence (AI) has
been applied in many areas of human life. Neural network or artificial neural
network (NN), the main technique behind DL, has been extensively studied to
facilitate computer vision and natural language recognition. However, the more
we rely on information technology, the more vulnerable we are. That is,
malicious NNs could bring huge threat in the so-called coming AI era. In this
paper, for the first time in the literature, we propose a novel approach to
design and insert powerful neural-level trojans or PoTrojan in pre-trained NN
models. Most of the time, PoTrojans remain inactive, not affecting the normal
functions of their host NN models. PoTrojans could only be triggered in very
rare conditions. Once activated, however, the PoTrojans could cause the host NN
models to malfunction, either falsely predicting or classifying, which is a
significant threat to human society of the AI era. We would explain the
principles of PoTrojans and the easiness of designing and inserting them in
pre-trained deep learning models. PoTrojans doesn't modify the existing
architecture or parameters of the pre-trained models, without re-training.
Hence, the proposed method is very efficient.},
 author = {Zou, Minhui and Shi, Yang and Wang, Chengliang and Li, Fangyu and Song, WenZhan and Wang, Yu},
 journal = {arxiv},
 month = {2},
 title = {PoTrojan: powerful neural-level trojan designs in deep learning models},
 url = {http://arxiv.org/pdf/1802.03043v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1802.07384v2,
 abstract = {We present a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU activations
to change its output. We argue that such a correction is a useful way to
provide feedback to a user when the network's output is different from a
desired output. Our algorithm generates such a correction by solving a series
of linear constraint satisfaction problems. The technique is evaluated on three
neural network models: one predicting whether an applicant will pay a mortgage,
one predicting whether a first-order theorem can be proved efficiently by a
solver using certain heuristics, and the final one judging whether a drawing is
an accurate rendition of a canonical drawing of a cat.},
 author = {Zhang, Xin and Solar-Lezama, Armando and Singh, Rishabh},
 journal = {arxiv},
 month = {2},
 title = {Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic
  Corrections},
 url = {http://arxiv.org/pdf/1802.07384v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.04263v3,
 abstract = {Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research.},
 author = {Weld, Daniel S. and Bansal, Gagan},
 journal = {arxiv},
 month = {3},
 title = {The Challenge of Crafting Intelligible Intelligence},
 url = {http://arxiv.org/pdf/1803.04263v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
 abstract = {Issues regarding explainable AI involve four components: users, laws &
regulations, explanations and algorithms. Together these components provide a
context in which explanation methods can be evaluated regarding their adequacy.
The goal of this chapter is to bridge the gap between expert users and lay
users. Different kinds of users are identified and their concerns revealed,
relevant statements from the General Data Protection Regulation are analyzed in
the context of Deep Neural Networks (DNNs), a taxonomy for the classification
of existing explanation methods is introduced, and finally, the various classes
of explanation methods are analyzed to verify if user concerns are justified.
Overall, it is clear that (visual) explanations can be given about various
aspects of the influence of the input on the output. However, it is noted that
explanation methods or interfaces for lay users are missing and we speculate
which criteria these methods / interfaces should satisfy. Finally it is noted
that two important concerns are difficult to address with explanation methods:
the concern about bias in datasets that leads to biased DNNs, as well as the
suspicion about unfair outcomes.},
 author = {Ras, Gabrielle and Gerven, Marcel van and Haselager, Pim},
 journal = {arxiv},
 month = {3},
 title = {Explanation Methods in Deep Learning: Users, Values, Concerns and
  Challenges},
 url = {http://arxiv.org/pdf/1803.07517v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1804.02527v1,
 abstract = {Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions.},
 author = {Choo, Jaegul and Liu, Shixia},
 journal = {arxiv},
 month = {4},
 title = {Visual Analytics for Explainable Deep Learning},
 url = {http://arxiv.org/pdf/1804.02527v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1805.07468v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., explaining knowledge representations hidden in middle conv-layers of the
CNN. Given feature maps of a certain conv-layer of the CNN, the explainer
performs like an auto-encoder, which first disentangles the feature maps into
object-part features and then inverts object-part features back to features of
higher conv-layers of the CNN. More specifically, the explainer contains
interpretable conv-layers, where each filter disentangles the representation of
a specific object part from chaotic input feature maps. As a paraphrase of CNN
features, the disentangled representations of object parts help people
understand the logic inside the CNN. We also learn the explainer to use
object-part features to reconstruct features of higher CNN layers, in order to
minimize loss of information during the feature disentanglement. More
crucially, we learn the explainer via network distillation without using any
annotations of sample labels, object parts, or textures for supervision. We
have applied our method to different types of CNNs for evaluation, and
explainers have significantly boosted the interpretability of CNN features.},
 author = {Zhang, Quanshi and Yang, Yu and Liu, Yuchen and Wu, Ying Nian and Zhu, Song-Chun},
 journal = {arxiv},
 month = {5},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks},
 url = {http://arxiv.org/pdf/1805.07468v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.00069v3,
 abstract = {There has recently been a surge of work in explanatory artificial
intelligence (XAI). This research area tackles the important problem that
complex machines and algorithms often cannot provide insights into their
behavior and thought processes. XAI allows users and parts of the internal
system to be more transparent, providing explanations of their decisions in
some level of detail. These explanations are important to ensure algorithmic
fairness, identify potential bias/problems in the training data, and to ensure
that the algorithms perform as expected. However, explanations produced by
these systems is neither standardized nor systematically assessed. In an effort
to create best practices and identify open challenges, we provide our
definition of explainability and show how it can be used to classify existing
literature. We discuss why current approaches to explanatory methods especially
for deep neural networks are insufficient. Finally, based on our survey, we
conclude with suggested future research directions for explanatory artificial
intelligence.},
 author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 journal = {arxiv},
 month = {5},
 title = {Explaining Explanations: An Overview of Interpretability of Machine
  Learning},
 url = {http://arxiv.org/pdf/1806.00069v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.05337v2,
 abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance
due to their ability to learn complex, non-linear relationships between
variables. However, the inability to effectively visualize these relationships
has led to DNNs being characterized as black boxes and consequently limited
their applications. To ameliorate this problem, we introduce the use of
hierarchical interpretations to explain DNN predictions through our proposed
method, agglomerative contextual decomposition (ACD). Given a prediction from a
trained DNN, ACD produces a hierarchical clustering of the input features,
along with the contribution of each cluster to the final prediction. This
hierarchy is optimized to identify clusters of features that the DNN learned
are predictive. Using examples from Stanford Sentiment Treebank and ImageNet,
we show that ACD is effective at diagnosing incorrect predictions and
identifying dataset bias. Through human experiments, we demonstrate that ACD
enables users both to identify the more accurate of two DNNs and to better
trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to
adversarial perturbations, implying that it captures fundamental aspects of the
input and ignores spurious noise.},
 author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
 journal = {arxiv},
 month = {6},
 title = {Hierarchical interpretations for neural network predictions},
 url = {http://arxiv.org/pdf/1806.05337v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07470v1,
 abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI
(XAI) construct explanations based on the importance of features in
classification tasks. However, in a high-dimensional feature space this
approach may become unfeasible without restraining the set of important
features. We propose to utilize the human tendency to ask questions like "Why
this output (the fact) instead of that output (the foil)?" to reduce the number
of features to those that play a main role in the asked contrast. Our proposed
method utilizes locally trained one-versus-all decision trees to identify the
disjoint set of rules that causes the tree to classify data points as the foil
and not as the fact. In this study we illustrate this approach on three
benchmark classification tasks.},
 author = {Waa, Jasper van der and Robeer, Marcel and Diggelen, Jurriaan van and Brinkhuis, Matthieu and Neerincx, Mark},
 journal = {arxiv},
 month = {6},
 title = {Contrastive Explanations with Local Foil Trees},
 url = {http://arxiv.org/pdf/1806.07470v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.07538v2,
 abstract = {Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.},
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 journal = {arxiv},
 month = {6},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {http://arxiv.org/pdf/1806.07538v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.09809v1,
 abstract = {Natural language explanations of deep neural network decisions provide an
intuitive way for a AI agent to articulate a reasoning process. Current textual
explanations learn to discuss class discriminative features in an image.
However, it is also helpful to understand which attributes might change a
classification decision if present in an image (e.g., "This is not a Scarlet
Tanager because it does not have black wings.") We call such textual
explanations counterfactual explanations, and propose an intuitive method to
generate counterfactual explanations by inspecting which evidence in an input
is missing, but might contribute to a different classification decision if
present in the image. To demonstrate our method we consider a fine-grained
image classification task in which we take as input an image and a
counterfactual class and output text which explains why the image does not
belong to a counterfactual class. We then analyze our generated counterfactual
explanations both qualitatively and quantitatively using proposed automatic
metrics.},
 author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
 journal = {arxiv},
 month = {6},
 title = {Generating Counterfactual Explanations with Natural Language},
 url = {http://arxiv.org/pdf/1806.09809v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.10758v2,
 abstract = {Interpretability methods should be both meaningful to a human and correctly
explain model behavior. In this work, we propose a benchmark to evaluate the
latter. We introduce ROAR, RemOve And Retrain, a formal measure of the relative
accuracy of interpretability methods that estimate feature importance in deep
neural networks. We evaluate commonly used interpretability methods and a set
of recently proposed ensemble-based derivative approaches. Our results across
several large-scale image classification datasets are consistent and
thought-provoking -- we find that the formal methods we consider produce
estimates that are less accurate or on par with a random designation of feature
importance. However, certain derivative approaches that ensemble these
estimates far outperform such a random guess. The manner of ensembling remains
critical, we show that some approaches do no better than the underlying method
but carry a far higher computational burden.},
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 journal = {arxiv},
 month = {6},
 title = {Evaluating Feature Importance Estimates},
 url = {http://arxiv.org/pdf/1806.10758v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.03418v1,
 abstract = {Interpretability of deep neural networks is a recently emerging area of
machine learning research targeting a better understanding of how models
perform feature selection and derive their classification decisions. In this
paper, two neural network architectures are trained on spectrogram and raw
waveform data for audio classification tasks on a newly created audio dataset
and layer-wise relevance propagation (LRP), a previously proposed
interpretability method, is applied to investigate the models' feature
selection and decision making. It is demonstrated that the networks are highly
reliant on feature marked as relevant by LRP through systematic manipulation of
the input data. Our results show that by making deep audio classifiers
interpretable, one can analyze and compare the properties and strategies of
different models beyond classification accuracy, which potentially opens up new
ways for model improvements.},
 author = {Becker, Sören and Ackermann, Marcel and Lapuschkin, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
 journal = {arxiv},
 month = {7},
 title = {Interpreting and Explaining Deep Neural Networks for Classification of
  Audio Signals},
 url = {http://arxiv.org/pdf/1807.03418v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.04178v1,
 abstract = {The Defense Advanced Research Projects Agency (DARPA) recently launched the
Explainable Artificial Intelligence (XAI) program that aims to create a suite
of new AI techniques that enable end users to understand, appropriately trust,
and effectively manage the emerging generation of AI systems.
  In this paper, inspired by DARPA's XAI program, we propose a new paradigm in
security research: Explainable Security (XSec). We discuss the ``Six Ws'' of
XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and
complex characteristics: XSec involves several different stakeholders (i.e.,
the system's developers, analysts, users and attackers) and is multi-faceted by
nature (as it requires reasoning about system model, threat model and
properties of security, privacy and trust as well as about concrete attacks,
vulnerabilities and countermeasures). We define a roadmap for XSec that
identifies several possible research directions.},
 author = {Viganò, Luca and Magazzeni, Daniele},
 journal = {arxiv},
 month = {7},
 title = {Explainable Security},
 url = {http://arxiv.org/pdf/1807.04178v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.},
 author = {Bharadhwaj, Homanga and Joshi, Shruti},
 journal = {arxiv},
 month = {7},
 title = {Explanations for Temporal Recommendations},
 url = {http://arxiv.org/pdf/1807.06161v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06978v1,
 abstract = {An important task for a recommender system to provide interpretable
explanations for the user. This is important for the credibility of the system.
Current interpretable recommender systems tend to focus on certain features
known to be important to the user and offer their explanations in a structured
form. It is well known that user generated reviews and feedback from reviewers
have strong leverage over the users' decisions. On the other hand, recent text
generation works have been shown to generate text of similar quality to human
written text, and we aim to show that generated text can be successfully used
to explain recommendations.
  In this paper, we propose a framework consisting of popular review-oriented
generation models aiming to create personalised explanations for
recommendations. The interpretations are generated at both character and word
levels. We build a dataset containing reviewers' feedback from the Amazon books
review dataset. Our cross-domain experiments are designed to bridge from
natural language processing to the recommender system domain. Besides language
model evaluation methods, we employ DeepCoNN, a novel review-oriented
recommender system using a deep neural network, to evaluate the recommendation
performance of generated reviews by root mean square error (RMSE). We
demonstrate that the synthetic personalised reviews have better recommendation
performance than human written reviews. To our knowledge, this presents the
first machine-generated natural language explanations for rating prediction.},
 author = {Ouyang, Sixun and Lawlor, Aonghus and Costa, Felipe and Dolog, Peter},
 journal = {arxiv},
 month = {7},
 title = {Improving Explainable Recommendations with Synthetic Reviews},
 url = {http://arxiv.org/pdf/1807.06978v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.07404v1,
 abstract = {Predictive geometric models deliver excellent results for many Machine
Learning use cases. Despite their undoubted performance, neural predictive
algorithms can show unexpected degrees of instability and variance,
particularly when applied to large datasets. We present an approach to measure
changes in geometric models with respect to both output consistency and
topological stability. Considering the example of a recommender system using
word2vec, we analyze the influence of single data points, approximation methods
and parameter settings. Our findings can help to stabilize models where needed
and to detect differences in informational value of data points on a large
scale.},
 author = {Regneri, Michaela and Hoffmann, Malte and Kost, Jurij and Pietsch, Niklas and Schulz, Timo and Stamm, Sabine},
 journal = {arxiv},
 month = {7},
 title = {Analyzing Hypersensitive AI: Instability in Corporate-Scale Machine
  Learning},
 url = {http://arxiv.org/pdf/1807.07404v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.01591v1,
 abstract = {Recurrent neural networks (RNNs) are temporal networks and cumulative in
nature that have shown promising results in various natural language processing
tasks. Despite their success, it still remains a challenge to understand their
hidden behavior. In this work, we analyze and interpret the cumulative nature
of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation
(LISA) for explaining decisions and detecting the most likely (i.e., saliency)
patterns that the network relies on while decision making. We demonstrate (1)
LISA: "How an RNN accumulates or builds semantics during its sequential
processing for a given text example and expected response" (2) Example2pattern:
"How the saliency patterns look like for each category in the data according to
the network in decision making". We analyse the sensitiveness of RNNs about
different inputs to check the increase or decrease in prediction scores and
further extract the saliency patterns learned by the network. We employ two
relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to
explain RNN predictions via the LISA and example2pattern.},
 author = {Gupta, Pankaj and Schütze, Hinrich},
 journal = {arxiv},
 month = {8},
 title = {LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse
  Semantic Accumulation and Example to Pattern Transformation},
 url = {http://arxiv.org/pdf/1808.01591v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
 abstract = {PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.},
 author = {Harbecke, David and Schwarzenberg, Robert and Alt, Christoph},
 journal = {arxiv},
 month = {8},
 title = {Learning Explanations from Language Data},
 url = {http://arxiv.org/pdf/1808.04127v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.05054v1,
 abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who
book our next appointment at the hair salon or at that restaurant for dinner -
machine learning systems are becoming increasingly ubiquitous. The main reason
for this is that these methods boast remarkable predictive capabilities.
However, most of these models remain black boxes, meaning that it is very
challenging for humans to follow and understand their intricate inner workings.
Consequently, interpretability has suffered under this ever-increasing
complexity of machine learning models. Especially with regards to new
regulations, such as the General Data Protection Regulation (GDPR), the
necessity for plausibility and verifiability of predictions made by these black
boxes is indispensable. Driven by the needs of industry and practice, the
research community has recognised this interpretability problem and focussed on
developing a growing number of so-called explanation methods over the past few
years. These methods explain individual predictions made by black box machine
learning models and help to recover some of the lost interpretability. With the
proliferation of these explanation methods, it is, however, often unclear,
which explanation method offers a higher explanation quality, or is generally
better-suited for the situation at hand. In this thesis, we thus propose an
axiomatic framework, which allows comparing the quality of different
explanation methods amongst each other. Through experimental validation, we
find that the developed framework is useful to assess the explanation quality
of different explanation methods and reach conclusions that are consistent with
independent research.},
 author = {Honegger, Milo},
 journal = {arxiv},
 month = {8},
 title = {Shedding Light on Black Box Machine Learning Algorithms: Development of
  an Axiomatic Framework to Assess the Quality of Methods that Explain
  Individual Predictions},
 url = {http://arxiv.org/pdf/1808.05054v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.07292v2,
 abstract = {In this paper, we study two challenging problems. The first one is how to
implement \textit{k}-means in the neural network, which enjoys efficient
training based on the stochastic algorithm. The second one is how to enhance
the interpretability of network design for clustering. To solve the problems,
we propose a neural network which is a novel formulation of the vanilla
$k$-means objective. Our contribution is in twofold. From the view of neural
networks, the proposed \textit{k}-meansNet is with explicit interpretability in
neural processing. We could understand not only why the network structure is
presented like itself but also why it could perform data clustering. Such an
interpretable neural network remarkably differs from the existing works that
usually employ visualization technique to explain the result of the neural
network. From the view of \textit{k}-means, three highly desired properties are
achieved, i.e. robustness to initialization, the capability of handling new
coming data, and provable convergence. Extensive experimental studies show that
our method achieves promising performance comparing with 12 clustering methods
on some challenging datasets.},
 author = {Peng, Xi and Tsang, Ivor W. and Zhou, Joey Tianyi and Zhu, Hongyuan},
 journal = {arxiv},
 month = {8},
 title = {k-meansNet: When k-means Meets Differentiable Programming},
 url = {http://arxiv.org/pdf/1808.07292v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09551v1,
 abstract = {Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .},
 author = {Godin, Fréderic and Demuynck, Kris and Dambre, Joni and Neve, Wesley De and Demeester, Thomas},
 journal = {arxiv},
 month = {8},
 title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
  They Discover Linguistic Rules?},
 url = {http://arxiv.org/pdf/1808.09551v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.09744v1,
 abstract = {Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network's performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.},
 author = {Sushil, Madhumita and Šuster, Simon and Daelemans, Walter},
 journal = {arxiv},
 month = {8},
 title = {Rule induction for global explanation of trained models},
 url = {http://arxiv.org/pdf/1808.09744v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1809.02479v1,
 abstract = {Recently machine learning is being applied to almost every data domain one of
which is Question Answering Systems (QAS). A typical Question Answering System
is fairly an information retrieval system, which matches documents or text and
retrieve the most accurate one. The idea of open domain question answering
system put forth, involves convolutional neural network text classifiers. The
Classification model presented in this paper is multi-class text classifier.
The neural network classifier can be trained on large dataset. We report series
of experiments conducted on Convolution Neural Network (CNN) by training it on
two different datasets. Neural network model is trained on top of word
embedding. Softmax layer is applied to calculate loss and mapping of
semantically related words. Gathered results can help justify the fact that
proposed hypothetical QAS is feasible. We further propose a method to integrate
Convolutional Neural Network Classifier to an open domain question answering
system. The idea of Open domain will be further explained, but the generality
of it indicates to the system of domain specific trainable models, thus making
it an open domain.},
 author = {Amin, Muhammad Zain and Nadeem, Noman},
 journal = {arxiv},
 month = {9},
 title = {Convolutional Neural Network: Text Classification Model for Open Domain
  Question Answering System},
 url = {http://arxiv.org/pdf/1809.02479v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1809.08037v1,
 abstract = {We present an analysis into the inner workings of Convolutional Neural
Networks (CNNs) for processing text. CNNs used for computer vision can be
interpreted by projecting filters into image space, but for discrete sequence
inputs CNNs remain a mystery. We aim to understand the method by which the
networks process and classify text. We examine common hypotheses to this
problem: that filters, accompanied by global max-pooling, serve as ngram
detectors. We show that filters may capture several different semantic classes
of ngrams by using different activation patterns, and that global max-pooling
induces behavior which separates important ngrams from the rest. Finally, we
show practical use cases derived from our findings in the form of model
interpretability (explaining a trained model by deriving a concrete identity
for each filter, bridging the gap between visualization tools in vision tasks
and NLP) and prediction interpretability (explaining predictions).},
 author = {Jacovi, Alon and Shalom, Oren Sar and Goldberg, Yoav},
 journal = {arxiv},
 month = {9},
 title = {Understanding Convolutional Neural Networks for Text Classification},
 url = {http://arxiv.org/pdf/1809.08037v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00024v1,
 abstract = {Establishing unique identities for both humans and end systems has been an
active research problem in the security community, giving rise to innovative
machine learning-based authentication techniques. Although such techniques
offer an automated method to establish identity, they have not been vetted
against sophisticated attacks that target their core machine learning
technique. This paper demonstrates that mimicking the unique signatures
generated by host fingerprinting and biometric authentication systems is
possible. We expose the ineffectiveness of underlying machine learning
classification models by constructing a blind attack based around the query
synthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an
attack in under 130 queries on a state-of-the-art face authentication system,
and under 100 queries on a host authentication system. We examine how these
attacks can be defended against and explore their limitations. XAI provides an
effective means for adversaries to infer decision boundaries and provides a new
way forward in constructing attacks against systems using machine learning
models for authentication.},
 author = {Garcia, Washington and Choi, Joseph I. and Adari, Suman K. and Jha, Somesh and Butler, Kevin R. B.},
 journal = {arxiv},
 month = {9},
 title = {Explainable Black-Box Attacks Against Model-based Authentication},
 url = {http://arxiv.org/pdf/1810.00024v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
 abstract = {Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).},
 author = {Ross, Andrew Slavin},
 journal = {arxiv},
 month = {9},
 title = {Training Machine Learning Models by Regularizing their Explanations},
 url = {http://arxiv.org/pdf/1810.00869v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
 abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian
predictive models by projecting the information in the predictive distribution
locally to a simpler, interpretable explanation model. The proposed approach
combines the recent Local Interpretable Model-agnostic Explanations (LIME)
method with ideas from Bayesian projection predictive variable selection
methods. The information theoretic basis helps in navigating the trade-off
between explanation fidelity and complexity. We demonstrate the method in
explaining MNIST digit classifications made by a Bayesian deep convolutional
neural network.},
 author = {Peltola, Tomi},
 journal = {arxiv},
 month = {10},
 title = {Local Interpretable Model-agnostic Explanations of Bayesian Predictive
  Models via Kullback-Leibler Projections},
 url = {http://arxiv.org/pdf/1810.02678v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.09312v1,
 abstract = {Convolutional neural networks have been successfully applied to various NLP
tasks. However, it is not obvious whether they model different linguistic
patterns such as negation, intensification, and clause compositionality to help
the decision-making process. In this paper, we apply visualization techniques
to observe how the model can capture different linguistic features and how
these features can affect the performance of the model. Later on, we try to
identify the model errors and their sources. We believe that interpreting CNNs
is the first step to understand the underlying semantic features which can
raise awareness to further improve the performance and explainability of CNN
models.},
 author = {Koupaee, Mahnaz and Wang, William Yang},
 journal = {arxiv},
 month = {10},
 title = {Analyzing and Interpreting Convolutional Neural Networks in NLP},
 url = {http://arxiv.org/pdf/1810.09312v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13192v4,
 abstract = {The developments of deep neural networks (DNN) in recent years have ushered a
brand new era of artificial intelligence. DNNs are proved to be excellent in
solving very complex problems, e.g., visual recognition and text understanding,
to the extent of competing with or even surpassing people. Despite inspiring
and encouraging success of DNNs, thorough theoretical analyses still lack to
unravel the mystery of their magics. The design of DNN structure is dominated
by empirical results in terms of network depth, number of neurons and
activations. A few of remarkable works published recently in an attempt to
interpret DNNs have established the first glimpses of their internal
mechanisms. Nevertheless, research on exploring how DNNs operate is still at
the initial stage with plenty of room for refinement. In this paper, we extend
precedent research on neural networks with piecewise linear activations (PLNN)
concerning linear regions bounds. We present (i) the exact maximal number of
linear regions for single layer PLNNs; (ii) a upper bound for multi-layer
PLNNs; and (iii) a tighter upper bound for the maximal number of liner regions
on rectifier networks. The derived bounds also indirectly explain why deep
models are more powerful than shallow counterparts, and how non-linearity of
activation functions impacts on expressiveness of networks.},
 author = {Hu, Qiang and Zhang, Hao},
 journal = {arxiv},
 month = {10},
 title = {Nearly-tight bounds on linear regions of piecewise linear neural
  networks},
 url = {http://arxiv.org/pdf/1810.13192v4},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13373v1,
 abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
  We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.},
 author = {Barrett, David G. T. and Morcos, Ari S. and Macke, Jakob H.},
 journal = {arxiv},
 month = {10},
 title = {Analyzing biological and artificial neural networks: challenges with
  opportunities for synergy?},
 url = {http://arxiv.org/pdf/1810.13373v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.13425v2,
 abstract = {Techniques for understanding the functioning of complex machine learning
models are becoming increasingly popular, not only to improve the validation
process, but also to extract new insights about the data via exploratory
analysis. Though a large class of such tools currently exists, most assume that
predictions are point estimates and use a sensitivity analysis of these
estimates to interpret the model. Using lightweight probabilistic networks we
show how including prediction uncertainties in the sensitivity analysis leads
to: (i) more robust and generalizable models; and (ii) a new approach for model
interpretation through uncertainty decomposition. In particular, we introduce a
new regularization that takes both the mean and variance of a prediction into
account and demonstrate that the resulting networks provide improved
generalization to unseen data. Furthermore, we propose a new technique to
explain prediction uncertainties through uncertainties in the input domain,
thus providing new ways to validate and interpret deep learning models.},
 author = {Thiagarajan, Jayaraman J. and Kim, Irene and Anirudh, Rushil and Bremer, Peer-Timo},
 journal = {arxiv},
 month = {10},
 title = {Understanding Deep Neural Networks through Input Uncertainties},
 url = {http://arxiv.org/pdf/1810.13425v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.00196v1,
 abstract = {Building explainable systems is a critical problem in the field of Natural
Language Processing (NLP), since most machine learning models provide no
explanations for the predictions. Existing approaches for explainable machine
learning systems tend to focus on interpreting the outputs or the connections
between inputs and outputs. However, the fine-grained information is often
ignored, and the systems do not explicitly generate the human-readable
explanations. To better alleviate this problem, we propose a novel generative
explanation framework that learns to make classification decisions and generate
fine-grained explanations at the same time. More specifically, we introduce the
explainable factor and the minimum risk training approach that learn to
generate more reasonable explanations. We construct two new datasets that
contain summaries, rating scores, and fine-grained reasons. We conduct
experiments on both datasets, comparing with several strong neural network
baseline systems. Experimental results show that our method surpasses all
baselines on both datasets, and is able to generate concise explanations at the
same time.},
 author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
 journal = {arxiv},
 month = {11},
 title = {Towards Explainable NLP: A Generative Explanation Framework for Text
  Classification},
 url = {http://arxiv.org/pdf/1811.00196v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.02783v1,
 abstract = {We introduce a novel approach to feed-forward neural network interpretation
based on partitioning the space of sequences of neuron activations. In line
with this approach, we propose a model-specific interpretation method, called
YASENN. Our method inherits many advantages of model-agnostic distillation,
such as an ability to focus on the particular input region and to express an
explanation in terms of features different from those observed by a neural
network. Moreover, examination of distillation error makes the method
applicable to the problems with low tolerance to interpretation mistakes.
Technically, YASENN distills the network with an ensemble of layer-wise
gradient boosting decision trees and encodes the sequences of neuron
activations with leaf indices. The finite number of unique codes induces a
partitioning of the input space. Each partition may be described in a variety
of ways, including examination of an interpretable model (e.g. a logistic
regression or a decision tree) trained to discriminate between objects of those
partitions. Our experiments provide an intuition behind the method and
demonstrate revealed artifacts in neural network decision making.},
 author = {Zharov, Yaroslav and Korzhenkov, Denis and Shvechikov, Pavel and Tuzhilin, Alexander},
 journal = {arxiv},
 month = {11},
 title = {YASENN: Explaining Neural Networks via Partitioning Activation Sequences},
 url = {http://arxiv.org/pdf/1811.02783v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.06471v2,
 abstract = {Deep learning adoption in the financial services industry has been limited
due to a lack of model interpretability. However, several techniques have been
proposed to explain predictions made by a neural network. We provide an initial
investigation into these techniques for the assessment of credit risk with
neural networks.},
 author = {Modarres, Ceena and Ibrahim, Mark and Louie, Melissa and Paisley, John},
 journal = {arxiv},
 month = {11},
 title = {Towards Explainable Deep Learning for Credit Lending: A Case Study},
 url = {http://arxiv.org/pdf/1811.06471v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.07253v1,
 abstract = {Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.},
 author = {Xiao, Yijun and Wang, William Yang},
 journal = {arxiv},
 month = {11},
 title = {Quantifying Uncertainties in Natural Language Processing Tasks},
 url = {http://arxiv.org/pdf/1811.07253v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.08120v1,
 abstract = {Latent factor models (LFMs) such as matrix factorization achieve the
state-of-the-art performance among various Collaborative Filtering (CF)
approaches for recommendation. Despite the high recommendation accuracy of
LFMs, a critical issue to be resolved is the lack of explainability. Extensive
efforts have been made in the literature to incorporate explainability into
LFMs. However, they either rely on auxiliary information which may not be
available in practice, or fail to provide easy-to-understand explanations. In
this paper, we propose a fast influence analysis method named FIA, which
successfully enforces explicit neighbor-style explanations to LFMs with the
technique of influence functions stemmed from robust statistics. We first
describe how to employ influence functions to LFMs to deliver neighbor-style
explanations. Then we develop a novel influence computation algorithm for
matrix factorization with high efficiency. We further extend it to the more
general neural collaborative filtering and introduce an approximation algorithm
to accelerate influence analysis over neural network models. Experimental
results on real datasets demonstrate the correctness, efficiency and usefulness
of our proposed method.},
 author = {Cheng, Weiyu and Shen, Yanyan and Zhu, Yanmin and Huang, Linpeng},
 journal = {arxiv},
 month = {11},
 title = {Explaining Latent Factor Models for Recommendation with Influence
  Functions},
 url = {http://arxiv.org/pdf/1811.08120v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.09725v1,
 abstract = {Deep learning is currently playing a crucial role toward higher levels of
artificial intelligence. This paradigm allows neural networks to learn complex
and abstract representations, that are progressively obtained by combining
simpler ones. Nevertheless, the internal "black-box" representations
automatically discovered by current neural architectures often suffer from a
lack of interpretability, making of primary interest the study of explainable
machine learning techniques. This paper summarizes our recent efforts to
develop a more interpretable neural model for directly processing speech from
the raw waveform. In particular, we propose SincNet, a novel Convolutional
Neural Network (CNN) that encourages the first layer to discover more
meaningful filters by exploiting parametrized sinc functions. In contrast to
standard CNNs, which learn all the elements of each filter, only low and high
cutoff frequencies of band-pass filters are directly learned from data. This
inductive bias offers a very compact way to derive a customized filter-bank
front-end, that only depends on some parameters with a clear physical meaning.
Our experiments, conducted on both speaker and speech recognition, show that
the proposed architecture converges faster, performs better, and is more
interpretable than standard CNNs.},
 author = {Ravanelli, Mirco and Bengio, Yoshua},
 journal = {arxiv},
 month = {11},
 title = {Interpretable Convolutional Filters with SincNet},
 url = {http://arxiv.org/pdf/1811.09725v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.10799v1,
 abstract = {Recent efforts in Machine Learning (ML) interpretability have focused on
creating methods for explaining black-box ML models. However, these methods
rely on the assumption that simple approximations, such as linear models or
decision-trees, are inherently human-interpretable, which has not been
empirically tested. Additionally, past efforts have focused exclusively on
comprehension, neglecting to explore the trust component necessary to convince
non-technical experts, such as clinicians, to utilize ML models in practice. In
this paper, we posit that reinforcement learning (RL) can be used to learn what
is interpretable to different users and, consequently, build their trust in ML
models. To validate this idea, we first train a neural network to provide risk
assessments for heart failure patients. We then design a RL-based clinical
decision-support system (DSS) around the neural network model, which can learn
from its interactions with users. We conduct an experiment involving a diverse
set of clinicians from multiple institutions in three different countries. Our
results demonstrate that ML experts cannot accurately predict which system
outputs will maximize clinicians' confidence in the underlying neural network
model, and suggest additional findings that have broad implications to the
future of research into ML interpretability and the use of ML in medicine.},
 author = {Lahav, Owen and Mastronarde, Nicholas and Schaar, Mihaela van der},
 journal = {arxiv},
 month = {11},
 title = {What is Interpretable? Using Machine Learning to Design Interpretable
  Decision-Support Systems},
 url = {http://arxiv.org/pdf/1811.10799v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11705v1,
 abstract = {Despite the growing popularity of modern machine learning techniques (e.g.
Deep Neural Networks) in cyber-security applications, most of these models are
perceived as a black-box for the user. Adversarial machine learning offers an
approach to increase our understanding of these models. In this paper we
present an approach to generate explanations for incorrect classifications made
by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is
used to find the minimum modifications (of the input features) required to
correctly classify a given set of misclassified samples. The magnitude of such
modifications is used to visualize the most relevant features that explain the
reason for the misclassification. The presented methodology generated
satisfactory explanations that describe the reasoning behind the
mis-classifications, with descriptions that match expert knowledge. The
advantages of the presented methodology are: 1) applicable to any classifier
with defined gradients. 2) does not require any modification of the classifier
model. 3) can be extended to perform further diagnosis (e.g. vulnerability
assessment) and gain further understanding of the system. Experimental
evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and
Multilayer perceptron classifiers. The results are shown using intuitive
visualizations in order to improve the interpretability of the results.},
 author = {Marino, Daniel L. and Wickramasinghe, Chathurika S. and Manic, Milos},
 journal = {arxiv},
 month = {11},
 title = {An Adversarial Approach for Explainable AI in Intrusion Detection
  Systems},
 url = {http://arxiv.org/pdf/1811.11705v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.11839v2,
 abstract = {The need for interpretable and accountable intelligent system gets sensible
as artificial intelligence plays more role in human life. Explainable
artificial intelligence systems can be a solution by self-explaining the
reasoning behind the decisions and predictions of the intelligent system.
Researchers from different disciplines work together to define, design and
evaluate interpretable intelligent systems for the user. Our work supports the
different evaluation goals in interpretable machine learning research by a
thorough review of evaluation methodologies used in machine-explanation
research across the fields of human-computer interaction, visual analytics, and
machine learning. We present a 2D categorization of interpretable machine
learning evaluation methods and show a mapping between user groups and
evaluation measures. Further, we address the essential factors and steps for a
right evaluation plan by proposing a nested model for design and evaluation of
explainable artificial intelligence systems.},
 author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
 journal = {arxiv},
 month = {11},
 title = {A Survey of Evaluation Methods and Measures for Interpretable Machine
  Learning},
 url = {http://arxiv.org/pdf/1811.11839v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1811.12615v1,
 abstract = {We propose a possible solution to a public challenge posed by the Fair Isaac
Corporation (FICO), which is to provide an explainable model for credit risk
assessment. Rather than present a black box model and explain it afterwards, we
provide a globally interpretable model that is as accurate as other neural
networks. Our "two-layer additive risk model" is decomposable into subscales,
where each node in the second layer represents a meaningful subscale, and all
of the nonlinearities are transparent. We provide three types of explanations
that are simpler than, but consistent with, the global model. One of these
explanation methods involves solving a minimum set cover problem to find
high-support globally-consistent explanations. We present a new online
visualization tool to allow users to explore the global model and its
explanations.},
 author = {Chen, Chaofan and Lin, Kangcheng and Rudin, Cynthia and Shaposhnik, Yaron and Wang, Sijia and Wang, Tong},
 journal = {arxiv},
 month = {11},
 title = {An Interpretable Model with Globally Consistent Explanations for Credit
  Risk},
 url = {http://arxiv.org/pdf/1811.12615v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
 abstract = {Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque "black boxes". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.},
 author = {Horel, Enguerrand and Mison, Virgile and Xiong, Tao and Giesecke, Kay and Mangu, Lidia},
 journal = {arxiv},
 month = {12},
 title = {Sensitivity based Neural Networks Explanations},
 url = {http://arxiv.org/pdf/1812.01029v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.04801v1,
 abstract = {Interactions such as double negation in sentences and scene interactions in
images are common forms of complex dependencies captured by state-of-the-art
machine learning models. We propose Mah\'e, a novel approach to provide
Model-agnostic hierarchical \'explanations of how powerful machine learning
models, such as deep neural networks, capture these interactions as either
dependent on or free of the context of data instances. Specifically, Mah\'e
provides context-dependent explanations by a novel local interpretation
algorithm that effectively captures any-order interactions, and obtains
context-free explanations through generalizing context-dependent interactions
to explain global behaviors. Experimental results show that Mah\'e obtains
improved local interaction interpretations over state-of-the-art methods and
successfully explains interactions that are context-free.},
 author = {Tsang, Michael and Sun, Youbang and Ren, Dongxu and Liu, Yan},
 journal = {arxiv},
 month = {12},
 title = {Can I trust you more? Model-Agnostic Hierarchical Explanations},
 url = {http://arxiv.org/pdf/1812.04801v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.07169v1,
 abstract = {This paper presents a method to explain the knowledge encoded in a
convolutional neural network (CNN) quantitatively and semantically. The
analysis of the specific rationale of each prediction made by the CNN presents
a key issue of understanding neural networks, but it is also of significant
practical values in certain applications. In this study, we propose to distill
knowledge from the CNN into an explainable additive model, so that we can use
the explainable model to provide a quantitative explanation for the CNN
prediction. We analyze the typical bias-interpreting problem of the explainable
model and develop prior losses to guide the learning of the explainable
additive model. Experimental results have demonstrated the effectiveness of our
method.},
 author = {Chen, Runjin and Chen, Hao and Huang, Ge and Ren, Jie and Zhang, Quanshi},
 journal = {arxiv},
 month = {12},
 title = {Explaining Neural Networks Semantically and Quantitatively},
 url = {http://arxiv.org/pdf/1812.07169v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.10537v2,
 abstract = {In the present paper, a method of defining the industrial process parameters
for a new product using machine learning algorithms will be presented. The
study will describe how to go from the product characteristics till the
prediction of the suitable machine parameters to produce a good quality of this
product, and this is based on an historical training dataset of similar
products with their respective process parameters. In the first part of our
study, we will focus on the ultrasonic welding process definition, welding
parameters and on how it operate. While in second part, we present the design
and implementation of the prediction models such multiple linear regression,
support vector regression, and we compare them to an artificial neural networks
algorithm. In the following part, we present a new application of Convolutional
Neural Networks (CNN) to the industrial process parameters prediction. In
addition, we will propose the generalization approach of our CNN to any
prediction problem of industrial process parameters. Finally the results of the
four methods will be interpreted and discussed.},
 author = {Khdoudi, Abdelmoula and Masrour, Tawfik},
 journal = {arxiv},
 month = {12},
 title = {Prediction of Industrial Process Parameters using Artificial
  Intelligence Algorithms},
 url = {http://arxiv.org/pdf/1812.10537v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1901.03838v1,
 abstract = {Prediction accuracy and model explainability are the two most important
objectives when developing machine learning algorithms to solve real-world
problems. The neural networks are known to possess good prediction performance,
but lack of sufficient model explainability. In this paper, we propose to
enhance the explainability of neural networks through the following
architecture constraints: a) sparse additive subnetworks; b) orthogonal
projection pursuit; and c) smooth function approximation. It leads to a sparse,
orthogonal and smooth explainable neural network (SOSxNN). The multiple
parameters in the SOSxNN model are simultaneously estimated by a modified
mini-batch gradient descent algorithm based on the backpropagation technique
for calculating the derivatives and the Cayley transform for preserving the
projection orthogonality. The hyperparameters controlling the sparse and smooth
constraints are optimized by the grid search. Through simulation studies, we
compare the SOSxNN method to several benchmark methods including least absolute
shrinkage and selection operator, support vector machine, random forest, and
multi-layer perceptron. It is shown that proposed model keeps the flexibility
of pursuing prediction accuracy while attaining the improved interpretability,
which can be therefore used as a promising surrogate model for complex model
approximation. Finally, the real data example from the Lending Club is employed
as a showcase of the SOSxNN application.},
 author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
 journal = {arxiv},
 month = {1},
 title = {Enhancing Explainability of Neural Networks through Architecture
  Constraints},
 url = {http://arxiv.org/pdf/1901.03838v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.06560v1,
 abstract = {There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don't explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs' use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.},
 author = {Gilpin, Leilani H. and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
 journal = {arxiv},
 month = {1},
 title = {Explaining Explanations to Society},
 url = {http://arxiv.org/pdf/1901.06560v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.07538v1,
 abstract = {This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., the explainer uses interpretable visual concepts to explain features in
middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the
explainer performs like an auto-encoder, which decomposes the feature maps into
object-part features. The object-part features are learned to reconstruct CNN
features without much loss of information. We can consider the disentangled
representations of object parts a paraphrase of CNN features, which help people
understand the knowledge encoded by the CNN. More crucially, we learn the
explainer via knowledge distillation without using any annotations of object
parts or textures for supervision. In experiments, our method was widely used
to interpret features of different benchmark CNNs, and explainers significantly
boosted the feature interpretability without hurting the discrimination power
of the CNNs.},
 author = {Zhang, Quanshi and Yang, Yu and Wu, Ying Nian},
 journal = {arxiv},
 month = {1},
 title = {Unsupervised Learning of Neural Networks to Explain Neural Networks
  (extended abstract)},
 url = {http://arxiv.org/pdf/1901.07538v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.08547v1,
 abstract = {Transfer learning which aims at utilizing knowledge learned from one problem
(source domain) to solve another different but related problem (target domain)
has attracted wide research attentions. However, the current transfer learning
methods are mostly uninterpretable, especially to people without ML expertise.
In this extended abstract, we brief introduce two knowledge graph (KG) based
frameworks towards human understandable transfer learning explanation. The
first one explains the transferability of features learned by Convolutional
Neural Network (CNN) from one domain to another through pre-training and
fine-tuning, while the second justifies the model of a target domain predicted
by models from multiple source domains in zero-shot learning (ZSL). Both
methods utilize KG and its reasoning capability to provide rich and human
understandable explanations to the transfer procedure.},
 author = {Geng, Yuxia and Chen, Jiaoyan and Jimenez-Ruiz, Ernesto and Chen, Huajun},
 journal = {arxiv},
 month = {1},
 title = {Human-centric Transfer Learning Explanation via Knowledge Graph
  [Extended Abstract]},
 url = {http://arxiv.org/pdf/1901.08547v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09813v1,
 abstract = {Word embeddings generated by neural network methods such as word2vec (W2V)
are well known to exhibit seemingly linear behaviour, e.g. the embeddings of
analogy "woman is to queen as man is to king" approximately describe a
parallelogram. This property is particularly intriguing since the embeddings
are not trained to achieve it. Several explanations have been proposed, but
each introduces assumptions that do not hold in practice. We derive a
probabilistically grounded definition of paraphrasing and show it can be
re-interpreted as word transformation, a mathematical description of "$w_x$ is
to $w_y$". From these concepts we prove existence of the linear relationship
between W2V-type embeddings that underlies the analogical phenomenon, and
identify explicit error terms in the relationship.},
 author = {Allen, Carl and Hospedales, Timothy},
 journal = {arxiv},
 month = {1},
 title = {Analogies Explained: Towards Understanding Word Embeddings},
 url = {http://arxiv.org/pdf/1901.09813v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1901.09839v1,
 abstract = {While the success of deep neural networks (DNNs) is well-established across a
variety of domains, our ability to explain and interpret these methods is
limited. Unlike previously proposed local methods which try to explain
particular classification decisions, we focus on global interpretability and
ask a universally applicable question: given a trained model, which features
are the most important? In the context of neural networks, a feature is rarely
important on its own, so our strategy is specifically designed to leverage
partial covariance structures and incorporate variable dependence into feature
ranking. Our methodological contributions in this paper are two-fold. First, we
propose an effect size analogue for DNNs that is appropriate for applications
with highly collinear predictors (ubiquitous in computer vision). Second, we
extend the recently proposed "RelATive cEntrality" (RATE) measure (Crawford et
al., 2019) to the Bayesian deep learning setting. RATE applies an information
theoretic criterion to the posterior distribution of effect sizes to assess
feature significance. We apply our framework to three broad application areas:
computer vision, natural language processing, and social science.},
 author = {Ish-Horowicz, Jonathan and Udwin, Dana and Flaxman, Seth and Filippi, Sarah and Crawford, Lorin},
 journal = {arxiv},
 month = {1},
 title = {Interpreting Deep Neural Networks Through Variable Importance},
 url = {http://arxiv.org/pdf/1901.09839v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02041v1,
 abstract = {We ask whether the neural network interpretation methods can be fooled via
adversarial model manipulation, which is defined as a model fine-tuning step
that aims to radically alter the explanations without hurting the accuracy of
the original model. By incorporating the interpretation results directly in the
regularization term of the objective function for fine-tuning, we show that the
state-of-the-art interpreters, e.g., LRP and Grad-CAM, can be easily fooled
with our model manipulation. We propose two types of fooling, passive and
active, and demonstrate such foolings generalize well to the entire validation
set as well as transfer to other interpretation methods. Our results are
validated by both visually showing the fooled explanations and reporting
quantitative metrics that measure the deviations from the original
explanations. We claim that the stability of neural network interpretation
method with respect to our adversarial model manipulation is an important
criterion to check for developing robust and reliable neural network
interpretation method.},
 author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
 journal = {arxiv},
 month = {2},
 title = {Fooling Neural Network Interpretations via Adversarial Model
  Manipulation},
 url = {http://arxiv.org/pdf/1902.02041v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02384v1,
 abstract = {A barrier to the wider adoption of neural networks is their lack of
interpretability. While local explanation methods exist for one prediction,
most global attributions still reduce neural network decisions to a single set
of features. In response, we present an approach for generating global
attributions called GAM, which explains the landscape of neural network
predictions across subpopulations. GAM augments global explanations with the
proportion of samples that each attribution best explains and specifies which
samples are described by each attribution. Global explanations also have
tunable granularity to detect more or fewer subpopulations. We demonstrate that
GAM's global explanations 1) yield the known feature importances of simulated
data, 2) match feature weights of interpretable statistical models on real
data, and 3) are intuitive to practitioners through user studies. With more
transparent predictions, GAM can help ensure neural network decisions are
generated for the right reasons.},
 author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
 journal = {arxiv},
 month = {2},
 title = {Global Explanations of Neural Networks: Mapping the Landscape of
  Predictions},
 url = {http://arxiv.org/pdf/1902.02384v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.02497v1,
 abstract = {With the widespread applications of deep convolutional neural networks
(DCNNs), it becomes increasingly important for DCNNs not only to make accurate
predictions but also to explain how they make their decisions. In this work, we
propose a CHannel-wise disentangled InterPretation (CHIP) model to give the
visual interpretation to the predictions of DCNNs. The proposed model distills
the class-discriminative importance of channels in networks by utilizing the
sparse regularization. Here, we first introduce the network perturbation
technique to learn the model. The proposed model is capable to not only distill
the global perspective knowledge from networks but also present the
class-discriminative visual interpretation for specific predictions of
networks. It is noteworthy that the proposed model is able to interpret
different layers of networks without re-training. By combining the distilled
interpretation knowledge in different layers, we further propose the Refined
CHIP visual interpretation that is both high-resolution and
class-discriminative. Experimental results on the standard dataset demonstrate
that the proposed model provides promising visual interpretation for the
predictions of networks in image classification task compared with existing
visual interpretation methods. Besides, the proposed method outperforms related
approaches in the application of ILSVRC 2015 weakly-supervised localization
task.},
 author = {Cui, Xinrui and Wang, Dan and Wang, Z. Jane},
 journal = {arxiv},
 month = {2},
 title = {CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional
  Neural Networks},
 url = {http://arxiv.org/pdf/1902.02497v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.03380v2,
 abstract = {Discovering and exploiting the causality in deep neural networks (DNNs) are
crucial challenges for understanding and reasoning causal effects (CE) on an
explainable visual model. "Intervention" has been widely used for recognizing a
causal relation ontologically. In this paper, we propose a causal inference
framework for visual reasoning via do-calculus. To study the intervention
effects on pixel-level feature(s) for causal reasoning, we introduce pixel-wise
masking and adversarial perturbation. In our framework, CE is calculated using
features in a latent space and perturbed prediction from a DNN-based model. We
further provide a first look into the characteristics of discovered CE of
adversarially perturbed images generated by gradient-based methods.
Experimental results show that CE is a competitive and robust index for
understanding DNNs when compared with conventional methods such as
class-activation mappings (CAMs) on the ChestX-ray 14 dataset for
human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds
promises for detecting adversarial examples as it possesses distinct
characteristics in the presence of adversarial perturbations.},
 author = {Yang, Chao-Han Huck and Liu, Yi-Chieh and Chen, Pin-Yu and Ma, Xiaoli and Tsai, Yi-Chang James},
 journal = {arxiv},
 month = {2},
 title = {When Causal Intervention Meets Image Masking and Adversarial
  Perturbation for Deep Neural Networks},
 url = {http://arxiv.org/pdf/1902.03380v2},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.00519v1,
 abstract = {Despite a growing literature on explaining neural networks, no consensus has
been reached on how to explain a neural network decision or how to evaluate an
explanation. In fact, most works rely on manually assessing the explanation to
evaluate the quality of a method. This injects uncertainty in the explanation
process along several dimensions: Which explanation method to apply? Who should
we ask to evaluate it and which criteria should be used for the evaluation? Our
contributions in this paper are twofold. First, we investigate schemes to
combine explanation methods and reduce model uncertainty to obtain a single
aggregated explanation. Our findings show that the aggregation is more robust,
well-aligned with human explanations and can attribute relevance to a broader
set of features (completeness). Second, we propose a novel way of evaluating
explanation methods that circumvents the need for manual evaluation and is not
reliant on the alignment of neural networks and humans decision processes.},
 author = {Rieger, Laura and Hansen, Lars Kai},
 journal = {arxiv},
 month = {3},
 title = {Aggregating explainability methods for neural networks stabilizes
  explanations},
 url = {http://arxiv.org/pdf/1903.00519v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.10246v1,
 abstract = {We review computational and robotics models of early language learning and
development. We first explain why and how these models are used to understand
better how children learn language. We argue that they provide concrete
theories of language learning as a complex dynamic system, complementing
traditional methods in psychology and linguistics. We review different modeling
formalisms, grounded in techniques from machine learning and artificial
intelligence such as Bayesian and neural network approaches. We then discuss
their role in understanding several key mechanisms of language development:
cross-situational statistical learning, embodiment, situated social
interaction, intrinsically motivated learning, and cultural evolution. We
conclude by discussing future challenges for research, including modeling of
large-scale empirical data about language acquisition in real-world
environments.
  Keywords: Early language learning, Computational and robotic models, machine
learning, development, embodiment, social interaction, intrinsic motivation,
self-organization, dynamical systems, complexity.},
 author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
 journal = {arxiv},
 month = {3},
 title = {Computational and Robotic Models of Early Language Development: A Review},
 url = {http://arxiv.org/pdf/1903.10246v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.11420v1,
 abstract = {Explainable Artificial Intelligence (XAI) brings a lot of attention recently.
Explainability is being presented as a remedy for lack of trust in model
predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise
instance level interpretability for any complex machine learning model. But how
certain are these explanations? Can we rely on additive explanations for
non-additive models? In this paper, we examine the behavior of model explainers
under the presence of interactions. We define two sources of uncertainty, model
level uncertainty, and explanation level uncertainty. We show that adding
interactions reduces explanation level uncertainty. We introduce a new method
iBreakDown that generates non-additive explanations with local interaction.},
 author = {Gosiewska, Alicja and Biecek, Przemyslaw},
 journal = {arxiv},
 month = {3},
 title = {iBreakDown: Uncertainty of Model Explanations for Non-additive
  Predictive Models},
 url = {http://arxiv.org/pdf/1903.11420v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.02323v2,
 abstract = {Deep learning is increasingly used in decision-making tasks. However,
understanding how neural networks produce final predictions remains a
fundamental challenge. Existing work on interpreting neural network predictions
for images often focuses on explaining predictions for single images or
neurons. As predictions are often computed based off of millions of weights
that are optimized over millions of images, such explanations can easily miss a
bigger picture. We present Summit, the first interactive system that scalably
and systematically summarizes and visualizes what features a deep learning
model has learned and how those features interact to make predictions. Summit
introduces two new scalable summarization techniques: (1) activation
aggregation discovers important neurons, and (2) neuron-influence aggregation
identifies relationships among such neurons. Summit combines these techniques
to create the novel attribution graph that reveals and summarizes crucial
neuron associations and substructures that contribute to a model's outcomes.
Summit scales to large data, such as the ImageNet dataset with 1.2M images, and
leverages neural network feature visualization and dataset examples to help
users distill large, complex neural network models into compact, interactive
visualizations. We present neural network exploration scenarios where Summit
helps us discover multiple surprising insights into a state-of-the-art image
classifier's learned representations and informs future neural network
architecture design. The Summit visualization runs in modern web browsers and
is open-sourced.},
 author = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng},
 journal = {arxiv},
 month = {4},
 title = {Summit: Scaling Deep Learning Interpretability by Visualizing Activation
  and Attribution Summarizations},
 url = {http://arxiv.org/pdf/1904.02323v2},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.04063v1,
 abstract = {The EMNLP 2018 workshop BlackboxNLP was dedicated to resources and techniques
specifically developed for analyzing and understanding the inner-workings and
representations acquired by neural models of language. Approaches included:
systematic manipulation of input to neural networks and investigating the
impact on their performance, testing whether interpretable knowledge can be
decoded from intermediate representations acquired by neural networks,
proposing modifications to neural network architectures to make their knowledge
state or generated output more explainable, and examining the performance of
networks on simplified or formal languages. Here we review a number of
representative studies in each category.},
 author = {Alishahi, Afra and Chrupała, Grzegorz and Linzen, Tal},
 journal = {arxiv},
 month = {4},
 title = {Analyzing and Interpreting Neural Networks for NLP: A Report on the
  First BlackboxNLP Workshop},
 url = {http://arxiv.org/pdf/1904.04063v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.05488v1,
 abstract = {Current deep neural networks suffer from two problems; first, they are hard
to interpret, and second, they suffer from overfitting. There have been many
attempts to define interpretability in neural networks, but they typically lack
causality or generality. A myriad of regularization techniques have been
developed to prevent overfitting, and this has driven deep learning to become
the hot topic it is today; however, while most regularization techniques are
justified empirically and even intuitively, there is not much underlying
theory. This paper argues that to extract the features used in neural networks
to make decisions, it's important to look at the paths between clusters
existing in the hidden spaces of neural networks. These features are of
particular interest because they reflect the true decision making process of
the neural network. This analysis is then furthered to present an ensemble
algorithm for arbitrary neural networks which has guarantees for test accuracy.
Finally, a discussion detailing the aforementioned guarantees is introduced and
the implications to neural networks, including an intuitive explanation for all
current regularization methods, are presented. The ensemble algorithm has
generated state-of-the-art results for Wide-ResNet on CIFAR-10 and has improved
test accuracy for all models it has been applied to.},
 author = {Tao, Sean},
 journal = {arxiv},
 month = {4},
 title = {Deep Neural Network Ensembles},
 url = {http://arxiv.org/pdf/1904.05488v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.08939v1,
 abstract = {A neuroscience method to understanding the brain is to find and study the
preferred stimuli that highly activate an individual cell or groups of cells.
Recent advances in machine learning enable a family of methods to synthesize
preferred stimuli that cause a neuron in an artificial or biological brain to
fire strongly. Those methods are known as Activation Maximization (AM) or
Feature Visualization via Optimization. In this chapter, we (1) review existing
AM techniques in the literature; (2) discuss a probabilistic interpretation for
AM; and (3) review the applications of AM in debugging and explaining networks.},
 author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
 journal = {arxiv},
 month = {4},
 title = {Understanding Neural Networks via Feature Visualization: A survey},
 url = {http://arxiv.org/pdf/1904.08939v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1904.09273v1,
 abstract = {By their nature, the composition of black box models is opaque. This makes
the ability to generate explanations for the response to stimuli challenging.
The importance of explaining black box models has become increasingly important
given the prevalence of AI and ML systems and the need to build legal and
regulatory frameworks around them. Such explanations can also increase trust in
these uncertain systems. In our paper we present RICE, a method for generating
explanations of the behaviour of black box models by (1) probing a model to
extract model output examples using sensitivity analysis; (2) applying
CNPInduce, a method for inductive logic program synthesis, to generate logic
programs based on critical input-output pairs; and (3) interpreting the target
program as a human-readable explanation. We demonstrate the application of our
method by generating explanations of an artificial neural network trained to
follow simple traffic rules in a hypothetical self-driving car simulation. We
conclude with a discussion on the scalability and usability of our approach and
its potential applications to explanation-critical scenarios.},
 author = {Paçacı, Görkem and Johnson, David and McKeever, Steve and Hamfelt, Andreas},
 journal = {arxiv},
 month = {4},
 title = {"Why did you do that?": Explaining black box models with Inductive
  Synthesis},
 url = {http://arxiv.org/pdf/1904.09273v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1905.00122v1,
 abstract = {Converting malware into images followed by vision-based deep learning
algorithms has shown superior threat detection efficacy compared with classical
machine learning algorithms. When malware are visualized as images,
visual-based interpretation schemes can also be applied to extract insights of
why individual samples are classified as malicious. In this work, via two case
studies of dynamic malware classification, we extend the local interpretable
model-agnostic explanation algorithm to explain image-based dynamic malware
classification and examine its interpretation fidelity. For both case studies,
we first train deep learning models via transfer learning on malware images,
demonstrate high classification effectiveness, apply an explanation method on
the images, and correlate the results back to the samples to validate whether
the algorithmic insights are consistent with security domain expertise. In our
first case study, the interpretation framework identifies indirect calls that
uniquely characterize the underlying exploit behavior of a malware family. In
our second case study, the interpretation framework extracts insightful
information such as cryptography-related APIs when applied on images created
from API existence, but generate ambiguous interpretation on images created
from API sequences and frequencies. Our findings indicate that current
image-based interpretation techniques are promising for explaining vision-based
malware classification. We continue to develop image-based interpretation
schemes specifically for security applications.},
 author = {Chen, Li and Yagemann, Carter and Downing, Evan},
 journal = {arxiv},
 month = {4},
 title = {To believe or not to believe: Validating explanation fidelity for
  dynamic malware analysis},
 url = {http://arxiv.org/pdf/1905.00122v1},
 year = {2019}
}

