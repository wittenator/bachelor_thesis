@article{http://arxiv.org/abs/1710.10967v3,
 abstract = {Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy
network" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its "reinforcement-learning value
network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.},
 author = {Mitsuru Igami},
 journal = {arxiv},
 month = {10},
 title = {Artificial Intelligence as Structural Estimation: Economic
  Interpretations of Deep Blue, Bonanza, and AlphaGo},
 url = {http://arxiv.org/pdf/1710.10967v3},
 year = {2017}
}

@article{http://arxiv.org/abs/1802.00560v2,
 abstract = {Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.},
 author = {Xuan Liu, Xiaoguang Wang, Stan Matwin},
 journal = {arxiv},
 month = {2},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 url = {http://arxiv.org/pdf/1802.00560v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07517v2,
 abstract = {Issues regarding explainable AI involve four components: users, laws &
regulations, explanations and algorithms. Together these components provide a
context in which explanation methods can be evaluated regarding their adequacy.
The goal of this chapter is to bridge the gap between expert users and lay
users. Different kinds of users are identified and their concerns revealed,
relevant statements from the General Data Protection Regulation are analyzed in
the context of Deep Neural Networks (DNNs), a taxonomy for the classification
of existing explanation methods is introduced, and finally, the various classes
of explanation methods are analyzed to verify if user concerns are justified.
Overall, it is clear that (visual) explanations can be given about various
aspects of the influence of the input on the output. However, it is noted that
explanation methods or interfaces for lay users are missing and we speculate
which criteria these methods / interfaces should satisfy. Finally it is noted
that two important concerns are difficult to address with explanation methods:
the concern about bias in datasets that leads to biased DNNs, as well as the
suspicion about unfair outcomes.},
 author = {Gabrielle Ras, Marcel van Gerven, Pim Haselager},
 journal = {arxiv},
 month = {3},
 title = {Explanation Methods in Deep Learning: Users, Values, Concerns and
  Challenges},
 url = {http://arxiv.org/pdf/1803.07517v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1803.07980v2,
 abstract = {We interpret part of the experimental results of Shwartz-Ziv and Tishby
[2017]. Inspired by these results, we established a conjecture of the dynamics
of the machinary of deep neural network. This conjecture can be used to explain
the counterpart result by Saxe et al. [2018].},
 author = {Tianchen Zhao},
 journal = {arxiv},
 month = {3},
 title = {Information Theoretic Interpretation of Deep learning},
 url = {http://arxiv.org/pdf/1803.07980v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.00069v3,
 abstract = {There has recently been a surge of work in explanatory artificial
intelligence (XAI). This research area tackles the important problem that
complex machines and algorithms often cannot provide insights into their
behavior and thought processes. XAI allows users and parts of the internal
system to be more transparent, providing explanations of their decisions in
some level of detail. These explanations are important to ensure algorithmic
fairness, identify potential bias/problems in the training data, and to ensure
that the algorithms perform as expected. However, explanations produced by
these systems is neither standardized nor systematically assessed. In an effort
to create best practices and identify open challenges, we provide our
definition of explainability and show how it can be used to classify existing
literature. We discuss why current approaches to explanatory methods especially
for deep neural networks are insufficient. Finally, based on our survey, we
conclude with suggested future research directions for explanatory artificial
intelligence.},
 author = {Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, Lalana Kagal},
 journal = {arxiv},
 month = {5},
 title = {Explaining Explanations: An Overview of Interpretability of Machine
  Learning},
 url = {http://arxiv.org/pdf/1806.00069v3},
 year = {2018}
}

@article{http://arxiv.org/abs/1807.06161v1,
 abstract = {Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.},
 author = {Homanga Bharadhwaj, Shruti Joshi},
 journal = {arxiv},
 month = {7},
 title = {Explanations for Temporal Recommendations},
 url = {http://arxiv.org/pdf/1807.06161v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
 abstract = {Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).},
 author = {Andrew Slavin Ross},
 journal = {arxiv},
 month = {9},
 title = {Training Machine Learning Models by Regularizing their Explanations},
 url = {http://arxiv.org/pdf/1810.00869v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
 abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian
predictive models by projecting the information in the predictive distribution
locally to a simpler, interpretable explanation model. The proposed approach
combines the recent Local Interpretable Model-agnostic Explanations (LIME)
method with ideas from Bayesian projection predictive variable selection
methods. The information theoretic basis helps in navigating the trade-off
between explanation fidelity and complexity. We demonstrate the method in
explaining MNIST digit classifications made by a Bayesian deep convolutional
neural network.},
 author = {Tomi Peltola},
 journal = {arxiv},
 month = {10},
 title = {Local Interpretable Model-agnostic Explanations of Bayesian Predictive
  Models via Kullback-Leibler Projections},
 url = {http://arxiv.org/pdf/1810.02678v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1902.02384v1,
 abstract = {A barrier to the wider adoption of neural networks is their lack of
interpretability. While local explanation methods exist for one prediction,
most global attributions still reduce neural network decisions to a single set
of features. In response, we present an approach for generating global
attributions called GAM, which explains the landscape of neural network
predictions across subpopulations. GAM augments global explanations with the
proportion of samples that each attribution best explains and specifies which
samples are described by each attribution. Global explanations also have
tunable granularity to detect more or fewer subpopulations. We demonstrate that
GAM's global explanations 1) yield the known feature importances of simulated
data, 2) match feature weights of interpretable statistical models on real
data, and 3) are intuitive to practitioners through user studies. With more
transparent predictions, GAM can help ensure neural network decisions are
generated for the right reasons.},
 author = {Mark Ibrahim, Melissa Louie, Ceena Modarres, John Paisley},
 journal = {arxiv},
 month = {2},
 title = {Global Explanations of Neural Networks: Mapping the Landscape of
  Predictions},
 url = {http://arxiv.org/pdf/1902.02384v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.12069v1,
 abstract = {Artificial intelligence (AI) will pave the way to a new era in medicine.
However, currently available AI systems do not interact with a patient, e.g.,
for anamnesis, and thus are only used by the physicians for predictions in
diagnosis or prognosis. However, these systems are widely used, e.g., in
diabetes or cancer prediction. In the current study, we developed an AI that is
able to interact with a patient (virtual doctor) by using a speech recognition
and speech synthesis system and thus can autonomously interact with the
patient, which is particularly important for, e.g., rural areas, where the
availability of primary medical care is strongly limited by low population
densities. As a proof-of-concept, the system is able to predict type 2 diabetes
mellitus (T2DM) based on non-invasive sensors and deep neural networks.
Moreover, the system provides an easy-to-interpret probability estimation for
T2DM for a given patient. Besides the development of the AI, we further
analyzed the acceptance of young people for AI in healthcare to estimate the
impact of such system in the future.},
 author = {Sebastian Spänig, Agnes Emberger-Klein, Jan-Peter Sowa, Ali Canbay, Klaus Menrad, Dominik Heider},
 journal = {arxiv},
 month = {3},
 title = {The Virtual Doctor: An Interactive Artificial Intelligence based on Deep
  Learning for Non-Invasive Prediction of Diabetes},
 url = {http://arxiv.org/pdf/1903.12069v1},
 year = {2019}
}

