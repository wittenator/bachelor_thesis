% !TeX root = ../main/thesis_main.tex
% ---------------------------------------------------
% ----- Chapters of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing.
% 
% Set up python path for all used code in this session and track all external files
\begin{pycode}
import sys
sys.path.insert(0, '../chapters/literature_analysis/code/')
# track code
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_publishers.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_time_dist.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_keywords.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_mappings.py')
# track data
pytex.add_dependencies('../chapters/literature_analysis/data/meta_search.csv')
pytex.add_dependencies('../chapters/literature_analysis/data/stage1.bib')
pytex.add_dependencies('../chapters/literature_analysis/code/params.json')
pytex.add_dependencies('../chapters/literature_analysis/data/mapping.xlsx')
\end{pycode}

\chapter{Literature mapping study}

\section{Motivation}

In order to access current methods in the fast-moving field of interpretability research in machine learning in a reproducible and structured fashion I will conduct a literature mapping study according to Petersen et. al \cite{petersenSystematicMappingStudies}, which consists of a number of sequential steps which should result in a representative corpus and an analysis using it.

\section{Methodology}

The recommended process is augmented by further steps in order to tailor it to the existing use case and consists of the following seven procedures:
\begin{enumerate}
	
	\item Definition of research questions:
	
	The overall process starts by defining clear questions which should guide the development of the whole literature mapping study and subsequently the result as well. Since I am interested in gaining an overview over the existing interpretability techniques, I chose the following questions:
	
	\begin{enumerate}
		\item What kind of explainability techniques are mentioned in the corpus?
		\item What kind of models are enhanced by explainability techniques?
		\item Which techniques are applicable to results produced by the pipeline or the pipeline itself?
	\end{enumerate}
	
	\item Construction of a search string:
	
	Based on the questions one is able to gather a set of key words which are most relevant to the field which is analyzed. Each word is augmented by synonyms which are concatenated with boolean OR operators and several of these synonymous groups are again connected via logical ANDs. Applying this method to the previously found questions yields the following search string:
	
\begin{pycode}
import json
with open('../chapters/literature_analysis/code/params.json') as json_file:
	params = json.load(json_file)
	search_string = ' AND '.join( '( ' + ' OR '.join([ f'"{item}"' for item in ors]) + ' )' for ors in params["stage1"])
	
	print(r"\textit{")
	print(search_string)
	print(r"}")
\end{pycode}
	
	
	\item Analysis of the main publishers using a meta search and the search string:
	
	Due to the presumed distributed nature of interpretability research it is not easy to pinpoint the main publishers of scientific articles. In order to mitigate this, a pre-search in the meta-search engine 'Google Scholar' is conducted. It should be noted at this point that any biases which are apparent in the meta search engine therefore apply to this analysis as well. One can see in \autoref{fig:top_publisher} that the main publishers are respectivly Arxiv, IEEE, Springer and ACM. Since all of these publishers are mainly focused on publications in computer science, mathematics and engineering, this speaks in favor of the hypothesis that most of the research is still very technical and research from social sciences rarely influences it. Even though Arxiv is not a credible publisher per se, it seems like the research community uses it as the first place to publish work and therefore it should not be excluded in this analysis. 
	
\begin{pycode}
from analyze_top_publishers import show_top_publishers
show_top_publishers('../chapters/literature_analysis/data/meta_search.csv', 'top_publisher')
\end{pycode}

	\item Sourcing of publications in scientific databases:
	
	Based on the insights from the previous step each of the main publisher's databases is scraped using the search string and their respective 'advanced search' interfaces or their APIs. Since most searches result in more than 1000 publications only the top 100 results ordered by the relevance scoring of the database are taken into account. These publications then form the corpus which is the basis for further analysis.

\begin{pycode}
from analyze_time_dist import print_time_dist
print_time_dist('../chapters/literature_analysis/data/stage1.bib', 'time_dist')
\end{pycode}

\begin{pycode}
from analyze_top_keywords import print_top_keywords
print_top_keywords('../chapters/literature_analysis/data/stage1.bib', 'top_keywords')
\end{pycode}
	
	\item Filtering of these publications by keywording their abstracts:
	
	Most scientific databases do a full text search on publications and possibly find the supplied keywords from the search string in sections of the paper which are not relevant e.g. the bibliography or in the outlook. Therefore another filtering step is necessary which searches for the search string in the abstracts of the papers of the corpus.
	
	In order to enhance the quality of the filtering process, the search string is enhances with key words generated by an analysis of the current corpus. As seen in \autoref{fig:top_keywords} the previous search string already contains most of the relevant keywords.
	
	\item Definition and application of inclusion and exclusion criteria to narrow down the pool of publications further:
	
	The next step serves as another filtering step enhancing the quality of the hitherto automatic selection by using human decision making. A combination of the guiding questions, which were defined in the beginning of the process and a first pass over the whole corpus, in which I skimmed the papers, gave me a clear set of criteria, as seen in \autoref{tbl:incl_excl}, which can be used to filter the corpus further. In a second pass each paper was evaluated and included in the next step if and only if it satisfied at least one inclusion criterion and none of the exclusion criteria.	
	
	\begin{table}
		\centering
		\begin{tabular}{  p{5cm} | p{5cm} }
			Inclusion criteria & Exclusion criteria  \\ \hline
			
			\begin{itemize}
				\item Reviews the current state of explainability research
				\item Presents a specific method for enhancing explainability for models
			\end{itemize}
			
			&
			
			\begin{itemize}
				\item Is not scientific literature
				\item Does not describe the used explainability method
				\item The publication does not focus on explainability
				\item The described method is neither general, nor focused on NLP
			\end{itemize}
		\end{tabular}
		\caption{\label{tbl:incl_excl} Table showing all used inclusion and exclusion criteria}
	\end{table}
	
	\item Quantitative assessment of the resulting corpus:
	
	In the last step the actual mapping is generated. In another pass I first skimmed and then read each paper and based on that classified each publication and its presented technique according to the Gamuth classification, the type of model to which the technique is applicable and the component where the technique could be applied in the topic extraction pipeline. Additionally each paper was classified as either "Theory", "Method", "Study" or "Report". 
	A "Method" paper presents a single explainability technique and demonstrates its impact in an exemplary use case. A "Theory" paper does so as well, but misses a presented application and evaluation. A "Report" on the other hand summarizes and presents multiple techniques. Finally, a "Study" paper shows the results of an evaluation of an interface which visualizes the results of explainability methods. Publications from the last category are therefore less technical and more concerned with the HCI aspects of explainability techniques and their visualization.
	
	Since most of the overview papers presented a huge amount of techniques which were already covered by the "Method" papers and the corpus was already large, I decided to exclude them from the last mapping step.

\end{enumerate}

\section{Results}

In order to answer my first question concerning the different kinds of researched explainability 

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'mapping1', 'Mapping of applicability and gamut classification', 'Gamut extended', 'Applicability')

print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'mapping2', 'Mapping of applicability and gamut classification', 'Gamut extended', 'Type')

print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'mapping3', 'Mapping of applicability and gamut classification', 'Gamut extended', 'Pipeline step')
\end{pycode}

