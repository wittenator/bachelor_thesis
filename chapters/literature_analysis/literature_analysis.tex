% !TeX root = ../main/thesis_main.tex
% ---------------------------------------------------
% ----- Chapters of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing.
% 
% Set up python path for all used code in this session and track all external files
\begin{pycode}
import sys
sys.path.insert(0, '../chapters/literature_analysis/code/')
# track code
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_publishers.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_time_dist.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_keywords.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_mappings.py')
# track data
pytex.add_dependencies('../chapters/literature_analysis/data/meta_search.csv')
pytex.add_dependencies('../chapters/literature_analysis/data/stage1.bib')
pytex.add_dependencies('../chapters/literature_analysis/code/params.json')
pytex.add_dependencies('../chapters/literature_analysis/data/mapping.xlsx')
\end{pycode}

\chapter{Literature mapping study}
\label{chap:literature_analysis}

\section{Motivation}

In order to access current methods in the fast-moving field of interpretability research in machine learning in a reproducible and structured fashion I will conduct a literature mapping study according to Petersen et. al \cite{petersenSystematicMappingStudies}, which consists of a number of sequential steps which should result in a representative corpus and an analysis using it.

\section{Methodology}

The process from Petersen et al. is augmented by further steps in order to tailor it to the existing use case and consists of the following seven procedures:
\begin{enumerate}
	
	\item Definition of research questions:
	
	The overall process starts by defining clear questions which should guide the development of the whole literature mapping study and subsequently the result as well. Since I am interested in gaining an overview over the existing interpretability techniques for NLP, I chose the following questions:
	
	\begin{enumerate}
		\item What categories of explainability techniques are mentioned in the corpus?
		\item What kind of models are enhanced by explainability techniques?
		\item Which techniques are applicable to results produced by the pipeline or the pipeline itself?
	\end{enumerate}
	
	\item Construction of a search string:
	
	Based on the questions one is able to gather a set of key words which are most relevant to the field which is analyzed. Each word is augmented by synonyms which are concatenated with boolean OR operators and several of these synonymous groups are again connected via logical ANDs. Applying this method to the previously found questions yields the following search string:
	
\begin{pycode}
import json
with open('../chapters/literature_analysis/code/params.json') as json_file:
	params = json.load(json_file)
	search_string = ' AND '.join( '( ' + ' OR '.join([ f'"{item}"' for item in ors]) + ' )' for ors in params["stage1"])
	
	print(r"\textit{")
	print(search_string)
	print(r"}")
\end{pycode}
	
	
	\item Analysis of the main publishers using a meta search and the search string:
	
	Due to the presumed distributed nature of interpretability research it is not easy to pinpoint the main publishers of scientific articles. In order to mitigate this, a pre-search in the meta-search engine 'Google Scholar' is conducted. It should be noted at this point that any biases which are apparent in the meta search engine therefore apply to this analysis as well. One can see in \autoref{fig:top_publisher} that the main publishers are respectivly Arxiv, IEEE, Springer and ACM. Since all of these publishers are mainly focused on publications in computer science, mathematics and engineering, this speaks in favor of the hypothesis that the majority of the research is still very technical and research from social sciences rarely influences it. Even though Arxiv is not a credible publisher per se, it seems like the research community uses it as the first place to publish work and therefore it should not be excluded in this analysis. 
	
\begin{pycode}
from analyze_top_publishers import show_top_publishers
show_top_publishers('../chapters/literature_analysis/data/meta_search.csv', 'top_publisher')
\end{pycode}

	\item Sourcing of publications in scientific databases:
	
	Based on the insights from the previous step each of the main publisher's databases is scraped using the search string and their respective 'advanced search' interfaces or their APIs. Since most searches result in more than 1000 publications only the top 100 results ordered by the relevance scoring of the database are taken into account. These publications then form the corpus which is the basis for further analysis.
	
	\item Intermediate assessment of the corpus:
	
	Looking at the distribution of tags in \autoref{fig:top_keywords} it is apparent that the chosen keywords represent the field well. There are no tags in the first 5 entries which are not constructable by the query.
	Plotting the distribution of publishing dates of the papers from the corpus in \autoref{fig:time_dist} reveals that the first publications were already written in 1980, while there is a surge of interest and research in the last 4 years. This speaks in favor of the premise that interpretability research is not necessarily a young, but a recently thriving field.
	
\begin{pycode}
from analyze_time_dist import print_time_dist
print_time_dist('../chapters/literature_analysis/data/stage1.bib', 'time_dist')
\end{pycode}

\begin{pycode}
from analyze_top_keywords import print_top_keywords
print_top_keywords('../chapters/literature_analysis/data/stage1.bib', 'top_keywords')
\end{pycode}
	
	\item Definition and application of inclusion and exclusion criteria to narrow down the pool of publications further:
	
	The next step serves as another filtering step enhancing the quality of the hitherto automatic selection by using human decision making. A combination of the guiding questions, which were defined in the beginning of the process and a first pass over the whole corpus, in which I skimmed the papers, gave me a clear set of criteria, as seen in \autoref{tbl:incl_excl}, which can be used to filter the corpus further. In a second pass each paper was evaluated and included in the next step if and only if it satisfied at least one inclusion criterion and none of the exclusion criteria.	In order to support my decision making and minimize the amount of work to classify each paper I developed a Jupyter-based interface, which takes a bibliography and a set of inclusion and exclusion criteria and iterates over all contained publications, shows its title and abstract and allows the user to select criteria which apply. If a closer examination is needed it opens the paper on demand. Furthermore it sorts each publication into either a bibliography for the next stage, a bibliography with rejected publications depending on the applying criteria or a bibliography containing interesting, but not directly relevant literature. I opensourced this framework on \href{https://github.com/wittenator/limap}{GitHub}.
	
	\begin{table}
		\centering
		\begin{tabular}{  p{5cm} | p{5cm} }
			Inclusion criteria & Exclusion criteria  \\ \hline
			
			\begin{itemize}
				\item Reviews the current state of explainability research
				\item Presents a specific method for enhancing explainability for models
			\end{itemize}
			
			&
			
			\begin{itemize}
				\item Is not scientific literature
				\item Does not describe the used explainability method
				\item The publication does not focus on explainability
				\item The described method is neither general, nor focused on NLP
			\end{itemize}
		\end{tabular}
		\caption{\label{tbl:incl_excl} Table showing all used inclusion and exclusion criteria}
	\end{table}
	
	\item Quantitative assessment of the resulting corpus:
	
	In the last step the actual mapping is generated. In another pass I first skimmed and then read each paper and based on that classified each publication and its presented technique in order to answer the initially posed questions. To answer the first question I categorized them according to the proposed categories of Hohman et. al. \cite{hohmanGamutDesignProbe2019}. These categories are not a perfect fit for a thesis dealing with explainability for non-technical experts since it also categorizes techniques according to their mathematical inner workings, but Hohman et al. extended the categories proposed by Lipton \cite{liptonMythosModelInterpretability2016a}, which formulated the starting hypothesis for this thesis and is the closest to a nontechnical assessment of interpretability research I could find. Furthermore each publication was assigned the type of model to which the technique is applicable, the component to which the technique could be applied in the topic extraction pipeline and each paper was classified as either "Theory", "Method", "Study" or "Report". 
	A "Method" paper presents a single explainability technique and demonstrates its impact in an exemplary use case. A "Theory" paper does so as well, but misses a presented application and evaluation. A "Report" on the other hand summarizes and presents multiple techniques. Finally, a "Study" paper shows the results of an interface evaluation which visualizes the output of explainability methods. Publications from the last category are therefore less technical and more concerned with the HCI aspects of explainability techniques and their visualization.
	
	Since most of the overview papers presented a huge amount of techniques which were already covered by the "Method" papers and the corpus was already large, I decided to exclude them from the last mapping step. This reduced the final corpus to a size of 72 publications.

\end{enumerate}

\section{Results}

In order to answer my first question concerning the different kinds of researched explainability 

Mapping the type of paper and the classification according to Gamut each on an axis (\autoref{fig:type_gamut}) shows clearly that there is a trend towards developing methods which explain single decision instances (38 paper). Furthermore most developed methods are tested on real world data (61 paper), but their application in an interface is rarely studied (6 paper). This speaks in favor of the hypothesis that most explainability methods are developed as mathematical theories and influences from HCI are rarely taken into consideration.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'type_gamut', 'Mapping of the type of publication and its Gamut classification', 'Gamut extended', 'Type')
\end{pycode}

The second question was concerned with the type of models which are enhanced by explainability techniques. In \autoref{fig:application_gamut} it is visible that neural architectures (NN, CNN, FNN, RNN, GCNN) dominate the field (40 paper). 19 papers try to explain a given model in an agnostic way as a black box, while a minority of publications deals with the explainability of clustering results, decision trees or linear models.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'application_gamut', 'Mapping of applicability and Gamut classification', 'Gamut extended', 'Applicability')
\end{pycode}

The third mapping in \autoref{fig:pipelinestep_gamut} shows the relation between the applicability of a method in the general topic extraction pipeline and its Gamut classification. Suprisingly, 51\% of the sourced publications are not applicable to the general topic extraction pipeline in any form. The two main reasons why a publication falls into this category is that it either presents a method in a subdomain of NLP which is not directly applicable \cite{goyalTransparentAISystems2016} \cite{itoTextVisualizingNeuralNetwork2018a} or its presented use case and context is too far off in order to be applied \cite{8591457} \cite{gengHumancentricTransferLearning}.
The second biggest category consists of techniques which could be applied to the document classification step using labeled data to train a model. Since any neural network can be used to classify vectorized documents, most of the publications on the "NN" axis in \autoref{fig:application_gamut} fall into this bucket as well. All in all, 18 publications remain which could be applied to an unsupervised topic extraction pipeline. 
The document embedding step could be made interpretable by decomposition, feature importance visualization or by explaining the embedding of single instances. 

Kim et al. \cite{kimStructureDeepNeural2019} decompose a pretrained network and extract simple features which they use as to train a neural network on another task in a transfer learning fashion. The predictions for new tasks can then be described as a combination of these extracted features.

In contrast to that, Zhang et al. \cite{zhangUnsupervisedLearningNeural2018} train another neural network to explain the output of any given neural network in a unsupervised way. They focus on CNNs and utilize the fact that these convolutional layers contain structural information. For each input they are able to disentagle the information from the applied convolutional filters and extract features which can be applied back to the input as masks to show influential parts. Given a document embedding technique, which uses CNNs, and a corpus this explainability technique could be used to highlight influential parts of the input document.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'pipelinestep_gamut', 'Mapping of pipeline step and Gamut classification', 'Gamut extended', 'Pipeline step')
\end{pycode}

