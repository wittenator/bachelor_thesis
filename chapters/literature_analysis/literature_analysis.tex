% !TeX root = ../main/thesis_main.tex
% ---------------------------------------------------
% ----- Chapters of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing.
% 
% Set up python path for all used code in this session and track all external files
\begin{pycode}
import sys
sys.path.insert(0, '../chapters/literature_analysis/code/')
# track code
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_publishers.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_time_dist.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_top_keywords.py')
pytex.add_dependencies('../chapters/literature_analysis/code/analyze_mappings.py')
# track data
pytex.add_dependencies('../chapters/literature_analysis/data/meta_search.csv')
pytex.add_dependencies('../chapters/literature_analysis/data/stage1.bib')
pytex.add_dependencies('../chapters/literature_analysis/code/params.json')
pytex.add_dependencies('../chapters/literature_analysis/data/mapping.xlsx')
\end{pycode}

\chapter{Literature mapping study}
\label{chap:literature_analysis}

\section{Motivation}

In order to assess current methods in the fast-moving field of interpretability research in machine learning in a reproducible and structured fashion I conduct a literature mapping study according to Petersen et. al \cite{petersenSystematicMappingStudies}, which consists of a number of sequential steps which should reduce the initially sourced corpus to a set of representative publications and an analysis using it.

\section{Method}

The process from Petersen et al. is augmented by further steps in order to tailor it to the existing use case and consists of the following seven procedures:
\begin{enumerate}
	
	\item Definition of research questions:
	
	The overall process starts by defining clear questions which should guide the development of the whole literature mapping study and subsequently the result as well. Since I am interested in gaining an overview over the existing interpretability techniques for NLP, I chose the following questions:
	
	\begin{enumerate}
		\item What categories of interpretability techniques are mentioned in the corpus?
		\item What kind of models are enhanced by interpretability techniques?
		\item Which interpretability techniques are applicable to the pipeline or to intermediate results of it?
	\end{enumerate}
	
	\item Construction of a search string:
	
	Based on the questions I developed a set of key words which are most relevant to the field which is analyzed. Each word is augmented by synonyms which are concatenated with boolean OR operators and several of these synonymous groups are again connected via logical ANDs. Applying this method to the previously found questions yields the following search string:
	
\begin{pycode}
import json
with open('../chapters/literature_analysis/code/params.json') as json_file:
	params = json.load(json_file)
	search_string = ' AND '.join( '( ' + ' OR '.join([ f'"{item}"' for item in ors]) + ' )' for ors in params["stage1"])
	
	print(r"\textit{")
	print(search_string)
	print(r"}")
\end{pycode}
	
	
	\item Analysis of the main publishers using a meta search and the search string:
	
	Due to the presumed distributed nature of interpretability research it is not easy to pinpoint the main publishers of scientific articles. In order to mitigate this, a pre-search in the meta-search engine 'Google Scholar' is conducted. It should be noted at this point that any biases which are apparent in the meta search engine therefore apply to this analysis as well. One can see in \autoref{fig:top_publisher} that the main publishers are respectivly ArXiv, IEEE, Springer and ACM. Since all of these publishers are mainly focused on publications in computer science, mathematics and engineering, this speaks in favor of the hypothesis that the majority of the research is still very technical and research from social sciences rarely influences it. Even though Arxiv is a preprint server and not a publisher per se, it seems like the research community uses it as the first place to publish work and therefore it should not be excluded in this analysis. 
	
\begin{pycode}
from analyze_top_publishers import show_top_publishers
show_top_publishers('../chapters/literature_analysis/data/meta_search.csv', 'top_publisher')
\end{pycode}

	\item Sourcing of publications in scientific databases:
	
	Based on the insights from the previous step each of the main publisher's databases is scraped using the search string and their respective 'advanced search' interfaces or their APIs. Since most searches result in more than 1000 publications only the top 100 results ordered by the relevance scoring of the database are taken into account. These publications then form the corpus which is the basis for further analysis.
	
	\item Definition and application of inclusion and exclusion criteria to narrow down the pool of publications further:
	
	The next step serves as another filtering step enhancing the quality of the hitherto automatic selection by using human decision making. A combination of the guiding questions, which were defined in the beginning of the process and a first pass over the whole corpus, in which I skimmed the papers, gave me a clear set of criteria, as seen in \autoref{tbl:incl_excl}, which can be used to filter the corpus further. In a second pass each paper was evaluated and included in the next step if and only if it satisfied at least one inclusion criterion and none of the exclusion criteria.	In order to support my decision making and minimize the amount of work to classify each paper I developed a Jupyter-based interface, which takes a bibliography and a set of inclusion and exclusion criteria and iterates over all contained publications, shows its title and abstract and allows the user to select criteria which apply. If a closer examination is needed it opens the paper on demand. Furthermore it sorts each publication into either a bibliography for the next stage, a bibliography with rejected publications depending on the applying criteria or a bibliography containing interesting, but not directly relevant literature. I opensourced this framework on \href{https://github.com/wittenator/limap}{GitHub} \footnote{https://github.com/wittenator/limap}.
	
	\begin{table}
		\centering
		\begin{tabular}{  p{5cm} | p{5cm} }
			Inclusion criteria & Exclusion criteria  \\ \hline
			
			\begin{itemize}
				\item Reviews the current state of interpretability research
				\item Presents a specific method for enhancing interpretability for models
			\end{itemize}
			
			&
			
			\begin{itemize}
				\item Is not scientific literature
				\item Does not describe the used interpretability technique
				\item The publication does not focus on interpretability
				\item The described method is neither general, nor focused on NLP
			\end{itemize}
		\end{tabular}
		\caption{\label{tbl:incl_excl} Table showing all used inclusion and exclusion criteria}
	\end{table}

	\item Intermediate assessment of the corpus:
	
	Looking at the distribution of tags in \autoref{fig:top_keywords} it appears that the chosen keywords represent the field well. There are no tags in the first five entries which are not constructable by the query.
	Plotting the distribution of publishing dates of the papers from the corpus in \autoref{fig:time_dist} reveals that the first publications were already written in 1999, while there is a surge of interest and research in the last four years. This speaks in favor of the premise that interpretability research is not necessarily a young, but a recently thriving field. Investigating the earliest paper \cite{tahaSymbolicInterpretationArtificial1999} reveals that it does not differ significantly from the rest of the corpus concerning the techniques or the understanding of interpretability it uses.

\begin{pycode}
from analyze_time_dist import print_time_dist
print_time_dist('../chapters/literature_analysis/data/stage2.bib', 'time_dist')
\end{pycode}

\begin{pycode}
from analyze_top_keywords import print_top_keywords
print_top_keywords('../chapters/literature_analysis/data/stage2.bib', 'top_keywords')
\end{pycode}
	
	\item Quantitative assessment of the resulting corpus:
	
	In the last step the actual mapping is generated. In another pass I first skimmed and then read each paper and based on that classified each publication and its presented technique in order to answer the initially posed questions. To answer the first question I categorized them according to the proposed categories of Hohman et. al. \cite{hohmanGamutDesignProbe2019}, who also conducted a literature study.. 
	They propose a set of categories which fit the description of explanation strategies in the initially proposed relational model. These categories, henceforth referenced as Gamut classification, are not a perfect fit for a thesis dealing with interpretability for non-technical experts since it also categorizes techniques according to their mathematical inner workings, but Hohman et al. extended the categories proposed by Lipton \cite{liptonMythosModelInterpretability2016a}, which formulated the starting hypothesis for this thesis and is the closest to a nontechnical assessment of interpretability research I could find. Furthermore there are not many such classifications to begin with, so I chose one which was recently introduced and presented at a major conference (CHI 2019).  
	Additionally, I expanded the proposed set of categories by one strategy called 'decomposition', because during the analysis of the corpus I encountered the situation of not being able to classify a technique in one given bin. Investigating the non-assigned techniques led me to understand that most of them explain the interplay between components in the model they try to explain or decompose a model and use existing techniques to explain the components. For the rest of this thesis I refer to this set of categories as Gamut+.
	Each publication was therefore assigned the type of explanation strategy it supports, the type of model to which the technique is applicable, the component to which the technique could be applied in the topic extraction pipeline and each paper was classified as either "Theory", "Method", "Study" or "Report". 
	A "Method" paper presents a single interpretability techniques and demonstrates its impact in an exemplary use case. A "Theory" paper does so as well, but misses a presented application and evaluation. A "Report" on the other hand summarizes and presents multiple techniques. Finally, a "Study" paper shows the results of an interface evaluation which visualizes the output of interpretability techniques. Publications from the last category are therefore less technical and more concerned with the HCI aspects of interpretability techniques and their visualization.
	
	Since most of the overview papers presented a huge amount of techniques which were already covered by the "Method" papers and the corpus was already large, I decided to exclude them from the last mapping step. This reduced the final corpus to a size of 72 publications.

\end{enumerate}

\section{Results}

In order to answer the posed question in the beginning of the literature mapping study I visualize the resulting corpus as a mapping on a grid. Each dimension of this grid stands for one of the selected categories and these dimensions are divided into all possible tags in the respective categories. The intersections contain bubbles which visualize, by size and as an explicit number in the middle of the bubble, the amount of publications which were assigned both tags.

Mapping the type of paper and the classification according to Gamut+ each on an axis (\autoref{fig:type_gamut}) shows clearly that there is a trend towards developing methods which explain single decision instances (38 paper). Furthermore most developed methods are tested on real world data (61 paper), but their application in an interface is rarely studied (6 paper). This speaks in favor of the hypothesis that most interpretability techniques are developed as mathematical theories and influences from HCI are rarely taken into consideration and that the findings of Miller \cite{millerExplainableAIBeware2017} also hold true in the subdomain of NLP.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'type_gamut', 'Mapping of the type of publication and its Gamut+ classification', 'Gamut extended', 'Type')
\end{pycode}

The second question was concerned with the type of models which are enhanced by interpretability techniques. In \autoref{fig:application_gamut} it is visible that neural architectures (NN, CNN, FNN, RNN, GCNN) dominate the field (40 paper). 19 papers try to explain a given model in an agnostic way as a black box, while a minority of publications deals with the interpretability of clustering results, decision trees or linear models.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'application_gamut', 'Mapping of applicability and Gamut+ classification', 'Gamut extended', 'Applicability')
\end{pycode}

The third mapping in \autoref{fig:pipelinestep_gamut} shows the relation between the applicability of a method in the general topic extraction pipeline and its Gamut+ classification. Surprisingly, 51\% of the sourced publications are not applicable to the general topic extraction pipeline in any form. The two main reasons why a publication falls into this category is that it either presents a method in a subdomain of NLP which is not directly applicable \cite{goyalTransparentAISystems2016} \cite{itoTextVisualizingNeuralNetwork2018a} or its presented use case and context is too far off in order to be applied \cite{8591457} \cite{gengHumancentricTransferLearning}.
The second biggest category consists of techniques which could be applied to the document classification step using labeled data to train a model. Since any neural network can be used to classify vectorized documents, most of the publications on the "NN" axis in \autoref{fig:application_gamut} fall into this bucket as well. All in all, 9 publications remain which could be applied to an unsupervised topic extraction pipeline. 
The document embedding step could be made interpretable by decomposition or by explaining the embedding of single instances. 
The corpus delivers no techniques which could be applied to a unsupervised dimensionality reduction, but unveils 7 potential techniques for unsupervised classification, also known as clustering. This step can has also the most variety of potential interpretability techniques only missing a technique which uses instance explanation comparison as an explanation strategy.

\begin{pycode}
from analyze_mappings import print_mapping
print_mapping('../chapters/literature_analysis/data/mapping.xlsx', 'pipelinestep_gamut', 'Mapping of pipeline step and Gamut+ classification', 'Gamut extended', 'Pipeline step')
\end{pycode}

The literature mapping study revealed a few interesting insights into the field of interpretability research in NLP, while yielding surprisingly few applicable techniques. This may be due to the selection of only the first 100 publications in the scientific database or due to introducing a false bias to the results by including key words like 'neural network' or 'deep learning'. These techniques are normally connected to supervised learning which is not applicable to this use case from the start. These findings, especially the analysis showing the distribution of explanation strategies over models, makes it possible to tackle the problem of the proof-of-concept pipeline having theoretical shortcomings while simultaneously improving the interpretability of each component.
