% ---------------------------------------------------
% ----- Abstract (German) of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing. 
%
\pagestyle{empty}

\subsection*{Zusammenfassung}

Mit der steigenden Anwendung von Machine Learning-Systemen (ML) in unserem täglichen Leben steigt auch die Nachfrage, die Nutzung und die Ergebnisse dieser Systeme für Menschen mit unterschiedlichem Hintergrund (ML-Experten, nicht-technische Experten usw.) interpretierbar zu machen. Es gibt einen breiten Fundus an Forschung, insbesondere in der ML-Forschung, zu spezifischen Interpretationstechniken (z.B. Extraktion und Darstellung von Informationen aus ML-Pipelines). Häufig ist jedoch ein Bildungshintergrund im Bereich des maschinellen Lernens oder der Mathematik erforderlich, um die Ergebnisse der Interpretierbarkeitstechnik selbst zu interpretieren. Daher fehlt es dringend an Techniken, die nichttechnischen Experten bei der Nutzung solcher Systeme helfen können.

Die grundlegende Hypothese dieser Arbeit ist, dass, insbesondere für nicht-technische Experten, der Kontext einen großen Einfluss darauf hat wie Menschen komplexe algorithmische Systeme verstehen. Daher ist eine Interaktion zwischen einem Nutzer und einer Anwendung in diesem Modell tatsächlich ein Zusammenspiel zwischen einem Benutzer und seinem historischen Kontext, dem Kontext der Situation, in die die Interaktion eingebettet ist, und dem algorithmischen System. Interpretationstechniken sind das gemeinsame Bindeglied, welches all diese verschiedenen Aspekte zusammenführt.

Um die Annahme zu evaluieren, dass der Großteil der aktuellen Interpretierbarkeitsforschung auf ein technisches Publikum zugeschnitten ist und einen Überblick über bestehende Interpretierbarkeitstechniken zu erhalten, habe ich eine systematische Literaturübersichtsstudie durchgeführt, die den Stand der Interpretierbarkeitsforschung im Bereich der natürlichen Sprachverarbeitung (NLP) untersucht. Die Ergebnisse dieser Analyse deuten darauf hin, dass die meisten Techniken in der Tat nicht in einem Kontext bewertet werden, in dem ein nicht-technischer Experte sie verwenden kann, und dass die meisten Publikationen keine angemessene Definition der Interpretierbarkeit liefern.

Daher präsentiere und implementiere ich drei Methoden, um die Topic Modeling-Pipeline besser interpretierbar zu machen, indem ich mich von den allgemeinen Strategien inspirieren lasse, die in den, aus der systematischen Literaturübersichtsstudie stammenden, Publikationen verwendet wurden.

Da diese Arbeit davon ausgeht, dass die Interpretation ein komplexer soziotechnischer Prozess ist, muss auch eine Validierung beide Seiten berücksichtigen. Eine erste systemzentrierte Bewertung mit Coherence Scores ergab, dass die Pipeline in der Tat in der Lage ist, Projekte semantisch zu verknüpfen und wichtige Merkmale aussagekräftig darzustellen. Dieser theoretischen Analyse folgte eine humanzentrierte Validierung, bei der ein Cognitive Walkthrough verwendet wurde, um die Interaktion zwischen einem nicht-technischen Experten und der Anwendung zu simulieren. Diese Usability-Technik enthüllte, dass, obwohl die drei Interpretierbarkeitstechniken mit einer bestimmten Strategie im Hinterkopf entwickelt wurden, der Kontext es ermöglicht, die Ergebnisse von Interpretierbarkeitstechniken neu zu interpretieren und in einer neuen Weise zur Interpretierung des Systems zu verwenden. Dieses Ergebnis spricht für die Hypothese, dass solche kontext-seichten Usability-Techniken die Beziehung zwischen Benutzer und Anwendung nicht vollständig untersuchen können, was einen Bedarf an Validierungsmethoden schafft, die in den gleichen Kontext eingebettet sind, die der Nutzer hätte.

\cleardoublepage