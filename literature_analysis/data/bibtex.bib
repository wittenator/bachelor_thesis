@article{http://arxiv.org/abs/1710.09511v2,
 abstract = {Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.},
 author = {Shane Barratt},
 journal = {arxiv},
 month = {10},
 title = {InterpNET: Neural Introspection for Interpretable Deep Learning},
 url = {http://arxiv.org/pdf/1710.09511v2},
 year = {2017}
}

@article{http://arxiv.org/abs/1802.00560v2,
 abstract = {Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.},
 author = {Xuan Liu, Xiaoguang Wang, Stan Matwin},
 journal = {arxiv},
 month = {2},
 title = {Interpretable Deep Convolutional Neural Networks via Meta-learning},
 url = {http://arxiv.org/pdf/1802.00560v2},
 year = {2018}
}

@article{http://arxiv.org/abs/1806.01933v1,
 abstract = {Machine Learning algorithms are increasingly being used in recent years due
to their flexibility in model fitting and increased predictive performance.
However, the complexity of the models makes them hard for the data analyst to
interpret the results and explain them without additional tools. This has led
to much research in developing various approaches to understand the model
behavior. In this paper, we present the Explainable Neural Network (xNN), a
structured neural network designed especially to learn interpretable features.
Unlike fully connected neural networks, the features engineered by the xNN can
be extracted from the network in a relatively straightforward manner and the
results displayed. With appropriate regularization, the xNN provides a
parsimonious explanation of the relationship between the features and the
output. We illustrate this interpretable feature--engineering property on
simulated examples.},
 author = {Joel Vaughan, Agus Sudjianto, Erind Brahimi, Jie Chen, Vijayan N. Nair},
 journal = {arxiv},
 month = {6},
 title = {Explainable Neural Networks based on Additive Index Models},
 url = {http://arxiv.org/pdf/1806.01933v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1808.04127v1,
 abstract = {PatternAttribution is a recent method, introduced in the vision domain, that
explains classifications of deep neural networks. We demonstrate that it also
generates meaningful interpretations in the language domain.},
 author = {David Harbecke, Robert Schwarzenberg, Christoph Alt},
 journal = {arxiv},
 month = {8},
 title = {Learning Explanations from Language Data},
 url = {http://arxiv.org/pdf/1808.04127v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.00869v1,
 abstract = {Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).},
 author = {Andrew Slavin Ross},
 journal = {arxiv},
 month = {9},
 title = {Training Machine Learning Models by Regularizing their Explanations},
 url = {http://arxiv.org/pdf/1810.00869v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1810.02678v1,
 abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian
predictive models by projecting the information in the predictive distribution
locally to a simpler, interpretable explanation model. The proposed approach
combines the recent Local Interpretable Model-agnostic Explanations (LIME)
method with ideas from Bayesian projection predictive variable selection
methods. The information theoretic basis helps in navigating the trade-off
between explanation fidelity and complexity. We demonstrate the method in
explaining MNIST digit classifications made by a Bayesian deep convolutional
neural network.},
 author = {Tomi Peltola},
 journal = {arxiv},
 month = {10},
 title = {Local Interpretable Model-agnostic Explanations of Bayesian Predictive
  Models via Kullback-Leibler Projections},
 url = {http://arxiv.org/pdf/1810.02678v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1812.01029v1,
 abstract = {Although neural networks can achieve very high predictive performance on
various different tasks such as image recognition or natural language
processing, they are often considered as opaque "black boxes". The difficulty
of interpreting the predictions of a neural network often prevents its use in
fields where explainability is important, such as the financial industry where
regulators and auditors often insist on this aspect. In this paper, we present
a way to assess the relative input features importance of a neural network
based on the sensitivity of the model output with respect to its input. This
method has the advantage of being fast to compute, it can provide both global
and local levels of explanations and is applicable for many types of neural
network architectures. We illustrate the performance of this method on both
synthetic and real data and compare it with other interpretation techniques.
This method is implemented into an open-source Python package that allows its
users to easily generate and visualize explanations for their neural networks.},
 author = {Enguerrand Horel, Virgile Mison, Tao Xiong, Kay Giesecke, Lidia Mangu},
 journal = {arxiv},
 month = {12},
 title = {Sensitivity based Neural Networks Explanations},
 url = {http://arxiv.org/pdf/1812.01029v1},
 year = {2018}
}

@article{http://arxiv.org/abs/1902.02384v1,
 abstract = {A barrier to the wider adoption of neural networks is their lack of
interpretability. While local explanation methods exist for one prediction,
most global attributions still reduce neural network decisions to a single set
of features. In response, we present an approach for generating global
attributions called GAM, which explains the landscape of neural network
predictions across subpopulations. GAM augments global explanations with the
proportion of samples that each attribution best explains and specifies which
samples are described by each attribution. Global explanations also have
tunable granularity to detect more or fewer subpopulations. We demonstrate that
GAM's global explanations 1) yield the known feature importances of simulated
data, 2) match feature weights of interpretable statistical models on real
data, and 3) are intuitive to practitioners through user studies. With more
transparent predictions, GAM can help ensure neural network decisions are
generated for the right reasons.},
 author = {Mark Ibrahim, Melissa Louie, Ceena Modarres, John Paisley},
 journal = {arxiv},
 month = {2},
 title = {Global Explanations of Neural Networks: Mapping the Landscape of
  Predictions},
 url = {http://arxiv.org/pdf/1902.02384v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1902.03501v1,
 abstract = {The increasing adoption of machine learning tools has led to calls for
accountability via model interpretability. But what does it mean for a machine
learning model to be interpretable by humans, and how can this be assessed? We
focus on two definitions of interpretability that have been introduced in the
machine learning literature: simulatability (a user's ability to run a model on
a given input) and "what if" local explainability (a user's ability to
correctly indicate the outcome to a model under local changes to the input).
Through a user study with 1000 participants, we test whether humans perform
well on tasks that mimic the definitions of simulatability and "what if" local
explainability on models that are typically considered locally interpretable.
We find evidence consistent with the common intuition that decision trees and
logistic regression models are interpretable and are more interpretable than
neural networks. We propose a metric - the runtime operation count on the
simulatability task - to indicate the relative interpretability of models and
show that as the number of operations increases the users' accuracy on the
local interpretability tasks decreases.},
 author = {Sorelle A. Friedler, Chitradeep Dutta Roy, Carlos Scheidegger, Dylan Slack},
 journal = {arxiv},
 month = {2},
 title = {Assessing the Local Interpretability of Machine Learning Models},
 url = {http://arxiv.org/pdf/1902.03501v1},
 year = {2019}
}

@article{http://arxiv.org/abs/1903.03894v1,
 abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on
graphs. GNNs combine node feature information with the graph structure by using
neural networks to pass messages through edges in the graph. However,
incorporating both graph structure and feature information leads to complex
non-linear models and explaining predictions made by GNNs remains to be a
challenging task. Here we propose GnnExplainer, a general model-agnostic
approach for providing interpretable explanations for predictions of any
GNN-based model on any graph-based machine learning task (node and graph
classification, link prediction). In order to explain a given node's predicted
label, GnnExplainer provides a local interpretation by highlighting relevant
features as well as an important subgraph structure by identifying the edges
that are most relevant to the prediction. Additionally, the model provides
single-instance explanations when given a single prediction as well as
multi-instance explanations that aim to explain predictions for an entire class
of instances/nodes. We formalize GnnExplainer as an optimization task that
maximizes the mutual information between the prediction of the full model and
the prediction of simplified explainer model. We experiment on synthetic as
well as real-world data. On synthetic data we demonstrate that our approach is
able to highlight relevant topological structures from noisy graphs. We also
demonstrate GnnExplainer to provide a better understanding of pre-trained
models on real-world tasks. GnnExplainer provides a variety of benefits, from
the identification of semantically relevant structures to explain predictions
to providing guidance when debugging faulty graph neural network models.},
 author = {Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec},
 journal = {arxiv},
 month = {3},
 title = {GNN Explainer: A Tool for Post-hoc Explanation of Graph Neural Networks},
 url = {http://arxiv.org/pdf/1903.03894v1},
 year = {2019}
}

